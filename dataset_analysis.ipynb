{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90f06ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# df = pd.read_parquet('data/acl-publication-info.74k.v2.parquet')\n",
    "# df = pd.read_parquet('data/acl-publication-info.main.parquet')\n",
    "df = pd.read_parquet('data/acl-publication-info.machine-translation.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9295105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "acl_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "full_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "corpus_paper_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pdf_hash",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "numcitedby",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "publisher",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "address",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "year",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "month",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "booktitle",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pages",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "number",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "volume",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "journal",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "editor",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "isbn",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "ENTRYTYPE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "note",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "853e0c88-41f8-4cca-a0dd-5ffbb24a5eb3",
       "rows": [
        [
         "0",
         "W05-0819",
         "In this paper, we describe a word alignment algorithm for English-Hindi parallel data. The system was developed to participate in the shared task on word alignment for languages with scarce resources at the ACL 2005 workshop, on \"Building and using parallel texts: data driven machine translation and beyond\". Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours. We trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in Hindi sentences. The system scored 77.03% precision and 60.68% recall on the shared task unseen test data.",
         "In this paper, we describe a word alignment algorithm for English-Hindi parallel data. The system was developed to participate in the shared task on word alignment for languages with scarce resources at the ACL 2005 workshop, on \"Building and using parallel texts: data driven machine translation and beyond\". Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours. We trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in Hindi sentences. The system scored 77.03% precision and 60.68% recall on the shared task unseen test data. Introduction This paper describes a word alignment system developed as a part of shared task on word alignment for languages with scarce resources at the ACL 2005 workshop on \"building and using parallel texts: data driven machine translation and beyond\". Participants in the shared task were provided with common sets of training data, consisting of English-Inuktitut, Romanian-English, and English-Hindi parallel texts and the participating teams could choose to evaluate their system on one, two, or all three language pairs. Our system is for aligning English-Hindi parallel data at the word level. The word-alignment algorithm described here is based on a hybridmulti-feature approach, which groups Hindi words locally within a Hindi sentence and uses dictionary lookup (DL) as the main method of aligning words along with other methods such as Transliteration Similarity (TS), Expected English Words (EEW) and Nearest Aligned Neighbors (NAN). We used the training data supplied to derive rules for local word grouping in Hindi sentences and to find Named Entities (NE) and cognates using our TS approach. In the following sections we briefly describe our approach. Training Data The training data set was composed of approximately 3441 English-Hindi parallel sentence pairs drawn from the EMILLE (Enabling Minority Language Engineering) corpus (Baker et al., 2004) . The data was pre-tokenized. For the English data, a token was a sequence of characters that matches any of the \"Dr.\", \"Mr.\", \"Hon.\", \"Mrs.\", \"Ms.\", \"etc.\", \"i.e.\", \"e.g.\", \"[a-zA-Z0-9]+\", words ending with apostrophe and all special characters except the currency symbols £ and $. Similarly for the Hindi, a token consisted of a sequence of characters with spaces on both ends and all special characters except the currency symbols £ and $. Word Alignment Given a pair of parallel sentences, the task of word alignment can be described as finding one-to-one, one-to-many, and many-to-many correspondences between the words of source and target sentences. It becomes more complicated when aligning phrases of one language with the corresponding words or phrases in the target language. For some words, it is also possible not to find any translation in the target language. Such words are aligned to null. The algorithm presented in this paper, is a blend of various methods. We categorize words of a Hindi sentence into one of four different categories and use different techniques to deal with each of them. These categories include: 1) NEs and cognates 2) Hindi words for which it is possible to predict their corresponding English words 3) Hindi words that match certain pre-specified regular expression patterns specified in a rule file (explained in section 3.3.) and finally 4) words which do not fit in any of the above categories. In the following sections we explain different methods to deal with words from each of these categories. Named Entities and Cognates According to WWW1, the Named Entity Task is the process of annotating expressions in the text that are \"unique identifiers\" of entities (e.g. Organization, Person, Location etc.). For example: \"Mr. Niraj Aswani\", \"United Kingdom\", and \"Microsoft\" are examples of NEs. In most text processing systems, this task is achieved by using local pattern-matching techniques e.g. a word that is in upper initial orthography or a Title followed by the two adjacent words that are in upper initial or in all upper case. We use a Hindi gazetteer list that contains a large set of NEs. This gazetteer list is distributed as a part of Hindi Gazetteer processing resource in GATE (Maynard et al., 2003) . The Gazetteer list contains various NEs including person names, locations, organizations etc. It also contains other entities such as time units -months, dates, and number expressions. Cognates can be defined as two words having a common etymology and thus are similar or identical. In most cases they are pronounced in a similar way or with a minor change. For example \"Bungalow\" in English is derived from the word \"बं गला\" in Hindi, which means a house in the Bengali style (WWW2). We use our TS method to locate such words. Section 3.2 describes the TS approach. Transliteration Similarity For the English-Hindi alphabets, it is possible to come up with a table consisting of correspondences between the letters of the two alphabets. This table is generated based on the various sounds that each letter can produce. For example a letter \"c\" can be mapped to two letters in Hindi, \"क\" and \"स\". This mapping is not restricted to one-to-one but also includes many-tomany correspondences. It is also possible to map a sequence of two or more characters to a single character or to a sequence two or more characters. For example \"tio\" and \"sh\" in English correspond to the character \"श\" in Hindi. Prior to executing our word alignment algorithm, we use the TS approach to build a table of NEs and cognates. We consider one pair of parallel sentences at a time and for each word in a Hindi sentence, we generate different English words using our TS table. We found that before comparing words of two languages, it is more accurate to eliminate vowels from the words except those that appear at the start of words. We use a dynamic programming algorithm called \"edit-distance\" to measure the similarity between these words (WWW3). We calculate the similarity measure for each word in a Hindi sentence by comparing it with each and every word of an English sentence. We come up with an m x n matrix, where m and n refer to the number of words in Hindi and English respectively. This matrix contains a similarity measure for each word in a Hindi sentence corresponding to each word in a parallel English sentence. From our experiments of comparing more than 100 NE and cognate pairs, we found that the word pairs should be considered valid matches only if the similarity is greater than 75%. Therefore, we consider only those pairs which have the highest similarity among the other pairs with similarity greater than 75%. The following example shows how TS is used to compare a pair of English-Hindi words. For example consider a pair \"aswani अ।सवानी\" and the TS table entries as shown below: A अ, S स, SS स, V व, W व and N न We remove vowels from both words: \"aswn असवन\", and then convert the Hindi word into possible English words. This gives four different combinations: \"asvn\", \"assvn\", \"aswn\" and \"asswn\". These words are then compared with the actual English word \"aswn\". Since we are able to locate at least one word with similarity greater than 75%, we consider \"aswani अ।सवानी\" as a NE. Once a list of NEs and cognates is ready, we switch to our next step: local word grouping, where all words in Hindi sentences, either those available in the gazetteer list or in the list derived using TS approach, are aligned using TS approach. Local Word Grouping Hindi is a partially free order language (i.e. the order of the words in a Hindi sentence is not fixed but the order of words in a group/phrase is fixed). Unlike English where the verbs are used in different inflected forms to indicate different tenses, Hindi uses one or two extra words after the verb to indicate the tense. Therefore, if the English verb is not in its base form, it needs to be aligned with one or more words in a parallel Hindi sentence. Sometimes a phrase is aligned with another phrase. For example \"customer benefits\" aligns with \" ाहक के फायदे \". In this example the first word \"customer\" aligns with the first word \" ाहक\" and the second word \"benefits\" aligns with the third word \"फायदे \". Considering \"customer satisfaction\" and \" ाहक के फायदे \" as phrases to be aligned with each other, \"के \" is the word that indicates the relation between the two words \" ाहक\" and \"फायदे \", which means the \"benefits of customer\" in English. These words in a phrase need to be grouped together in order to align them correctly. In the case of certain prepositions, pronouns and auxiliaries, it is possible to predict the respective Hindi postpositions, pronouns and other words. We derived a set of more than 250 rules to group such patterns by consulting the provided training data and other grammar resources such as Bal Anand (2001) . The rule file contains the following information for each rule: 1) Hindi Regular Expression for a word or phrase. This must match one or more words in the Hindi sentence. 2) Group name or a part-of-speech category. 3) Expected English word(s) that this Hindi word group may align to. 4) In case a group of one or more English words aligns with a group of one or more Hindi words, information about the key words in both groups. Key words must match each other in order to align English-Hindi groups. 5) A rule to convert Hindi word into its base form. We list some of the derived rules below: 1) Group a sequence of [X + Postposition], where X can be any category in the above list except postposition or verb. For example: \"For X\" = \"X के िलये \", where \"For\" = \"के िलये \". 2) Root Verb + (रहा, रही or रहे ) + (PH). Present continuous tense. We use \"PH\" as an abbreviation to refer to the present/past tense conjunction of the verb \"होना\" -ं , ह , है , हो, etc. 3) Group two words that are identical to each other. For example: \"अलग अलग\", which means \"different\" in English. Such bi-grams are common in Hindi and are used to stress the importance of a word/activity in a sentence. Once the words are grouped in a Hindi sentence, we identify those word groups which do not fit in any of the TS and EEW categories. Such words are then aligned using the DL approach. Dictionary lookup Since the most dictionaries contain verbs in their base forms, we use a morphological analyzer to convert verbs in their base forms. The English-Hindi dictionary is obtained from (WWW4). The dictionary returns, on average, two to four Hindi words referring to a particular English word. The formula for finding the lemma of any Hindi verb is: infinitive = root verb + \"ना\". Since in most cases, our dictionary contains Hindi verbs in their infinitive forms, prior to comparing the word with the unaligned words, we remove the word \"ना\" from the end of it. Due to minor spelling mistakes it is also possible that the word returned from dictionary does not match with any of the words in a Hindi sentence. In this case, we use edit-distance algorithm to obtain similarity between the two words. If the similarity is greater than 75%, we consider them similar. We use EEW approach for the words which remain unaligned after the DL approach. Expected English words Candidates for the EEW approach are the Hindi word groups (HWG) that are created by our Hindi local word grouping algorithm (explained in section 3.3). The HWGs such as postpositions, number expressions, month-units, day-units etc. are aligned using the EEW approach. For example, for the Hind word \"बावन\" in a Hindi sentence, which means \"fifty two\" in English, the algorithm tries to locate \"fifty two\" in its parallel English sentence and aligns them if found. For the remaining unaligned Hindi words we use the NAN approach. Nearest Aligned Neighbors In certain cases, words in English-Hindi phrases follow a similar order. The NAN approach works on this principle and aligns one or more words with one of the English words. Considering one HWG at a time, we find the nearest Hindi word that is already aligned with one or more English word(s). Aligning a phrase \"customer benefits\" with \" ाहक के फायदे \" (example explained in section 3.3) is an example of NAN approach. Similarly consider a phrase \"tougher controls\", where for its equivalent Hindi phrase \"अिधक िनयं ण\", the dictionary returns a correct pair \"controls िनयं ण\", but fails to locate \"tougher अिधक\". For aligning the word \"tougher\", NAN searches for the nearest aligned word, which, in this case, is \"controls\". Since the word \"controls\" is already aligned with the word \"िनयं ण\", the NAN method aligns the word \"tougher\" with the nearest unaligned word \"अिधक\". Test Data results We executed our algorithm on the test data consisting of 90 English-Hindi sentence pairs. We obtained the following results for non-null alignment pairs.",
         "1215281",
         "b20450f67116e59d1348fc472cfc09f96e348f55",
         "15",
         "https://aclanthology.org/W05-0819",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Aswani, Niraj  and\nGaizauskas, Robert",
         "Aligning Words in {E}nglish-{H}indi Parallel Corpora",
         "115--118",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "aswani-gaizauskas-2005-aligning",
         null,
         null
        ],
        [
         "1",
         "W05-0821",
         "Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model.",
         "Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model. Introduction Statistical machine translation (SMT) makes use of a noisy channel model where a sentence ē in the desired language can be conceived of as originating as a sentence f in a source language. The goal is to find, for every input utterance f, the best hypothesis ē * such that ē * = argmax ēP (ē| f ) = argmax ēP ( f |ē)P (ē) (1) P ( f |ē) is the translation model expressing probabilistic constraints on the association of source and target strings. P (ē) is a language model specifying the probability of target language strings. Usually, a standard word trigram model of the form P (e 1 , ..., e l ) ≈ l i=3 P (e i |e i−1 , e i−2 ) (2) is used, where ē = e 1 , ..., e l . Each word is predicted based on a history of two preceding words. Most work in SMT has concentrated on developing better translation models, decoding algorithms, or minimum error rate training for SMT. Comparatively little effort has been spent on language modeling for machine translation. In other fields, particularly in automatic speech recognition (ASR), there exists a large body of work on statistical language modeling, addressing e.g. the use of word classes, language model adaptation, or alternative probability estimation techniques. The goal of this study was to use some of the language modeling techniques that have proved beneficial for ASR in the past and to investigate whether they transfer to statistical machine translation. In particular, this includes language models that make use of morphological and part-of-speech information, so-called factored language models. Factored Language Models A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework. Assuming that each word w can be decomposed into k features, i.e. w ≡ f 1:K , a trigram model can be defined as p(f 1:K 1 , f 1:K 2 , ..., f 1:K T ) ≈ T t=3 p(f 1:K t |f 1:K t−1 , f 1:K t−2 ) (3) Each word is dependent not only on a single stream of temporally preceding words, but also on additional parallel streams of features. This representation can be used to provide more robust probability estimates when a particular word n-gram has not been observed in the training data but its corresponding feature combinations (e.g. stem or tag trigrams) has been observed. FLMs are therefore designed to exploit sparse training data more effectively. However, even when a sufficient amount of training data is available, a language model utilizing morphological and POS information may bias the system towards selecting more fluent translations, by boosting the score of hypotheses with e.g. frequent POS combinations. In FLMs, word feature information is integrated via a new generalized parallel backoff technique. In standard Katz-style backoff, the maximum-likelihood estimate of an n-gram with too few observations in the training data is replaced with a probability derived from the lower-order (n − 1)gram and a backoff weight as follows: p BO (w t |w t−1 , w t−2 ) (4) = d c p M L (w t |w t−1 , w t−2 ) if c > τ α(w t−1 , w t−2 )p BO (w t |w t−1 ) otherwise where c is the count of (w t , w t−1 , w t−2 ), p M L denotes the maximum-likelihood estimate, τ is a count threshold, d c is a discounting factor and α(w t−1 , w t−2 ) is a normalization factor. During standard backoff, the most distant conditioning variable (in this case w t−2 ) is dropped first, followed by the second most distant variable etc., until the unigram is reached. This can be visualized as a backoff path (Figure 1(a) ). If additional conditioning variables are used which do not form a temporal sequence, it is not immediately obvious in which order they should be eliminated. In this case, several backoff paths are possible, which can be summarized in a backoff graph (Figure 1 is also possible to choose multiple paths and combine their probability estimates. This is achieved by replacing the backed-off probability p BO in Equation 2 by a general function g, which can be any non-negative function applied to the counts of the lower-order n-gram. Several different g functions can be chosen, e.g. the mean, weighted mean, product, minimum or maximum of the smoothed probability distributions over all subsets of conditioning factors. In addition to different choices for g, different discounting parameters can be selected at different levels in the backoff graph. One difficulty in training FLMs is the choice of the best combination of conditioning factors, backoff path(s) and smoothing options. Since the space of different combinations is too large to be searched exhaustively, we use a guided search procedure based on Genetic Algorithms (Duh and Kirchhoff, 2004) , which optimizes the FLM structure with respect to the desired criterion. In ASR, this is usually the perplexity of the language model on a held-out dataset; here, we use the BLEU scores of the oracle 1-best hypotheses on the development set, as described below. FLMs have previously shown significant improvements in perplexity and word error rate on several ASR tasks (e.g. (Vergyri et al., 2004) ). W 1 t W − 2 t W − 3 t W − t W 1 t W − 2 t W − t W 1 t W − t W (a) F 1 F 2 F 3 F F F 1 F 2 F F 1 F 3 F F 2 F 3 F F 1 F F 3 F F 2 F (b) Baseline System We used a fairly simple baseline system trained using standard tools, i.e. GIZA++ (Och and Ney, 2000) for training word alignments and Pharaoh (Koehn, 2004) for phrase-based decoding. The training data was that provided on the ACL05 Shared MT task website for 4 different language pairs (translation from Finnish, Spanish, French into English); no additional data was used. Preprocessing consisted of lowercasing the data and filtering out sentences with a length ratio greater than 9. The total number of training sentences and words per language pair ranged between 11.3M words (Finnish-English) and 15.7M words (Spanish-English). The development data consisted of the development sets provided on the website (2000 sentences each). We trained our own word alignments, phrase table, language model, and model combination weights. The language model was a trigram model trained using the SRILM toolkit, with modified Kneser-Ney smoothing and interpolation of higher-and lowerorder ngrams. Combination weights were trained using the minimum error weight optimization procedure provided by Pharaoh. We use a two-pass decoding approach: in the first pass, Pharaoh is run in N-best mode to produce N-best lists with 2000 hypotheses per sentence. Seven different component model scores are collected from the outputs, including the distortion model score, the first-pass language model score, word and phrase penalties, and bidirectional phrase and word translation scores, as used in Pharaoh (Koehn, 2004 ). In the second pass, the N-best lists are rescored with additional language models. The resulting scores are then combined with the above scores in a log-linear fashion. The combination weights are optimized on the development set to maximize the BLEU score. The weighted combined scores are then used to select the final 1-best hypothesis. The individual rescoring steps are described in more detail below. Language Models We trained two additional language models to be used in the second pass, one word-based 4-gram model, and a factored trigram model. Both were trained on the same training set as the baseline system. The 4-gram model uses modified Kneser-Ney smoothing and interpolation of higher-order and lower-order n-gram probabilities. The potential advantage of this model is that it models n-grams up to length 4; since the BLEU score is a combination of n-gram precision scores up to length 4, the integration of a 4-gram language model might yield better results. Note that this can only be done in a rescoring framework since the first-pass decoder can only use a trigram language model. For the factored language models, a feature-based word representation was obtained by tagging the text with Rathnaparki's maximum-entropy tagger (Ratnaparkhi, 1996) and by stemming words using the Porter stemmer (Porter, 1980) . Thus, the factored language models use two additional features per word. A word history of up to 2 was considered (3gram FLMs). Rather than optimizing the FLMs on the development set references, they were optimized to achieve a low perplexity on the oracle 1-best hypotheses (the hypotheses with the best individual BLEU scores) from the first decoding pass. This is done to avoid optimizing the model on word combinations that might never be hypothesized by the firstpass decoder, and to bias the model towards achieving a high BLEU score. Since N-best lists differ for different language pairs, a separate FLM was trained for each language pair. While both the 4-gram language model and the FLMs achieved a 8-10% reduction in perplexity on the dev set references compared to the baseline language model, their perplexities on the oracle 1-best hypotheses were not significantly different from that of the baseline model. N-best List Rescoring For N-best list rescoring, the original seven model scores are combined with the scores of the secondpass language models using the framework of discriminative model combination (Beyerlein, 1998) . This approach aims at an optimal (with respect to a given error criterion) integration of different information sources in a log-linear model, whose combination weights are trained discriminatively. This combination technique has been used successfully in ASR, where weights are typically optimized to minimize the empirical word error count on a heldout set. In this case, we use the BLEU score of the N-best hypothesis as an optimization criterion. Optimization is performed using a simplex downhill method known as amoeba search (Nelder and Mead, 1965) Results The results from the first decoding pass on the development set are shown in Conclusions We have demonstrated improvements in BLEU score by utilizing more complex language models in the rescoring pass of a two-pass SMT system. We noticed that FLMs performed worse than wordbased 4-gram models. However, only trigram FLM were used in the present experiments; larger improvements might be obtained by 4-gram FLMs. The weights assigned to the second-pass language models during weight optimization were larger than those assigned to the first-pass language model, suggesting that both the word-based model and the FLM provide more useful scores than the baseline language model. Finally, we observed that the overall improvement represents only a small portion of the possible increase in BLEU score as indicated by the oracle results, suggesting that better language models do not have a significant effect on the overall system performance unless the translation model is improved as well. Acknowledgements This work was funded by the National Science Foundation, Grant no. IIS-0308297. We are grateful to Philip Koehn for assistance with Pharaoh.",
         "1966857",
         "2a05c9c5373a3e1e01b8161e6687b960ab3d2ff5",
         "55",
         "https://aclanthology.org/W05-0821",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Kirchhoff, Katrin  and\nYang, Mei",
         "Improved Language Modeling for Statistical Machine Translation",
         "125--128",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "kirchhoff-yang-2005-improved",
         null,
         null
        ],
        [
         "2",
         "W05-0820",
         "Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis.",
         "Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis. Statistical machine translation is currently the dominant paradigm in machine translation research. Annual competitions are held for Chinese-English and Arabic-English by NIST (sponsored by the US military funding agency DARPA), which creates a forum to present and compare novel ideas and leads to steady progress in the field. One of the advantages of statistical machine translation is that the currently applied methods are fairly language-independent. Building a new machine translation system for a new language pair is not much more than a matter of running a training process on a training corpus of parallel text (a text in one language paired with a translation in another). It is therefore possible to hold a competition where research groups have only a few weeks to build machine translation systems for language pairs that they have not previously worked on. We effectively demonstrated this with our shared task. For instance, seven teams built Finnish-English machine translation systems, a language pair that was certainly not of their immediate concern before. In contrast to the bigger NIST competition, we wanted to keep the barrier of entry as low as possible. We provided not only training data from the Europarl corpus (Koehn, 2005) , but also additional resources: sentence and word alignments, the decoder Pharaoh 1 (Koehn, 2004b) , and a language model, so that participation was feasible even as a graduate level class project. Using about 15 million words of translated text, participants were asked to build a phrase-based statistical machine translation system. The focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided -for more on phrase-based statistical machine translation, refer to Koehn et al. (2003) . The participants' systems were compared by how well they translated 2000 previously unseen test sentences from the same domain. The shared task operated within an extremely short timeframe. The workshop and hence the shared task was accepted on February 22, 2005 and announced on March 3. The official test data was made available on April 3, results were due one week later. Despite this tight schedule, eleven research groups participated and built a total of 32 machine translation systems for the four language pairs. Goals When setting up this competition, we were motivated by a number of goals. We set out to: Create a platform to demonstrate the effectiveness of novel ideas: The research community is easily balkanized, where different groups work on different data sets and under different conditions, so that it becomes often hard to assess, how effective a novel method is. By creating an environment with common test and training sets, language model, preprocessing, and even decoder, the effect of other model choices can be more easily demonstrated. Work on new language pairs, new problems: Different language pairs pose different challenges. We picked Finnish-English and German-English for the special problems of rich morphology, word order, which are a challenge to current phrase-based SMT methods. Enable more researchers to get engaged in SMT research: One of our main goals with providing as many resources as possible was to keep the barrier of entry low. Participants could use the word alignment and other resources and focus on phrase extraction. We hoped to attract researchers that are relatively new to the field. We were satisfied to learn that many entries are by graduate students working on their own. Promote and create free resources: Academic research thrives on freely available resources. The field of statistical machine translation has been blessed with a long tradition of freely available software tools -such as GIZA++ (Och and Ney, 2003) -and parallel corpora -such as the Canadian Hansards 2 . Following this lead, we made word alignments and a language model available for this competition in addition to our previously published resources (Europarl and Pharaoh). The competition created resources as well. Most teams agreed to share system output and their model files. You can download them from the competition web site 3 . Promote work on European language pairs: Finally, we wanted to promote work on European languages. The increasing economic and political ties within the European Union create a huge need for translation services. We would like to see researchers rise to the challenge of creating high quality machine translation systems to fill these needs. We are very grateful for the strong participation, especially by researchers who are relatively new to the field. Rules of Engagement We set up a machine translation competition for four language pairs. We chose Spanish-English and French-English, because many researchers would be familiar with these languages. We chose German-English for its special problems with word order (such as nested constructions and split verb groups) and morphology. Finally, we picked Finnish-English for the rich agglutinative morphology of Finnish. Statistical machine translation systems are typically trained on sentence-aligned parallel corpora. We selected Europarl 4 , a freely available parallel corpus in eleven languages. In addition, we also made a word alignment available, which was derived using a variant of the current default method for word alignment -Och and Ney (2003)'s refined method. Figure 1 details some properties of the parallel corpora. The training corpus is most of the Europarl corpus, only the text of sessions from last quarter of the year 2000 was reserved for testing. The corpus has the size of roughly 15 million English words in 700,000 sentences -these numbers differ for each of the four parallel corpora due to the different number of discarded sentences during sentence alignment and after enforcing a 40 word length limit for sentences. The number of foreign words differs even more dramatically. The effect of Finnish morphology manifests itself in a low number of words (just over 11 million), but a high number of distinct words (more than 5 times as many as in the English half). The test corpus consists of 2000 sentences aligned across all five languages. Note that the output of each system is compared against the same English references for all source languages. The number of total words, distinct words, and words not seen in the training data reflects again the morphology effect. For researchers willing to create their own word alignment, we suggested the use of GIZA++ 5 , an implementation of the IBM word-based machine translation models, which also assisted the creation of the provided word alignments. We trained a language model on the English part Spanish-English French-English Finnish of the Europarl corpus using the SRI language modeling toolkit (Stolke, 2002) . Finally, we suggested the use of Pharaoh (Koehn, 2004b) , a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year's NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004) ), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005) , additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3-10. We allowed late submissions up to April 17. Results Eleven teams from eight institutions in Europe and North America participated, see Figure 2 for a complete list. The figure also indicates, if a team used the Pharaoh decoder (eight teams), the provided language model (seven teams) and the provided word alignment (four did, three of those with additional preprocessing or additional data). Translation performance was measured using the BLEU score (Papineni et al., 2002) , which measures n-gram overlap with a reference translation. In our case, we only used a single reference translation, since the test set was taken from a held-out portion of the Europarl corpus. On the other hand we used a relatively large number of test sentences to guarantee that the BLEU results are stable despite the fact that we used only one reference translation for each sentence. Shared tasks like this one, of course, bring out the competitive spirit of participants and can draw criticisms about being a horse race. From an outside perspective, however, it is far more interesting to learn which methods and ideas proved to be successful, than who won the competition. Taking stock of the results -see Figure 3 -one observes a very packed field at the top. While the participants from the University of Washington produced the best translations for every single language pair, the distance to many other participant scores ID Team Pharaoh is within a BLEU percentage point or two. As one might have expected, the scores are best for Spanish and French, and worst for Finnish. Figure 4 shows some typical output of the submitted systems. The proceedings to the workshop include detailed system descriptions of all participants. Novel phrase extraction approaches were proposed, along with better preprocessing, language modeling, rescoring, and other ideas. We are certain that better performance can be achieved by combining some of the methods used by different participants. And hence, we would like to pose the challenge to the research community to build and test better systems using the provided resources. We will gladly list additional results on the competition web site. Survey Following the end of the competition, we sent out a questionnaire to the participants. One of the questions what they would like to see different in a potential future competition. We listed four potential changes: 70% of the respondends checked translation from English, 50% checked out of domain test data, 40% checked more language pairs, 0% checked fewer language pairs. Additional suggestions were: alternatives to the BLEU scoring method (maybe human judgment by participants themselves), transitive translation using pivot languages, translation of resource-poor languages, and more time to prepare for the task. Outlook Given the short timeframe, one should view the system performances (albeit very competitive with the state of the art) as a baseline effort on the task of open domain text translation between European languages. We hope that future researchers will use the provided environment as a test bed for their machine translation systems. We will continue to publish any scores reported to us. Since we placed much of the systems' output online, the interested reader may be inspired to more closely explore the quality and shortcomings. Even some of the model files have been made available, so it is even possible to download and install some of the systems. Spanish-English Reference We know all too well that the present Treaties are inadequate and that the Union will need a better and different structure in future , a more constitutional structure which clearly distinguishes the powers of the Member States and those of the Union . Input Spanish Sabemos muy bien que los Tratados actuales no bastan y que , en el futuro , será necesario desarrollar una estructura mejor y diferente para la Unión Europea , una estructura más constitucional que también deje bien claras cuáles son las competencias de los Estados miembros y cuáles pertenecen a la Unión . Best system (Spanish-English) we all know very well that the current treaties are not enough and that , in the future , it will be necessary to develop a structure better and different for the european union , a structure more constitutional also make it clear what the competences of the member states and what belongs to the union . Worst System (Spanish-English) we know very well that the current treaties not enough and that , in the future , will be necessary develop a better structure and different to the european union , a structure more constitutional that also be well clear the powers of the member states and what belong to the union . Input French Nous savons très bien que les Traités actuels ne suffisent pas et qu ' il sera nécessaire à l ' avenir de développer une structure plus efficace et différente pour l ' Union , une structure plus constitutionnelle qui indique clairement quelles sont les compétences des états membres et quelles sont les compétences de l ' Union . Best system (French-English) we know very well that the current treaties are not enough and that it will be needed in the future to develop a structure more effective and different for the union , a structure more constitutional which clearly indicates what are the competence of member states and what are the powers of the union . Input Finnish Tiedämme oikein hyvin , että nykyiset perustamissopimukset eivät ole riittäviä ja että tulevaisuudessa on tarpeen kehittää unionille parempi ja toisenlainen rakenne , siis perustuslaillisempi rakenne , jossa mys ilmaistaan selkeämmin , mitä jäsenvaltioiden ja unionin toimivaltaan kuuluu Best system (Finnish-English) we know very well that the existing founding treaties do not need to be developed for the union and a different structure , therefore perustuslaillisempi structure , which also expresses clearly what the member states and the union 's competence is not sufficient and that better in the future . Input German Uns ist sehr wohl bewusst , dass die geltenden Verträge unzulänglich sind und künftig eine andere , effizientere Struktur für die Union entwickelt werden muss , nämlich eine stärker konstitutionell ausgeprägte Struktur mit einer klaren Abgrenzung zwischen den Befugnissen der Mitgliedstaaten und den Kompetenzen der Union . Best system (German-English) the union must be developed , with a major institutional structure with a clear demarcation between the powers of the member states and the competences of the union is well aware that the existing treaties are inadequate and in the future , a different , more efficient structure for us .",
         "9271055",
         "5025a4b2b724867f7b6b24f3e9253a61b76a3406",
         "83",
         "https://aclanthology.org/W05-0820",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Koehn, Philipp  and\nMonz, Christof",
         "Shared Task: Statistical Machine Translation between {E}uropean Languages",
         "119--124",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "koehn-monz-2005-shared",
         null,
         null
        ],
        [
         "3",
         "W05-0823",
         "This work discusses translation results for the four Euparl data sets which were made available for the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\". All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model.",
         "This work discusses translation results for the four Euparl data sets which were made available for the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\". All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003) . Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002) . The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mariño (2002) , and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This model is described in section 2. Translation results from the four source languages made available for the shared task (es: Spanish, fr: French, de: German, and fi: Finnish) into English (en) are presented and discussed. The paper is structured as follows. Section 2 describes the bilingual n-gram translation model. Section 3 presents a brief overview of the whole SMT procedure. Section 4 presents and discusses the shared task results and other interesting experimentation. Finally, section 5 presents some conclusions and further work. Bilingual N-gram Translation Model As already mentioned, the translation model used here is based on bilingual n-grams. It actually constitutes a language model of bilingual units which are referred to as tuples (de Gispert and Mariño, 2002) . This model approximates the joint probability between source and target languages by using 3grams as it is described in the following equation: p(T, S) ≈ N n=1 p((t, s) n |(t, s) n−2 , (t, s) n−1 ) (1) where t refers to target, s to source and (t, s) n to the n th tuple of a given bilingual sentence pair. Tuples are extracted from a word-to-word aligned corpus according to the following two constraints: first, tuple extraction should produce a monotonic segmentation of bilingual sentence pairs; and second, the produced segmentation is maximal in the sense that no smaller tuples can be extracted without violating the previous constraint (Crego et al., 2004) . According to this, tuple extraction provides a unique segmentation for a given bilingual sentence pair alignment. Figure 1 illustrates this idea with a simple example. Two important issues regarding this translation model must be mentioned. First, when extracting tuples, some words always appear embedded into tuples containing two or more words, so no translation probability for an independent occurrence of such words exists. To overcome this problem, the tuple 3-gram model is enhanced by incorporating 1-gram translation probabilities for all the embedded words (de Gispert et al., 2004) . Second, some words linked to NULL end up producing tuples with NULL source sides. This cannot be allowed since no NULL is expected to occur in a translation input. This problem is solved by preprocessing alignments before tuple extraction such that any target word that is linked to NULL is attached to either its precedent or its following word. SMT Procedure Description This section describes the procedure followed for preprocessing the data, training the models and optimizing the translation system parameters. Preprocessing and Alignment The Euparl data provided for this shared task (Euparl, 2003) was preprocessed for eliminating all sentence pairs with a word ratio larger than 2.4. As a result of this preprocessing, the number of sentences in each training set was slightly reduced. However, no significant reduction was produced. In the case of French, a re-tokenizing procedure was performed in which all apostrophes appearing alone were attached to their corresponding words. For example, pairs of tokens such as l ' and qu ' were reduced to single tokens such as l' and qu'. Once the training data was preprocessed, a wordto-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000) . As an approximation to the most probable alignment, the Viterbi alignment was considered. Then, the intersection and union of alignment sets in both directions were computed for each training set. Feature Function Computation The considered translation system implements a total of five feature functions. The first of these models is the tuple 3-gram model, which was already described in section 2. Tuples for the translation model were extracted from the union set of alignments as shown in Figure 1 . Once tuples had been extracted, the tuple vocabulary was pruned by using histogram pruning. The same pruning parameter, which was actually estimated for Spanish-English, was used for the other three language pairs. After pruning, the tuple 3-gram model was trained by using the SRI Language Modeling toolkit (Stolcke, 2002) . Finally, the obtained model was enhanced by incorporating 1-gram probabilities for the embedded word tuples, which were extracted from the intersection set of alignments. Table 1 presents the total number of running words, distinct tokens and tuples, for each of the four training data sets. The second feature function considered was a target language model. This feature actually consisted of a word 3-gram model, which was trained from the target side of the bilingual corpus by using the SRI Language Modeling toolkit. The third feature function was given by a word penalty model. This function introduces a sentence length penalization in order to compensate the sys-tem preference for short output sentences. More specifically, the penalization factor was given by the total number of words contained in the translation hypothesis. Finally, the fourth and fifth feature functions corresponded to two lexicon models based on IBM Model 1 lexical parameters p(t|s) (Brown et al., 1993) . These lexicon models were calculated for each tuple according to the following equation: p lexicon ((t, s) n ) = 1 (I + 1) J J j=1 I i=0 p(t i n |s j n ) (2) where s j n and t i n are the j th and i th words in the source and target sides of tuple (t, s) n , being J and I the corresponding total number words in each side of it. The forward lexicon model uses IBM Model 1 parameters obtained from source-to-target alignments, while the backward lexicon model uses parameters obtained from target-to-source alignments. Decoding and Optimization The search engine for this translation system was developed by Crego et al. (2005) . It implements a beam-search strategy based on dynamic programming and takes into account all the five feature functions described above simultaneously. It also allows for three different pruning methods: threshold pruning, histogram pruning, and hypothesis recombination. For all the results presented in this work the decoder's monotonic search modality was used. An optimization tool, which is based on a simplex method (Press et al., 2002) , was developed and used for computing log-linear weights for each of the feature functions described above. This algorithm adjusts the log-linear weights so that BLEU (Papineni et al., 2002) is maximized over a given development set. One optimization for each language pair was performed by using the 2000-sentence development sets made available for the shared task. Shared Task Results Table 2 presents the BLEU scores obtained for the shared task test data. Each test set consisted of 2000 sentences. The computed BLEU scores were case insensitive and used one translation reference. es -en fr -en de -en fi -en 0.3007 0.3020 0.2426 0.2031 As can be seen from Table 2 the best ranked translations were those obtained for French, followed by Spanish, German and Finnish. A big difference is observed between the best and the worst results. Differences can be observed from translation outputs too. Consider, for example, the following segments taken from one of the test sentences: es-en: We know very well that the present Treaties are not enough and that , in the future , it will be necessary to develop a structure better and different for the European Union... fr-en: We know very well that the Treaties in their current are not enough and that it will be necessary for the future to develop a structure more effective and different for the Union... de-en: We very much aware that the relevant treaties are inadequate and , in future to another , more efficient structure for the European Union that must be developed... fi-en: We know full well that the current Treaties are not sufficient and that , in the future , it is necessary to develop the Union better and a different structure... It is evident from these translation outputs that translation quality decreases when moving from Spanish and French to German and Finnish. A detailed observation of translation outputs reveals that there are basically two problems related to this degradation in quality. The first has to do with reordering, which seems to be affecting Finnish and, specially, German translations. The second problem has to do with vocabulary. It is well known that large vocabularies produce data sparseness problems (Koehn, 2002) . As can be confirmed from Tables 1 and 2 , translation quality decreases as vocabulary size increases. However, it is not clear yet, in which degree such degradation is due to monotonic decoding and/or vocabulary size. Finally, we also evaluated how much the full feature function system differs from the baseline tuple 3-gram model alone. In this way, BLEU scores were computed for translation outputs obtained for the baseline system and the full system. Since the English reference for the test set was not available, we computed translations and BLEU scores over de-velopment sets. Table 3 presents the results for both the full system and the baseline. 1   Table 3 : Baseline-and full-system BLEU scores (computed over development sets). language pair baseline full es -en 0.2588 0.3004 fr -en 0.2547 0.2938 de -en 0.1844 0.2350 fi -en 0.1526 0.1989 From Table 3 , it is evident that the four additional feature functions produce important improvements in translation quality. Conclusions and Further Work As can be concluded from the presented results, performance of the translation system used is much better for French and Spanish than for German and Finnish. As some results suggest, reordering and vocabulary size are the most important problems related to the low translation quality achieved for German and Finnish. It is also evident that the bilingual n-gram model used requires the additional feature functions to produce better translations. However, more experimentation is required in order to fully understand each individual feature's influence on the overall loglinear model performance. Acknowledgments This work has been funded by the European Union under the integrated project TC-STAR -Technology and Corpora for Speech to Speech Translation -(IST-2002-FP6-506738, http://www.tc-star.org). The authors also want to thank José A. R. Fonollosa and Marta Ruiz Costa-jussà for their participation in discussions related to this work.",
         "8303276",
         "bd500b912faec09be996f76ab49e33eb5a8b5f8f",
         "22",
         "https://aclanthology.org/W05-0823",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Banchs, Rafael E.  and\nCrego, Josep M.  and\nde Gispert, Adri{\\`a}  and\nLambert, Patrik  and\nMari{\\~n}o, Jos{\\'e} B.",
         "Statistical Machine Translation of {E}uparl Data by using Bilingual N-grams",
         "133--136",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "banchs-etal-2005-statistical",
         null,
         null
        ],
        [
         "4",
         "2009.mtsummit-posters.23",
         "Rule based machine translation methods require a set of sophisticated transfer rules for good accuracy. To manually build such a bilingual resource, one requires many man-years of work performed by linguistic specialists. This cost is too high, especially in case of less represented language pairs, such as Hungarian and Japanese. This paper proposes a simple and robust method to automatically build a large coverage transfer rule set for the Hungarian-Japanese language pair. Our method uses a small parsed bilingual corpus and a bilingual dictionary of the selected languages. We concentrate on accurately inducing the most frequent target language translation rules from all instances of a source language rule. We achieved good accuracy especially for low level rules, which are especially important in case of agglutinative languages.",
         "Rule based machine translation methods require a set of sophisticated transfer rules for good accuracy. To manually build such a bilingual resource, one requires many man-years of work performed by linguistic specialists. This cost is too high, especially in case of less represented language pairs, such as Hungarian and Japanese. This paper proposes a simple and robust method to automatically build a large coverage transfer rule set for the Hungarian-Japanese language pair. Our method uses a small parsed bilingual corpus and a bilingual dictionary of the selected languages. We concentrate on accurately inducing the most frequent target language translation rules from all instances of a source language rule. We achieved good accuracy especially for low level rules, which are especially important in case of agglutinative languages. Introduction Creating a set of transfer rules for a rule-based or pattern-based system could take many man-years of work (Prószéky and Tihanyi, 2002) ; we attempt to simplify this process by automatically generating these rules in form of transfer rules, including word-level rule correspondences, such as inflection and conjugation rules. This is particularly crucial with agglutinative languages, such as Hungarian or Japanese. Both languages manifest a high degree of verb and adjective inflection, governed by grammatical rules for which bilingual transfer rule implementation can be very costly. Moreover, both languages present specific linguistic features that are again very costly to organize in a bilingual environment. For example, Hungarian has one of the most grammatical cases, estimated to be between 17 and 24 (László, 1977) (Table 1 ). In Japanese particles before the respective nouns are used instead of grammatical cases. We attempt to generate the transfer rules using a small or medium sized parallel corpus and a bilingual dictionary, concentrating mainly on wordlevel, inflectional correspondences. Because of the limited bilingual resources, our transfer rules will be incorporated into a rule based machine translation framework for assimilation purposes. Thus we target the most simple and general transfer rules that cover most of the language. In this stage we do not attempt to create rules for grammatical exceptions or idiomatic expressions. This paper is structured as follows: first we discuss the most significant related studies, after which we focus on the problems of current translation template generation methods, followed by a brief description of our method. Finally we evaluate our method and conclude with our findings. Related work There are numerous relatively successful examples of shallow translation template extraction methods for closely related languages (Altintas and Güvenir, 2003; Cicekli, 2005) . Initial in-depth structure alignment methods attempt to identify complex, hierarchical structures such as phrase structures (Kaji et al., 1992) or dependency structures (Watanabe et al., 2000) . Other methods include the Translation Template Learner (TTL) algorithm, which analyzes similarities and differences between translation pairs (Cicekli and Güvenir, 2003; Öz and Cicekli, 1998; Ong et al., 2007) . Most of these heuristic methods attempt to generate transfer rules from each sentence pair. With distant languages they notoriously fail, because after recognizing partial matches, these methods estimate that the remaining, unmatched fragments are also equivalent, producing many erroneous, useless and even contradictory results. As a possible solution to the drawbacks of the pure statistical machine translation (weak on reordering; lack of target language fluency), syntactic approaches were proposed that work with traditional statistic models: syntax-based statistical machine translation (Yamada and Knight, 2001) ; string-based (Galley et al., 2004; Galley et al., 2006) ; tree-based (Lin, 2004; Liu et al., 2006) forest-based (Mi et al., 2008) ; forest pruning based systems (Mi et al., 2008) . These methods perform better than the non-statistical ones, but require large bilingual corpora. One other major problem of statistical methods is their difficulty in applicability with agglutinative languages. For example, in Hungarian one noun can have more than 2000 possible forms (combi-nations of number, case, number or person of grammatical possessors or possessed, etc), thus simply collecting enough statistical data is an enormous task. Lemmatizers could facilitate this task, but because of the complexity of the inflection rules and the high number of exceptions from the rule, efficient Hungarian lemmatizers are not yet available. Proposed method In order to achieve high precision, our method analyzes all instances of a certain rule, attempting to extract the most frequent, and thus the most suitable transfer rules. During this process, it looks for the most general rule as possible, subcategorizing or exemplifying only when needed. Our method follows a bottom-to-top mechanism, looking to identify not only general translation templates, but also partial rules, or frequent subsequences of a certain pattern, mainly targeting inflections and conjugations. Resource details To generate the transfer rules, the proposed method uses an automatically generated bilingual dictionary (Varga and Yokoyama, 2007) and a parsed bilingual corpus. There is no known large digital bilingual corpus for Hungarian and Japanese, therefore we needed to improvise with a small, manually created corpus using bilingual language books for Japanese or Hungarian learners that has only about 5000 sentence pairs. Although the sentences are short, it might be suitable for transfer rule extraction, since its data is grammatically rich and well prepared due to its initial educational purpose. For Hungarian we used MetaMorph (Prószéky and Novák, 2005) and for Japanese Cabocha (Kudo and Matsumoto, 2000) parsers. Our method's bilingual corpus requires a specific format. To ensure robustness, we opted for label-bracketed phrase structure rules, since these retain a relatively detailed syntax of the language. For example, the format for the Japanese sentence 夜はオペラに行った [Yoru ha opera ni itta 'Last night I went to the opera'] is (S (PP (N 夜) (Part は)) (VP (NP (N オペラ)(Part に)) (V 行った)))). Both parsers needed minor modifications to accommodate the output. Moreover, the Japanese parser had to be adapted to the Yamada-grammar (Moriyama, 2000) , so that one word should represent one concept, similarly with Hungarian. To be able to generate low-level inflection rules, we extended both Hungarian and Japanese parses with additional, optional information for inflected or conjugated words. The additional information are: (1) information about the inflection or conjugation and (2) the stem of the inflected or conjugated word to be easily identified from the dictionary. For example, the additional information for the Japanese verb 行った [itta; 'went'], becomes (V<2p> 行った<行く>). The bracket after the part-of-speech (POS) information represents the grammatical category of the inflection (2: time; p: past, hence <2p>: past tense), while the bracket after the inflected word represents the stem of the word. Transfer rule generation Our method is composed of two steps: first we generate the language models for each language; next the transfer rules are generated. Both steps are entirely automated. Step 1: Language model generation In this step we are looking to build the language model of the two languages. We compute every sentence, saving all partial rules. We can distinguish four types of rules: 1. head rules: rules where the parent is the sentence itself. The number of terminal rules is equal with the number of words that the sentence contains (ex: V→sleep); 4. regular rules: every rule that is not head, lexical or terminal rule (ex: NP→Adj+NP). We count the frequency of each rule, saving also the sentences from which they were generated. Head, lexical and regular rules need to be solved (transfer rules need to be generated to each of them). We consider a rule to be solved, when there is a correspondence with it in the target language. The transfer rules for the terminal rules are the bilingual dictionaries, and thus they are already solved. Step 2: Transfer rule generation In this step we build the transfer rules between the two languages. Using a recursive algorithm (solve_rule) we build the transfer rule candidates followed by a noisy rule elimination process (clean_candidates). This is performed twice, once as Hungarian and once as Japanese set as source language. solve_rule() solve_rule() solve_rule() solve_rule() For each source head rule (S=s→children(s)), we retrieve all S[] instances (retrieve_instance) in which this rule appears (line 7). If there are S'[] children that are not solved (not_solved), we attempt to solve its children's rules (line 9). For example, in the case of S→PP+VP as a Japanese head rule, an instance where this rule appears is the PP(夜は)+VP(オペラに行った) [yoru ha + opera ni itta 'Last night + I went to the opera'] sentence. Since the PP→N(夜)+Part(は) and VP→PP( オ ペ ラ に )+V( 行 っ た ) are not solved yet, the algorithm attempts to solve these first (line 9). When and if these two rules will be solved, the parent rule (S→PP+VP) will be reattempted. If all children are solved, transfer rule candidates (generate_candidates) are generated (line 13) using all translations of the rule (re-trieve_translation) (line 11). identify_match identify_match identify_match identify_match() () () () If a source rule that is investigated has only solved children, we retrieve all instances of the rule, to- gether with their target language correspondences (TS[]). These target language correspondences are whole sentences, we need to identify which parts of these sentences correspond with the source rule. All rules are identified by their children's rules; therefore if the rule in question is a lexical rule, the identification is done using the bilingual dictionary. The stemmed expression is vital in this case. If no information about the stem is available, there is a risk that the word will not be retrieved from the dictionary. If the rule is a head rule, the identification is done using the already solved rules, while with regular rules both resources are needed. In case of lexical rules, we look up each lexical category's instance (stemmed word, if it is available) from the lexical rule and mark the eventual correspondences. After all such correspondences are marked, we investigate the lowest level phrasal categories in the target language, counting how many identified instances it has in its sub-tree. The node or nodes with the maximum value are selected together with the sub-tree(s) as a transfer rule candidate (Figure 1 ). For example, to identify the s 1 →s 2 +s 3 lexical rule from the t 1 →t 2 +t 3 subtree, the s 2 →w 4 and s 3 →w 5 terminal rules are looked up using the dictionary. The process can have multiple scenarios: the words correspond to the same sub-tree (scenario1), or to different subtree (scenario2). In the latter case, multiple candidates (t 2 →t 4 +t 5 and t 3 →t 6 +t 7 ) are saved. In case of head and regular rules the only difference is the number of children's children. While with lexical rules this was one (one instance for each PoS), for non-lexical rules this is generally at least two (Figure 2 ). If no correspondences are found, no transfer rule candidate is returned. instantiate instantiate instantiate instantiate() () () () In case of lexical rules there are cases when the translations are not registered in the dictionary. In these cases, assuming that the dictionaries are correct, these words have a grammatical, rather than a lexical function. In this case the corresponding lexical category is instantiated, being replaced by its instance. Instantiation is performed also when inflection or conjugation information are available.  For example, if our initial s 1 →s 2 (w 1 )+s 3 (w 2 ) rule's w 2 word did not have any correspondence, the rule becomes s 1 →s 2 (w 1 )+w 2 . For example, in case of the Japanese PP→N+Part, there is no regular rule for a noun plus a particle, therefore the method correctly makes the judgment that the particle needs to be instantiated and new rules are generated for each instance (Table 2 ). clean_ca clean_ca clean_ca clean_candidates ndidates ndidates ndidates() () () () If there are unmatched instances in the second language and their translation can not be found in the first language's rule, the transfer rule candidate is deleted. For example, none of the translations of japán (Japanese person; Japanese language) from example#4 could be found in the Japanese rule, thus the transfer rule was considered erroneous. On the other hand, definite articles (a, az) (English: the) also don't have translations in the Japanese rule, but it they have no translation in the dictionary either (there is no corresponding Japanese translation), therefore they were allowed. The remaining candidates are grouped by their common nodes and are saved with three values: total nr of candidates; total nr of transfer rule instances; nr of instances for the current rule. Since we do not use any thresholds within our method, these three numbers indicate the confidence level of the transfer rule. For example, for PP→N+は [ha] only one transfer rules could be generated: NP→DET+N<2s>. The corresponding values are (7, 2, 2). Evaluation For evaluation, we fragmented our corpus into 5 fragments. First we randomly separated 100 sentences that we used these as our evaluation data. Next we randomly separated 4 training corpora of 100, 500, 1000 and 2000 sentence pairs, to analyze the score differences across various sized corpora. Due to the small size of the available Hungarian-Japanese corpus, performing BLEU score was not adequate, since not enough statistical data was available. Instead, we performed automatic recall evaluation and a manual adequacy evaluation to evaluate our method. We used the rules whose number of instances for the current rule is at least 2. Recall evaluation We investigated to what percentage our method's output rules (R o ) manage to cover the training data's phrase structure rules (R T ). We performed a weighted recall evaluation, weighting each rule (r) with its frequency (frequency(r)) in the training corpus. Because of the instantiation feature many new rules are generated that are not part of the training data's phrase structure rules, during evaluation only we added these new rules to the training data. ( ( ) 100 ⋅ = ∑ ∑ ∈ ∈ T O R r R r r frequency r frequency recall (1) ) We analyzed the Japanese coverage, separately evaluating the head, regular and lexical rules. Lexical rules performed best, improving rapidly from 47.71% to 64.23% when the training data increased from 100 to 2000 sentence pairs (Table 3 ). Head rules performed worst, with only up to a third of them managing to move up the parse trees all the way to the root. Adequacy evaluation We automatically simulated a basic rule based machine translation system with 100 Japanese sentences whose rule head has a transfer rule. These sentences were randomly selected from the test data of our bilingual corpus. Our simple machine translation system exhaustively applied all suitable transfer rules, also performing lexical transfer, based on the bilingual dictionary. During this process, all intermediate data (lexical, regular and head rules) was separately saved. As a result, multiple Hungarian translations became available for a single Japanese sentence. Next, all Hungarian translations, together with the reference retrieved from the bilingual corpus, were manually checked by 3 independent, Hungarian native speakers. We used a 5 to 1 scoring criteria, where 5 is a perfect, 1 is a totally wrong output sentence. The interpretation of the intermediate scores was left to the judgment of each evaluator. We separately evaluated the head, regular and lexical rules. We performed the same evaluation on four training corpora: 100, 500, 1000 and 2000 samples. Assuming that a language model would correctly identify the best translation, we considered only the best scoring Hungarian translation for each Japanese source. Lexical items scored best, since understandably the errors on the lower level reflected within the regular and head rules as well. However, with the increase in size of the training data, the accuracy of the lexical rules increased faster than the other two types of rules. We could not observe any major difference in behaviour between the regular and head rules (Table 4 ). Discussions Our method showed its biggest weakness during recall evaluation. Many rules could not be identified in the transfer rules, especially the ones which direct over larger sub-trees. There are two major reasons for this: linguistic differences and resource issues. Regarding precision, the once recalled rules showed a surprising accuracy, especially lexical ones. Precision problems can be mainly attributed to resource issues. Linguistic differences Our first observation is that the biggest reasons for the low recall value are the linguistic differences between Hungarian and Japanese. Syntax is different, sentence construction is also different; therefore one sub-tree in a certain language does not necessarily match another sub-tree in the other parse. Other linguistic differences, such as expression of pronouns or the sentence topic manifest differently across languages, our method does not always generate the proper transfer rule in these cases. For example, with the 私[watashi]+は [ha] (me, myself) sub-tree rarely has any correspondence in Hungarian, since the agent (pronoun in this case) is expressed within the verb. Resource issues There are two types of resource issues. The first problem concerns the parsers and dictionary that we used. The parsers do not have a perfect accuracy, the noise produced by them reflected in the recall and accuracy results. The methodology of the parsers itself is different, with sub-trees not always matching a sub-tree in the other language, even when both parsers performed correctly. Our bilingual dictionary is noisy, because it was automatically generated using a pivot language (Varga and Yokoyama, 2007) . No manual cleaning was performed in order to raise its recall or accuracy; many translations could not be identified. The second problem concerns our corpus. Precision scores with smaller training data were low, because many erroneous transfer rules were generated besides the correct ones, but with the increase of the training data the frequency of correct rules also climbed rapidly. However, even with our biggest training data (2000 sentence pairs) the recall and precision values were not very high, but it is promising that from the second largest training data (1000 sentence pairs) the relative recall increase was between 3%-11%, the relative adequacy increase between 4%-13%. This significant increase shows that with an average sized corpus much better results can be achieved. Conclusions We presented a transfer rule generating method that uses a parsed bilingual corpus and a bilingual dictionary as resources. Although our biggest aim is low-cost in this research, during bilingual corpus acquisition we found ourselves in a contradictory situation: to generate low-cost transfer rules, we needed to manually create a small bilingual corpus. However, the cost of creating this corpus is insignificant when we think of the costs that a transfer rule system would require. As a compromise between having a small or medium sized corpus with noisy parses and the desire to achieve a good performance, we didn't handle grammatical exceptions or idiomatic expressions. As a result, with a small corpus we managed to achieve medium recall and good precision, with basic conjugation and inflectional rules being highly accurate. We showed that with the minimal increase in size of the bilingual corpus, overall adequacy, together with recall can quickly increase.",
         "34218208",
         "2e4ffd53b290960a18679c8d6f3db4e03bbacdc8",
         "3",
         "https://aclanthology.org/2009.mtsummit-posters.23",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Varga, Istv{\\'a}n  and\nYokoyama, Shoichi",
         "Transfer rule generation for a {J}apanese-{H}ungarian machine translation system",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "varga-yokoyama-2009-transfer",
         null,
         null
        ],
        [
         "5",
         "W05-0822",
         "This paper describes the participation of the Portage team at NRC Canada in the shared task 1 of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task.",
         "This paper describes the participation of the Portage team at NRC Canada in the shared task 1 of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task. Introduction The rapid growth of the Internet has led to a rapid growth in the need for information exchange among different languages. Machine Translation (MT) and related technologies have become essential to the information flow between speakers of different languages on the Internet. Statistical Machine Translation (SMT), a data-driven approach to producing translation systems, is becoming a practical solution to the longstanding goal of cheap natural language processing. In this paper, we describe Portage, a statistical phrase-based machine translation system, which we evaluated on all different language pairs that were provided for the shared task. As Portage is a very 1 http://www.statmt.org/wpt05/mt-shared-task/ new system, our main goal in participating in the workshop was to test it out on different language pairs, and to establish baseline performance for the purpose of comparison against other systems and against future improvements. To do this, we used a fairly standard configuration for phrase-based SMT, described in the next section. Of the language pairs in the shared task, French-English is particularly interesting to us in light of Canada's demographics and policy of official bilingualism. We therefore divided our participation into two parts: one stream for French-English and another for Finnish-, German-, and Spanish-English. For the French-English stream, we tested the use of additional data resources along with hand-coded rules for translating numbers and dates. For the other streams, we used only the provided resources in a purely statistical framework (although we also investigated several automatic methods of coping with Finnish morphology). The remainder of the paper is organized as follows. Section 2 describes the architecture of the Portage system, including its hand-coded rules for French-English. Experimental results for the four pairs of languages are reported in Section 3. Section 4 concludes and gives pointers to future work. Portage Portage operates in three main phases: preprocessing of raw data into tokens, with translation suggestions for some words or phrases generated by rules; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. (A fourth postprocessing phase was not needed for the shared task.) Preprocessing Preprocessing is a necessary first step in order to convert raw texts in both source and target languages into a format suitable for both model training and decoding (Foster et al., 2003) . For the supplied Europarl corpora, we relied on the existing segmentation and tokenization, except for French, which we manipulated slightly to bring into line with our existing conventions (e.g., converting l ' an into l' an). For the Hansard corpus used to supplement our French-English resources (described in section 3 below), we used our own alignment based on Moore's algorithm (Moore, 2002) , segmentation, and tokenization procedures. Languages with rich morphology are often problematic for statistical machine translation because the available data lacks instances of all possible forms of a word to efficiently train a translation system. In a language like German, new words can be formed by compounding (writing two or more words together without a space or a hyphen in between). Segmentation is a crucial step in preprocessing languages such as German and Finnish texts. In addition to these simple operations, we also developed a rule-based component to detect numbers and dates in the source text and identify their translation in the target text. This component was developed on the Hansard corpus, and applied to the French-English texts (i.e. Europarl and Hansard), on the development data in both languages, and on the test data. Decoding Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P(t|s). Our model for P(t|s) is a log-linear combination of four main components: one or more trigram language models, one or more phrase translation models, a distortion model, and a word-length feature. The trigram language model is implemented in the SRILM toolkit (Stolcke, 2002) . The phrase-based translation model is similar to the one described in (Koehn, 2004) , and relies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings. s To set weights on the components of the loglinear model, we implemented Och's algorithm (Och, 2003) . This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell's algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations. Unfortunately, our implementation of this algorithm converged only very slowly to a satisfactory final nbest list, so we used two different ad hoc strategies for setting weights: choosing the best values encountered during , with the exception of a ch as the ability to decode either w ards. translarent language pairs of the sha d t hared t the iterations of Och's algorithm (French-English), and a grid search (all other languages). To perform the actual translation, we used our decoder, Canoe, which implements a dynamicprogramming beam search algorithm based on that of Pharaoh (Koehn, 2004) . Canoe is input-output compatible with Pharaoh few extensions su back ards or forw Rescoring To improve raw output from Canoe, we used a rescoring strategy: have Canoe generate a list of nbest translations rather than just one, then reorder the list using a model trained with Och's method to optimize BLEU score. This is identical to the final pass of the algorithm described in the previous section, except for the use of a more powerful loglinear model than would have been feasible to use inside the decoder. In addition to the four basic features of the initial model, our rescoring model included IBM2 model probabilities in both directions (i.e., P(s|t) and P(t|s)); and an IBM1-based feature designed to detect whether any words in one language seemed to be left without satisfactory tions in the other language. This missing-word feature was also applied in both directions. Experiments on the Shared Task We conducted experiments and evaluations on Portage using the diffe re ask. In addition to the provided data, a set of 6,056,014 sentences extracted from Hansard corpus, the official record of Canada's parliamentary debates, was used in both French and English languages. This c guage and translation models for use in decoding and rescoring. The development test data was split into two parts: The first part that includes 1,000 sentences in each language with reference translations into English served in the optimization of weights for both the decoding and rescoring models. In this study, number of n-best lists was set to 1,000. The second part, which includes 1,000 sentences in each language with referenc used in the evaluation of the performance of the translation models. Experiments on the French-English Task Our goal for this language pair was to conduct experiments on hniques: 1 indicates the method, the second column gives results for decoding with Canoe only, and the third column for decoding and rescoring with Canoe. For comparison between the four methods, there was an improvement in terms of BLEU scores when using two language models and two translation models generated from Europarl and Hansard corpora; however, parsing numbers and dates had a negative impact on the ranslation els. of increased trade within North merica but also functions as a good counterpoint for French-English. ble 1. BLEU scores for the French-English test sentences A noteworthy feature of these results is that the improvement given by the out-of-domain Hansard corpus was very slight. Although we suspect that somewhat better performance could have been achieved by better weight optimization, this result clearly underscores the importance of matching training and test domains. A related point is that our number and date translation rules actually caused a performance drop due to the fact that they were optimized for typographical conventions prevalent in Hansard, which are quite different from those used in Europarl. Our best result ranked third in the shared WPT05 French-English task , with a difference of 0.74 in terms of BLEU score from the first rank participant, and a difference of 0.67 in terms o BLEU score from the second ranked participant. Experiments on other Pairs of Languages The WPT05 workshop provides a good opportunity to achieve our benchmarking goals with corpora that provide challenging difficulties. German and Finnish are languages that make considerable use of compounding. Finnish, in addition, has a particularly complex morphology that is organized on principles that are quite different from any in English. This results in much longer word forms each of which occurs very infrequently. Our original intent was to propose a number of possible statistical approaches to analyzing and splitting these word forms and improving our results. Since none of these yielded results as good as the baseline, we will continue this work until we understand what is really needed. We also care very much about translating between French and English in Canada and plan to spend a lot of extra effort on difficulties that occur in this case. Translation between Spanish and English is also becoming more mportant as a result ble 2 BLEU scores for the Finnish-English, German-English and Spanish-English test sentences To establish our baseline, the only preprocessing we did was lowercasing (using the provided tokenization). Canoe was run without any special settings, although weights for distortion, word penalty, language model, and translation model were optimized using a grid search, as described above. Rescoring was also done, and usually resulted in at least an extra BLEU point. Our final results are shown in Table 2 . Ranks at the shared WPT05 Finnish-, German-, and Spanish-English tasks were assigned as second, third and fourth, with differences of 1.06, 1.87 ter s of BLEU sc Conclusion We have reported on our participation in the shared task of the ACL 2005 Workshop on Building and Using Parallel Texts, conducting evaluations of Portage, our statistical machine translation system, on all four language pairs. Our best BLEU scores for the French-, Finnish-, German-, and Spanish-English at this stage were 29.5, 20.95, 23.21 and 29.08 , respectively. In total, eleven teams took part at the shared task and most of them submitted results for all pairs of languages. Our results distinguished the NRC team at the third, second, third and fourth ranks with slight differences with the first ranked participants. A major goal of this work was to evaluate Portage at its first stage of implementation on different pairs of languages. This evaluation has served to identify some problems with our system in the areas of weight optimization and number and date rules. It has also indicated the limits of using out-ofdomain corpora, and the difficulty of morphologically complex languages like Finnish. Current and planned future work includes the exploitation of comparable corpora for statistica machine transl knowledge, and better features for nbest rescoring.",
         "1289925",
         "dd91cae2486eb393f524c3cc32006991568e8ba5",
         "47",
         "https://aclanthology.org/W05-0822",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Sadat, Fatiha  and\nJohnson, Howard  and\nAgbago, Akakpo  and\nFoster, George  and\nKuhn, Roland  and\nMartin, Joel  and\nTikuisis, Aaron",
         "{PORTAGE}: A Phrase-Based Machine Translation System",
         "129--132",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "sadat-etal-2005-portage",
         null,
         null
        ],
        [
         "6",
         "W05-0826",
         "We describe the Spanish-to-English LDV-COMBO system for the Shared Task 2: \"Exploiting Parallel Texts for Statistical Machine Translation\" of the ACL-2005 Workshop on \"Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond\". Our approach explores the possibility of working with alignments at different levels of abstraction, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on Word-Net which is used for unknown words.",
         "We describe the Spanish-to-English LDV-COMBO system for the Shared Task 2: \"Exploiting Parallel Texts for Statistical Machine Translation\" of the ACL-2005 Workshop on \"Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond\". Our approach explores the possibility of working with alignments at different levels of abstraction, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on Word-Net which is used for unknown words. Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. Many other authors have tried to do so. See (Och and Ney, 2000) , (Yamada and Knight, 2001) , (Koehn and Knight, 2002) , (Koehn et al., 2003) , (Schafer and Yarowsky, 2003) and (Gildea, 2003) . Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al., 1993) . Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit. Apart from redefining the scope of the alignment unit, we may use different degrees of linguistic annotation. We introduce the general concept of data view, which is defined as any possible representation of the information contained in a bitext. We enrich data view tokens with features further than lexical such as PoS, lemma, and chunk label. As an example of the applicability of data views, suppose the case of the word 'plays' being seen in the training data acting as a verb. Representing this information as 'plays V BZ ' would allow us to distinguish it from its homograph 'plays N N S ' for 'plays' as a noun. Ideally, one would wish to have still deeper information, moving through syntax onto semantics, such as word senses. Therefore, it would be possible to distinguish for instance between two realizations of 'plays' with different meanings: 'he P RP plays V BG guitar N N ' and 'he P RP plays V BG basketball N N '. Of course, there is a natural trade-off between the use of data views and data sparsity. Fortunately, we hava data enough so that statistical parameter estimation remains reliable. System Description The LDV-COMBO system follows the SMT architecture suggested by the workshop organizers. First, training data are linguistically annotated for the two languages involved (See subsection 2.1). 10 different data views have been built. Notice that it is not necessary that the two parallel counterparts of a bitext share the same data view, as long as they share the same granularity. However, in all our experiments we have annotated both sides with the same linguistic information. See token descriptions: (W) word, (WL) word and lemma, (WP) word and PoS, (WC) word and chunk label, (WPC) word, PoS and chunk label, (Cw) chunk of words (Cwl), chunk of words and lemmas, (Cwp) chunk of words and PoS (Cwc) chunk of words and chunk labels (Cwpc) chunk of words, PoS and chunk labels. By chunk label we refer to the IOB label associated to every word inside a chunk, e.g. 'I B−N P declare B−V P resumed I−V P the B−N P session I−N P of B−P P the B−N P European I−N P Parliament I−N P . O '). We build chunk tokens by explicitly connecting words in the same chunk, e.g. '(I) N P (declare resumed) V P (the session) N P (of) P P (the European Parliament) N P '. See examples of some of these data views in Table 1 . Then, running GIZA++, we obtain token alignments for each of the data views. Combined phrasebased translation models are built on top of the Viterbi alignments output by GIZA++. See details in subsection 2.2. Combo-models must be then postprocessed in order to remove the additional linguistic annotation and split chunks back into words, so they fit the format required by Pharaoh. Moreover, we have used the Multilingual Central Repository (MCR), a multilingual lexical-semantic database (Atserias et al., 2004) , to build a wordbased translation model. We back-off to this model in the case of unknown words, with the goal of improving system recall. See subsection 2.3. Data Representation In order to achieve robustness the same tools have been used to linguistically annotate both languages. The SVMTool 1 has been used for PoS-tagging (Giménez and Màrquez, 2004) . The Freeling 2 package (Carreras et al., 2004) has been used for lemmatizing. Finally, the Phreco software by (Carreras et al., 2005) has been used for shallow parsing. No additional tokenization or pre-processing steps other than case lowering have been performed. Special treatment of named entities, dates, numbers, 1 The SVMTool may be freely downloaded at http://www.lsi.upc.es/˜nlp/SVMTool/ . 2 Freeling Suite of Language Analyzers may be downloaded at http://www.lsi.upc.es/˜nlp/freeling/ currency, etc., should be considered so as to further enhance the system. Building Combined Translation Models Because data views capture different, possibly complementary, aspects of the translation process it seems reasonable to combine them. We consider two different ways of building such combo-models: LPHEX Local phrase extraction. To build a separate phrase-based translation model for each data view alignment, and then combine them. There are two ways of combining translation models: MRG Merging translation models. We work on a weighted linear interpolation of models. These weights may be tuned, although a uniform weight selection yields good results. Additionally, phrase-pairs may be filtered out by setting a score threshold. noMRG Passing translation models directly to the Pharaoh decoder. However, we encountered many problems with phrasepairs that were not seen in all single models. This obliged us to apply arbitrary smoothing values to score these pairs. GPHEX Global phrase extraction. To build a single phrased-based translation model from the union of alignments from several data views. In its turn, any MRG operation performed on a combo-model results again in a valid combo-model. In any case, phrase extraction 3 is performed as depicted by (Och, 2002) . Using the MCR Outer knowledge may be supplied to the Pharaoh decoder by annotating the input with alternative translation options via XML-markup. We enrich every unknown word by looking up every possible translation for all of its senses in the MCR. These are scored by relative frequency according to the number of senses that lexicalized in the same manner. Let w f , p f be the source word and PoS, and w e be the target word, we define a function  Scount(w f , p f , w e ) which counts the number of senses for (w f , p f ) which can lexicalize as w e . A translation pair is scored as: score(w f , p f |w e ) = Scount(w f , p f , w e ) (w f ,p f ) Scount(w f , p f , w e ) (1) Better results would be expected working with word sense disambiguated text. We are not at this point yet. A first approach could be to work with the most frequent sense heuristic. Experimental Results Data and Evaluation Metrics We have used the data sets and language model provided by the organization. No extra training or development data were used in our experiments. We evaluate results with 3 different metrics: GTM F 1 -measure (e = 1, 2), BLEU score (n = 4) as provided by organizers, and NIST score (n = 5). Experimenting with Data Views Table 2 presents MT results for the 10 elementary data views devised in Section 2. Default parameters are used for λ tm , λ lm , and λ w . No tuning has been performed. As expected, word-based views obtain significatively higher results than chunk-based. All data views at the same level of granularity obtain comparable results. In is performed. We refer to the W model as our baseline. In this view, only words are used. The 5W-MRG and 5W-GPHEX models use a combination of the 5 word-based data views, as in MRG and GPHEX, respectively. The 5C-MRG and 5C-GPHEX system use a combination of the 5 chunk based data views, as in MRG and GPHEX, respectively. The 10-MRG system uses all 10 data views combined as in MRG. The 10-GPHEX/MRG system uses the 5 word based views combined as in GPHEX, the 5 chunk based views combined as in GPHEX, and then a combination of these two combo-models as in MRG.  It can be seen that results improve by combining several data views. Furthermore, global phrase extraction (GPHEX) seems to work much finer than local phrase extraction (LPHEX). Table 4 shows MT results after optimizing λ tm , λ lm , λ w , and the weights for the MRG operation, by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) . Observe that tuning the system improves the performance considerably. The λ w parameter is particularly sensitive to tuning. Even though the performance of chunk-based models is poor, the best results are obtained by combinining the two levels of abstraction, thus proving that syntactically motivated phrases may help. 10-MRG and 10-GPHEX models achieve a similar performance. The 10-MRG-best W N system corresponds to the 10-MRG model using WordNet. The 10-MRGsub W N system is this same system at the time of submission. Results using WordNet, taking into account that the number of unknown 4 words in the development set was very small, are very promising. Conclusions We have showed that it is possible to obtain better phrase-based translation models by utilizing alignments built on top of different linguistic data views. These models can be robustly combined, significantly outperforming all of their components in isolation. We leave for further work the experimentation of new data views such as word senses and semantic roles, as well as their natural porting and evolution from the alignment step to phrase extraction and decoding. Acknowledgements This research has been funded by the Spanish Ministry of Science and Technology (ALIADO TIC2002-04447-C02). Authors are thankful to Patrik Lambert for providing us with the implementation of the Simplex Method used for tuning.",
         "81127",
         "c6a3e8627dd17293efa7c49d426befb52e0263fa",
         "16",
         "https://aclanthology.org/W05-0826",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Gim{\\'e}nez, Jes{\\'u}s  and\nM{\\`a}rquez, Llu{\\'\\i}s",
         "Combining Linguistic Data Views for Phrase-based {SMT}",
         "145--148",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "gimenez-marquez-2005-combining",
         null,
         null
        ],
        [
         "7",
         "W05-0824",
         "Thanks to the profusion of freely available tools, it recently became fairly easy to built a statistical machine translation (SMT) engine given a bitext. The expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another. We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT sharedtask.",
         "Thanks to the profusion of freely available tools, it recently became fairly easy to built a statistical machine translation (SMT) engine given a bitext. The expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another. We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT sharedtask. Introduction Machine translation is nowadays mature enough that it is possible without too much effort to devise automatically a statistical translation system from just a parallel corpus. This is possible thanks to the dissemination of valuable packages. The performance of such a system may however greatly vary from one pair of languages to another. Indeed, there is no free lunch for system developers, and if a black box approach can sometimes be good enough for some applications (we can surely accomplish translation gisting with the French-English and Spanish-English systems we developed during this exercice), making use of the output of such a system for, let's say, quality translation is another kettle of fish (especially in our case with the Finnish-English system we ended-up with). We devoted two weeks to the SMT shared task, the aim of which was precisely to see how well systems can do across different language families. We began with a core system which is described in the next section and from which we obtained baseline performances that we tried to improve upon. Since the French-and Spanish-English systems produced output that were comprehensible enough 1 , we focussed on the two languages whose translations were noticeably worse: German and Finnish. For German, we tried to move around words in order to mimic English word order; and we tried to split compound words. This is described in section 4. For the Finnish/English pair, we tried to decompose Finnish words into smaller substrings (see section 5). In parallel to that, we tried to smooth a phrasebased model (PBM) making use of WORDNET. We report on this experiment in section 3. We describe in section 6 the final setting of the systems we used for submitting translations and their official results as computed by the organizers. Finally, we conclude our two weeks of efforts in section 7. The core system We assembled up a phrase-based statistical engine by making use of freely available packages. The translation engine we used is the one suggested within the shared task: PHARAOH (Koehn, 2004) which control the decoder. For acquiring a PBM, we followed the approach described by Koehn et al. (2003) . In brief, we relied on a bi-directional word alignment of the training corpus to acquire the parameters of the model. We used the word alignment produced by Giza (Och and Ney, 2000) out of an IBM model 2. We did try to use the alignment produced with IBM model 4, but did not notice significant differences over our experiments; an observation consistent with the findings of Koehn et al. (2003) . Each parameter in a PBM can be scored in several ways. We considered its relative frequency as well as its IBM-model 1 score (where the transfer probabilities were taken from an IBM model 2 transfer table). The language model we used was the one provided within the shared task. We obtained baseline performances by tuning the engine on the top 500 sentences of the development corpus. Since we only had a few parameters to tune, we did it by sampling the parameter space uniformly. The best performance we obtained, i.e., the one which maximizes the BLEU metric as measured by the mteval script 2 is reported for each pair of languages in Table 1 . Smoothing PBMs with WORDNET Among the things we tried but which did not work well, we investigated whether smoothing the transfer table of an IBM model (2 in our case) with WORDNET would produce better estimates for rare words. We adapted an approach proposed by Cao et al. (2005) for an Information Retrieval task, and computed for any parameter (e i , f j ) be-longing to the original model the following approximation: ṗ(e i |f j ) ≈ e∈E p wn (e i |e) × p n (e|f j ) where E is the English vocabulary, p n designates the native distribution and p wn is the probability that two words in the English side are linked together. We estimated this distribution by cooccurrence counts over a large English corpus 3 . To avoid taking into account unrelated but cooccurring words, we used WORDNET to filter in only the co-occurrences of words that are in relation according to WORDNET. However, since many words are not listed in this resource, we had to smooth the bigram distribution, which we did by applying Katz smoothing (Katz, 1997) : p katz (e i |e) = ċ(e i ,e|W,L) P e j c(e j ,e|W,L) if c(e i , e|W, L) > 0 α(e)p katz (e i ) otherwise where ċ(a, b|W, L) is the good-turing discounted count of times two words a and b that are linked together by a WORDNET relation, co-occur in a window of 2 sentences. We used this smoothed model to score the parameters of our PBM instead of the native transfer table. The results were however disappointing for both the G-E and S-E translation directions we tested. One reason for that, may be that the English corpus we used for computing the co-occurrence counts is an out-of-domain corpus for the present task. Another possible explanation lies in the fact that we considered both synonymic and hyperonymic links in WORDNET; the latter kind of links potentially introducing too much noise for a translation task. The German-English task We identified two major problems with our approach when faced with this pair of languages. First, the tendency in German to put verbs at the end of a phrase happens to ruin our phrase acquisition process, which basically collects any box of aligned source and target adjacent words. This can be clearly seen in the alignment matrix of figure 1 where the verbal construction could clarify is translated by two very distant German words könnten and erläutern. Second, there are many compound words in German that greatly dilute the various counts embedded in the PBM Figure 1 : Bidirectional alignment matrix. A cross in this matrix designates an alignment valid in both directions, while the symbol indicates an uni-directional alignment (for has been aligned with einen, but not the other way round). Moving around German words For the first problem, we applied a memory-based approach to move around words in the German side in order to better synchronize word order in both languages. This involves, first, to learning transformation rules from the training corpus, second, transforming the German side of this corpus; then training a new translation model. The same set of rules is then applied to the German text to be translated. The transformation rules we learned concern a few (five in our case) verbal constructions that we expressed with regular expressions built on POS tags in the English side. Once the locus e v u of a pattern has been identified, a rule is collected whenever the following conditions apply: for each word e in the locus, there is a target word f which is aligned to e in both alignment directions; these target words when moved can lead to a diagonal going from the target word (l) associated to e u−1 to the target word r which is aligned to e v+1 . The rules we memorize are triplets (c, i, o) where c = (l, r) is the context of the locus and i and o are the input and output German word order (that is, the order in which the tokens are found, and the order in which they should be moved). For instance, in the example of Figure 1 , the Verb Verb pattern match the locus could clarify and the following rule is acquired: (sie einen, könnten erläutern, könnten erläutern), a paraphrase of which is: \"whenever you find (in this order) the word könnten and erläutern in a German sentence containing also (in this order) sie and einen, move könnten and erläutern between sie and einen. A set of 124 271 rules have been acquired this way from the training corpus (for a total of 157 970 occurrences). The most frequent rule acquired is (ich herrn, möchte danken, möchte danken), which will transform a sentence like \"ich möchte herrn wynn für seinen bericht danken.\" into \"ich möchte danken herrn wynn für seinen bericht.\". In practice, since this acquisition process does not involve any generalization step, only a few rules learnt really fire when applied to the test material. Also, we devised a fairly conservative way of applying the rules, which means that in practice, only 3.5% of the sentences of the test corpus where actually modified. The performance of this procedure as measured on the development set is reported in Table 2 . As simple as it is, this procedure yields a relative gain of 7% in BLEU. Given the crudeness of our approach, we consider this as an encouraging improvement. Compound splitting For the second problem, we segmented German words before training the translation models. Empirical methods for compound splitting applied to German have been studied by Koehn and Knight (2003) . They found that a simple splitting strategy based on the frequency of German words was the most efficient method of the ones they tested, when embedded in a phrase-based translation engine. Therefore, we applied such a strategy to split German words in our corpora. The results of this approach are shown in Table 2 . Note: Both the swapping strategy and the compound splitting yielded improvements in terms of BLEU score. Only after the deadline did we find time to train new models with a combination of both techniques; the results of which are reported in the last line of Table 2 . The Finnish-English task The worst performances were registered on the Finnish-English pair. This is due to the agglutinative nature of Finnish. We tried to segment the Finnish material into smaller units (substrings) by making use of the frequency of all Finnish substrings found in the training corpus. We maintained a suffix tree structure for that purpose. We proceeded by recursively finding the most promising splitting points in each Finnish token of C characters F C 1 by computing split(F C 1 ) where: split(F j i ) =    |F j i | if j − i < 2 max c∈[i+2,j−2] |F c i |× split(F j c+1 ) otherwise This approach yielded a significant degradation in performance that we still have to analyze. Submitted translations At the time of the deadline, the best translations we had were the baselines ones for all the language pairs, except for the German-English one where the moving of words ranked the best. This defined the configuration we submitted, whose results (as provided by the organizers) are reported in Table 3 Conclusion We found that, while comprehensible translations were produced for pairs of languages such as French-English and Spanish-English; things did not go as well for the German-English pair and especially not for the Finnish-English pair. We had a hard time improving our baseline performance in such a tight schedule and only managed to improve our German-English system. We were less lucky with other attempts we implemented, among them, the smoothing of a transfer table with WORDNET, and the segmentation of the Finnish corpus into smaller units.",
         "9374715",
         "177bfa5b71e3e4fff964c4c96f08d971885b41a9",
         "3",
         "https://aclanthology.org/W05-0824",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Langlais, Philippe  and\nCao, Guihong  and\nGotti, Fabrizio",
         "{RALI}: {SMT} Shared Task System Description",
         "137--140",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "langlais-etal-2005-rali",
         null,
         null
        ],
        [
         "8",
         "W05-0827",
         "Nowadays, most of the statistical translation systems are based on phrases (i.e. groups of words). In this paper we study different improvements to the standard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\" (ACL Workshop on Parallel Texts 2005).",
         "Nowadays, most of the statistical translation systems are based on phrases (i.e. groups of words). In this paper we study different improvements to the standard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\" (ACL Workshop on Parallel Texts 2005). Introduction Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), ẽ = argmax e P (e|f ) (1) If we use Bayes rule to reformulate the translation probability, we obtain, ẽ = argmax e P (f |e)P (e) (2) This translation model is known as the sourcechannel approach [1] and it consists on a language model P (e) and a separate translation model P (f |e) [5] . In the last few years, new systems tend to use sequences of words, commonly called phrases [8] , aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. The features functions, h m , are the system models (translation model, language model and others) and weigths, λ i , are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understanding task. It has the advantatge that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phrase-extraction algorythm in [11] . It also combines several interesting features and it reports an important improvement from the baseline. It is organized as follows. Section 2 introduces the baseline; the following section explains the modification in the phrase extraction; section 4 shows the different features which have been taken into account; section 5 presents the evaluation framework; and the final section shows some conclusions on the experiments in the paper and on the results in the shared task. Baseline The baseline is based on the source-channel approach, and it is composed of the following models which later will be combined in the decoder. The Translation Model. It is based on bilingual phrases, where a bilingual phrase (BP ) is simply two monolingual phrases (M P ) in which each one is supposed to be the translation of each other. A monolingual phrase is a sequence of words. Therefore, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [17] . During training, the system has to learn a dictionary of phrases. We begin by aligning the training corpus using GIZA++ [6] , which is done in both translation directions. We take the union of both alignments to obtain a symmetrized word alignment matrix. This alignment matrix is the starting point for the phrase based extraction. Next, we define the criterion to extract the set of BP of the sentence pair (f j2 j1 ; e i2 i1 ) and the alignment matrix A ⊆ J * I, which is identical to the alignment criterion described in [11] . BP (f J 1 , e I 1 , A) = {(f j2 j1 , e i2 i1 ) : ∀(j, i) A : j 1 ≤ j ≤ j 2 ↔ i 1 ≤ i ≤ i2 ∧∃(j, i) A : j 1 ≤ j ≤ j 2 ∧ i 1 ≤ i ≤ i2} The set of BP is consistent with the alignment and consists of all BP pairs where all words within the foreign language phrase are only aligned to the words of the English language phrase and viceversa. At least one word in the foreign language phrase has to be aligned with at least one word of the English language. Finally, the algorithm takes into account possibly unaligned words at the boundaries of the foreign or English language phrases. The target language model. It is combined with the translation probability as showed in equation (2) . It gives coherence to the target text obtained by the concatenated phrases. Phrase Extraction Motivation. The length of a M P is defined as its number of words. The length of a BP is the greatest of the lengths of its M P . As we are working with a huge amount of data (see corpus statistics), it is unfeasible to build a dictionary with all the phrases longer than length 4. Moreover, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [8] . X-length In our system we considered two length limits. We first extract all the phrases of length 3 or less. Then, we also add phrases up to length 5 if they cannot be generated by smaller phrases. Empirically, we chose 5, as the probability of reappearence of larger phrases decreases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments. For example, from the following sentence, Cuando el Parlamento Europeo , que tan frecuentemente insiste en los derechos de los trabajadores y en la debida protección social , (...) NULL ( ) When ( 1 ) the ( 2 ) European ( 4 ) Parliament ( 3 4 ) , ( 5 ) that ( 6 ) so ( 7 ) frequently ( 8 ) insists ( 9 ) on ( 10 ) workers ( 11 15 ) ' ( 14 ) rights ( 12 ) and ( 16 ) proper ( 19 ) social ( 21 ) protection ( 20 ) , ( 22 ) (...) where the number inside the clauses is the aligned word(s). And the phrase that we are looking for is the following one. los derechos de los trabajadores # workers ' rights which only could appear in the case the maximum length was 5. Phrase ranking Conditional probability P (f |e) Given the collected phrase pairs, we estimated the phrase translation probability distribution by relative frecuency. P (f |e) = N (f, e) N (e) (4) where N(f,e) means the number of times the phrase f is translated by e. If a phrase e has N > 1 possible translations, then each one contributes as 1/N [17] . Note that no smoothing is performed, which may cause an overestimation of the probability of rare phrases. This is specially harmful given a BP where the source part has a big frecuency of appearence but the target part appears rarely. For example, from our database we can extract the following BP : \"you # la que no\", where the English is the source language and the Spanish, the target language. Clearly, \"la que no\" is not a good translation of \"you\", so this phrase should have a low probability. However, from our aligned training database we obtain, P (f |e) = P (you|la que no) = 0.23 This BP is clearly overestimated due to sparseness. On the other, note that \"la que no\" cannot be considered an unusual trigram in Spanish. Hence, the language model does not penalise this target sequence either. So, the total probability (P (f |e)P (e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of the posterior [12] and lexical probabilities [1] [10] as additional features. Feature P (e|f ) In order to estimate the posterior phrase probability, we compute again the relative frequency but replacing the count of the target phrase by the count of the source phrase. P (e|f ) = N (f, e) N (f ) (5) where N'(f,e) means the number of times the phrase e is translated by f. If a phrase f has N > 1 possible translations, then each one contributes as 1/N. Adding this feature function we reduce the number of cases in which the overall probability is overestimated. This results in an important improvement in translation quality. IBM Model 1 We used IBM Model 1 to estimate the probability of a BP . As IBM Model 1 is a word translation and it gives the sum of all possible alignment probabilities, a lexical co-ocurrence effect is expected. This captures a sort of semantic coherence in translations. Therefore, the probability of a sentence pair is given by the following equation. P (f |e; M 1) = 1 (I + 1) J J j=1 I i=0 p(f j |e i ) (6) The p(f j |e i ) are the source-target IBM Model 1 word probabilities trained by GIZA++. Because the phrases are formed from the union of source-totarget and target-to-source alignments, there can be words that are not in the P (f j |e i ) table. In this case, the probability was taken to be 10 −40 . In addition, we have calculated the IBM −1 Model 1. P (e|f ; M 1) = 1 (J + 1) I I I=1 J j=0 p(e i |f j ) (7) Language Model The English language model plays an important role in the source channel model, see equation (2) , and also in its modification, see equation (3) . The English language model should give an idea of the sentence quality that is generated. As default language model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation (by using SRILM [16] ). The phrase penalty is a constant cost per produced phrase. Here, a negative weight, which means reducing the costs per phrase, results in a preference for adding phrases. Alternatively, by using a positive scaling factors, the system will favor less phrases. Word and Phrase Penalty Evaluation framework Corpus Statistics Experiments were performed to study the effect of our modifications in the phrases. The training material covers the transcriptions from April 1996 to September 2004. This material has been distributed by the European Parlament. In our experiments, we have used the distribution of RWTH of Aachen under the project of TC-STAR 1 . The test material was used in the first evaluation of the project in March 2005. In our case, we have used the development divided in two sets. This material corresponds to the transcriptions of the sessions from October the 21st to October the 28th. It has been distributed by ELDA 2 . Results are reported for Spanish-to-English translations. 1 http://www.tcstar.org/ 2 http://www.elda.org/ Experiments The decoder used for the presented translation system is reported in [2] . This decoder is called MARIE and it takes into account simultaneously all the 7 features functions described above. It implements a beam-search strategy. As evaluation criteria we use: the Word Error Rate (WER), the BLEU score [15] and the NIST score [3] . As follows we report the results for several experiments that show the performance of: the baseline, adding the posterior probability, IBM Model 1 and IBM1 −1 , and, finally, the modification of the phrases extraction. Optimisation. Significant improvements can be obtained by tuning the parameters of the features adequately. In the complet system we have 7 parameters to tune: the relatives frecuencies P (f |e) and P (e|f ), IBM Model 1 and its inverse, the word penalty, the phrase penalty and the weight of the language model. We applied the widely used algorithm SIMPLEX to optimise [9] . In Table 2 (line 5th), we see the final results. Baseline. We report the results of the baseline. We use the union alignment and we extract the BP of length 3. As default language model feature, we use the standard trigram with smoothing Kneser-Ney and interpolation. Also we tune the parameters (only two parameters) with the SIM-PLEX algorithm (see Table 2 ). Posterior probability. Table 2 shows the effect of using the posterior probability: P (e|f ). We use all the features but the P (e|f ) and we optimise the parameters. We see the results without this feature decrease around 1.1 points both in BLEU and WER (see line 2rd and 5th in Table 2 ). IBM Model 1. We do the same as in the paragraph above, we do not consider the IBM Model 1 and the IBM1 −1 . Under these conditions, the translation's quality decreases around 1.3 points both in BLEU and WER (see line 3th and 5th in Table 2 ). Modification of the Phrase Extraction. Finally, we made an experiment without modification of the phrases' length. We can see the comparison between: (1) the phrases of fixed maximum length of 3; and (2) including phrases with a maximum length of 5 which can not be generated by smaller phrases. We can see it in Table 2 (lines 4th and 5th). We observe that there is no much difference between the number of phrases, so this approach does not require more resources. However, we get slightly better scores. Shared Task This section explains the participation of \"Exploiting Parallel Texts for Statistical Machine Translation\". We used the EuroParl data provided for this shared task [4] . A word-to-word alignment was performed in both directions as explained in section 2. The phrase-based translation system which has been considered implements a total of 7 features (already explained in section 4). Notice that the language model has been trained with the training provided in the shared task. However, the optimization in the parameters has not been repeated, and we used the parameters obtained in the subsection above. We have obtained the results in the Table 3 . Conclusions We reported a new method to extract longer phrases without increasing the quantity of phrases (less than 0.5%). We also reported several features as P (e|f ) which in combination with the functions of the source-channel model provides significant improvement. Also, the feature IBM1 in combination with IBM1 −1 provides improved scores, too. Finally, we have optimized the parameters, and we provided the final results which have been presented in the Shared Task: Exploiting Parallel Acknowledgements The authors want to thank José B. Mariño, Adrià de Gispert, Josep M. Crego, Patrik Lambert and Rafael E. Banchs (members of the TALP Research Center) for their contribution to this work.",
         "12386445",
         "142a06b58eaa48bd4e53ade28dd175befe7a4124",
         "6",
         "https://aclanthology.org/W05-0827",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Ruiz Costa-juss{\\`a}, Marta  and\nFonollosa, Jos{\\'e} A. R.",
         "Improving Phrase-Based Statistical Translation by Modifying Phrase Extraction and Including Several Features",
         "149--154",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "ruiz-costa-jussa-fonollosa-2005-improving",
         null,
         null
        ],
        [
         "9",
         "2009.mtsummit-posters.17",
         "Most of the existing, easily available parallel texts to train a statistical machine translation system are from international organizations that use a particular jargon. In this paper, we consider the automatic adaptation of such a translation model to the news domain. The initial system was trained on more than 200M words of UN bitexts. We then explore large amounts of in-domain monolingual texts to modify the probability distribution of the phrase-table and to learn new task-specific phrase-pairs. This procedure achieved an improvement of 3.5 points BLEU on the test set in an Arabic/French statistical machine translation system. This result compares favorably with other large state-of-the-art systems for this language pair.",
         "Most of the existing, easily available parallel texts to train a statistical machine translation system are from international organizations that use a particular jargon. In this paper, we consider the automatic adaptation of such a translation model to the news domain. The initial system was trained on more than 200M words of UN bitexts. We then explore large amounts of in-domain monolingual texts to modify the probability distribution of the phrase-table and to learn new task-specific phrase-pairs. This procedure achieved an improvement of 3.5 points BLEU on the test set in an Arabic/French statistical machine translation system. This result compares favorably with other large state-of-the-art systems for this language pair. Introduction Adaptation of a statistical machine translation system (SMT) is a topic of increasing interest during the last years. Statistical (n-gram) language models are used in many domains and several approaches to adapt such models were proposed in the literature, for instance in the framework of automatic speech recognition. Many of these approaches were successfully used to adapt the language model of an SMT system. On the other hand, it seems more challenging to adapt the other components of an SMT system, namely the translation and reordering model. In this work we consider the adaptation of the translation model of a phrase-based SMT system. While rule-based machine translation rely on rules and linguistic resources built for that purpose, SMT systems can be developed without the need of any language-specific expertise and are only based on bilingual sentence-aligned data (\"bitexts\") and large monolingual texts. However, while monolingual data is usually available in large amounts and for a variety of tasks, bilingual texts are a sparse resource for most language pairs. Current parallel corpora mostly come from one domain (proceedings of the Canadian or European Parliament, or of the United Nations). This is problematic when SMT systems trained on such corpora are used for general translations, as the language jargon heavily used in these corpora is not appropriate for everyday life translations or translations in some other domain. This problem could be attacked by either searching for more in-domain training data, e.g. by exploring comparable corpora or the WEB, or by adapting the translation model to the task. In this work we consider translation model adaptation without using additional bilingual data. One can distinguish two types of translation model adaptation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007) , for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008) . This mixture approach has the advan-tage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005) . Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language (Snover et al., 2008) . Cross-lingual information retrieval was applied to find texts in the target language that are related to the domain of the source texts. However, it was difficult to get the alignments between the source and target phrases and an over-generalizing IBM1-style approach was used. Another direction of research is self-enhancing of the translation model. This was first proposed by (Ueffing, 2006) . The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing, 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008) . A somehow related approach was named lightlysupervised training (Schwenk, 2008) . In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. We could obtain small improvements in the BLEU score in a French/English translation system. Although this technique seems to be close to self enhancing as proposed by (Ueffing, 2006) , there is a conceptual difference. We do not use the test data to adapt the translation model, but large amounts of monolingual training data in the source language and we create a complete new model that can be applied to any test data without additional modification of the system. This kind of adapted system can be used in WEB service. In this paper, we use the same type of approach to adapt an generic Arabic/French translation system to the news domain. This task is interesting for several reasons: there is only a limited amount of in-domain bitexts available (about 1.2M words), but large amounts of out-of-domain bitexts (≈150M words of UN data) and both languages have a rich morphology. Usually, the Arabic source words are decomposed to detach pre-and suffixes. This helps to significantly reduce the size of the translation vocabulary and is reported to improve the translation quality. This morphological decomposition also results in many different and infrequent phrases which may lead to bad relative frequency estimates of the phrase translation probabilities. We are aiming in improving those estimates by using large amount of monolingual in-domain data. Finally, there seems to be a real need to translate between Arabic and French for the population in the Mediterranean area. This paper is organized as follows. In the next section we first describe the considered task and the available bilingual and monolingual resources. Section 3 describes the baseline SMT systems. The following section describe our adaptation technique. Results are summarized in section 5 and the paper concludes with a discussion and perspectives of this work. Task Description and resources In this paper, we consider the translation of news texts from Arabic into French. We are not aware of easily available aligned parallel corpora for this language pair. Fortunately, Arabic and French are both official languages of the United Nations. We crawled data from various sources of the United Nations over the period 1988-2008. This totals in almost 150M Arabic words. The Arabic and French texts were automatically sentence aligned. This amount of parallel texts is usually considered as more than sufficient to train an SMT system. Note however, that a particular jargon is used in the UN texts that is not appropriate for news-text translation. The French TRAMES 1 project considered the translation of Arabic Speeches to French. In the framework of this project, about 90h of Arabic TV and radio broadcast news were recorded, transcribed and translated into French. The sources are Orient, Qatar, BBC, Alarabiya, Aljazeera and Alalam. These high-quality domain specific bitexts of about 262k Arabic words were made available to us by the DGA. 2  Additional bilingual training data was obtained from the Project Syndicate WEB-site. 3 This data source is already used to build SMT systems to translate between European languages, in particular in the framework of the evaluations organized in junction with the workshops on statistical machine translation (Callison-Burch et al., 2007; Callison-Burch et al., 2008) . Some of the texts are also translated into Arabic. The scripts to access this WEBsite were kindly made available by P. Koehn. We crawled and aligned a total of 1.6M words. Note that these texts are not exactly broadcast news texts. The characteristics of the translation model training data is summarized in table 1. The number of words is given after tokenization. Arabic French Words Vocab Words Vocab DGA TRAMES 262k 30k 400k 18k News commentary 1.1M 67k 1.3M 41k UN 149M 712k 212M 420k The DGA also provided a test set that was created in the same way than the in-domain bitexts. Four high-quality reference translations are available. We randomly split this data into a development set for system tuning and an internal test set. The details of the development and test set are given in Table 2 . We are only aware of one other large Arabic/French news translation system, the one that was developed during the TRAMES project (Hasan and Ney, 2008) 3 Baseline system The baseline system is a standard phrase-based SMT system based on the the Moses SMT toolkit (Koehn et al., 2007) . It uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. It constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008) . 4 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. All the bitexts were concatenated. The parameters of Moses are tuned on the development data using the CMERT tool. Tokenization There is a large body of work in the literature showing that a morphological decomposition of the Arabic words can improve the word coverage and by these means the translation quality, see for instance (Habash and Sadat, 2006) . It is clear that such a decomposition is most helpful when the translation model training data is limited, but this is less obvious for tasks where several hundreds of millions of words of bitexts are available. Most of the published work is based on the freely available tools, like the Buckwalter transliterator and the MADA and TOKAN tools for morphological analysis from Columbia University. In this work, we compare two different tokenization of the Arabic source text: a full word mode and the morphological decomposition provided by the sentence analysis module of SYSTRAN's rulebased Arabic/English translation software. Sentence analysis represents a large share of the computations in a rule-based system. This process first applies decomposition rules coupled with a word dictionary. For words that are not known in the dictionary, the most likely decomposition is guessed. In general, all possible decompositions of each word are generated and then filtered in the context of the sentence. This step uses lexical knowledge and a global analysis of the sentences. This morphological decomposition drastically reduced the vocabulary of the Arabic bitexts: it was almost divided by two. The French texts were tokenized using the tools of the Moses suite. Punctuation and case were preserved. LM training In contrast to the translation model, many resources are available to train a LM optimized on French broadcast news texts. We used the French side of the bitexts, data form the European and the Canadian parliament, news-data crawled in the framework for the 2009 WMT evaluation, 5 other WEB data collected by ourselves and finally LDC's Gigaword corpus. Separate 4-gram back-off LMs were build on each data source with the SRI LM toolkit (Stolcke, 2002) coefficients with an EM procedure. The perplexities of these LMs are given in Table 3 . Translation model adaptation by lightly-supervised training The goal of this work is to adapt the translation model without using additional bilingual data. Instead we will use in-domain monolingual data in the source language. Usually it is relatively easy to obtain large collections of such data, in particular in the news domain as considered in this work. We use parts of the LDC Arabic Gigaword corpus, but more recent texts could be easily found on the Internet. These texts are translated by an initial, unadapted SMT system. We then need to filter the automatic translations in order to keep only the \"good ones\" for addition to the translation model training data. This selection could take advantage of word-based confidence scores (Ueffing, 2007) . We use the sentencelength normalized log-likelihoods of the decoder. These selected translations are used as additional indomain bitexts and the standard procedure to build a new SMT system is performed, i.e. word alignment with GIZA++, phrase extraction and tuning of the system parameters. Alternately, we could reuse the alignments established by the translation process since the Moses decoder is able to output the phrase and word alignments. This would speed up the process of creating the adapted SMT system since we skip the timeconsuming word alignment performed by GIZA++. It could also be that the decoder-induced word alignments are more appropriate than those performed by GIZA++. This was partially investigated in the framework of pivot translation to produce artificially bitexts in another language (Bertoldi et al., 2008) . Finally, instead of only using the 1-best translation we could also use the n-best list. LDC's Arabic and French Gigaword corpora are described in Table 4 . There is only one source that does exist in both languages: the AFP collection. It is likely that the Arabic and French texts partially cover the same facts, but they are usually not direct translations. 6 In fact, we were informed that journalists at AFP have the possibility to freely change the sentences when they report on a fact based on text already available in another language. Nevertheless, it can be expected that using these texts in the target language model helps the SMT system to produce good translations. This language model training data can be considered as some form of light supervision and we will therefore use the term lightly-supervised training (Schwenk, 2008) . This can be compared to the research in speech recognition where the same term was used for supervision that either comes from approximate transcriptions of the audio signal (closed captions) or related language model training data. Source Arabic French AFP 145M 570M APW -200M ASB 7M - HYT 175M - NHR 188M - UMH 1M - XIN 58M - In this work, we have processed the AFP Arabic text only, but the other texts (ASB, HYT, . . .) could be processed in the same manner. In fact it is an interesting question whether the availability of related or even \"comparable\" texts for the target language model is a necessary condition for our approach to work. All these issues will be explored in future research. Experimental Evaluation We first performed experiments using different amounts of bitexts to train the translation model and analyzed the benefits of the morphological decomposition of the Arabic words. The results are summarized in Table 5 . The TRAMES training corpus contains several lines with more than 100 words. These can't be processed by the GIZA++ tool and the were discarded. This was the case for about 6% of the data. In future research, we will try to split those lines into shorter sentences. As expected, the morphological decomposition of the Arabic words is very helpful when only a small amount of training data is available: using only the TRAMES and news-commentary in-domain data we observed an improvement of 4.6 BLEU points (first and second line in Table 5 ). Note that in this case we have actually less training data since the morphological decomposition leads to longer phrases out of which many are discarded by the 100 words limit of GIZA++. Somewhat surprisingly, the morphological decomposition still achieves a significant improvement of 1 BLEU point when more than 200M words of bitexts are available (last two lines in Table 5). Translation model adaptation The best system we were able to build with all human provided translations was used to translate all the AFP news texts from Arabic to French. The phrase table of whole AFP Gigaword corpus with such a large system is a computational challenge since it is impossible to charge the whole phrase table into memory. The Moses system supports two procedures to deal with this problem: filtering of the phrase table or binary on-disk phrase tables. Neither technique can be applied here. The phrase table is still too big in the first case and the binary representation of the whole phrase table occupies too much space on disk. This problem could be eventually approached with a distributed representation of the data structures. We finally adopted a combination of both techniques: the AFP corpus is split into parts with 50k lines (approximately 1.5M words), the phrase table is filtered for this data and then binarized. This made it possible to load the LM into memory and to have a process size of less then 20GB. The total translation time was more than 2700 hours. 7 These automatic translations were filtered according to the sentence-length normalized log-likelihood and the most likely ones were used as bitexts. Different amounts of data can be obtained by varying the threshold on the likelihood. The BLEU scores on the development and test data in function of the total size of the bitexts are shown in figure 1 . In these experiments we only use the in-domain humanprovided bitexts (TRAMES and news-commentary) -the UN data being replaced by the automatic translations. The best value on the development data was obtained for a total of 48M words of bitexts. The BLEU score on the development data is 45.44 and 43.68 on the test data respectively (see also table 6 ). This is an improvement of 3.5 BLEU points on both data sets. We analyzed the phrase table of the original system trained on all human provided data, including UN, and the one of the automatically adapted system. This is summarized in table 7. The original phrase table had 329M entries out of which 22.9M could be potentially applied on the test data. The phrase table of the adapted system on the other hand used only 700k out of a total of 8.6M phrases. It seems clear that the phrase table obtained by training on the UN data contains many entries that are not useful, or eventually even correspond to wrong translations. Surprisingly, the phrase table of the adapted system is not only substantially smaller, but even contains about 11% more entries (18029 with respect to 16263). All these entries correspond to new sequences of known words since lightlysupervised training cannot extend the source or target side vocabulary. We conjecture that this is particularly important with the morphological decomposition of the Arabic words. This decomposition reduces the vocabulary size of the source language, but produces on the other hand many possible se- Source: ‫ق.‬ ِ ‫ساب‬ ّ ‫ال‬ ‫ي‬ ّ ِ ‫راق‬ َ ِ ‫لع‬ َ ‫ا‬ ‫ِيس‬ ‫رئ‬ َ ‫ل‬ َ ‫ا‬ ّ ‫ضد‬ ِ ‫م‬ َ ‫ُه‬ ‫ت‬ ِ ‫حة‬ َ ِ ‫لئ‬ َ ِ ‫جيه‬ ِ ‫َو‬ ‫ت‬ ِ ‫ب‬ ‫ِيل‬ ‫َل‬ ‫ق‬ ُ ‫منذ‬ ُ ‫ت‬ َ ‫َأ‬ ‫َد‬ ‫ب‬ ‫ّة‬ ‫ِي‬ ‫راق‬ َ ِ ‫الع‬ ‫ة‬ ُ ‫م‬ َ َ ‫محك‬ َ ‫ل‬ َ ‫ا‬ Base: le tribunal irakien a commencé depuis peu par la direction du règlement des accusations contre l'ancien président irakien. Adapt: le tribunal irakien a commencé depuis peu une liste d'accusations contre l'ancien président irakien. Ref: La Cour irakienne a commencé à dresser la liste des inculpations de l'ancien président irakien. Source: ‫ِي‬ ‫ف‬ ‫ه‬ ّ ‫لل‬ َ ‫ا‬ ‫رام‬ َ ‫ِي‬ ‫ف‬ ً ‫شطا‬ ِ ‫َا‬ ‫ن‬ ً ‫يل‬ َ ‫ل‬ ‫ل‬ َ َ ‫َق‬ ‫ِعت‬ ‫ا‬ ‫ي‬ ّ ِ ‫ِيل‬ ‫رائ‬ َ ‫س‬ ِ ‫ال‬ ‫ش‬ َ ‫جي‬ َ ‫ال‬ ‫ن‬ ّ َ ‫أ‬ ‫ّة‬ ‫ِي‬ ‫ِيل‬ ‫رائ‬ َ ‫س‬ ِ ‫إ‬ ‫ة‬ ٌ ّ ‫ي‬ ِ ‫ر‬ َ ‫َسك‬ ‫ع‬ ‫ر‬ ُ ِ ‫صاد‬ َ ‫م‬ َ ‫َت‬ ‫َاد‬ ‫ف‬ َ ‫أ‬ ‫ن‬ َ ‫رو‬ ُ ‫ض‬ ّ ‫ح‬ َ ُ ‫ي‬ ‫ُوا‬ ‫ان‬ َ ‫ك‬ ‫رين‬ َ ‫خ‬ َ ‫آ‬ ‫ن‬ ِ ‫ي‬ َ ‫شط‬ ِ ‫َا‬ ‫ن‬ ‫ل‬ ُ ‫َا‬ ‫ِق‬ ‫اعت‬ ‫م‬ ّ َ ‫ت‬ ‫ما‬ َ َ ‫ك‬ ‫ّة‬ ‫ـي‬ ِ ‫َرب‬ ‫الغ‬ ِ ‫ّة‬ ‫ضف‬ ّ ‫ال‬ Base: De source militaire israélienne a indiqué que l'armée israélienne a arrêté dans la nuit militants à Ramallah en Cisjordanie ont été arrêtés autres militants qui ... Adapt: Selon des sources militaires israéliennes, l'armée israélienne a arrêté dans la nuit de militants à Ramallah, en Cisjordanie, a également été arrêté deux autres activistes qui ... Ref: Des sources militaires israéliennes ont indiqué que l'armée israélienne a arrêté de nuit un activiste à Ramallah en Cisjordanie, ainsi que deux autres activistes qui ... Source: ‫من.‬ َ َ ‫لي‬ َ ‫ا‬ ‫َة،‬ ‫حاف‬ َ ‫ص‬ ّ ‫ال‬ ‫ة‬ ُ َ ‫جول‬ َ ‫ي،‬ ِ ‫َار‬ ‫ُب‬ ‫الغ‬ ُ ‫مد‬ ّ ‫ح‬ َ ‫م‬ ُ Base: Mohammed du brouillard, le cycle de la presse, au Yémen. Adapt: Mohammed, une tournée de la presse le Yémen. Ref: Mohamed Al-Ghobari, tour de la presse, Yémen. Source:  . quences of tokens. It seems important to a include sequences of these tokens in the phrase table that appear in in-domain data. As a side effect, the smaller phrase-table of the adapted system also leads to a 40% faster translation. We compared the translations of the unadapted and adapted systems: the TER is about 30 in both directions, meaning that the outputs differ substantially. Some example translations are shown in figure 2. The adapted system clearly produces better output in these examples. There remain of course some errors in these sentences, but we argue that the quality is high enough for an human being to capture most of the meaning of the sentences. ‫راق.‬ َ ِ ‫الع‬ ‫ن‬ َ ‫م‬ ِ ‫َا‬ ‫ِه‬ ‫ُود‬ ‫جن‬ ُ ‫ب‬ ِ ‫سح‬ َ ‫ِي‬ ‫ف‬ ً ‫يضا‬ َ ‫أ‬ ‫لند‬ َ ‫َاي‬ ‫ت‬ ‫َت‬ ‫رع‬ َ ‫ش‬ َ ‫رى‬ َ ‫خ‬ ُ ‫أ‬ ٍ ‫ة‬ َ ‫جه‬ ِ ‫من‬ ِ Base: d' Conclusion Statistical machine translation is today used to rapidly create automatic translations systems for a variety of tasks. In principle, we only need aligned example translations and monolingual data in the target language. However, for many application domains and language pairs there are no appropriate in-domain parallel texts to train the translation model. On the other hand, large generic bitexts may be available. In this work we consider such a configuration: the translation of broadcast news texts from Arabic to French. We have a little more than 1M words of in-domain bitexts and about 150M words of generic bitexts from the United Nations. This system is automatically adapted to the news domain by using large amounts of monolingual texts, namely LDC's collection of Arabic AFP texts from 1994 to 2006. These texts were processed by the initial SMT system and the most reliable automatic translations were added to the bitexts and a new system was trained. By these means we achieved an improvement in the BLEU score of 3.5 points on the test set. This system actually uses less bitexts than the generic one since the generic UN bitexts are not used any more. An analysis of the created phrase table seems to indicate that the adaptation of the translation model leads to much smaller and more concise phrases. Our best system achieves a BLEU score of 43.68 on the test set which compares favorably with other large state-of-the-art systems for this language pair. The proposed algorithm is generic and could be applied to other language pairs and applications domains. We only need a good initial generic SMT system and in-domain monolingual texts in the source language.r It is interesting to compare our approach to self-learning as proposed in (Ueffing, 2006) Selflearning was applied to small amounts of test data only while we use several hundreds of million words of training data in the source language. We build a complete new phrase table instead of interpolating a small \"adapted phrase table\" with a generic one. Finally, self-learning was applied during the translation process and must be repeated for each new test data. This is computationally expensive and is difficult to use in on-line translation. The approach proposed in this paper applies translation model adaptation once and builds a new SMT system that can be then applied to any test data (ideally from the same domain). Several extensions of the proposed approach can be envisioned, namely improved confidence scores for filtering the most reliable translations, processing of n-best lists instead of using the most likely translation only, and reuse of the decoder-induced word alignments instead of rerunning GIZA++. We are currently working on these issues. Acknowledgments The Arabic/French corpus of broadcasts news as well as the corresponding test set were made available to us by the DGA. This work has been partially funded by the French Government under the project INSTAR (ANR JCJC06 143038) and the European Commission under the project EuromatrixPlus.",
         "28006881",
         "53d6c70d1a4871a8a8164f86d824313ef655a8b7",
         "41",
         "https://aclanthology.org/2009.mtsummit-posters.17",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Schwenk, Holger  and\nSenellart, Jean",
         "Translation Model Adaptation for an {A}rabic/{F}rench News Translation System by Lightly- Supervised Training",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "schwenk-senellart-2009-translation",
         null,
         null
        ],
        [
         "10",
         "W05-0828",
         "We motivate our contribution to the shared MT task as a first step towards an integrated architecture that combines advantages of statistical and knowledge-based approaches. Translations were generated using the Pharaoh decoder with tables derived from the provided alignments for all four languages, and for three of them using web-based and locally installed commercial systems. We then applied statistical and heuristic algorithms to select the most promising translation out of each set of candidates obtained from a source sentence. Results and possible refinements are discussed.",
         "We motivate our contribution to the shared MT task as a first step towards an integrated architecture that combines advantages of statistical and knowledge-based approaches. Translations were generated using the Pharaoh decoder with tables derived from the provided alignments for all four languages, and for three of them using web-based and locally installed commercial systems. We then applied statistical and heuristic algorithms to select the most promising translation out of each set of candidates obtained from a source sentence. Results and possible refinements are discussed. Motivation and Long-term Perspective \"The problem of robust, efficient and reliable speech-to-speech translation can only be cracked by the combined muscle of deep and shallow processing approaches.\" (Wahlster, 2001) Although this statement has been coined in the context of VerbMobil, aiming at translation for direct communication, it appears also realistic for many other translation scenarios, where demands on robustness, coverage, or adaptability on the input side and quality on the output side go beyond today's technological possibilities. The increasing availability of MT engines and the need for better quality has motivated considerable efforts to combine multiple engines into one \"super-engine\" that is hopefully better than any of its ingredients, an idea pionieered in (Frederking and Nirenburg, 1994) . So far, the larger group of related publications has focused on the task of selecting, from a set of translation candidates obtained from different engines, one translation that looks most promising (Tidhar and Küssner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004) . But also the more challenging problem of decomposing the candidates and re-assembling from the pieces a new sentence, hopefully better than any of the given inputs, has recently gained considerable attention (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005) . Although statistical MT approaches currently come out as winners in most comparative evaluations, it is clear that the achievable quality of methods relying purely on lookup of fixed phrases will be limited by the simple fact that for any given combination of topic, application scenario, language pair, and text style there will never be sufficient amounts of pre-existing translations to satisfy the needs of purely data-driven approaches. Rule-based approaches can exploit the effort that goes into single entries in their knowledge repositories in a broader way, as these entries can be unfolded, via rule applications, into large numbers of possible usages. However, this increased generality comes at significant costs for the acquisition of the required knowledge, which needs to be encoded by specialists in formalisms requiring extensive training to be used. In order to push the limits of today's MT technology, integrative approaches will have to be developed that combine the relative advantages of both paradigms and use them to compensate for their disadvantages. In particular, it should be possible to turn single instances of words and constructions found in training data into internal representations that allow them to be used in more general ways. In a first step towards the development of integrated solutions, we need to investigate the relative strengths and weaknesses of competing systems on the level of the target text, i.e. find out which sentences and which constructions are rendered well by which type of engine. In a second step, such an analysis will then make it possible to take the outcomes of various engines apart and re-assemble from the building blocks new translations that avoid errors made by the individual engines, i.e. to find integrated solutions that improve over the best of the candidates they have been built from. Once this can be done, the third and final step will involve feed back of corrections into the individual systems, such that differences between system behaviour can trigger (potentially after manual resolution of unclear cases) system updates and mutual learning. In the long term, one would hope to achieve a setup where a group of MT engines can converge to a committee that typically disagrees only in truly difficult cases. In such a committee, remaining dissent between the members would be a symptom of unresolved ambiguity, that would warrant the cost of manual intervention by the fact that the system as a whole can actually learn from the additional evidence. We expect this setup to be particularly effective when existing MT engines have to be ported to new application domains. Here, a rule-based engine would be able to profit from its more generic knowledge during the early stages of the transition and could teach unseen correspondences of known words and phrases to the SMT engine, whereas the SMT system would bring in its abilities to apply known phrase pairs in novel contexts and quickly learn new vocabulary from examples. Collecting Translation Candidates Setting up Statistical MT In the general picture laid out in the preceding section, statistical MT plays an important role for several reasons. On one hand, the construction of a relatively well-performing phrase-based SMT system from a given set of parallel corpora is no more overly difficult, especially if -as in the case in this shared task -word alignments and a decoder are provided. Furthermore, once the second task in our chain will have been surmounted, it will be relatively easy to feed back building blocks of improved translations into the phrase table, which constitutes the central resource of the SMT system Therefore, SMT facilitates experiments aiming at dynamic and interactive adaptation, the results of which should then also be applicable to MT engines that represent knowledge in a more condensed form. In order to collect material for testing these ideas, we constructed phrase tables for all four languages, following roughly the procedure given in (Koehn, 2004 ) but deviating in one detail related to the treatment of unaligned words at the beginning or end of the phrases 1 . We used the Pharaoh decoder as described on http://www.statmt.org/wpt05/mt-sharedtask/ after normalization of all tables to lower case. Using Commercial Engines As our main interest is in the integration of statistical and rule-based MT, we tried to collect results from \"conventional\" MT systems that had more or less uniform characteristics across the languages involved. We could not find MT engines supporting all four source languages, and therefore decided to drop Finnish for this part of the experiment. We sent the texts of the other three languages through several incarnations of Systran-based MT Web-services 2 and through an installation of Lernout & Hauspie Power Translator Pro, Version 6.43. 3 1 We used slightly more restrictive conditions that resulted in a 5.76% reduction of phrase table size 2 The results were incomplete and different, but sufficiently close to each other so that it did not seem worthwhile to explore the differences systematically. Instead we ranked the services according to errors in an informal comparison and took for each sentence the first available translation in this order. 3 After having collected or computed all translations, we observed that in the case of French, both systems were quite sensitive to the fact that the apostrophes were formatted as separate tokens in the source texts (l ' homme instead of l'homme). We therefore modified and retranslated the French texts, but did not explore possible effects of similar transformations in the other languages. Heuristic Selection Approach We implemented two different ways to select, out of a set of alternative translations of a given sentence, one that looks most promising. The first approach is purely heuristic and is limited to the case where more than two candidates are given. For each candidate, we collect a set of features, consisting of words and word n-grams (n ∈ {2, 3, 4}). Each of these features is weighted by the number of candidates it appears in, and the candidate with the largest feature weight per word is taken. This can be seen as the similarity of each of the candidate to a prototypical version composed as a weighted mixture of the collection, or as being remotely related to a sentence-specific language model derived from the candidates. The heuristic measure was used to select \"favorite\" from each group of competing translations obtained from the same source sentence, yielding a fourth set of translations for the sentences given in DE, FR, and ES. A particularity of the shared task is the fact that the source sentences of the development and test sets form a parallel corpus. Therefore, we can not only integrate multiple translations of the same source sentence into a hopefully better version, but we can merge the translations of corresponding parts from different source languages into a target form that combines their advantages. This approach, called triangulation in (Kay, 1997) , can be motivated by the fact that most cases of translation for dissemination involve multiple target languages; hence one can assume that, except for the very first of them, renderings in multiple languages exist and can be used as input to the next step 4 . See also (Och and Ney, 2001) for some related empirical evidence. In order to obtain a first impression of the potential of triangulation in the domain of parliament debates, we applied the selection heuristics to a set of four translations, one from Finnish, the other three the result of the selections mentioned above. Results and Discussion The BLEU scores (Papineni et al., 2002) 1 . These results show that in each group of translations for a given source language, the statistical engine came out best. Furthermore, our heuristic approach for the selection of the best among a small set of candidate translations did not result in an increase of the measured BLEU score, but typically gave a score that was only slightly better than the second best of the ingredients. This somewhat disappointing result can be explained in two ways. Apparently, the selection heuristic does not give effective estimates of translation quality for the candidates. Furthermore, the granularity on which the choices have to bee made is too coarse, i.e. the pieces for which the symbolic engines do produce better translations than the SMT engine are accompanied by too many bad choices so that the net effect is negative. Statistical Selection The other score we used was based on probabilities as computed by the trigram language model for English provided by the organizers of the task, in a representation compatible with the SRI LM toolkit (Stolcke, 2002) . However, a correct implementation for obtaining these estimates was not available in time, so the selections generated from the statistical language model could not be used for official submissions, but were generated and evaluated after the closing date. The results, also displayed in Table 1 , show that this approach can lead to slight improvements of the BLEU score, which however turn out not to be statistically sigificant in then sense of (Zhang et al., 2004) . Next Steps When we started the experiments reported here, the hope was to find relatively simple methods to select the best among a small set of candidate translations and to achieve significant improvements of a hybrid architecture over a purely statistical approach. Although we could indeed measure certain improvements, these are not yet big enough for a conclusive \"proof of concept\". We have started a refinement of our approach that can not only pick the best among translations of complete sentences, but also judge the quality of the building blocks from which the translations are composed. First informal results look very promising. Once we can replace single phrases that appear in one translation by better alternatives taken from a competing candidate, chances are good that a significant increase of the overall translation quality can be achieved. Acknowledgements This work has been funded by the Deutsche Forschungsgemeinschaft. We want to thank two anonymous reviewers for numerous pointers to relevant literature, Bogdan Sacaleanu for his help with the collection of translations from on-line MT engines, as well as the organizers of the shared task for making these interesting experiments possible.",
         "9766288",
         "cb55a2758d26d96dadcf301a11dadee9f703f3c0",
         "18",
         "https://aclanthology.org/W05-0828",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Eisele, Andreas",
         "First Steps towards Multi-Engine Machine Translation",
         "155--158",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "eisele-2005-first",
         null,
         null
        ],
        [
         "11",
         "W05-0829",
         "This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach.",
         "This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993) . Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training. In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. Translation Likelihood as a Statistical Test Given a bilingual corpus of language pair F (Foreign, source language) and E (English, target language), if we know the word alignment for each sentence pair we can calculate the co-occurrence frequency for each source/target word pair type C(f, e) and the marginal frequency C(f ) = e C(f, e) and C(e) = f C(f, e). We can apply various statistical tests (Manning and Schütze, 1999) to measure how likely is the association between f and e, in other words how likely they are mutual translations. In the following sections, we will use χ 2 statistics to measure the the mutual translation likelihood (Church and Hanks, 1990) . The Core of the Integrated Phrase Segmentation and Alignment The competitive linking algorithm (CLA) (Melamed, 1997 ) is a greedy word alignment algorithm. It was designed to overcome the problem of indirect associations using a simple heuristic: whenever several word tokens f i in one half of the bilingual corpus co-occur with a particular word token e in the other half of the corpus, the word that is most likely to be e's translation is the one for which the likelihood L(f, e) of translational equivalence is highest. The simplicity of this algorithm depends on a one-to-one alignment assumption. Each word translates to at most one other word. Thus when one pair {f, e} is \"linked\", neither f nor e can be aligned with any other words. This assumption renders CLA unusable in phrase level alignment. We propose an extension, the competitive grouping, as the core component in the ISA model. Competitive Grouping Algorithm (CGA) The key modification to the competitive linking algorithm is to make it less greedy. When a word pair is found to be the winner of the competition, we allow it to invite its neighbors to join the \"winner's club\" and group them together as an aligned phrase pair. The one-to-one assumption is thus discarded in CGA. In addition, we introduce the locality assumption for phrase alignment. Locality states that a source phrase of adjacent words can only be aligned to a target phrase composed of adjacent words. This is not true of most language pairs in cases such as the relative clause, passive tense, and prepositional clause, etc.; however this assumption renders the problem tractable. Here is a description of CGA: For a sentence pair {f , e}, represent the word pair statistics for each word pair {f, e} in a two dimensional matrix L I×J , where L(i, j) = χ 2 (f i , e j ) in our implementation. 1 1. Find i * and j * such that L(i * , j * ) is the highest. Create a seed phrase pair [i * , i * , j * , j * ] which is simply the word pair {f i * , e j * } itself. 2. Expand the current phrase pair [i start , i end , j start , j end ] to the neighboring territory to include adjacent source and target words in the phrase alignment group. There are 8 ways to group new words into the phrase pair. For example, one can expand to the north by including an additional source word f istart−1 to be aligned with all the target words in the current group; or one can expand to the northeast by including f istart−1 and e j end +1 (Figure 1 ). Two criteria have to be satisfied for each expansion: (a) If a new source word f i is to be grouped, max jstart≤j≤j end L(i , j) should be no smaller than max 1≤j≤J L(i , j). Since CGA is a greedy algorithm as described below, this is to guarantee that f i will not \"regret\" the decision of joining the phrase pair because it does not have other \"better\" target words to be aligned with. Similar constraint is applied if a new target word e j is to be grouped. (b) The highest value in the newly-expanded area needs to be \"similar\" to the seed value L(i * , j * ). Expand the current phrase pair to the largest extend possible as long as both criteria are satisfied. 3. The locality assumption means that the aligned phrase cannot be aligned again. Therefore, all the source and target words in the phrase pair are marked as \"invalid\" and will be skipped in the following steps. 4. If there is another valid pair {f i , e j }, then repeat from Step 1. Figure 2 and Figure 3 show a simple example of applying CGA on the sentence pair {je déclare reprise la session/i declare resumed the session}. Exploring all possible groupings The similarity criterion 2-(b) described previously is used to control the granularity of phrase pairs. In cases where the pairs {f 1 f 2 , e 1 e 2 }, {f 1 , e 1 } and {f 2 , e 2 } are all valid translations pairs, similarity is used to control whether we want to align {f 1 f 2 , e 1 e 2 } as one phrase pair or two shorter ones. The granularity of the phrase pairs is hard to optimize especially when the test data is unknown. On the one hand, we prefer long phrases since interaction among the words in the phrase, for example word sense, morphology and local reordering could be encapsulated. On the other hand, long phrase pairs are less likely to occur in the test data than the shorter ones and may lead to low coverage. To have both long and short phrases in the alignment, we apply a range of similarity thresholds for each of the expansion operations. By applying a low similarity threshold, the expanded phrase pairs tend to be large, while a higher similarity threshold results in shorter phrase pairs. As described above, CGA is a greedy algorithm and the expansion of the seed pair restricts the possible alignments for the rest of the sentence. Figure 4 shows an example as we explore all the possible grouping choices in a depth-first search. In the end, all unique phrase pairs along the path traveled are output as phrase translation candidates for the current sentence pair. Phrase translation probabilities Each aligned phrase pair { f , ẽ} is assigned a likelihood score L( f , ẽ), defined as: i max j log L(f i , e j ) + j max i log L(f i , e j ) | f | + |ẽ| where i ranges over all words in f and similarly j in ẽ. Given the collected phrase pairs and their likelihood, we estimate the phrase translation probability by their weighted frequency: P ( f |ẽ) = count( f , ẽ) • L( f , ẽ) f count( f , ẽ) • L( f , ẽ) No smoothing is applied to the probabilities. Learning co-occurrence information In most cases, word alignment information is not given and is treated as a hidden parameter in the training process. We initialize a word pair cooccurrence frequency by assuming uniform alignment for each sentence pair, i.e. for sentence pair (f , e) where f has I words and e has J words, each word pair {f, e} is considered to be aligned with frequency 1 I×J . These co-occurrence frequencies will be accumulated over the whole corpus to calculate the initial L(f, e). Then we iterate the ISA model: 1. Apply the competitive grouping algorithm to each sentence pair to find all possible phrase pairs. (French-English, Finnish-English, German-English and Spanish-English). Conclusion In this paper, we introduced the competitive grouping algorithm which is at the core of the ISA phrase alignment model. As an extension to the competitive linking algorithm which is used for word-to-word alignment, CGA overcomes the assumption of oneto-one mapping and makes it possible to align phrase 3 http://www.isi.edu/licensed-sw/pharaoh/ pairs. Despite its simplicity, the ISA model has achieved competitive translation results. We plan to release ISA toolkit 4 to the community in the near future.",
         "2455968",
         "633bf26c3315fcf80a6d51e1d7f8fa9ab2e72412",
         "11",
         "https://aclanthology.org/W05-0829",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Zhang, Ying  and\nVogel, Stephan",
         "Competitive Grouping in Integrated Phrase Segmentation and Alignment Model",
         "159--162",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "zhang-vogel-2005-competitive",
         null,
         null
        ],
        [
         "12",
         "W05-0831",
         "This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly improves translation quality.",
         "This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly improves translation quality. Introduction Reordering is of crucial importance for machine translation. Already (Knight et al., 1998) use full unweighted permutations on the level of source words in their early weighted finite-state transducer approach which implemented single-word based translation using conditional probabilities. In a refinement with additional phrase-based models, (Kumar et al., 2003) define a probability distribution over all possible permutations of source sentence phrases and prune the resulting automaton to reduce complexity. A second category of finite-state translation approaches uses joint instead of conditional probabilities. Many joint probability approaches originate in speech-to-speech translation as they are the natural choice in combination with speech recognition models. The automated transducer inference techniques OMEGA (Vilar, 2000) and GIATI (Casacuberta et al., 2004 ) work on phrase level, but ignore the reordering problem from the view of the model. Without reordering both in training and during search, sentences can only be translated properly into a language with similar word order. In (Bangalore et al., 2000) weighted reordering has been applied to target sentences since defining a permutation model on the source side is impractical in combination with speech recognition. In order to reduce the computational complexity, this approach considers only a set of plausible reorderings seen on training data. Most other phrase-based statistical approaches like the Alignment Template system of Bender et al. (2004) rely on (local) reorderings which are implicitly memorized with each pair of source and target phrases in training. Additional reorderings on phrase level are fully integrated into the decoding process, which increases the complexity of the system and makes it hard to modify. Zens et al. (2003) reviewed two types of reordering constraints for this type of translation systems. In our work we follow a phrase-based translation approach, applying source sentence reordering on word level. We compute a reordering graph ondemand and take it as input for monotonic translation. This approach is modular and allows easy introduction of different reordering constraints and probabilistic dependencies. We will show that it performs at least as well as the best statistical machine translation system at the IWSLT Evaluation. In the next section we briefly review the basic theory of our translation system based on weighted finite-state transducers (WFST). In Sec. 3 we introduce new methods for reordering and alignment monotonization in training. To compare different reordering constraints used in the translation search process we develop an on-demand computable framework for permutation models in Sec. 4. In the same section we also define and analyze unrestricted and restricted permutations with some of them being first published in this paper. We conclude the paper by presenting and discussing a rich set of experimental results. Machine Translation using WFSTs Let f J 1 and e I i be two sentences from a source and target language. Assume that we have word level alignments A of all sentence pairs from a bilingual training corpus. We denote with ẽJ 1 the segmentation of a target sentence e I 1 into J phrases such that f J 1 and ẽJ 1 can be aligned to form bilingual tuples (f j , ẽj ). If alignments are only functions of target words A : {1, . . . , I} → {1, . . . , J}, the bilingual tuples (f j , ẽj ) can be inferred with e. g. the GIATI method of (Casacuberta et al., 2004) , or with our novel monotonization technique (see Sec. 3). Each source word will be mapped to a target phrase of one or more words or an \"empty\" phrase ε. In particular, the source words which will remain non-aligned due to the alignment functionality restriction are paired with the empty phrase. We can then formulate the problem of finding the best translation êI 1 of a source sentence f J 1 : êI 1 = argmax e I 1 P r(f J 1 , e I 1 ) = argmax ẽJ 1 A∈A P r(f J 1 , ẽJ 1 , A) ∼ = argmax ẽJ 1 max A∈A P r(A) • P r(f J 1 , ẽJ 1 |A) ∼ = argmax ẽJ 1 max A∈A f j :j=1...J P r(f j , ẽj |f j−1 1 , ẽj−1 1 , A) = argmax ẽJ 1 max A∈A f j :j=1...J p(f j , ẽj |f j−1 j−m , ẽj−1 j−m , A) In other words: if we assume a uniform distribution for P r(A), the translation problem can be mapped to the problem of estimating an m-gram language model over a learned set of bilingual tuples (f j , ẽj ). Mapping the bilingual language model to a WFST T is canonical and it has been shown in (Kanthak et al., 2004 ) that the search problem can then be rewritten using finite-state terminology: êI 1 = project-output(best(f J 1 • T )) . This implementation of the problem as WFSTs may be used to efficiently solve the search problem in machine translation. Reordering in Training When the alignment function A is not monotonic, target language phrases ẽ can become very long. For example in a completely non-monotonic alignment all target words are paired with the last aligned source word, whereas all other source words form tuples with the empty phrase. Therefore, for language pairs with big differences in word order, probability estimates may be poor. This problem can be solved by reordering either source or target training sentences such that alignments become monotonic for all sentences. We suggest the following consistent source sentence reordering and alignment monotonization approach in which we compute optimal, minimum-cost alignments. First, we estimate a cost matrix C for each sentence pair (f J 1 , e I 1 ). The elements of this matrix c ij are the local costs of aligning a source word f j to a target word e i . Following (Matusov et al., 2004) , we compute these local costs by interpolating state occupation probabilities from the source-to-target and target-to-source training of the HMM and IBM-4 models as trained by the GIZA++ toolkit (Och et al., 2003) . For a given alignment A ⊆ I × J, we define the costs of this alignment c(A) as the sum of the local costs of all aligned word pairs: c(A) = (i,j)∈A c ij (1) The goal is to find an alignment with the minimum costs which fulfills certain constraints. Source Sentence Reordering To reorder a source sentence, we require the alignment to be a function of source words A 1 : {1, . . . , J} → {1, . . . , I}, easily computed from the cost matrix C as: A 1 (j) = argmin i c ij (2) We do not allow for non-aligned source words. A 1 naturally defines a new order of the source words f J 1 which we denote by f J 1 . By computing this permutation for each pair of sentences in training and applying it to each source sentence, we create a corpus of reordered sentences. Alignment Monotonization In order to create a \"sentence\" of bilingual tuples ( f J 1 , ẽJ 1 ) we required alignments between reordered source and target words to be a function of target words A 2 : {1, . . . , I} → {1, . . . , J}. This alignment can be computed in analogy to Eq. 2 as: A 2 (i) = argmin j cij (3) where cij are the elements of the new cost matrix C which corresponds to the reordered source sentence. We can optionally re-estimate this matrix by repeating EM training of state occupation probabilities with GIZA++ using the reordered source corpus and the original target corpus. Alternatively, we can get the cost matrix C by reordering the columns of the cost matrix C according to the permutation given by alignment A 1 . In alignment A 2 some target words that were previously unaligned in A 1 (like \"the\" in Fig. 1 ) may now still violate the alignment monotonicity. The monotonicity of this alignment can not be guaranteed for all words if re-estimation of the cost matrices had been performed using GIZA++. The general GIATI technique (Casacuberta et al., 2004) is applicable and can be used to monotonize the alignment A 2 . However, in our experiments the following method performs better. We make use of the cost matrix representation and compute a monotonic minimum-cost alignment with a dynamic programming algorithm similar to the Levenshtein string edit distance algorithm. As costs of each \"edit\" operation we consider the local alignment costs. The resulting alignment A 3 represents a minimum-cost monotonic \"path\" through the cost matrix. To make A 3 a function of target words we do not consider the source words non-aligned in A 2 and also forbid \"deletions\" (\"many-to-one\" source word alignments) in the DP search. An example of such consistent reordering and monotonization is given in Fig. 1 . Here, we reorder the German source sentence based on the initial alignment A 1 , then compute the function of target words A 2 , and monotonize this alignment to A 3 Reordering in Search When searching the best translation ẽJ 1 for a given source sentence f J 1 , we permute the source sentence as described in (Knight et al., 1998) : êI 1 = project-output(best(permute(f J 1 ) • T )) Permuting an input sequence of J symbols results in J! possible permutations and representing the permutations as a finite-state automaton requires at least 2 J states. Therefore, we opt for computing the permutation automaton on-demand while applying beam pruning in the search. Lazy Permutation Automata For on-demand computation of an automaton in the flavor described in (Kanthak et al., 2004) it is sufficient to specify a state description and an algorithm that calculates all outgoing arcs of a state from the state description. In our case, each state represents a permutation of a subset of the source words f J 1 , which are already translated. This can be described by a bit vector b J 1 (Zens et al., 2002) . Each bit of the state bit vector corresponds to an arc of the linear input automaton and is set to one if the arc has been used on any path from the initial to the current state. The bit vectors of two states connected by an arc differ only in a single bit. Note that bit vectors elegantly solve the problem of recombining paths in the automaton as states with the same bit vectors can be merged. As a result, a fully minimized permutation automaton has only a single initial and final state. Even with on-demand computation, complexity using full permutations is unmanagable for long sentences. We further reduce complexity by additionally constraining permutations. Refer to Figure 2 for visualizations of the permutation constraints which we describe in the following. IBM Constraints The IBM reordering constraints are well-known in the field of machine translation and were first described in (Berger et al., 1996) . The idea behind these constraints is to deviate from monotonic translation by postponing translations of a limited number of words. More specifically, at each state we can translate any of the first l yet uncovered word positions. The implementation using a bit vector is straightforward. For consistency, we associate window size with the parameter l for all constraints presented here. Inverse IBM Constraints The original IBM constraints are useful for a large number of language pairs where the ability to skip some words reflects the differences in word order between the two languages. For some other pairs, it is beneficial to translate some words at the end of the sentence first and to translate the rest of the sentence nearly monotonically. Following this idea we can define the inverse IBM constraints. Let j be the first uncovered position. We can choose any position for translation, unless l − 1 words on positions j > j have been translated. If this is the case we must translate the word in position j. The inverse IBM constraints can also be expressed by invIBM(x) = transpose(IBM(transpose(x))) . As the transpose operation can not be computed on-demand, our specialized implementation uses bit vectors b J 1 similar to the IBM constraints. Local Constraints For some language pairs, e.g. Italian -English, words are moved only a few words to the left or right. The IBM constraints provide too many alternative permutations to chose from as each word can be moved to the end of the sentence. A solution that allows only for local permutations and therefore has very low complexity is given by the following permutation rule: the next word for translation comes from the window of l positions 1 counting from the first yet uncovered position. Note, that the local constraints define a true subset of the permutations defined by the IBM constraints. ITG Constraints Another type of reordering can be obtained using Inversion Transduction Grammars (ITG) (Wu, 1997) . These constraints are inspired by bilingual bracketing. They proved to be quite useful for machine translation, e.g. see (Bender et al., 2004) . Here, we interpret the input sentence as a sequence of segments. In the beginning, each word is a segment of its own. Longer segments are constructed by recursively combining two adjacent segments. combination step, we either keep the two segments in monotonic order or invert the order. This process continues until only one segment for the whole sentence remains. The on-demand computation is implemented in spirit of Earley parsing. We can modify the original ITG constraints to further limit the number of reorderings by forbidding segment inversions which violate IBM constraints with a certain window size. Thus, the resulting reordering graph contains the intersection of the reorderings with IBM and the original ITG constraints. Weighted Permutations So far, we have discussed how to generate the permutation graphs under different constraints, but permutations were equally probable. Especially for the case of nearly monotonic translation it is make sense to restrict the degree of non-monotonicity that we allow when translating a sentence. We propose a simple approach which gives a higher probability to the monotone transitions and penalizes the nonmonotonic ones. A state description b J 1 , for which the following condition holds: M on(j) : b j = δ(j ≤ j) ∀ 1 ≤ j ≤ J represents the monotonic path up to the word f j . At each state we assign the probability α to that outgoing arc where the target state description fullfills M on(j +1) and distribute the remaining probability mass 1 − α uniformly among the remaining arcs. In case there is no such arc, all outgoing arcs get the same uniform probability. This weighting scheme clearly depends on the state description and the outgoing arcs only and can be computed on-demand. Experimental Results Corpus Statistics The translation experiments were carried out on the Basic Travel Expression Corpus (BTEC), a multilingual speech corpus which contains tourism-related sentences usually found in travel phrase books. We tested our system on the so called Chinese-to-English (CE) and Japanese-to-English (JE) Supplied Tasks, the corpora which were provided during the International Workshop on Spoken Language Translation (IWSLT 2004) (Akiba et al., 2004) . In addition, we performed experiments on the Italian-to-English (IE) task, for which a larger corpus was kindly provided to us by ITC/IRST. The corpus statistics for the three BTEC corpora are given in Tab. 1. The development corpus for the Italian-to-English translation had only one reference translation of each Italian sentence. A set of 506 source sentences and 16 reference translations is used as a development corpus for Chinese-to-English and Japanese-to-English and as a test corpus for Italianto-English tasks. The 500 sentence Chinese and Japanese test sets of the IWSLT 2004 evaluation campaign were translated and automatically scored against 16 reference translations after the end of the campaign using the IWSLT evaluation server. Evaluation Criteria For the automatic evaluation, we used the criteria from the IWSLT evaluation campaign (Akiba et al., 2004) , namely word error rate (WER), positionindependent word error rate (PER), and the BLEU and NIST scores (Papineni et al., 2002; Doddington, 2002) . The two scores measure accuracy, i. e. larger scores are better. The error rates and scores were computed with respect to multiple reference transla- tions, when they were available. To indicate this, we will label the error rate acronyms with an m. Both training and evaluation were performed using corpora and references in lowercase and without punctuation marks. Experiments We used reordering and alignment monotonization in training as described in Sec. 3. To estimate the matrices of local alignment costs for the sentence pairs in the training corpus we used the state occupation probabilities of GIZA++ IBM-4 model training and interpolated the probabilities of source-to-target and target-to-source training directions. After that we estimated a smoothed 4-gram language model on the level of bilingual tuples f j , ẽj and represented it as a finite-state transducer. When translating, we applied moderate beam pruning to the search automaton only when using reordering constraints with window sizes larger than 3. For very large window sizes we also varied the pruning thresholds depending on the length of the input sentence. Pruning allowed for fast translations and reasonable memory consumption without a significant negative impact on performance. In our first experiments, we tested the four reordering constraints with various window sizes. We aimed at improving the translation results on the development corpora and compared the results with two baselines: reordering only the source training sentences and translation of the unreordered test sentences; and the GIATI technique for creating bilingual tuples (f j , ẽj ) without reordering of the source sentences, neither in training nor during translation. Highly Non-Monotonic Translation (JE) Fig. 3 (left) shows word error rate on the Japanese-to-English task as a function of the window size for different reordering constraints. For each of the constraints, good results are achieved using a window size of 9 and larger. This can be attributed to the Japanese word order which is very different from English and often follows a subjectobject-verb structure. For small window sizes, ITG or IBM constraints are better suited for this task, for larger window sizes, inverse IBM constraints perform best. The local constraints perform worst and require very large window sizes to capture the main word order differences between Japanese and English. However, their computational complexity is low; for instance, a system with local constraints and window size of 9 is as fast (25 words per second) as the same system with IBM constraints and window size of 5. Using window sizes larger than 10 is computationally expensive and does not significantly improve the translation quality under any of the constraints. Tab. 2 presents the overall improvements in translation quality when using the best setting: inverse IBM constraints, window size 9. The baseline without reordering in training and testing failed completely for this task, producing empty translations for 37 % of the sentences 2 . Most of the original alignments in training were non-monotonic which resulted in mapping of almost all Japanese words to ε when using only the GIATI monotonization technique. Thus, the proposed reordering methods are of crucial importance for this task. Further improvements were obtained with a rescoring procedure. For rescoring, we produced a k-best list of translation hypotheses and used the word penalty and deletion model features, the IBM Model 1 lexicon score, and target language n-gram models of the order up to 9. The scaling factors for all features were optimized on the development corpus for the NIST score, as described in (Bender et al., 2004) . Moderately Non-Mon. Translation (CE) Word order in Chinese and English is usually similar. However, a few word reorderings over quite large distances may be necessary. This is especially true in case of questions, in which question words like \"where\" and \"when\" are placed at the end of a sentence in Chinese. The BTEC corpora contain many sentences with questions. The inverse IBM constraints are designed to perform this type of reordering (see Sec. 4.3). As shown in Fig. 3 , the system performs well under these con- straints already with relatively small window sizes. Increasing the window size beyond 4 for these constraints only marginally improves the translation error measures for both short (under 8 words) and long sentences. Thus, a suitable language-pair-specific choice of reordering constraints can avoid the huge computational complexity required for permutations of long sentences. Tab. 2 includes error measures for the best setup with inverse IBM constraints with window size of 7, as well as additional improvements obtained by a kbest list rescoring. The best settings for reordering constraints and model scaling factors on the development corpora were then used to produce translations of the IWSLT Japanese and Chinese test corpora. These translations were evaluated against multiple references which were unknown to the authors. Our system (denoted with WFST, see Tab. 3) produced results competitive with the results of the best system at the evaluation campaign (denoted with AT (Bender et al., 2004) ) and, according to some of the error measures, even outperformed this system. Almost Monotonic Translation (IE) The word order in the Italian language does not differ much from the English. Therefore, the absolute translation error rates are quite low and translating without reordering in training and search already results in a relatively good performance. This is reflected in Tab. 4. However, even for this language pair it is possible to improve translation quality by performing reordering both in training and during translation. The best performance on the development corpus is obtained when we constrain the reodering with relatively small window sizes of 3 to 4 and use either IBM or local reordering constraints. On the test corpus, as shown in Tab. 4, all error measures can be improved with these settings. Especially for languages with similar word order it is important to use weighted reorderings (Sec. 4.6) in order to prefer the original word order. Introduction of reordering weights for this task results in notable improvement of most error measures using either the IBM or local constraints. The optimal probability α for the unreordered path was determined on the development corpus as 0.5 for both of these constraints. The results on the test corpus using this setting are also given in Tab. 4. Conclusion In this paper, we described a reordering framework which performs source sentence reordering on word level. We suggested to use optimal alignment functions for monotonization and improvement of translation model training. This allowed us to translate monotonically taking a reordering graph as input. We then described known and novel reordering constraints and their efficient finite-state implementations in which the reordering graph is computed ondemand. We also utilized weighted permutations. We showed that our monotonic phrase-based translation approach effectively makes use of the reordering framework to produce quality translations even from languages with significantly different word order. On the Japanese-to-English and Chinese-to-English IWSLT tasks, our system performed at least as well as the best machine translation system. Acknowledgement This work was partially funded by the Deutsche Forschungsgemeinschaft (DFG) under the project \"Statistische Textübersetzung\" (Ne572/5) and by the European Union under the integrated project TC-STAR -Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).",
         "1854610",
         "0cfe82145332522ed6665bd039f4fd089ae24141",
         "93",
         "https://aclanthology.org/W05-0831",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Kanthak, Stephan  and\nVilar, David  and\nMatusov, Evgeny  and\nZens, Richard  and\nNey, Hermann",
         "Novel Reordering Approaches in Phrase-Based Statistical Machine Translation",
         "167--174",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "kanthak-etal-2005-novel",
         null,
         null
        ],
        [
         "13",
         "W05-0830",
         "Part-of-Speech patterns extracted from parallel corpora have been used to enhance a translation resource for statistical phrase-based machine translation.",
         "Part-of-Speech patterns extracted from parallel corpora have been used to enhance a translation resource for statistical phrase-based machine translation. Introduction The use of structural and syntactic information in language processing implementations in recent years has been producing contradictory results. Whereas language generation has benefited from syntax [Wu, 1997; Alshawi et al., 2000] , the performance of statistical phrase-based machine translation when relying solely on syntactic phrases has been reported to be poor [Koehn et al., 2003] . We carry out a set of experiments to explore whether heuristic learning of part-of-speech patterns from a parallel corpus can be used to enhance phrase-based translation resources. System The resources used for our experiments are as follows. The statistical machine translation GIZA++ toolkit was used to generate a bilingual translation table from the French-English parallel and sentence-aligned Europarl corpus. Additionally, a phrase table generated from the Europarl French-English corpus, and a training test set of 2000 French and English sentences that were made available on the webpage of the ACL 2005 work-shop 1 were also used. Syntactic tagging was realized by the TreeTagger, which is a probabilistic part-of-speech tagger and lemmatizer. The decoder used to produce machine translations was Pharaoh, version 1.2.3. We used GIZA++ to generate a translation table from the parallel corpus. The table produced consisted of individual words and phrases, followed by their corresponding translation and a unique probability value. Specifically, every line of the said table consisted of a French entry (in the form of one or more tokens), followed by an English entry (in the form of one or more tokens), followed by P(f|e), which is the probability P of translation to the French entry f given the English entry e. We added the GIZA++-generated table to the phrasebased translation table downloaded from the workshop webpage. During this merging of translation tables, no word or phrase was omitted, replaced or altered. We chose to combine the two aforementioned translation tables in order to achieve better coverage. We called the resulting merged translation table lexical phrase table. In order to utilize the syntactic information stemming from our resources, we used the Tree-Tagger to tag both the parallel corpus and the lexical phrase table. The probability values included in the lexical phrase table were not tagged. The TreeTagger uses a slightly modified version of the Penn Treebank tagset, different for each language. In order to achieve tag-uniformity, we performed the following dual tag-smoothing operation. Firstly, we changed the French tags into their English equivalents, i.e. NOM (noun -French) became NN (noun -English). Secondly, we simplified the tags, so that they reflected nothing more than general part-of-speech information. For example, tags denoting predicate-argument structures, whmovement, passive voice, inflectional variation, and so on, were simplified. For example, NNS (noun -plural) became NN (noun). Once our resources were uniformly tagged, we used them to extract part-of-speech correspondences between the two languages. Specifically, we extracted a sentence-aligned parallel corpus of French and English part-of-speech patterns from the tagged Europarl parallel corpus. We called this corpus of parallel and corresponding part-ofspeech patterns pos-corpus. The format of the poscorpus remained identical to the format of the original parallel corpus, with the sole difference that individual words were replaced by their corresponding part-of-speech tag. Similarly, we extracted a translation table of part-of-speech patterns from the tagged lexical phrase table. We called this part-of-speech translation table pos-table. The pos-table had exactly the same format as the lexical phrase table, with the unique difference that individual words were replaced by their corresponding part-of-speech tag. The translation probability values included in the lexical phrase table were copied onto the pos-table intact. Each of the part-of-speech patterns contained in the pos-corpus was matched against the part-ofspeech patterns contained in the pos-table. Matching was realized similarly to conventional left-toright string matching operations. Matching was considered to be successful not simply when a part-of-speech pattern was found to be contained in, or part of a longer pattern, but when patterns were found to be absolutely identical. When a perfect match was found, the translation probability value of the specific pattern in the pos-table was increased to the maximum value of 1. If the score were already 1, it remained unchanged. When there were no matches, values remained unchanged. We chose to match identical part-ofspeech patterns, and not to accept partial pattern matches, because the latter would require a revision of our probability recomputation method. This point is discussed in section 3 of this paper. Once all matching was complete, the newly enhanced pos-table, which now contained translation probability scores reflecting the syntactic features of the relevant languages, was used to update the original lexical phrase table. This update consisted in matching each and every part-of-speech pattern with its original lexical phrase, and replacing the initial translation probability score with the values contained in the pos-table. The identification of the original lexical phrases that generated each and every part-of-speech pattern was facilitated by the use of pattern-identifiers (pos-ids) and phraseidentifiers (phrase-ids), which were introduced at a very early stage in the process for that purpose. The resulting translation phrase table contained exactly the same entries as the lexical phrase table, but had different probability scores assigned to some of these entries, in line with the parallel partof-speech co-occurrences and correspondences found in the Europarl corpus. We called this table enhanced phrase table. Table 1 illustrates the process described above with the example of a phrase, the part-of-speech analysis of which has been used to increase its original translation probability value from 0.333333 to 1. 1 : Extracting and matching a part-ofspeech pattern to increase translation probability. We used the Pharaoh decoder firstly with our lexical phrase table, and secondly with our enhanced phrase table in order to generate statistical machine translations of source and target language variations of the French and English training test set. We measured performance using the BLEU score [Papineri et al., 2001] , which estimates the accuracy of translation output with respect to a reference translation. For both source-target language combinations, the use of the lexical phrase table received a slightly lower score than the score achieved when using the enhanced phrase table. The difference between these two approaches is not significant (p-value > 0.05). The results of our experiments are displayed in Table 2 Discussion The motivation behind this investigation has been to test whether syntactic or structural language aspects can be reflected or represented in the resources used in statistical phrase-based machine translation. We adopted a line of investigation that concentrates on the correspondence of part-of-speech patterns between French and English. We measured the usability of syntactic structures for statistical phrase-based machine translation by comparing translation performance when a standard phrase table was used, and when a syntactically enhanced phrase table was used. Both approaches scored very similarly. This similarity in the performance is justified by the following three factors. Firstly, the difference between the two translation resources, namely the lexical phrase table and the enhanced phrase table, does not relate to their entries, and thus their coverage, but to a simple alteration of the translation probability values of some of their entries. The coverage of these resources is exactly identical. Secondly, a closer examination of the translation probability value alterations that took place in order to reflect part-of-speech correspondences reveals that the proportion of the entries of the phrase table that were matched syntactically to phrases from the parallel corpus, and thus underwent a modification in their translation probability score, was very low (less than 1%). The reason behind this is the fact that the part-of-speech patterns produced by the parallel corpus were long strings in their vast majority, while the part-ofspeech patterns found in the phrase table were significantly shorter strings. The inclusion of phrases longer than three words in translation resources has been avoided, as it has been shown not to have a strong impact on translation performance [Koehn et al., 2003] . Thirdly, the above described translation probability value modifications were not parameterized, but consisted in a straightforward increase of the translation probability to its maximum value. It remains to be seen how these probability value alterations can be expanded to a type of probability value 'reweighing', in line with specific parameters, such as the size of the resources involved, the frequency of part-of-speech patterns in the resources, the length of part-of-speech patterns, as well as the syntactic classification of the members of part-of-speech patterns. If one is to compare the impact that such parameters have had upon the performance of automatic information summarisation [Mani, 2001] and retrieval technology [Belew, 2000] , it may be worth experimenting with such parameter tuning when refining machine translation resources. A note should be made to the choice of tagger for our experiments. A possible risk when attempting any syntactic examination of a large set of data may stem from the overriding role that syntax often assumes over semantics. Statistical phrasebased machine translation has been faced with instances of this phenomenon, often disguised as linguistic idiosyncrasies. This phenomenon accounts for such instances as when nouns appear in pronominal positions, or as adverbial modifiers. On these occasions, and in order for the syntactic examination to be precise, words would have to be defined on the basis of their syntactic distribution rather than their semantic function. The TreeTagger abides by this convention, which is one of the main reasons why we chose it over a plethora of other freely available taggers, the remaining reasons being its high speed and low error rate. In addition, it should be clarified that there is no statistical, linguistic, or other reason why we chose to adopt the English version of the Penn TreeBank tagset over the French, as they are both equally conclusive and transparent. The overall driving force behind our investigation has been to test whether part-of-speech structures can be of assistance to the enhancement of translation resources for statistical phrase-based machine translation. We view our use of part-ofspeech patterns as a natural extension to the introduction of structural elements to statistical machine translation by Wang [1998] and Och et al. [1999] . Our empirical results suggest that the use of partof-speech pattern correspondences to enhance existing translation resources does not damage machine translation performance. What remains to be investigated is how this approach can be optimized, and how it would respond to known statistical machine translation issues, such as mapping nested structures, or the handling of 'unorthodox' language pairs, i.e. agglutinative-fusion languages. Conclusion Syntactic and structural language information contained in a bilingual parallel corpus has been extracted and used to refine the translation probability values of a translation phrase table, using simple heuristics. The usability of the said translation table in statistical phrase-based machine translation has been tested in the shared task of the second track of the ACL 2005 Workshop on Building and Using Parallel Corpora. Findings suggest that using part-of-speech information to alter translation probabilities has had no significant effect upon translation performance. Further investigation is required to reveal how our approach can be optimized in order to produce significant performance improvement.",
         "14118934",
         "f0025f479e9feba22c88d17cad8c06fd47ba9a2b",
         "7",
         "https://aclanthology.org/W05-0830",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Lioma, Christina  and\nOunis, Iadh",
         "Deploying Part-of-Speech Patterns to Enhance Statistical Phrase-Based Machine Translation Resources",
         "163--166",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "lioma-ounis-2005-deploying",
         null,
         null
        ],
        [
         "14",
         "2009.mtsummit-posters.21",
         "The contribution discusses variants of architectures of hybrid MT systems. The three main types of architectures are: coupling of systems (serial or parallel), architecture adaptations (integrating novel components into SMT or RMT architectures, either by pre/post-editing, or by system core modifications), and genuine hybrid systems, combining components of different paradigms. The interest is to investigate which resources are required for which types of systems, and to which extent the proposals contribute to an overall increase in MT quality.",
         "The contribution discusses variants of architectures of hybrid MT systems. The three main types of architectures are: coupling of systems (serial or parallel), architecture adaptations (integrating novel components into SMT or RMT architectures, either by pre/post-editing, or by system core modifications), and genuine hybrid systems, combining components of different paradigms. The interest is to investigate which resources are required for which types of systems, and to which extent the proposals contribute to an overall increase in MT quality. Introduction In recent years, significant research in Machine Translation has been carried out, mainly in the area of data-driven MT (example-based and statistical (SMT)), as opposed to knowledge-driven approaches (rule-based (RMT), knowledge-based). Recent evaluations (Callison-Burch et al. 2009 ) show that  both types of systems reach comparable translation quality, but:  the level of output acceptance (in terms of understandability) in the best language directions is at about 50%. So the state of the art in MT is far from acceptability by human readers, which limits the success of the MT technology significantly. Error analysis (Chen et al., 2007 , Thurmair 2005) shows that the errors made by the different system types are complementary.  RMT systems have weaknesses in lexical selection in transfer, and lack robustness in case of analysis failures sentences. However they translate more accurately by trying to represent every piece of the input.  SMT systems are more robust and always produce output. They read more fluent, due to the use of Language Models, and are better in lexical selection. However, they have difficulties to cope with phenomena which require linguistic knowledge, like morphology, syntactic functions, and word order. Also, they lose adequacy due to missing or spurious translations (Vilar et al. 2006 ). Systems which try to profit from the respective other approach, and avoid mistakes for which solutions already exist (albeit in another MT paradigm) must therefore be hybrid solutions, combining knowledge-driven and data-driven elements. The purpose of this paper is to discuss different architectures of hybrid systems which have been proposed recently. The interest is to discuss the way how the hybrid systems overcome the restrictions of their respective paradigms, how they contribute to improve MT quality (in terms of fluency and adequacy), which requirements they have for language resources and performance, and how they can be adapted to new domains. Chapter 2 will discuss systems which couple different systems (RMT and/or) SMT, either in a serial (PSE) or in a parallel way; the systems themselves are not modified. Chapter 3 considers systems which use either the SMT or the RMT paradigm as basic architecture, and extend it by either knowledge-driven or data-driven components. Chapter 4 presents approaches which have hybrid architectures, and combine components of RMT and SMT systems into novel architectures. Coupling Coupling means that two or more existing systems are used to produce improved MT output. Coupling can either be done in a serial way, the most researched approach being statistical post-editing (SPE) of a rule-based system. Or it can be done in a parallel way, whereby the best translation is selected/produced from the output of several systems. First systems using small domains (Simard et al. 2007 , data from Canadian Job Bank) showed that even with relatively small training data of several thousand sentences, significant improvements of the MT output can be achieved, and a RMT+SPE component outperforms pure SMT systems in cases where only limited data are available. Serial Coupling Experiments have continued since then, on a broader data base, resulting in the following picture: a. Combinations of RMT+SPE systems are highly competitive in MT quality (cf. Schwenk et al., 2009) . The output tends to be grammatical, and the main effect of the combination is an increase in lexical selection quality (Dugast et al. 2007) , one of the weak points of pure RMT systems. b. However, care must be taken to avoid the introduction of errors by the SMT postprocessor. Such errors are: the syntactic structure of the output can be confused by the PSE component (Ehara 2007) ; accuracy drops as some parts of the translation are omitted, and special care needs to be taken to keep e.g. Named Entities in the output (Dugast et al. 2009) . c. To avoid such deteriorations, (Federmann et al. 2009 ) use a RMT system's syntactic structure, and only try local alternatives (using POS information). This helps for the lexical selection problem of the RMT systems, but less for the parse-failure problem. PSE type systems require bilingual training data, to be able to align RMT output to good output. Parallel Coupling This coupling employs several MT systems in parallel, and uses some mechanism to select or produce the best output from the result set. Two main paradigms are followed in these approaches: The first approach identifies the best translations from a list of n-best translations (Hildebrand/Vogel 2008) . They search for the best n-grams in all output hypotheses available, and then select the best hypothesis from the candidate list. They report an improvement of 2-3 BLEU compared to the best single system, as the resulting text can integrate sentences from different MT system outputs. The second approach does not work on whole sentences but on smaller segments (phrases, words). It uses confusion networks, and generates an output sentence on the basis of the available MT outputs. In its first variant, a skeleton is selected as a basis, and for each position of the skeleton the best translation alternative is identified and composed to the overall output sentence. Skeletons can be selected on sentence level (cf. Rosti et al. 2007) but also on phrase level (Heafield et al. 2009) ; the choice of the best skeleton is critical as it determines the structure / word order of the target sentence. Although most MT output stems from SMT systems, RMT output seems to add interesting hypotheses (Leusch et al. 2009) , and is sometimes used itself as the skeleton (Chen et al. 2009) . To overcome the risk of skeleton selection, techniques have been applied to build confusion network such as to let every hypothesis be the skeleton, and calculate the overall best solution; this has been done in the context of consensus translation (Matusov et al., 2006) . The results of parallel coupling seem to improve BLEU by 2-3 points; however, the 2009 MT workshop results seem to indicate that system combinations can perform as well as the best individual systems but not significantly better (Callison-Burch et al. 2009) . On top, a parallel system approach seems to be difficult to be used in practical applications; mainly for reasons of computational resources, and availability of MT systems. In praxi, at most two systems would be able to run in parallel; and the reduced number of output candidates would lead to a loss in efficiency for the decision process. Architecture Extensions While coupling means that the architecture of the participating systems is not changed, by extension we mean that the system architecture basically follows the RMT or SMT paradigm but is modified by including resources of the respective other approach. Modifications can occur as pre-editing (i.e. the system data are pre-processed), or core modification (e.g. phrase tables are extended, dictionaries are enlarged etc. by the respective other approach). RMT Extensions Approaches to improve rule-based systems with data-driven procedures focus on two problems:  Pre-editing is tried, both on the dictionary side, by running Term-Extraction tools, and on the grammar side, by automatically extracting grammar rules from corpora.  Modification of the system core is attempted, both by adding probability information to the analysis / parsing process, and by manipulating the transfer selection process. Pre-Editing Pre-Editing refers to the preparation of the language resources for RMT. Main language resources, dictionaries and grammar rules, can be set up using data-driven technology. Learning of dictionary entries Pre-Editing in rule-based systems means to apply data-driven techniques for terminology extraction from corpora, either on a monolingual basis (to find missing entries in the system's dictionaries), or from bilingual corpora, to find translation candidates, and to load them into the system dictionary. Such approaches are already in use in RMT systems. The challenges are:  recognition of multiword terms: Most of the semantically meaningful words are multiword terms (like ‚nuclear power plant'), having an internal linguistic structure.  linguistic annotation of the recognised terms. Terms must be brought into correct citation form (i.e. lemmatised), and annotated with (POS etc.) Approaches are described in (Dugast et al. 2009 , Eisele et al., 2008) . Results reported show that the MT quality improves moderately, depending on the amount of reductions of the out-of-vocabulary words, which in turn depends on the size and coverage of the already existing dictionary. The approach helps to fill dictionary gaps, and to adapt to new domains. However, in MT systems with already large dictionaries 1 the problem of lexical selection aggravates, as the amount of translations between which to select increases. This problem turns out to be much more difficult to solve than the problem of dictionary gaps. Learning of rules in RMT Research on learning grammar rules by data-driven techniques does not seem to have improved MT output quality significantly. The challenge for learning grammar rules seems to be that very many rule candidates are identified, even for small corpora, and that it is difficult to select the lowfrequent ‚good' rules from noise produced by the extraction technique. Existing RMT already use large grammars covering a lot of specific linguistic phenomena. As with dictionaries, the main problem is less that some structures are not covered but much more that the grammar rules interact and lead to problems of combinatorics and unexpected side-effects which require massive pruning and often led to parse failures. RMT core system modifications Modifications of the system core of RMT systems have been tried in several respects. The option to use probabilistic information in parsing has already been implemented in several existing RMT systems. Transfer Current hybrid approaches focus more on translation selection in the transfer phase, which is one of the weaknesses of RMT systems, esp. if dictionaries grow. Traditional approaches to RMT transfer selection rely on two techniques:  Assignment of subject area codes to translations; if a text belongs to a given subject area (which can be automatically detected, cf. Thurmair 2006), the respective translation is activated. However, even in specific domains, general readings of the terms in question are also found, so that this method is not reliable.  Tests and actions on certain contextual / structural properties (like: presence of direct object, certain prepositions, passive voice etc.), which trigger a specific translation. However, often such conditions cannot be reliably stated for lexical selection, esp. if the number of alternative translation grows; in addition, such tests rely on correct parses of the input sentence which cannot be guaranteed. Therefore, additional and robust means for lexical selection need to be developed. An obvious means is to use the more frequently used translation of a given term as default. But this technique is not sensitive to the specific context in which a term must be translated, and mostly returns the default. A second option is to use contextual disambiguation in the lexical selection process. Relevant clusters of (source language) contextual terms for a given candidate translation are built at training time from a corpus; at runtime these contexts are matched against the context of the text to be translated, and the best translation is selected. This technique, (cf. Thurmair 2006) , requires broadening the analysis scope of the system (from sentence-based to paragraph-based contexts); it achieves very good disambiguation results for the terms it was built for. Improvements of accuracy are also reported by (Kim et al., 2002) ; they use a smaller contextual window and follow a (Probabilistic) Latent Semantic Analysis approach. As a result, core modifications in RMT can improve the transfer selection process significantly; however they are less successful in case of robustness / parse failures. SMT Extensions Like RMT systems, SMT systems have also been extended to improve translation quality. Again,  Pre-editing is tried to prepare the data; the most important steps are morphology, POSinformation / syntactic information, and word reordering.  System core modifications are tried as well, by adding RMT information to the phrase tables, and by using factored translation. Pre-editing Morphology: Morphology has been researched rather extensively, mainly in languages with rich morphological schemas. Lemmatisation and POS tagging was used both on the source side (e.g. de Gispert et al., 2006) and on the target side ( Vandeghinste et al. 2006 ); the aim is to reduce data sparseness using lemma-based language models instead of textform-based ones. It seems to improve results for smaller corpora. Also, it seems that both textform and lemma based analysis should be done, as surface information has also shown to be beneficial (Koehn/Hoang 2007) . Factored translation (cf. below) is able to work on both levels simultaneously. Another research area in morphology is compounding (of English) / decompounding (of German words), to parallelise alignment (Stymne et al. 2008 , Popović et al. 2006) . Moreover, in languages with agglutinative behaviour, like Turkish (Hakkani-Tür et al. 2004), Hungarian or Arabic (Habash 2007) , preprocessing is required to split complex word strings (including pronouns, case markers etc.) into meaningful parts to be able to align them. Syntax: Syntactic preprocessing id tried e.g. in (Hannemann et al. 2009 ); the idea is to parse source and target side of a corpus, and only let syntactically well-formed phrases enter the phrase table. Both corpora are parsed, matching subtrees (mainly on NP level) are identified and aligned in the phrase table. The parsed phrases are still a minority on the phrase table but can help improving the MT output, in particular for local reorderings. Reordering: Reordering is a major challenge for SMT systems, not just because languages have different word and constituent order (SVO vs. SOV etc.) but also because the constituent order is meaning-bearing (e.g. case marking in English). While standard phrase-based models can handle local reorderings (e.g. noun-adjective position) to some extent, longer distance reordering requires different means. Proposals have been made to extend the input word sequence into a lattice containing different reorderings of the input words (based e.g. on POS information). Distortion rules can be set up manually or automatically, for contiguous and discontinuous POS sequences (Niehues/Kolss 2009), by matching them on source and target side of the training corpus. The input lattice contains the re-spective distorted strings, with weights on the probability of the distortion. Al alternative approach is proposed e.g. in (Bangalore et al. 2007) ; they do not use position at all, and try a global alignment in a kind of sentencebased bag-of-words strategy. In decoding, they create all possible permutations allowed by the Language Model (in a given window). However, apart from practical problems (window size), as all source language information is missing, results are not too promising; in addition, multiple occurrences of words in the target (‚the') need to be handled. (Birch et al. 2009 ) even claim that reordering problems determine the selection of the translation models: Long term reordering is better handled by hierarchical models (Hiero) while for short and medium reorderings, phrase-based models show better results. This remains to be researched. SMT core system modifications Three approaches can be found to incorporate RMT resources into an SMT architecture: Extension of the Phrase Table, rule-based control of the Language-Model-based generation, and factored translation. Importing RMT resources into the phrase table It was proposed (e.g. by Eisele et al. 2008) to run RMT systems in addition to SMT systems, and enrich the SMT phrase tables by terms and phrases produced by RMT systems. This approach makes use of the knowledge coded in the bilingual dictionaries of the RMT systems. Results show that the coverage of the system can indeed be increased, esp. in cases of texts from different domains; however, as the SMT decoder runs last, the effect is that the output can be less grammatical than the one of the original RMT. The proposal reacts on the data sparseness problem of the SMT training; it does not react on the output grammaticality problem. Improving decoding using target grammars First rather dramatic improvements had been reported by (Charniak et al. 2003) where the number of grammatical translations was increased in tests by 45%. Other results were less encouraging (e.g. Och et al., 2003) but this may have been due to the selection of an problematic evaluation metric. In recent times, using syntax in decoding is a major topic of research. Several proposals exist how to learn grammar and transfer rules from bilingual corpora. (Lavoie et al. 2002 , Hannemann et al. 2008 ) identify structural contexts for translation selection from bitexts. Melamed 2004 adapts parsing to allow for multiple input strings (multitrees). Hierarchical translation (Chiang 2007 ) uses synchronous context-free grammars in decoding: Different grammar and parsing alternatives are given e.g. in (Zollmann /Venugopal 2006 , Galley et al. 2004 ). An opensource toolset for target langauge parsing, Joshua, has recently been presented (Li et al., 2009) . Including syntax into the decoding process, esp. in the context of hierarchical translation, is a promising approach to boost the grammaticality of the MT output. Factored Translation While using structural information for decoding attracts increasing interest, Factored Translation (cf. Koehn/Hoang 2007) aims at enriching systems 'bottom-up', by providing more information at word level. It treats words not just as simple textforms but as vectors of features, such features being the lemma, the POS, morphology, and others. The approach decomposes phrase translations into a sequence of mapping steps, with translation steps operating on phrase level, and generation steps on word level. Models are combined in a log-linear fashion. Several papers (e.g. Stymne et al. 2008) show that phenomena like NP-agreement and compounding can be handled efficiently within a factored translation framework. As a result, treating words as feature bundles in factored translation, and using structural information for both source-to-target mapping as well as target decoding, allows significant quality improvements for systems combining these factors. They would use both knowledge-driven (dictionaries and grammars) and data-driven (phrase tables, language models) information. However, they rely on the availability of (possibly even linguistically pre-processed) bilingual corpora. This fact may reduce their applicability. Genuine hybrid architectures do not just use addons to their system architecture but combine whole system components of the respective approaches into novel systems. They use three basic components: identification of source language ‚chunks' (words, phrases or equivalents thereof), transformation of such chunks into the target language by means of a bilingual resource, and generation of a target language sentence. Several proposals have been made how such systems could look like. Rule-based analysis, bilingual dictionary, target language model Such an approach has been investigated in the METIS projects (Vandeghinste et al. 2006) . Analysis is done using available NL tools (lemmatisers, taggers, chunkers); transfer is based on existing dictionaries (consisting basically of lemma and POS in source and target language, including single and multiword terms), and generation uses a language model (based on a tokenized and tagged English corpus (BCN)). To ensure that the LM based generation produces grammatical sentences, several approaches have been investigated for different languages; e.g.  in Greek-to-English (Tambouratsis et al., 2005 , Markantonatou et al., 2006) , a pattern matcher is applied to search for the best matching patterns containing the respective lexical head, and number + POS of modifiers; the selected pattern is then analysed recursively down the structure (sentence levelchunk levelunit level) for the best matching sub-patterns. The best patterns undergoes language.model-based target search.  in German-to-English, a 'structural transfer' type of mapping component is implemented to prepare good LM-based search Other languages explored in METIS implement other solutions, like bag-of-words. Evaluation of the technology show that results are similar to basic SMT systems but worse than a complete (rule-based) system like SYSTRAN in all language combinations (Vandeghinste et al. 2008 ). However this is not surprising comparing the effort invested in the two systems. However, it needs to be seen if the proposed architecture has the poten-tial to produce superior MT quality once the effort is increased. Data driven analysis and generation, bilingual dictionary Instead of rule-based analysis, an alternative data-driven approach has been proposed by (Carbonell et al. 2006) . The required resources are: a (full-form) bilingual dictionary, and a n-gram indexed target language corpus. In analysis, an ngram window is moved over the sentence, and all words in the window are translated using the bilingual dictionary; based on these translations, the target language corpus is searched for the closest n-gram (ideally containing all words of the source, and no additional ones). The result is a lattice of ngram translations. Of these, the segments with the strongest left and right overlaps, and the highest density of terms, are selected by the decoder. While this approach also circumvents the problem of the availability of bilingual resources and uses a dictionary as main translation resource, it does not attempt any 'phrase' analysis of the input (while METIS uses phrases produced by linguistic chunkers), and any knowledge-based analysis or generation resource. It needs to be seen how grammaticality of the output can be ensured (e.g. proper morphology, word order problems), and how accuracy can be produced, as the technique seems to ignore out-of-dictionary words (like proper names) and to insert spurious translations in the target language n-grams. Domain Adaptation A special issue to be considered is domain adaptation. All kinds of MT systems must cope with the fact that they will be used not only in the domain for which they had been developed but also for other domains. While RMT systems support adaptation by dictionary import and coding, which in turn can be based on domain corpus collections, the situation is less obvious for SMT-based systems, and a significant drop of quality (up to 10 BLEU) had been observed. The most promising approach for SMT systems seems to be to use large out-of-domain training data (e.g. Europarl), and with a small in-domain training set, build different resources for both kinds of data. While the phrase tables of the out-ofdomain data moderately improve the in-domain ones (by closing gaps in the translations), the most efficient approach seems to be to run a target Language Model trained only with in-domain data (Koehn/Schroeder 2007) . Experiments have also been made for integrating customer terminology (a bilingual list of terms) into an SMT system (Itagaki/Aikawa 2008). Artificial contexts are created to identify how the phrase tables would translate a given source language term; in a post-processing phase, the phrase table translations of the terms are replaced by the target expressions of the term list. While this seems to be a significant and error-prone effort, options to manipulate the phrase tables directly (a shown above) could be more promising. Conclusion The selection of the ‚best' architecture for a practical MT system depends on three basic factors:  the intended use case, e.g. the translation domain(s). A single-domain application with enough bilingual training data is the exception rather than the rule. SMT approaches to lowresource languages are presented e.g. in (Nießen/Ney 2004)  the translation quality which can be achieved, both for the domain in which the system was trained, and other domains in which the systems are supposed to be used  the availability of resources and data, both on monolingual and bilingual level. For the determination of the MT quality, most of the presented systems claim to outperform some baseline system; however, the results are difficult to compare, also due to the fact that the used metrics often are not adequate as some of them do not treat different system types equally (Dugast et al. 2009 ). In the present context, where several types of systems need to be compared, this is a drawback. Recently, several approaches for sentencebased metrics have been proposed (an overview is given in Callison-Burch et al. 2009) ; however there is no consolidated picture, and different metrics seem to perform best for different language directions. Much more relevant, from a practical point of view, is the availability of resources. For many language directions and many domains, sufficient amounts of bilingual data still do not exist, or cannot be accessed. In this case, architectures which rely on monolingual data and use bilingual dictionaries would have to be preferred. So the selection of the best alternative would depend on quality criteria, and on the availability of (training) data. However, whichever approach is taken, there is still a long way to go before machine translation systems reach acceptable quality.",
         "17162943",
         "156a816f1e0df790ca21f19df108ae7e8f1cc73e",
         "41",
         "https://aclanthology.org/2009.mtsummit-posters.21",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Thurmair, Gregor",
         "Comparing different architectures of hybrid Machine Translation systems",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "thurmair-2009-comparing",
         null,
         null
        ],
        [
         "15",
         "W05-0832",
         "Translation memories provide assistance to human translators in production settings, and are sometimes used as first-pass machine translation in assimilation settings because they produce highly fluent output very rapidly. In this paper, we describe and evaluate a simple whole-segment translation memory, placing it as a new lower bound in the well-populated space of machine translation systems. The result is a new way to gauge how far machine translation has progressed compared to an easily understood baseline system. The evaluation also sheds light on the evaluation metric and gives evidence showing that gaming translation with perfect fluency does not fool bleu the way it fools people.",
         "Translation memories provide assistance to human translators in production settings, and are sometimes used as first-pass machine translation in assimilation settings because they produce highly fluent output very rapidly. In this paper, we describe and evaluate a simple whole-segment translation memory, placing it as a new lower bound in the well-populated space of machine translation systems. The result is a new way to gauge how far machine translation has progressed compared to an easily understood baseline system. The evaluation also sheds light on the evaluation metric and gives evidence showing that gaming translation with perfect fluency does not fool bleu the way it fools people. Introduction and background Translation Memory (TM) systems provide roughly concordanced results from an archive of previously translated materials. They are typically used by translators who want computer assistance for searching large archives for tricky translations, and also to help ensure a group of translators rapidly arrive at similar terminology (Macklovitch et al., 2000) . Several companies provide commercial TMs and systems for using and sharing them. TMs can add value to computer assisted translation services (Drugan, 2004) . Machine Translation (MT) developers make use of similar historical archives (parallel texts, bitexts), to produce systems that perform a task very similar to TMs. But while TM systems and MT systems can appear strikingly similar, (Marcu, 2001 ) key differences exist in how they are used. TMs often need to be fast because they are typically used interactively. They aim to produce highly readable, fluent output, usable in document production settings. In this setting, errors of omission are more easily forgiven than errors of commission so, just like MT, TM output must look good to users who have no access to the information in source texts. MT, on the other hand, is often used in assimilation settings, where a batch job can often be run on multiple processors. This permits variable rate output and allows slower systems that produce better translations to play a part. Batch MT serving a single user only needs to run at roughly the same rate the reader can consume its output. Simple TMs operate on an entire translation segment, roughly the size of a sentence or two, while more sophisticated TMs operate on units of varying size: a word, a phrase, or an entire segment (Callison-Burch et al., 2004) . Modern approaches to MT, especially statistical MT, typically operate on more fine-grained units, words and phrases (Och and Ney, 2004) . The relationship between whole segment TM and MT can be viewed as a continuum of translation granularity: This classification motivates our work here. MT systems have well-studied and popular evaluation techniques such as bleu (Papineni et al., 2001) . In this paper we lay out a methodology for evaluating TMs along the lines of MT evaluation. This allows us to measure the raw relative value of TM and MT as translation tools, and to develop expectations for how TM performance increases as the size of the memory increases. There are many ways to perform TM segmentation and phrase extraction. In this study, we use the most obvious and simple condition-a full segment TM. This gives a lower bound on real TM performance, but a lower bound which is not trivial. Section 2 details the architecture of our simple TM. Section 3 describes experiments involving different strategies for IR, oracle upper bounds on TM performance as the memory grows, and techniques for rescoring the retrievals. Section 4 discusses the results of the experiments. A Simple Chinese-English Translation Memory For our experiments below, we constructed a simple translation memory from a sentencealigned parallel corpus. The system consists of three stages. A source-language input string is rewritten to form an information retrieval (IR) query. The IR engine is called to return a list of candidate translation pairs. Finally a single target-language translation as output is chosen. Query rewriting To retrieve a list of translation candidates from the IR engine, we first create a query which is a concatenation of all possible ngrams of the source sentence, for all ngram sizes from 1 to a fixed n. We rely on the fact that the Chinese data in the translation memory is tokenized and indexed at the unigram level. Each Chinese character in the source sentence is tokenized individually, and we make use of the IR engine's phrase query feature, which matches documents in which all terms in the phrase appear in consecutive order, to create the ngrams. For example, to produce a trigram + bigram + unigram query for a Chinese sentence of 10 characters, we would create a query consisting of eight threecharacter phrases, nine two-character phrases, and 10 single-character \"phrases\". All phrases are weighted equally in the query. This approach allows us to perform lookups for arbitrary ngram sizes. Depending on the specifics of how idf is calculated, this may yield different results from indexing ngrams directly, but it is advantageous in terms of space consumed and scalability to different ngram sizes without reindexing. This is a slight generalization of the successful approach to Chinese information retrieval using bigrams (Kwok, 1997) . Unlike that work, we perform no second stage IR after query expansion. Using a segmentation-independent engineering approach to Chinese IR allows us to sidestep the lack of a strong segmentation standard for our heterogeneous parallel corpus and prepares us to rapidly move to other languages with segmentation or lemmatization challenges. The IR engine Simply for performance reasons, an IR engine, or some other sort of index, is needed to implement a TM (Brown, 2004) . We use the opensource Lucene v 1. 4.3, (Apa, 2004) as our IR engine. Lucene scores candidate segments from the parallel text using a modified tf-idf formula that includes normalizations for the input segment length and the candidate segment length. We did not modify any Lucene defaults for these experiments. To form our translation memory, we indexed all sentence pairs in the translation memory corpora, each pair as a separate document. We Source TM output However , everything depended on the missions to be decided by the Security Council . The presentations focused on the main lessons learned from their activities in the field . It is wrong to commit suicide or to use ones own body as a weapon of destruction . There was practically full employment in all sectors . One reference translation (of four) Doug Collins said, \"He may appear any time. It really depends on how he feels.\" At present, his training is defense oriented but he also practices shots. He is elevating the intensity to test whether his body can adapt to it. So far as his knee is concerned, he thinks it heals a hundred percent after the surgery.\" indexed in such a way that IR searches can be restricted to just the source language side or just the target language side. Rescoring The IR engine returns a list of candidate translation pairs based on the query string, and the final stage of the TM process is the selection of a single target-language output sentence from that set. We consider a variety of selection metrics in the experiments below. For each metric, the source-language side of each pair in the candidate list is evaluated against the original source language input string. The target language segment of the pair with the highest score is then output as the translation. In the case of automated MT evaluation metrics, which are not necessarily symmetric, the source-language input string is treated as the reference and the source-language side of each pair returned by the IR engine as the hypothesis. All tie-breaking is done via tf-idf , i.e. if multiple entries share the same score, the one ranked higher by the search engine will be output. Table 1 gives a typical example of how the TM performs. Four contiguous source segments are presented, followed by TM output and finally one of the reference translations for those source segments. The only indicator of the translation quality available to monolingual English speakers is the awkwardness of the segments as a group. By design, the TM performs with perfect fluency at the segment level. Experiments We performed several experiments in the course of optimizing this TM, all using the same set of parallel texts for the TM database and multiple-reference translation corpus for evalutation. The parallel texts for the TM come from several Chinese-English parallel corpora, all available from the Linguistic Data Consortium (LDC). These corpora are described in Table 2. We discarded any sentence pairs that seemed trivially incomplete, corrupt, or otherwise invalid. In the case of LDC2002E18, in which sentences were aligned automatically and confidence scores produced for each alignment, we dropped all pairs with scores above 9, indicating poor alignment. No duplication checks were performed. Our final corpus contained approximately 7 million sentence pairs and contained 3.2 GB of UTF-8 data. Our evaluation corpus and reference corpus come from the data used in the NIST 2002 MT competition. (NIST, 2002) . The evaluation corpus is 878 segments of Chinese source text. The reference corpus consists of four independent human-generated reference English translations of the evaluation corpus. All performance measurements were made using a fast reimplementation of NIST's bleu. bleu exhibits a high correlation with human judgments of translation quality when measuring on large sections of text (Papineni et al., 2001) . Furthermore, using bleu allowed us to compare our performance to that of other systems that have been tested with the same evaluation data. An upper bound on whole-segment translation memory Our first experiment was to determine an upper bound for the entire translation memory corpus. In other words, given an oracle that picks the best possible translation from the translation memory corpus for each segment in the evaluation corpus, what is the bleu score for the resulting document? This score is unlikely to approach the maximum, bleu =100 because this oracle is constrained to selecting a translation from the target language side of the parallel corpus. All of the calculations for this experiment are performed on the target language side of the parallel text. We were able to take advantage of a trait particular to bleu for this experiment, avoiding many of bleu score calculations required to assess all of the 878 × 7.5 million combinations. bleu produces a score of 0 for any hypothesis string that doesn't share at least one 4-gram with one reference string. Thus, for each set of four references, we created a Lucene query that returned all translation pairs which matched at least one 4-gram with one of the references. We picked the top segment by calculating bleu scores against the references, and created a hypothesis document from these segments. Note that, for document scores, bleu's brevity penalty (BP) is applied globally to an entire document and not to individual segments. Thus, the document score does not necessarily increase monotonically with increases in scores of individual segments. As more than 99% of the segment pairs we evaluated yielded scores of zero, we felt this would not have a significant effect on our experiments. Also, the TM does not have much liberty to alter the length of the returned segments. Individual segments were chosen to optimize bleu score, and the resulting documents exhibited appropriately increasing scores. While there is no efficient strategy for whole-document bleu maximization, an iterative rescoring of the entire document while optimizing the choice of only one candidate segment at a time could potentially yield higher scores than those we report here. TM performance with varied Ngram length The second experiment was to determine the effect that different ngram sizes in the Chinese IR query have on the IR engine's ability to retrieve good English translations. We considered cumulative ngram sizes from 1 to 7, i.e. unigram, unigram + bigram, unigram + bigram + trigram, and so on. For each set of ngram sizes, we created a Lucene query for every segment of the (Chinese) evaluation corpus. We then produced a hypothesis document by combining the English sides of the top results returned by Lucene for each query. The hypothesis document was evaluated against the reference corpora by calculating a bleu score. While it was observed that IR performance is maximized by performing bigram queries (Kwok, 1997) , we had reason to believe the TM would not be similar. TMs must attempt to match short sequences of stop words that indicate grammar as well as more traditional content words. Note that our system performed neither stemming nor stop word (or ngram) removal on the input Chinese strings. An upper bound on TM N -best list rescoring The next experiment was to determine an upper bound on the performance of tf-idf for different result set sizes, i.e. for different (maximum) numbers of translation pairs returned by the IR engine. This experiment describes the trade-off between more time spent in the IR engine creating a longer list of returns and the potential increase in translation score. To determine how much IR was \"enough\" IR, we performed an oracle experiment on different IR query sizes. For each segment of the evaluation corpus, we performed a cumulative 4-gram query as described in Section 4.2. We produced the n-best list oracle's hypothesis document by selecting the English translation from this result set with the highest bleu score when evaluated against the corresponding segment from the reference corpus. We then evaluated the hypothesis documents against the reference corpus by computing bleu scores. N -best list rescoring with several MT evaluation metrics The fourth experiment was to determine whether we could improve upon tf-idf by applying automated MT metrics to pick the best sentence from the top n translation pairs returned by the IR engine. We compared a variety of metrics from MT evaluation literatures. All of these were run on the tokens in the source language side of the IR result, comparing against the single pseudo-reference, the original source language segment. While many of these metrics aren't designed to perform well with one reference, they stand in as good approximate string matching algorithms. The score that the IR engine associates with each segment is retained and marked as tf-idf in this experiment. Naturally, bleu (Papineni et al., 2001) was the first choice metric, as it was well-matched to the target language evaluation function. rouge was a reimplementation of ROUGE-L from (Lin and Och, 2004) . It computes an F-measure from precision and recall that are both based on the longest common subsequence of the hypothesis and reference strings. wer-g is a variation on traditional word error rate that was found to correlate very well with human judgments (Foster et al., 2003) , and per is the traditional position-independent error rate that was also shown to correlate well with human judgments (Leusch et al., 2003) . Finally, a random metric was added to show the bleu value one could achieve by selecting from the top n strictly by chance. After the individual metrics are calculated for these segments, a uniform-weight log-linear combination of the metrics is calculated and used to produce a new rank ordering under the belief that the different metrics will make predictions that are constructive in aggregate. Results An upper bound for whole-sentence TM Figure 1 shows the maximum possible bleu score that can an oracle can achieve by selecting the best English-side segment from the parallel text. The upper bound achieved here is a bleu score of 17.7, and this number is higher than the best performing system in the corresponding NIST evaluation. Note the log-linear growth in the resulting   bleu score of the TM with increasing database size. As the database is increased by a factor of ten, the TM gains approximately 5 points of bleu. While this trend has a natural limit at 20 orders of magnitude, it is unlikely that this amount of text, let alone parallel text, will be a indexed in the foreseeable future. This rate is more useful in interpolation, giving an idea of how much could be gained from adding to corpora that are smaller than 7.5 million segments. 4.2 The effect of ngram size on Chinese tf-idf retrieval Figure 2 shows that our best performance is realized when IR queries are composed of cumulative 4-grams (i.e. unigrams + bigrams + trigrams + 4-grams). As hypothesized, while longer sequences are not important in document retrieval in Chinese IR, they convey information that is useful in segment retrieval in the translation memory. For the remainder of the experiments, we restrict ourselves to cumulative 4-gram queries. Note that the 4-gram result here (bleu of 5.87) provides the baseline system performance measure as well as the value when the segments are reranked according to tf-idf . Upper bounds for tf-idf Figure 3 gives the n-best list rescoring bounds. The upper bound continues to increase up to the top 1000 results. The plateau achieved after 1000 IR results suggests that is little to be gained from further IR engine retrieval. Note the log-linear growth in the bleu score the oracle achieves as the n-best list extends on the left side of the figure. As the list length is increased by a factor of ten, the oracle upper bound on performance increases by roughly 3 points of bleu. Of course, for a system to perform as well as the oracle does becomes progressively harder as the n-best list size increases. Comparing this result with the experiment in section 4.1 indicates that making the oracle choose among Chinese source language IR results and limiting its view to the 1000 results given by the IR engine incurs only a minor reduction of the oracle's bleu score, from 17.7 to 16.3. This is one way to measure the impact of crossing this particular language barrier and using IR rather than exhaustive search. Surprisingly, tf-idf was outperformed only by bleu and the combination metric. While we hoped to gain much more from n-best list rescoring on this task, reaching toward the limits discovered in section 4.3, the combination metric was less than 0.5 bleu points below the lower range of systems that were entered in the NIST 2002 evals. The bleu scores of research systems in that competition roughly ranged between 7 and 15. Of course, each of the segments produced by the TM exhibit perfect fluency. Discussion The maximum bleu score attained by a TM we describe (6.56) would place it in last place in the NIST 2002 evals, but by less than 0.5 bleu. Successive NIST competitions have exhibited impressive system progress, but each year there have been newcomers who score near (or in some cases lower than) our simple TM baseline. We have presented several experiments that quantitatively describe how well a simple TM performs when measured with a standard MT evaluation measure, bleu. We showed that the translation performance of a TM grows as a loglinear function of corpus size below 7.5 million segments. We showed, somewhat surprisingly, only 1000 IR returns need be evaluated by a rescorer to get within 1 bleu point of the maximum possible score attainable by the TM. In future work, we expect to validate these results with other language pairs. One question is: how well does this simple IR query expansion address segmented languages and languages that allow more liberal word order? Supervised training of n-best reranking schemes would also determine how far the oracle bound can be pushed. The computationally more expensive reranking procedure that attempts to optimize bleu on the entire document should be investigated to determine how much can be gained by better global management of the brevity penalty. Finally, we believe it's worth noting the degree to which high fluency of the TM output could potentially mislead target-language-only readers in their estimation of the system's performance. Table 1 is representative of system output, and is a good example of why translations should not be judged solely on the fluency of a few segments of target language output.",
         "317123",
         "6158add37ce7e19122e06433a548d833a5e9c4ab",
         "1",
         "https://aclanthology.org/W05-0832",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Henderson, John  and\nMorgan, William",
         "Gaming Fluency: Evaluating the Bounds and Expectations of Segment-based Translation Memory",
         "175--182",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "henderson-morgan-2005-gaming",
         null,
         null
        ],
        [
         "16",
         "2007.mtsummit-papers.60",
         "Although many high-quality dictionaries contain a sufficient number of idioms for their intended users, the methods available for looking up entries in both paper and electronic dictionaries as well as in machine translation systems are not satisfactory. Providing an adequate automatic look-up function is complicated by the existence of idiom variants, which sometimes can be very creative. The problem is further complicated by the fact that the possible range of idiom variations has not been described in a computationally tractable way. Against this backdrop, we analysed the variation patterns of idioms using manually-created idiom variation data, and, on the basis of that, developed an idiom look-up system that automatically matches idiom variants in English texts with the canonical forms of idiom entries in dictionaries. The experimental results showed our system performs sufficiently well to be used in real-world settings, including as an aid for translators, which is our overall aim.",
         "Although many high-quality dictionaries contain a sufficient number of idioms for their intended users, the methods available for looking up entries in both paper and electronic dictionaries as well as in machine translation systems are not satisfactory. Providing an adequate automatic look-up function is complicated by the existence of idiom variants, which sometimes can be very creative. The problem is further complicated by the fact that the possible range of idiom variations has not been described in a computationally tractable way. Against this backdrop, we analysed the variation patterns of idioms using manually-created idiom variation data, and, on the basis of that, developed an idiom look-up system that automatically matches idiom variants in English texts with the canonical forms of idiom entries in dictionaries. The experimental results showed our system performs sufficiently well to be used in real-world settings, including as an aid for translators, which is our overall aim. Introduction We are currently developing a system that aids Englishto-Japanese volunteer translators who translate online documents and publish translated documents online. Among the many reference functions and aspects of reference content that require enhancement, translators have identified improvement in idiom look-up functions as a key issue they would like to see addressed by translation aid systems (Kageura et. al., 2006) . Although idioms provided by many high-quality dictionaries (e.g. Sanseido, 2004; McCaleb and Iwasaki, 2003) are basically satisfactory for translators, the methods available for looking up idiom entries are far from satisfactory, not only in paper dictionaries but also in electronic dictionaries. This is partly because the user has to guess the core constituent words of an idiom in order to consult a dictionary. Automatic look-up methods embodied in machine translation systems are not satisfactory, either. Take, for instance, the following examples: (1) He said that with his tongue in his cheek. (2) He said that with his big fat tongue in his big fat cheek. Although many available machine translation systems successfully detect the idiomatic expression \"with one's tongue in one's cheek\" in (1), none among those we checked (e.g. Excite, 2005; Fujitsu, 2005; LogoVista, 2005; Sharp, 2004; Toshiba, 2005) 1 could properly translate (2). Most existing methods for looking up idioms cannot deal with the rich variations in idioms, that are abundant in ordinary texts 2 . In the field of natural language processing (NLP), much research has been carried out into the automatic extraction of collocations and idioms (e.g. Piao, 2006; Smadja, 1993; Widdows and Dorow, 2005) , but not much work has been devoted to the automatic matching of idiom entries to their occurrences in running texts. As translators are basically satisfied with the idioms provided in existing high-quality dictionaries but frustrated with the poor look-up functions, 1 Sharp (2004) can detect some gapped idiom occurrences, but it fails to detect complex idiom variations. 2 For instance, our rough survey of online documents revealed that the idiom \"hang on\" in its basic form, \"hang on\" with insertion, and \"hang on\" with passivisation (with or without insertion) occur in roughly the same frequency. The same is true for \"dumb down\". to develop enhanced look-up functions for idioms is of utmost importance from the point of view of aiding translators. Although a few important related studies exist (Carl and Rascu, 2006; Jacquemin, 2001; Yoshihashi et. al., 2005) , and many translation memory systems realise flexible approximate matching of similar sentence or phrasal constructions (Similis, 2006; Trados, 2006) , the task of flexible automatic look-up of idioms is yet to be fully explored. Against this backdrop, we are developing a mechanism that automatically matches English idiom occurrences in texts and their possible variations with idiom entries in dictionaries, as part of an overall project that aims at developing a system to aid English-to-Japanese online volunteer translators. In the following, we will first clarify translators' basic requirements. Then we will provide the basic patterns of idiom variations based on manually-constructed idiom variation data, and explain the automatic idiom look-up system that takes into account major syntagmatic idiom variations. Finally, we will provide an evaluation of the method and outline areas for further improvement. Translators' basic requirements In order to clarify requirements for translation-aid system, we consulted eight translators working online. We also sent a questionnaire to other online translators and obtained 12 replies. In relation to idiom look-up functions, two important features became clear. Firstly, translators do not want the system to provide a single idiom entry that matches textual occurrences. This is more to do with the fact that checking multiple possibilities is an inherent and essential part of proper translation than with the fact that it is difficult to develop a satisfactorily high-performance automatic idiom look-up system. In other words, from the point of view of translators, for the system to provide multiple possibilities of matching idioms is not a defect but a necessity if the system is to be useful for them. After all, translators check many candidates that they do not eventually use in their translations. What is important is reducing translators' burden as well as the quality of candidates the system proposes. Secondly, translators -as language practitioners -want the system to be able to deal with variations in a more flexible way than described by linguists. For instance, for language practitioners \"shoot the breeze\" could be passivised (or they could imagine a situation in which they face the passive form of this idiom or they passivise the idiom by themselves in the process of writing), while according to Numberg et. al. (1994) , it is not possible. The idiom \"go halves\" can be used in the form \"go exact halves\", while Nicolas (1995) claimed that this is not possible. In a sense, even what descriptive (i.e. non-prescriptive) linguistics provides is too prescriptive for the reality of texts that translators face in their daily activities. This is especially the case for online documents (Aitchison and Lewis, 2003) , which are dealt with by the translators our system targets. From the point of view of system specifications, these two requirements mean that the system can -and shouldprovide overmatching results, with recall as close as 100%. They also mean that, in the evaluation of system performance, the concept of \"precision\" should be defined not in terms of the \"correct\" choice of candidate but in terms of its usefulness for translators. We will come back to this point later when we evaluate the performance of our system. Idiom variation patterns There are studies and reference books that describe (English) idiom variations at a variety of levels (Benson, 1985; Biber, 1999; Čermák, 1970; Fraser, 1970; Moon, 1998; Nicolas, 1995; Numberg et. al., 1994; Quirk et. al., 1985) . On the basis of these, and taking into account the comments we obtained from translators, we first classified the idiom variation patterns as follows: (1) Type variants or families: Types of idiom variations that are (or theoretically should be) registered in dictionary entries. An example is \"run around [round] like a bluearsed fly\". From the practical point of view, this type of variant can be dealt with by the variation indications in idiom entries in dictionaries. (2) Variations created by external factors: Passivisation (\"the breeze was shot\") and topicalisation (\"It is these strings that he pulled\") are typical examples of this type of variation. These variations are generally created by applying syntactic operations defined outside the idioms themselves, and could be dealt with uniformly by a few basic rules. (3) Variations applied to parts or within the construction of idioms: many syntagmatic insertions (\"go halves\" → \"go exact halves\") and paradigmatic replacements (\"head screwed on right\" → \"head screwed on wrong/left\") fall under this category. This type of variation is expected to be neither straightforwardly clear nor completely unmanageable. (4) Highly creative variations: \"point of view\" → \"ballpoint pen of view\". This class of variation is expected to be unmanageable for the time being. We focus on the third category (3) of idiom variations in this work, for reasons mentioned above. In order to develop a look-up function for idiom entries, existing linguistic studies have three limitations: (i) as mentioned, they tend to be too restrictive from the point of view of the reality of texts that translators deal with; (ii) the description of variations is not given in a computationally tractable way; and (iii) the number of variations given in these studies is small 3 . Given the dearth of basic idiom variation data, we started by constructing idiom variation data manually. We took idiom entries from a widely-used English-Japanese idiom dictionary (McCaleb and Iwasaki, 2003) , and asked three native English speakers (two of whom were professional editors) to create idiom variations with examples. We asked the informants to imagine they were writing or editing articles in the culture section of newspapers and be as creative as possible within that restriction. Table 1 shows the basic quantities of the idiom variation data. The quantity of data is rather small, and we are intending to augment it further in the same manner, asking informants to construct variations. Note, however, that using large corpora to collect the data is not our priority, for a few reasons: (i) it is difficult to extract idiom variations from large corpora, except for easily predictable regular types which could be covered by rules; (ii) it is difficult to identify the threshold of possible variations, which tend to occur in low frequencies (translators do not choose a text by the representativeness of the language in the text or by the overall frequencies in the corpora of idiom variations used in the text); (iii) translators are dealing with individual texts and not representative language expressions, so frequencies in large corpora do not automatically mean importance for translators; and (iv) the frequency of idiom variations detected in large corpora correlates with the frequency of individual idioms, and frequently occurring idioms tend to be the ones that translators are least interested in and therefore the variations of which are less important from the translators' point of view. Informants (a) idioms (b) variants (b)/( Our intention in referring to the data is not to observe common usage or dominant patterns but to define the tractable range of variation patterns, which would hopefully correspond to the range of practically possible variations; we have no interest in covering only frequently occurring patterns at the expense of less frequent but computationally tractable patterns. As such, it is expected that the use of large corpora would not cover up the shortcomings of the size of the manually constructed data. For instance, one of the informants provided the variation \"take the wild plunge\" of the idiom \"take the plunge,\" which only brings up four hits in a Google search. From the point of view of translators, this and other rare variations, if technically possible, should be covered when they occur 4 . This manually-constructed data was then analysed and variation patterns were identified (Kageura and Toyoshima, 2006) . Table 2 shows the basic variation patterns. In the table, such types as \"dependent multiple replacement\" etc. indicate that more than one type of mutually dependent variations was observed. As can be seen from Table 2 , the major patterns are syntagmatic augmentations and paradigmatic replacement. Here we focus on variations by syntagmatic augmentation, and formalise the descriptions of syntagmatic augmentation patterns. In doing so, we assume the use of POS-taggers and morphological analysers. Though high-performance parsers exist, we did not use them for two main reasons: (i) idiomatic expressions often cross over the border of constituents given by parsers (in which case we would need to flatten the parse tree anyway), and (ii) to achieve recall as close to 100% as possible is most important, and for that aim the loose definition of patterns provides a better starting point than the rigid description of variations using structural information. There are two different approaches for describing POS level patterns for variations of syntagmatic augmentations: (a) taking all constituents of the idiom into account, or (b) taking only binary constituents adjacent to words inserted into the idiom. For instance, assuming that the idiom expression \"take the plunge\" has as a variation \"take the wild plunge\", we can describe the POS level patterns as \"Verb Det Noun\" → \"Verb Det Adj Noun\" in approach (a), or \"Det Noun\" → \"Det Adj Noun\" in approach (b). In the current work we took approach (b) because we assume that: (i) inserted words are mostly bound by the adjacent words and their grammatical categories; (ii) to take into account the overall grammatical patterns would immediately lead us to taking into account the individual idioms with lexical substance; and (iii) the requirement of high recall is of utmost importance at the current stage. Using the POS-information of adjacent elements, we for-Ah, those who cannot read literary texts but still have the audacity to think themselves to be language specialists!\" As computational linguists we should try to bridge this gap between linguists and translators. mulated the basic patterns of idiom variations by syntagmatic augmentation as shown in Table 3 (\"Prep\", \"Adj\", \"Adv\", \"PosPro\" and \"PerPro\" denote preposition, adjective, adverb, possessive pronoun (e.g. \"my\", \"her\" etc.) and personal pronoun (e.g. \"I\", \"he\" etc.), respectively). POS Each variation rule in Table 3 indicates the POS sequence of constituents of an idiom and the POS tag of the inserted word. For instance, the POS pattern of (Det, Noun) in constituents of an idiom can take either a noun, adjective or adverb. This rule covers the variation from \"take the plunge\" to \"take the wild plunge\". Note that the described range of variation patterns, when incorporated into automatic matching algorithms, can be overgenerative. We can, however, reasonably expect that the overmatching will be within the manageable range, because, as mentioned, the computational problem is defined here as a problem of matching when both ends are given, rather than a problem of generating acceptable variations. The idiom look-up system The system consists of three processing modules: (1) the preprocessing module in which the input text is processed to facilitate automatic matching, including POS-tagging and normalisation of expressions, (2) the surface matching module in which all the possible idiom entries are detected by using AND matching of constituent elements of idioms with words occurring in texts, and (3) the filtering module in which the undesired candidates detected in the surface matching module are filtered out by using the POS-based variation restriction rules constructed based on the patterns given in Table 3 . The input of the target system is an English text and the output is idiom candidates occurring in the text with their Japanese translations provided in the dictionary. Figure 1 shows the overall flow of the system. We will elaborate each of these modules below and illustrate the system interface. Input text (in English) Pre-processing of the text Pre-processed text Idiom entries in a dictionary Idiom candidates Output (list of idioms) Surface matching Rule-based filtering Pre-processing In the pre-processing module, we first assign POS information to the input text by using Tree-tagger (TreeTagger, 2004) . After that, we apply the standardisation rules to the surface word forms that occur in texts. This is because, in many cases, the word form in the text is different from the word form in dictionary entries. For example, \"took his seat\" can be found in the text, but what is registered in the dictionary is \"take one's seat\". We adjusted the word form in the text so that it could be matched to the dictionary entries. Four types of formal standardisations are applied at this stage: (1) Inflected forms of verbs are transformed into basic forms. In addition, we added \"do\" or \"doing\" to absorb the matching of idioms whose entries are registered as something like \"cannot help doing\" in the dictionary. (2) Plural forms of nouns are transformed into singular forms. (3) Articles are paradigmatically expanded so that the occurrence \"a\", for instance, can be matched with an entry with \"the\". This may often lead to false matching as some idioms require the strict use of either definite or indefinite articles, but we found we can gain more than we lose by applying this processing, from the point of view of system requirements. (4) Personal pronouns are standardised into basic forms. Table 4 shows the basic standardisation patterns of word forms. Note that in the actual matching, we also retain the original forms. In addition to these, we applied a small amount of pre-processing such as splitting hyphenated words, etc. Input word Pattern (1) verb surface, basic form, do, doing (2) plural noun surface, singular noun (3) particle a, an, the (4) my, his, etc. surface, one's (4) myself, herself, etc. surface, oneself Table 4: Standardisation of words Surface matching In the surface matching module, we carry out an extensive retrieval in which all the possible idiom candidates can be detected. We retrieve idiom entries whose constituents all match the textual sequences of words in order. In the dictionary entries, such examples as \"make A of B\" or \"have ... in\" exist. In the surface matching module, we deal with these \"position fillers\" as wild cards. Figure 2 shows an example of surface matching. In Figure 2 , the dictionary entry \"have one's eye on\" is detected as an idiom candidate, because its constituent words all match the input words in the text. -when an idiom is detected -when words in the text do not correspond to the constituent words of the idiom input sentence idiom entry in dictionary input sentence idiom entry in dictionary ...... had his eye on ...... O have one's eye on ...... had his ears on ..... X have one's eye on Filtering with POS patterns As we emphasise exhaustive retrieval in the surface matching module, many of the idiom candidates detected in the surface matching module are expected to be non-relevant idioms. In the filtering module, we filter out many irrelevant idioms by using the rules constructed on the basis of POSpatterns given in Table 3 . Figure 3 shows an example of filtering. In this case, the surface matching module detected the three idiom candidates \"make a habit of doing\", \"wake up\", \"make after\" for the input text \"I make a habit of stretching after I wake up.\" The basic adjacent patterns given in Table 3 require that what can be inserted between a verb and a preposition is either  an adverb or an adjective. As the construction \"a habit of stretching\" is neither an adverb nor an adjective, the candidate \"make after\" is filtered out and excluded from the final output. Interface of the independent idiom look-up system Figure 4 shows the system interface. The interface consists of an input area, parameter specification area in which the user can specify a window size of a certain number of words within which idiom candidates are to be searched, and an output area. When the user inputs an English text, and clicks the \"search\" button, the system outputs the idiom candidates with their meaning (in Japanese). By moving the mouse over an output idiom, the matched parts of the input text in the input area are emphasised. Figure 4 shows that \"take the plunge\" and \"have one's eye on\" were detected for the input: \"I decided to take the wild plunge and buy the car I had my eye on.\" Integration to the translation editor environment In addition to the independent system interface, we incorporated the idiom look-up system into the integrated translation editor environment QRedit (Abekawa and Kageura, 2007) . Figure 5 shows the interface in which automatic idiom lookup functions within the integrated environment. The screen shot shows that the idiom entry \"(with) one's tongue in one's cheek\" matches the sentence \"He said that with his big fat tongue in his big fat cheek.\" Evaluation We carried out evaluation experiments in order to observe the overall performance of the system, as well as the following three aspects: (1) the effect of standardisation of words; (2) the effect of the POS-based filtering; (3) the overall performance of the system. Experimental setup We observed how many correct idiom candidates our system was able to locate with each set of data. The data set used for evaluation were: (a) 100 sentences containing idiom variations, randomly extracted from the idiom variation data mentioned in section 2, and (b) data consisting of 20 newspaper and journal articles (five articles taken from the BBC online news site, five articles from The Nation website, five articles from the The Independent website, and five articles from the New York Times website). We manually identified and tagged the idioms for these articles. The window size is set to a sentence. For both the data (a) and (b), we compared the method of surface matching only with the method of surface matching and filtering with POS-based information. Correct outputs were defined as follows: (2) Variations of (1) created by replacement of articles. For instance, when \"come to the point\" is the idiom actually used in the text, \"come to a point\" is evaluated as correct output. (3) Variations of (1) created by the singular/plural forms of nouns. When a constituent noun in the actual idiom is plural, such as \"in spirits\", \"in spirit\" is evaluated as correct output. (4) Embedded idioms. When the actual idiom is \"come to the point\", we evaluate \"to the point\" as correct output. These criteria were set on the basis of the consultation with eight online volunteer translators. Translators, when they come across expressions they cannot readily translate analytically, check several possible idiom candidates to reach the final correct translation. As mentioned, checking multiple candidates is not an optional, extraneous process that translators would like to omit if they can, but rather an essential process by which they make sure that their final decision is correct. As such, given that no automatic processing can substitute for human decision making, translators want the automatic idiom look-up system to show multiple candidates that are close to the set of candidates that translators actually check. The criteria given here are an approximation to this. Figure 6 shows the range of the correct output defined here 5 . 5 In an evaluation of a system that provides translation information for human translators, Sharoff et. al. (2006) introduces five grades: 5 = the suggestion is an appropriate translation as it is; 4 = the suggestion can be used with some minor amendment; 3 = the suggestion is useful as a hint for another, appropriate translation; 2 = the suggestion is not useful, even though it is still in the same domain; 1 = the suggestion is totally irrelevant. The grades \"5\", \"4\" and \"3\" can be interpreted as corresponding roughly to (C) in Figure 6 , which includes, but is not limited to, the correct idioms. Evaluation measures From the viewpoint of translation support, it is important for our system to detect all the idioms that appear in the text. Therefore, recall is the most important factor at this stage. Improvements in precision should be elaborated without negatively affecting recall. F-measure is irrelevant. precision = #CorrectSystemOutputs #SystemOutputs recall = #CorrectSystemOutputs #AllCorrectIdioms Result of the experiments Tables 5 and 6 show the results of the experiments. In both sets of data, the recall is as high as 0.97, although the precision varies between the data sets (a) and (b). The high recall is very promising, as the essential problem that prompted us to develop this system was the difficulty experienced by translators in looking up idioms, the improvement of which requires high performance in recall. The precision of 0.5 to 0.7 is in a practically useful range. This figure means that translators are provided with twice as many candidates as they need to check. For most texts this will not be an excessive number. Experimental use by a translator in an integrated editor environment has shown that the problem is more to do with the quality of unnecessary candidates rather than the quantity (because they reduce the translators' expectations of the system). The result of filtering with POS-based patterns for the data set (a) and (b) shows that this filtering improved precision greatly, without negatively affecting recall, which proves the usefulness of POS-based patterns for our aim. The result of experiments with filtering on data set (b) shows that the recall for data set (b) is higher than that for data set (a), but the precision for data set (b) is lower than that for data set (a). The cause of the low precision can be summarised as follows: (1) it is easy for our system to detect incorrect idioms for data set (b), because real-world English texts tend to have longer sentences; (2) there is room for improvement in the filtering rules. On the other hand, the higher recall for data set (b) can be explained by the fact that the manually constructed basic data include some highly creative examples, which are rather difficult to detect, while the real-world data contains less of these extremely creative variations. Diagnosis Upon analysing the results, certain patterns of errors and misses were identified. (1) Errors resulting from insufficiency of lemmatisation by the POS-tagger. For instance, \"She horribly damned him with faint praise\" is based on the idiom \"horribly damn with faint praise\". However, our system could not detect this idiom because \"damned\" was recognised as an adverb rather than the verb \"damn\". This could be avoided by the improvement of the POS-tagging performance. Another related pattern is errors resulting from the errors of POS-taggers and/or lack of parsing. For instance, the system wrongly output \"from high\" to the input text \"I graduated from high school\". This was because, on the one hand, as we do not give structures to input texts, information about the proper construction of \"from (high school)\" is not provided, while on the other hand the dictionary entry \"from high\" was wrongly tagged as \"from:prep high:adj\" instead of \"from:prep high:nn\". Although theoretically it is preferable to use a high-performance parser, many of these errors can practically be avoided by an improvement in the POS-tagging performance. (2) Errors resulting from the lack of restrictions on the side of such idiom entries as \"make A of B\" or \"have ... in\". The dictionary we used does not give detailed information for the slot \"A\", \"B\" and \"...\". As a result, the system output several irrelevant idioms. This problem can be solved by imposing restrictions on each idiom entry with place holders. (3) Misses resulting from input text variations in which long phrases are inserted into the idiom constructions. For instance, the dictionary entry \"take apart\" was not detected for the input text: \"She takes (her daughter-inlaw) apart with stinging criticism.\" In order to deal with this, we need to further our understanding of possible idiom variations. In summary, most of the errors can be avoided (a) if we impose further restrictions on the variation patterns and place holders of idiom entries and (b) if the POS-tagging performance is improved. On the other hand, if we systematically try to deal with the misses, further understanding of the potential range of idiom variations is needed. The experimental results show that the rules we have established cover most of the variations that can be described formally as POS-based patterns. The remaining variations may well be ones that are more context dependent, creative, and/or related to constructions larger than those that can be conveniently described by POS-based patterns. We are currently dealing with misses through experimental use of the system and modifications on the basis of user feedback. Conclusions This paper has reported a method for automatically looking up idiom entries in dictionaries vis-à-vis idiom occurrences in texts that may include variations. We started by defining translators' requirements, and then observed the range of idiom variations and formalised the variation patterns of syntagmatic augmentations as POS-based patterns. The result of the experiment showed that the system performance is very promising. The precision for real-world texts is slightly above 0.5, which is in a practically useful range, as users' satisfaction depends more on an improvement of what is currently provided than on \"ideal\" performance. As for the technical aspect, we used a morphological analyser but not a parser. The experimental evaluation and the error analyses suggested that most errors and misses can be dealt with without delving into the structural level information given by parsers, although this needs further analysis and examination. We are currently working in three mutually related directions: (1) Making the system available for experimental use by translators and obtaining feedback from them, including levels of satisfaction and detailed patterns of errors/misses. We have obtained feedback from two translators and two more translators will take part in the userbased evaluations; (2) Developing a mechanism that deals with paradigmatic replacements. A basic mechanism has already been developed, and we are currently carrying out evaluation experiments; and (3) Refining the algorithms for dealing with variations by syntagmatic augmentation. Acknowledgements This research is partly supported by grant-in-aid (A) 17200018 \"Construction of online multilingual reference tools for aiding translators\" by the Japan Society for the Promotion of Sciences (JSPS). We would like to thank Sanseido Publishing Company for allowing us to use the Grand Concise English-Japanese Dictionary for our experiments.",
         "12237644",
         "91e5acb223de864e644cd86a9d50911fabede9b1",
         "13",
         "https://aclanthology.org/2007.mtsummit-papers.60",
         null,
         "Copenhagen, Denmark",
         "2007",
         "September 10-14",
         "Proceedings of Machine Translation Summit XI: Papers",
         "Takeuchi, Koichi  and\nKanehila, Takashi  and\nHilao, Kazuki  and\nAbekawa, Takeshi  and\nKageura, Kyo",
         "Flexible automatic look-up of {E}nglish idiom entries in dictionaries",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "takeuchi-etal-2007-flexible",
         null,
         null
        ],
        [
         "17",
         "W05-0833",
         "Way and Gough, 2005) provide an indepth comparison of their Example-Based Machine Translation (EBMT) system with a Statistical Machine Translation (SMT) system constructed from freely available tools. According to a wide variety of automatic evaluation metrics, they demonstrated that their EBMT system outperformed the SMT system by a factor of two to one. Nevertheless, they did not test their EBMT system against a phrase-based SMT system. Obtaining their training and test data for English-French, we carry out a number of experiments using the Pharaoh SMT Decoder. While better results are seen when Pharaoh is seeded with Giza++ word-and phrase-based data compared to EBMT sub-sentential alignments, in general better results are obtained when combinations of this 'hybrid' data is used to construct the translation and probability models. While for the most part the EBMT system of (Gough & Way, 2004b) outperforms any flavour of the phrasebased SMT systems constructed in our experiments, combining the data sets automatically induced by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French-English.",
         "Way and Gough, 2005) provide an indepth comparison of their Example-Based Machine Translation (EBMT) system with a Statistical Machine Translation (SMT) system constructed from freely available tools. According to a wide variety of automatic evaluation metrics, they demonstrated that their EBMT system outperformed the SMT system by a factor of two to one. Nevertheless, they did not test their EBMT system against a phrase-based SMT system. Obtaining their training and test data for English-French, we carry out a number of experiments using the Pharaoh SMT Decoder. While better results are seen when Pharaoh is seeded with Giza++ word-and phrase-based data compared to EBMT sub-sentential alignments, in general better results are obtained when combinations of this 'hybrid' data is used to construct the translation and probability models. While for the most part the EBMT system of (Gough & Way, 2004b) outperforms any flavour of the phrasebased SMT systems constructed in our experiments, combining the data sets automatically induced by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French-English. 1 Introduction (Way and Gough, 2005) provide what are to our knowledge the first published results comparing Example-Based and Statistical models of Machine Translation (MT). Given that most MT research carried out today is corpus-based, it is somewhat surprising that until quite recently no qualitative research existed on the relative performance of the two approaches. This may be due to a number of factors: the relative unavailability of EBMT systems, the lack of participation of EBMT researchers in competitive evaluations or the dominance in the MT research community of the SMT approach-whenever one paradigm finds favour with the clear majority of MT practitioners, the assumption made by most of the community is that this way of doing things is clearly better than the alternatives. Like (Way and Gough, 2005) , we find this regrettable: the only basis on which such views should be allowed to permeate our field is following extensive testing and evaluation. Nonetheless, given that no EBMT systems are freely available, very few research groups are in the position of being able to carry out such work. This paper extends the work of (Way and Gough, 2005) by testing EBMT against phrase-based models of SMT, rather than the word-based models used in this previous work. In so doing, it provides a more complete evaluation of the main question at hand, namely whether an SMT system outperforms an EBMT system on reasonably large training and test sets. We obtained the same training and test data used in (Way and Gough, 2005) , and evaluated a number of SMT systems which use the Pharaoh decoder 1 against the Marker-Based EBMT system of (Gough & Way, 2004b) , for French-English and English-French. We provide results using a range of automatic evaluation metrics: BLEU (Papineni et al., 2002) , Precision and Recall (Turian et al., 2003) , and Word-and Sentence Error Rates. (Way and Gough, 2005) observe that EBMT tends to outperform a word-based SMT model, and our experiments show that a number of different phrase-based SMT systems still tend to fall short of the quality obtained via EBMT for these evaluation metrics. However, when Pharaoh is seeded with the data sets automatically induced by both Giza++ and their EBMT system, better results are seen for French-English than for the EBMT system per se. The remainder of the paper is constructed as follows. In section 2, we summarize the main ideas behind typical models of SMT and EBMT, as well as the EBMT system of (Gough & Way, 2004b) used in our experiments. In section 3, we revisit the experiments and results carried out by (Way and Gough, 2005) . In section 4, we describe our extensions to their work, and compare their findings to ours, and in section 5, present a number of hybrid SMT models. Finally, we conclude and offer some thoughts for future work in section 6, and in section 7 present some further comments on the narrowing gap between EBMT and phrase-based SMT. Example-Based and Statistical Models of Translation A sine qua non for both EBMT and SMT is a set of sentences in one language aligned with their translations in another. Although similar in that both models of translation automatically induce translation knowledge from this resource, there are significant differences regarding both the type of information learnt and how this is brought to bear in dealing with new input. EBMT Given a new input string, EBMT models use three separate processes in order to derive translations: 1 http://www.isi.edu/licensed-sw/pharaoh/ 1. Searching the source side of the bitext for 'close' matches and their translations; 2. Determining the sub-sentential translation links in those retrieved examples; 3. Recombining relevant parts of the target translation links to derive the translation. Searching for the best matches involves determining a similarity metric based on word occurrences and part-of-speech labels, generalised templates and bilingual dictionaries. The recombination process depends on the nature of the examples used in the first place, which may include aligning phrasestructure (sub-)trees (Hearne & Way, 2003) or dependency trees (Watanabe et al., 2003) , or using placeables (Brown, 1999) as indicators of chunk boundaries. Another method-and the one used in the EBMT system used in our experiments-is to use a set of closed-class words to segment aligned source and target sentences and to derive an additional set of lexical and phrasal resources. (Gough & Way, 2004b) base their work on the 'Marker Hypothesis' (Green, 1979) , a universal psycholinguistic constraint which posits that languages are 'marked' for syntactic structure at surface level by a closed set of specific lexemes and morphemes. In a preprocessing stage, (Gough & Way, 2004b) use 7 sets of marker words for English and French (e.g. determiners, quantifiers, conjunctions etc.) , which together with cognate matches and mutual information scores are used to derive three new data sources: sets of marker chunks, generalised templates and a lexicon. In order to describe this in more detail, we revisit an example from (Gough & Way, 2004a) , namely: (1) each layer has a layer number =⇒chaque couche a un nombre de la couche From the sentence pair in (1), the strings in (2) are generated, where marker words are automatically tagged with their marker categories: (2) <QUANT> each layer has <DET> a layer number =⇒<QUANT> chaque couche a <DET> un nombre <PREP> de la couche Taking into account marker tag information (label, and relative sentence position), and lexical similarity, the marker chunks in (3) are automatically generated from the marker-tagged strings in (2): (3) a. <QUANT> each layer has: <QUANT> chaque couche a b. <DET> a layer number: <DET> un nombre de la couche (3b) shows that n:m alignments are possible (the two French marker chunks un nombre and de la couche are absorbed into one following the lexical similarities between layer and couche and number and nombre, respectively) given the sub-sentential alignment algorithm of (Gough & Way, 2004b) . By generalising over the marker lexicon, a set of marker templates is produced by replacing the marker word by its relevant tag. From the examples in (3), the generalised templates in (4) are derived: (4) a. <QUANT> layer has: <QUANT> couche a b. <DET> layer number: <DET> nombre de la couche These templates increase the robustness of the system and make the matching process more flexible. Now any marker word can be inserted after the relevant tag if it appears with its translation in the lexicon, so that (say) the layer number can now be handled by the generalised template in (4b) and inserting a (or all) translation(s) for the in the system's lexicon. Word-and Phrase-Based SMT SMT systems require two large probability tables in order to generate translations of new input: 1. a translation model induced from a large amount of bilingual data; 2. a target language model induced from a(n even) large(r) quantity of separate monolingual text. Essentially, the translation model establishes the set of target language words (and more recently, phrases) which are most likely to be useful in translating the source string, while the language model tries to assemble these words (and phrases) in the most likely target word order. The language model is trained by determining all bigram and/or trigram frequency distributions occurring in the training data, while the translation model takes into account source and target word (and phrase) co-occurrence frequencies, sentence lengths and the relative sentence positions of source and target words. Until quite recently, SMT models of translation were based on the simple word alignment models of (Brown et al., 1990) . Nowadays, however, SMT practitioners also get their systems to learn phrasal as well as lexical alignments (e.g. (Koehn et al., 2003) ; (Och, 2003) ). Unsurprisingly, the quality obtained by today's phrase-based SMT systems is considerably better than that obtained by the poorer word-based models. 3 Comparing EBMT and Word-Based SMT (Way and Gough, 2005) obtained a large translation memory from Sun Microsystems containing 207,468 English-French sentence pairs, of which 3,939 sentence pairs were randomly extracted as a test set, with the remaining 203,529 sentences used as training data. The average sentence length for the English test set was 13.1 words and 15.2 words for the corresponding French test set. The EBMT system used was their Marker-based system as described in section 2.1 above. In order to create the necessary SMT language and translation models, they used: • Giza++ (Och & Ney, 2003) ; 2 • the CMU-Cambridge statistical toolkit; 3 • the ISI ReWrite Decoder. 4   Translation was performed from English-French and French-English, and the resulting translations were evaluated using a range of automatic metrics: BLEU (Papineni et al., 2002) , Precision and Recall (Turian et al., 2003) , and Word-and Sentence Error Rates. In order to see whether the amount of training data affected the (relative) performance of the EBMT and SMT systems, (Way and Gough, 2005) split the training data into three sets, of 50K (1.1M words), 100K (2.4M words) and 203K (4.8M words) sentence pairs (TS1-TS3 in what follows). English-French Results Table 1 : Comparing the EBMT system of (Gough & Way, 2004b ) with a Word-Based SMT (WB-SMT) system for English-French. The results obtained by (Gough & Way, 2004b ) for English-French for their EBMT system and word-based SMT (WB-SMT) are given in Table 1 . Essentially, all the automatic evaluation metrics bar one (Precision) suggest that EBMT can outperform SMT from English-French. Surprisingly, however, apart from SER, all evaluation scores are higher using 100K sentence pairs as training data rather than the full 203K sentences. It is generally assumed that increasing the size of the training data for corpusbased MT systems will improve the quality of the output translations. (Way and Gough, 2005) observe that while this dip in performance may be due to a degree of over-fitting, they intend to carry out some variance analysis on these results (e.g. performing bootstrap-resampling on the test set (Koehn, 2004 )), or re-test with different sample test sets in order to investigate whether the same phenomenon is observed. BLEU With respect to SER, however, for both SMT and EBMT, the figures improve as more training data is made available. However, the improvement is much more significant for EBMT (20.6%) than for SMT (0.1%). While the WER scores are much the same, indicating that both systems are identifying reasonable target vocabulary that should appear in the output translation, the vast differences in SER using TS3 indicate that a system containing essentially no information about target syntax has very little hope of arranging these target words in the right order. On the contrary, even a system containing some basic knowledge of how phrases fit together such as the Marker-based EBMT system of (Gough & Way, 2004b) will generate translations of far higher quality. French-English Results Table 2: Comparing the EBMT system of (Gough & Way, 2004b ) with a WB-SMT system for French-English. The results obtained by (Way and Gough, 2005) for French-English translations are presented in Table 2. Translating in this language direction is inherently 'easier' than for English-French as far fewer agreement errors and cases of boundary friction are likely. Accordingly, all WB-SMT results in Table 2 are better than for the reverse direction, while for EBMT, improved results are to be seen for BLEU, Recall and SER. BLEU While the majority of metrics obtained for English-French indicate that EBMT outperforms WB-SMT, the results for French-English are by no means as conclusive. Of the 15 tests, WB-SMT outperforms EBMT in nine. Comparing EBMT and Phrase-Based SMT From the results in the previous sections for French-English and for English-French, (Way and Gough, 2005) observe that EBMT outperforms WB-SMT in the majority of tests. If we are to treat each of the metrics as being equally significant, it can be said that EBMT appears to outperform WB-SMT by a factor of two to one. In fact, the only metric for which EBMT seems to consistently underperform is precision for French-English which, when we examine WER, indicates that the EBMT system's knowledge of word correspondences is incomplete and not as comprehensive as that of the WB-SMT system. However, it has been apparent for some time now that phrase-based SMT outperforms previous systems using word-based models. The results obtained by (Way and Gough, 2005) for SER also indicate that if phrase-based SMT were used, then improvements in translation quality ought to be seen. Accordingly, in this section we describe a set of experiments which extends the work of (Way and Gough, 2005) by evaluating the Marker-based EBMT system of (Gough & Way, 2004b ) against a phrase-based SMT system built using the following components: • Giza++, to extract the word-level correspondences; • The Giza++ word alignments are then refined and used to extract phrasal alignments ( (Och & Ney, 2003) ; or (Koehn et al., 2003) for a more recent implementation); • Probabilities of the extracted phrases are calculated from relative frequencies; • The resulting phrase translation table is passed to the Pharaoh phrase-based SMT decoder which along with SRI language modelling toolkit 5 performs translation. We seeded the phrase-based SMT system constructed from the publicly available resources listed above with the word-and phrase-alignments derived via both Giza++ and the Marker-Based EBMT system of (Gough & Way, 2004b) . Using the full 203K training set of (Gough & Way, 2004b) , and testing on their near 4K test set, the results are given in Table 3. It is clear to see that the Giza++ alignments obtain better scores than the EBMT sub-sentential data. Before one considers the full impact of these results, one should take into account that the size of the EBMT data set (word-and phrase-alignments) is 403,317, while there are over four times as many SMT sub-sentential alignments (1, 732, 715) . English-French Results Comparing these results with those in Table 1 , we can see that for the same training-test data, the phrase-based SMT system outperforms the WB-SMT system on most metrics, considerably so with respect to BLEU score (.3753 vs. .3223) . WER, however, is somewhat worse (.585 vs. .535), and SER remains disappointingly high. Compared to the EBMT system of (Gough & Way, 2004b) , the phrase-based SMT system still falls well short with respect to BLEU score (.4409 for EBMT vs. .3573 for SMT), and again, notably for SER (.656 EBMT, .868 SMT). Again, the phrase-based SMT system was seeded with the Giza++ and EBMT alignments, trained on the full 203K training set, and tested on the 4K test set. The results are given in Table 4 . As for English-French, the Giza++ alignments obtain better scores than when the EBMT sub-sentential data is used. French-English Results Comparing these results with those in Table 2 , we see that the phrase-based SMT system actually does worse than WB-SMT, which is an unexpected result 6 . As expected, therefore, the results for phrasebased SMT here are worse still compared to EBMT. Towards Hybridity: Merging SMT and EBMT Alignments We decided to experiment further by combining parts of the EBMT sub-sentential alignments with parts of the data induced by Giza++. In the following sections, for both English-French and French-English, we seed the Pharaoh phrase-based SMT system with: 1. the EBMT phrase-alignments with the Giza++ word-alignments; 2. all the EBMT and Giza++ sub-sentential alignments (both words and phrases). Giza++ Words and EBMT Phrases Here we seeded Pharaoh with the word-alignments induced by Giza++ and the EBMT phrasal chunks only (i.e. no Giza++ phrases and no EBMT lexical alignments). Using the full 203K training set of (Gough & Way, 2004b) , and testing on their near 4K test set, the results are given in Table 5 . Comparing these figures to those in Table 3 , we can see that all automatic evaluation metrics improve with this hybrid system configuration. Note that the data set size is 430,336, compared to 1.73M for the phrase-based SMT system seeded solely with Giza++ alignments. With respect to the EBMT system per se in Table 1 , these results remain slightly below those figures (except for precision). Running the same experimental set up for the reverse language direction gives the results in Table 6 . While recall drops slightly, all the other metrics show a slight increase compared to the performance obtained when Pharaoh is seeded with Giza++ wordand phrase-alignments (cf. Table 4 ). English-French Results French-English Results Merging All Data The following two experiments were carried out by seeding Pharaoh with all the EBMT and Giza++ sub-sentential alignments, i.e. both words and phrases. Inserting all Giza++ and EBMT data into Pharaoh's knowledge sources gives the results in Table 7. These are considerably better than the scores for the 'semi-hybrid' system described in section 5.1.1. This indicates that a phrase-based SMT system is likely to perform better when EBMT wordand phrase-alignments are used in the calculation of the translation and target language probability models. Note, however, that the size of the data set increases to over 2M items. Despite this, compared to the results for the EBMT system of (Gough & Way, 2004b) shown in Table 1 , these results for the 'fully hybrid' SMT system still fall somewhat short (except for Precision: .6727 vs. .7026). Carrying out a similar experiment for the reverse language direction gives the results in Table 8 . This time this hybrid SMT system does outperform the EBMT system of (Gough & Way, 2004b) , with respect to BLEU score (.4888 vs .4611) and Precision (.6927 vs. 6782), but the EBMT system still wins out where Recall, WER and SER are concerned. Regarding this latter, it seems that the correlation between low SER and high BLEU score is not as important as is claimed in (Way and Gough, 2005) . English-French Results French-English Results 6 Conclusions (Way and Gough, 2005) carried out a number of experiments designed to test their large-scale Marker-Based EBMT system described in (Gough & Way, 2004b) against a WB-SMT system constructed from publicly available tools. While the results were a little mixed, the EBMT system won out overall. Nonetheless, WB-SMT has long been abandoned in favour of phrase-based models. We extended the work of (Way and Gough, 2005) by performing a range of experiments using the Pharaoh phrasebased decoder. Our main observations are as follows: • Seeding Pharaoh with word-and phrasealignments induced via Giza++ generates better results than if EBMT sub-sentential data is used. • Seeding Pharaoh with a 'hybrid' dataset of Giza++ word alignments and EBMT phrases improves over the baseline phrase-based SMT system primed solely with Giza++ data. This would appear to indicate that the quality of the EBMT phrases is better than the SMT phrases, and that SMT practitioners should use EBMT phrasal data in the calculating of their language and translation models, if available. • Seeding Pharaoh with all data induced by Giza++ and the EBMT system leads to the bestperforming hybrid SMT system: for English-French, as well as EBMT phrasal data, EBMT word alignments also contribute positively, but the EBMT system per se still wins out (except for Precision); for French-English, however, our hybrid Example-Based SMT system outperforms the EBMT system of (Gough & Way, 2004b ) (cf. Table 9 ). A number of avenues of further work remain open to us. We would like to extend our investigations into hybrid example-based statistical approaches to machine translation by experiment with seeding the Marker-Based system of (Gough & Way, 2004b) with the SMT data, and combinations thereof with the EBMT sub-sentential alignments, to investigate the effect on translation quality. Given our findings here, we are optimistic that 'hybrid statistical EBMT' will outperform the baseline EBMT system, and that our findings will prompt EBMT practitioners to augment their data resources with SMT alignments, something which to our knowledge is currently not done. In addition, we intend to continue this line of research on different and larger data sets, and for other language pairs. Final Remarks Finally, as (Way and Gough, 2005) observe, it is difficult to explain why to this day SMT practitioners have not made full use of the large body of existing work on EBMT, from (Nagao, 1984) to (Carl & Way, 2003) and beyond, which has contributed greatly to the field of corpus-based MT. From its very inception EBMT has made use of a range of sub-sentential data -both phrasal and lexical -to perform translations whereas, until quite recently, SMT models of translation were based on the relatively simple word alignment models of (Brown et al., 1990) . With the advent of phrase-based SMT systems the line between EBMT and SMT has become significantly blurred, yet we are still unaware of any papers on SMT which acknowledge their debt to EBMT or which describe their approach as 'example-based'. Despite it becoming increasingly difficulty to distinguish between EBMT and (phrase-based) SMT models of translation, some differences still exist. Rather than using models of syntax in a post hoc fashion, as is the case with most SMT systems, an EBMT model of translation builds in syntax at its core. Given this, a phrase-based SMT system is more likely to 'learn' chunks that an EBMT system would not, as the system learns n-gram sequences rather than syntactically-motivated phrases per se. Furthermore, our research here has demonstrated quite clearly that if available, merging SMT and EBMT data improves the quality of the resulting hybrid SMT system, as phrases extracted by both methods that are more likely to function as syntactic units (and therefore be more beneficial during the translation process) are given a higher statistical significance. Conversely, the probabilities of those 'less useful' SMT n-grams that are not also gener-ated by the EBMT system are reduced. Essentially, the EBMT data helps the SMT system to make the best use of phrase alignments during translation. Moreover, we see the fact that it is becoming increasingly difficult to describe the differences between EBMT and SMT as a good thing, and that as here, this convergence can lead to hybrid systems capable of outperforming leading EBMT systems as well as state-of-the-art phrase-based SMT. We hope that the research presented here, together with that begun by (Way and Gough, 2005) , will lead to new areas of collaboration between both sets of researchers, to the clear benefit of the MT research community and the wider public. Acknowledgements We would like to thank Nano Gough for supplying us with our EBMT training data. Thanks also to three anonymous reviewers for their insightful comments. The work presented in this paper is partly supported by an IRCSET 7 PhD Fellowship Award.",
         "3011925",
         "cd0ec1b847811191c0cc4684fbe54ea997c938d9",
         "60",
         "https://aclanthology.org/W05-0833",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Groves, Declan  and\nWay, Andy",
         "Hybrid Example-Based {SMT}: the Best of Both Worlds?",
         "183--190",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "groves-way-2005-hybrid",
         null,
         null
        ],
        [
         "18",
         "W05-0834",
         "Word graphs have various applications in the field of machine translation. Therefore it is important for machine translation systems to produce compact word graphs of high quality. We will describe the generation of word graphs for state of the art phrase-based statistical machine translation. We will use these word graph to provide an analysis of the search process. We will evaluate the quality of the word graphs using the well-known graph word error rate. Additionally, we introduce the two novel graph-to-string criteria: the position-independent graph word error rate and the graph BLEU score. Experimental results are presented for two Chinese-English tasks: the small IWSLT task and the NIST large data track task. For both tasks, we achieve significant reductions of the graph error rate already with compact word graphs.",
         "Word graphs have various applications in the field of machine translation. Therefore it is important for machine translation systems to produce compact word graphs of high quality. We will describe the generation of word graphs for state of the art phrase-based statistical machine translation. We will use these word graph to provide an analysis of the search process. We will evaluate the quality of the word graphs using the well-known graph word error rate. Additionally, we introduce the two novel graph-to-string criteria: the position-independent graph word error rate and the graph BLEU score. Experimental results are presented for two Chinese-English tasks: the small IWSLT task and the NIST large data track task. For both tasks, we achieve significant reductions of the graph error rate already with compact word graphs. Introduction A statistical machine translation system usually produces the single-best translation hypotheses for a source sentence. For some applications, we are also interested in alternative translations. The simplest way to represent these alternatives is a list with the N -best translation candidates. These N -best lists have one major disadvantage: the high redundancy. The translation alternatives may differ only by a single word, but still both are listed completely. Usually, the size of the N -best list is in the range of a few hundred up to a few thousand candidate translations per source sentence. If we want to use larger N -best lists the processing time gets very soon infeasible. Word graphs are a much more compact representation that avoid these redundancies as much as possible. The number of alternatives in a word graph is usually an order of magnitude larger than in an Nbest list. The graph representation avoids the combinatorial explosion that make large N -best lists infeasible. Word graphs are an important data structure with various applications: • Word Filter. The word graph is used as a compact representation of a large number of sentences. The score information is not contained. • Rescoring. We can use word graphs for rescoring with more sophisticated models, e.g. higher-order language models. • Discriminative Training. The training of the model scaling factors as described in (Och and Ney, 2002) was done on N -best lists. Using word graphs instead could further improve the results. Also, the phrase translation probabilities could be trained discrimatively, rather than only the scaling factors. • Confidence Measures. Word graphs can be used to derive confidence measures, such as the posterior probability (Ueffing and Ney, 2004) . • Interactive Machine Translation. Some interactive machine translation systems make use of word graphs, e.g. (Och et al., 2003) . State Of The Art. Although there are these many applications, there are only few publications directly devoted to word graphs. The only publication, we are aware of, is (Ueffing et al., 2002) . The shortcomings of (Ueffing et al., 2002) are: • They use single-word based models only. Current state of the art statistical machine translation systems are phrase-based. • Their graph pruning method is suboptimal as it considers only partial scores and not full path scores. • The N -best list extraction does not eliminate duplicates, i.e. different paths that represent the same translation candidate. • The rest cost estimation is not efficient. It has an exponential worst-case time complexity. We will describe an algorithm with linear worstcase complexity. Apart from (Ueffing et al., 2002) , publications on weighted finite state transducer approaches to machine translation, e.g. (Bangalore and Riccardi, 2001; Kumar and Byrne, 2003) , deal with word graphs. But to our knowledge, there are no publications that give a detailed analysis and evaluation of the quality of word graphs for machine translation. We will fill this gap and give a systematic description and an assessment of the quality of word graphs for phrase-based machine translation. We will show that even for hard tasks with very large vocabulary and long sentences the graph error rate drops significantly. The remaining part is structured as follows: first we will give a brief description of the translation system in Section 2. In Section 3, we will give a definition of word graphs and describe the generation. We will also present efficient pruning and N -best list extraction techniques. In Section 4, we will describe evaluation criteria for word graphs. We will use the graph word error rate, which is well known from speech recognition. Additionally, we introduce the novel position-independent word graph error rate and the graph BLEU score. These are generalizations of the commonly used string-to-string evaluation criteria in machine translation. We will present experimental results in Section 5 for two Chinese-English tasks: the first one, the IWSLT task, is in the domain of basic travel expression found in phrasebooks. The vocabulary is limited and the sentences are short. The second task is the NIST Chinese-English large data track task. Here, the domain is news and therefore the vocabulary is very large and the sentences are with an average of 30 words quite long. Translation System In this section, we give a brief description of the translation system. We use a phrase-based translation approach as described in (Zens and Ney, 2004) . The posterior probability P r(e I 1 |f J 1 ) is modeled directly using a weighted log-linear combination of a trigram language model and various translation models: a phrase translation model and a wordbased lexicon model. These translation models are used for both directions: p(f |e) and p(e|f ). Additionally, we use a word penalty and a phrase penalty. With the exception of the language model, all models can be considered as within-phrase models as they depend only on a single phrase pair, but not on the context outside of the phrase. The model scaling factors are optimized with respect to some evaluation criterion (Och, 2003) . We extended the monotone search algorithm from (Zens and Ney, 2004 ) such that reorderings are possible. In our case, we assume that local reorderings are sufficient. Within a certain window, all possible permutations of the source positions are allowed. These permutations are represented as a reordering graph, similar to (Zens et al., 2002) . Once we have this reordering graph, we perform a monotone phrase-based translation of this graph. More details of this reordering approach are described in (Kanthak et al., 2005) . Word Graphs Definition A word graph is a directed acyclic graph G = (V, E) with one designated root node n 0 ∈ V . The edges are labeled with words and optionally with scores. We will use (n, n , w) to denote an edge from node n to node n with word label w. Each path through the word graph represents a translation candidate. If the word graph contains scores, we accumulate the edge scores along a path to get the sentence or string score. The score information the word graph has to contain depends on the application. If we want to use the word graph as a word filter, we do not need any score information at all. If we want to extract the single-or N -best hypotheses, we have to retain the string or sentence score information. The information about the hidden variables of the search, e.g. the phrase segmentation, is not needed for this purpose. For discriminative training of the phrase translation probabilities, we need all the information, even about the hidden variables. Generation In this section, we analyze the search process in detail. Later, in Section 5, we will show the (experimental) complexity of each step. We start with the source language sentence that is represented as a linear graph. Then, we introduce reorderings into this graph as described in (Kanthak et al., 2005) . The type of reordering should depend on the language pair. In our case, we assume that only local reorderings are required. Within a certain window, all possible reorderings of the source positions are allowed. These permutations are represented as a reordering graph, similar to (Knight and Al-Onaizan, 1998) and (Zens et al., 2002) . Once we have this reordering graph, we perform a monotone phrase-based translation of this graph. This translation process consists of the following steps that will be described afterward: 1. segment into phrase 2. translate the individual phrases 3. split the phrases into words 4. apply the language model Now, we will describe each step. The first step is the segmentation into phrases. This can be imagined as introducing \"short-cuts\" into the graph. The phrase segmentation does not affect the number of nodes, because only additional edges are added to the graph. In the segmented graph, each edge represents a source phrase. Now, we replace each edge with one edge for each possible phrase translation. The edge scores are the combination of the different translation probabilities, namely the within-phrase models mentioned in Section 2. Again, this step does not increase the number of nodes, but only the number of edges. So far, the edge labels of our graph are phrases. In the final word graph, we want to have words as edge labels. Therefore, we replace each edge representing a multi-word target phrase with a sequence of edges that represent the target word sequence. Obviously, edges representing a single-word phrase do not have to be changed. As we will show in the results section, the word graphs up to this point are rather compact. The score information in the word graph so far consists of the reordering model scores and the phrase translation model scores. To obtain the sentence posterior probability p(e I 1 |f J 1 ), we apply the target language model. To do this, we have to separate paths according to the language model history. This increases the word graph size by an order of magnitude. Finally, we have generated a word graph with full sentence scores. Note that the word graph may contain a word sequence multiple times with different hidden variables. For instance, two different segmentations into source phrases may result in the same target sentence translation. The described steps can be implemented using weighted finite state transducer, similar to (Kumar and Byrne, 2003) . Pruning To adjust the size of the word graph to the desired density, we can reduce the word graph size using forward-backward pruning, which is well-known in the speech recognition community, e.g. see (Mangu et al., 2000) . This pruning method guarantees that the good strings (with respect to the model scores) remain in the word graph, whereas the bad ones are removed. The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002) . The forward probabilities F (n) and backward probabilities B(n) of a node n are defined by the following recursive equations: F (n) = (n ,n,w)∈E F (n ) • p(n , n, w) B(n) = (n,n ,w)∈E B(n ) • p(n, n , w) The forward probability of the root node and the backward probabilities of the final nodes are initialized with one. Using a topological sorting of the nodes, the forward and backward probabilities can be computed with linear time complexity. The posterior probability q(n, n , w) of an edge is defined as: q(n, n , w) = F (n) • p(n, n , w) • B(n ) B(n 0 ) The posterior probability of an edge is identical to the sum over the probabilities of all full paths that contain this edge. Note that the backward probability of the root node B(n 0 ) is identical to the sum over all sentence probabilities in the word graph. Let q * denoted the maximum posterior probability of all edges and let τ be a pruning threshold, then we prune an edge (n, n , w) if: q(n, n , w) < q * • τ N -Best List Extraction In this section, we describe the extraction of the Nbest translation candidates from a word graph. (Ueffing et al., 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea: use a modified A* algorithm with an optimal rest cost estimation. As rest cost estimation, the negated logarithm of the backward probabilities is used. The algorithm in (Ueffing et al., 2002) has two disadvantages: it does not care about duplicates and the rest cost computation is suboptimal as the described algorithm has an exponential worst-case complexity. As mentioned in the previous section, the backward probabilities can be computed in linear time. In (Mohri and Riley, 2002) the word graph is represented as a weighted finite state automaton. The word graph is first determinized, i.e. the nondeterministic automaton is transformed in an equivalent deterministic automaton. This process removes the duplicates from the word graph. Out of this determinized word graph, the N best candidates are extracted. In (Mohri and Riley, 2002) , -transitions are ignored, i.e. transitions that do not produce a word. These -transitions usually occur in the backing-off case of language models. The -transitions have to be removed before using the algorithm of (Mohri and Riley, 2002) . In the presence of -transitions, two path representing the same string are considered equal only if the -transitions are identical as well. Evaluation Criteria String-To-String Criteria To evaluate the single-best translation hypotheses, we use the following string-to-string criteria: word error rate (WER), position-independent word error rate (PER) and the BLEU score. More details on these standard criteria can be found for instance in (Och, 2003) . Graph-To-String Criteria To evaluate the quality of the word graphs, we generalize the string-to-string criteria to work on word graphs. We will use the well-known graph word error rate (GWER), see also (Ueffing et al., 2002) . Additionally, we introduce two novel graphto-string criteria, namely the position-independent graph word error rate (GPER) and the graph BLEU score (GBLEU). The idea of these graph-to-string criteria is to choose a sequence from the word graph and compute the corresponding string-to-string criterion for this specific sequence. The choice of the sequence is such that the criterion is the optimum over all possible sequences in the word graph, i.e. the minimum for GWER/GPER and the maximum for GBLEU. The GWER is a generalization of the word error rate. It is a lower bound for the WER. It can be computed using a dynamic programming algorithm which is quite similar to the usual edit distance computation. Visiting the nodes of the word graph in topological order helps to avoid repeated computations. The GPER is a generalization of the positionindependent word error rate. It is a lower bound for the PER. The computation is not as straightforward as for the GWER. In (Ueffing and Ney, 2004) , a method for computing the string-to-string PER is presented. This method cannot be generalized for the graph-to-string computation in a straightforward way. Therefore, we will first describe an alternative computation for the string-to-string PER and then use this idea for the graph-to-string PER. Now, we want to compute the number of positionindependent errors for two strings. As the word order of the strings does not matter, we represent them as multisets 1 A and B. To do this, it is sufficient to know how many words are in A but not in B, i.e. a := |A − B|, and how many words are in B but not in A, i.e. b := |B −A|. The number of substitutions, insertions and deletions are then: sub = min{a, b} ins = a − sub del = b − sub error = sub + ins + del = a + b − min{a, b} = max{a, b} It is obvious that there are either no insertions or no deletions. The PER is then computed as the number of errors divided by the length of the reference string. Now, back to the graph-to-string PER computation. The information we need at each node of the word graph are the following: the remaining multiset of words of the reference string that are not yet produced. We denote this multiset C. The cardinality of this multiset will become the value a in the preceding notation. In addition to this multiset, we also need to count the number of words that we have produced on the way to this node but which are not in the reference string. The identity of these words is not important, we simply have to count them. This count will become the value b in the preceding notation. If we make a transition to a successor node along an edge labeled w, we remove that word w from the set of remaining reference words C or, if the word w is not in this set, we increase the count of words that are in the hypothesis but not in the reference. To compute the number of errors on a graph, we use the auxiliary quantity Q(n, C), which is the count of the produced words that are not in the reference. We use the following dynamic programming recursion equations: 1 A multiset is a set that may contain elements multiple times. Q(n 0 , C 0 ) = 0 Q(n, C) = min n ,w:(n ,n,w)∈E Q(n , C ∪ {w}), Q(n , C) + 1 Here, n 0 denote the root node of the word graph, C 0 denotes the multiset representation of the reference string. As already mentioned in Section 3.1, (n , n, w) denotes an edge from node n to node n with word label w. In the implementation, we use a bit vector to represent the set C for efficiency reasons. Note that in the worst-case the size of the Q-table is exponential in the length of the reference string. However, in practice we found that in most cases the computation is quite fast. The GBLEU score is a generalization of the BLEU score. It is an upper bound for the BLEU score. The computation is similar to the GPER computation. We traverse the word graph in topological order and store the following information: the counts of the matching n-grams and the length of the hypothesis, i.e. the depth in the word graph. Additionally, we need the multiset of reference n-grams that are not yet produced. To compute the BLEU score, the n-gram counts are collected over the whole test set. This results in a combinatorial problem for the computation of the GBLEU score. We process the test set sentence-wise and accumulate the n-gram counts. After each sentence, we take a greedy decision and choose the ngram counts that, if combined with the accumulated n-gram counts, result is the largest BLEU score. This gives a conservative approximation of the true GBLEU score. Word Graph Size To measure the word graph size we use the word graph density, which we define as the number of edges in the graph divided by the source sentence length. Experimental Results Tasks We will show experimental results for two Chinese-English translation tasks. IWSLT Chinese-English Task. The first task is the Chinese-English supplied data track task of the International Workshop on Spoken Language Translation (IWSLT 2004) (Akiba et al., 2004) . The domain is travel expressions from phrase-books. This is a small task with a clean training and test corpus. The vocabulary is limited and the sentences are relatively short. The corpus statistics are shown in Table 1. The Chinese part of this corpus is already segmented into words. NIST Chinese-English Task. The second task is the NIST Chinese-English large data track task. For this task, there are many bilingual corpora available. The domain is news, the vocabulary is very large and the sentences have an average length of 30 words. We train our statistical models on various corpora provided by LDC. The Chinese part is segmented using the LDC segmentation tool. After the preprocessing, our training corpus consists of about three million sentences with somewhat more than 50 million running words. The corpus statistics of the preprocessed training corpus are shown in Table 2 . We use the NIST 2002 evaluation data as test set. Search Space Analysis In Table 3 , we show the search space statistics of the IWSLT task for different reordering window sizes. Each line shows the resulting graph densities after the corresponding step in our search as described in Section 3.2. Our search process starts with the reordering graph. The segmentation into phrases increases the graph densities by a factor of two. Doing the phrase translation results in an increase of the densities by a factor of twenty. Unsegmenting the phrases, i.e. replacing the phrase edges with a sequence of word edges doubles the graph sizes. Applying the language model results in a significant increase of the word graphs. Another interesting aspect is that increasing the window size by one roughly doubles the search space. Word Graph Error Rates In Figure 1 , we show the graph word error rate for the IWSLT task as a function of the word graph density. This is done for different window sizes for the reordering. We see that the curves start with a single-best word error rate of about 50%. For the monotone search, the graph word error rate goes down to about 31%. Using local reordering during the search, we can further decrease the graph word error rate down to less than 17% for a window size of 5. This is almost one third of the single-best word error rate. If we aim at halving the single-best word error rate, word graphs with a density of less than  In Figure 2 , we show the same curves for the NIST task. Here, the curves start from a single-best word error rate of about 64%. Again, dependent on the amount of reordering the graph word error rate goes down to about 36% for the monotone search and even down to 23% for the search with a window of size 5. Again, the reduction of the graph word error rate compare to the single-best error rate is dramatic. For comparison we produced an N -best list of size 10 000. The N -best list error rate (or oraclebest WER) is still 50.8%. A word graph with a density of only 8 has about the same GWER. In Figure 3 , we show the graph positionindependent word error rate for the IWSLT task. As this error criterion ignores the word order it is not affected by reordering and we show only one curve. We see that already for small word graph densities the GPER drops significantly from about 42% down to less than 14%.  In Figure 4 , we show the graph BLEU scores for the IWSLT task. We observe that, similar to the GPER, the GBLEU score increases significantly already for small word graph densities. We attribute this to the fact that the BLEU score and especially the PER are less affected by errors of the word order than the WER. This also indicates that producing translations with correct word order, i.e. syntactically well-formed sentences, is one of the major problems of current statistical machine translation systems. Conclusion We have described word graphs for statistical machine translation. The generation of word graphs during the search process has been described in detail. We have shown detailed statistics of the individual steps of the translation process and have given insight in the experimental complexity of each step. We have described an efficient and optimal pruning method for word graphs. Using these technique, we have generated compact word graphs for two Chinese-English tasks. For the IWSLT task, the graph error rate drops from about 50% for the singlebest hypotheses to 17% of the word graph. Even for the NIST task, with its very large vocabulary and long sentences, we were able to reduce the graph error rate significantly from about 64% down to 23%. Acknowledgment This work was partly funded by the European Union under the integrated project TC-Star (Technology and Corpora for Speech to Speech Translation, IST-2002-FP6-506738, http://www.tc-star.org).",
         "1659910",
         "27b6985d455ae63ef1042fec2a7c8e79a75954d0",
         "31",
         "https://aclanthology.org/W05-0834",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Zens, Richard  and\nNey, Hermann",
         "Word Graphs for Statistical Machine Translation",
         "191--198",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "zens-ney-2005-word",
         null,
         null
        ],
        [
         "19",
         "R13-1062",
         "Real-word errors or context sensitive spelling errors, are misspelled words that have been wrongly converted into another word of vocabulary. One way to detect and correct real-word errors is using Statistical Machine Translation (SMT), which translates a text containing some real-word errors into a correct text of the same language. In this paper, we improve the results of mentioned SMT system by employing some discourseaware features into a log-linear reranking method. Our experiments on a real-world test data in Persian show an improvement of about 9.5% and 8.5% in the recall of detection and correction respectively. Other experiments on standard English test sets also show considerable improvement of real-word checking results.",
         "Real-word errors or context sensitive spelling errors, are misspelled words that have been wrongly converted into another word of vocabulary. One way to detect and correct real-word errors is using Statistical Machine Translation (SMT), which translates a text containing some real-word errors into a correct text of the same language. In this paper, we improve the results of mentioned SMT system by employing some discourseaware features into a log-linear reranking method. Our experiments on a real-world test data in Persian show an improvement of about 9.5% and 8.5% in the recall of detection and correction respectively. Other experiments on standard English test sets also show considerable improvement of real-word checking results. 1 Introduction Kukich (1992) has categorized errors of a text into five categories: 1. isolated error 2. syntactic error 3. real-word error 4. discourse structure and 5. pragmatic error. In this paper, we focus on the third category, which is also referred as contextsensitive spelling error. This type of error includes misspelled words that are converted to another word of the dictionary (e.g., typing \"arm\" instead of \"are\" in the sentence \"we arm good\"). In order to detect and correct this kind of error, context analysis of the text is crucial. Here, we propose a language-independent method, which is based on a phrase-based Statistical Machine Translation (SMT). In this case, the input and output sentences are both in the same language and the input sentence contains some real-word errors. Phrase-based SMT is weak in handling longdistance dependencies between the sentence words. In order to capture this kind of dependencies, which affects detecting the correct candidate word, mentioned SMT is augmented with a discourse-aware reranking method for reranking the N-best results of SMT. Our work can be regarded as an extension of the method introduced by Ehsan and Faili (2013) , in which they use SMT to detect and correct the spelling errors of a document. But here, we use the N-best results of SMT as a candidate list for each erroneous word and rerank the list by using a discourse-aware reranking system which is just a log-linear ranker. Shortly, the contributions of this paper can be summarized as follow: The N-best results of SMT are regarded as a candidate list of suspicious word, which is reranked by using a discourse-aware reranking system. Two discourse-aware features are employed in a loglinear ranker. The keywords in whole document surrounding the erroneous sentence are considered as the context window. We have achieved about 5% improvement over the SMTbased approach in detection and correction recall and 1% in precision on English experiment. The state-of-the-art results are achieved for Persian context-sensitive spell checker respect to Fmeasure and Mean Reciprocal Rank metrics. This paper is organized as follows: Section 2 presents an overview of related works. In Section 3, we explain attributes of Persian language. In section 4, we will describe how to use SMT for generating candidate words. In Section 5, we discuss the approach for reranking the N-best result of SMT. Finally, we illustrate the experimental results and compare the results with the SMT-based approach. Related Works Most of the previous works in real-word error detection and correction are classified into two categories : 1. based-on statistical approaches (Bassil & Alwani, 2012 and 2. based-on separate resource such as WordNet (Fellbaum, 2010) in (Pedler, 2007) . Statistical methods use several features, such as N-gram models (Bassil & Alwani, 2012; Islam & Inkpen, 2009) , POS tagging (Golding & Schabes, 1996) , Bayesian classifiers (Gale, Church, & Yarowsky, 1992) , decision lists (Yarowsky, 1994) , Bayesian hybrid method (Golding, 1995) , latent semantic analysis (Jones & Martin, 1997) . The N-gram and POSbased method are combined by Golding and Schabes (1996) and a better result achieved. Pedler (2007) used WordNet as a separate resource to extract the semantic relations of the words. These methods consider fixed-length windows instead of the whole sentence as the context window. Most of these methods use confusion set for detecting real-word errors. The confusion set is a set of words that are confusable with the headword of the set. The words of the set are not necessarily confusable with each other (Faili, 2010) . When the error checker comes across one of the words in a confusion set, it should select an appropriate word in the sentence. A machinelearning method and the Winnow algorithm is proposed in (Golding & Roth, 1999) , to solve word disambiguities based-on surrounding words of the spelling errors. This method uses several features of surrounding words, such as POS tag. +/-10 words from the corresponding confusable word in confusion set are considered as the context window. Wilcox-O\"Hearn et al. ( 2008 ) report a reconsideration of the work of (Mays et al., 1991) . They use three different lengths for the context window. Also, they use 6, 10 and 14 words as the context window and accommodate all the trigrams that overlap with the words in the window. Some statistical methods use Google Web 1T N-gram data set to detect and select the best correct word for a real-word error (Bassil & Alwani, 2012; Islam & Inkpen, 2009) . Google Web 1T N-gram consists of N-gram word sequences, extracted from the World Wide Web. 5-gram and 3-gram are used in these papers, thus the context window in these methods is 9 and 5 words respectively. There are few spell checkers for Persian, such as the works presented by Ehsan and Faili (2013) ; Kashefi, Minaei-Bidgoli, and Sharifi (2010) . In Kashefi et al. (2010) , a new metric based-on string distance for Persian is presented to rank spelling suggestions. This ranking is based-on the effect of keyboard layout or on the typographical spelling errors. A language-independent approach based on a SMT framework is presented by (Ehsan & Faili, 2013) . This method achieved the state-of-the-art results for grammar checking and contextsensitive spell checking for Persian language. Here, we also use SMT as a candidate generator for spell checking of real word errors, but our approach is different from that work in the following causes: we consider the keywords of whole document as the context-aware features. SMT is used as a candidate generator. We train a log-linear reranking system as a post-processing system to rerank the candidate list. Our experiments on a real-world test data in Persian show an improvement of about 9.5% and 8.5% in the recall of detection and correction respectively over the method of Ehsan and Faili (2013) . Persian Language Persian or Farsi is an Indo-European language. It is mostly spoken in Iran, Afghanistan and Tajikistan with dialects Farsi, Dari and Tajik respectively. The Persian language has a rich morphology (Megerdoomian, 2000) in which words can be combined with a very large number of affixes. Combination, derivation, and inflection rules in Persian are uncertain (Lazard & Lyon, 1992; Mahootian, 2003) . The alphabet of Farsi is the same as Arabic with four additional letters. The alphabet contains 26 consonants and 6 vowels. Also there are some homophone and homograph letters. For example, \"‫ز‬ˮ, ‫,\"ذ\"‬ ‫\"ظ\"‬ and ‫\"ض\"‬ are homophones which all sound as \"/z\" and \"‫/\"ب‬b, \"‫/\"پ‬p, \"‫/\"ت‬t and \"‫/\"ث‬s are homograph letters which just differ in number and place of dots. These phonetic and graphical similarities cause many spelling errors. In the next section, we will describe how to use the SMT to detect context-sensitive spelling errors in a sentence and generate candidates. SMT as a Candidate generator SMT framework can be used to model contextsensitive spell checker, which translates a word that does not fit in a sentence with some suggestions for the suspicious word. SMT uses parallel corpora as the training data. It learns phrases of the language and some features such as phrase probability, reordering probability. In order to use SMT framework, a confusion set for each word is defined. Confusion set of a headword,w i is a set of words {w i1 ,w i2 ,…,w in }, in which each word w ij is a word that could be converted to w i with one editing operation of insertion, deletion, substitution or transposition. The Damerau-Levenshtein distance metric (Damerau, 1964) has been used for calculating the distance between two words. If their distance is lower than a pre-defined threshold, one editing operation, two words have been considered similar and then w j is added to the confusion set of w i . For example, confusable words in confusion set of the word ‫روز‬ ruz \"day\" are as follows: ‫روزه‬ ruze \"fast\", ‫روش‬ ravesh \"method\", ‫رود‬ rud \"river\", ‫روح‬ ruh \"spirit\". If E={w 1, w 2 ,…,w i ,…,w n } is a sentence and w i is a real-word error in the sentence, it could appear in several confusion sets, thus, there are several headwords as candidates for the suspicious word. In other words, each headword that has w i in its confusion set can be suggested as the correct word. To formulate this, consider C={ w 1, w 2 ,…,w i ' ,…,w n } is the correct sentence then w i ' is defined as follows (Ehsan & Faili, 2013) : 𝑤 𝑖 ′ = 𝑤 𝑖 𝑜𝑟 (𝑤 𝑗 ,0 𝑠𝑢𝑐 𝑡𝑎𝑡 ∃ 𝑗 ,𝑘 𝑤 𝑗 ,𝑘 = 𝑤 𝑖 ) (1) Equation ( 1 ) implies that the correct word, w i ' , is either w i or one of the headwords that contain w i . For each erroneous sentence E, which contains real-word error w i , we can define the Nbest candidate sentences 𝐶 as follows: 𝐶 = 𝑁 − 𝑎𝑟𝑔𝑚𝑎𝑥 𝐶 𝑃 𝐸 𝐶 𝑃 𝐶 𝑃(𝐸) (2) 2 ) is probability of occurring the erroneous sentence, which is constant for each candidate sentence and can be removed from Equation (2). P(E|C) can be defined as follows: P(E) in Equation ( 𝑃 𝐸|𝐶 = 𝑃 𝑤 1 , … , 𝑤 𝑖 , … , 𝑤 𝑛 𝑤 1 , … , 𝑤 𝑖 ′ , … , 𝑤 𝑛 (3) In Equation (3), each w is a word. In order to estimate 𝑃 𝐸|𝐶 in Equation (3) we can convert E and C from word base to phrase base, E = 𝑒 1 , 𝑒 2 , … , 𝑒 𝐼 and C = 𝑐 1 , 𝑐 2 , … , 𝑐 𝐼 . Using phrasebased SMT, we can capture some local dependencies among the words resulting better detection and correction on real-word errors. Let assume that w i is in j-th phrase of E, then, we can estimate 𝑃 𝐸|𝐶 as follows: 𝑃 𝐸|𝐶 = 𝑃 𝑒 𝑗 | 𝑐 𝑗 = 𝑐𝑜𝑢𝑛𝑡 (𝑒 𝑗 ,𝑐 𝑗 ) 𝑐𝑜𝑢𝑛𝑡 (𝑒 𝑗 ,𝑐 𝑗 ) 𝑒 𝑗 (4) Equation ( 4 ) is the same as phrasal translation model in phrasal SMT systems. Therefore, we can use a phrasal SMT to correct contextsensitive spelling errors. In this paper, Moses (Koehn et al., 2007) is used as the phrasal SMT. When using SMT as a context-sensitive spell checker, source and target sentences are in same language. The source sentences contain realword error while the target sentences contain their correct form. After generating candidate sentences by retrieving the N-best results of the mentioned SMT, we rerank the candidate list by discourse-aware features, which are described in next section. Discourse-aware Features For any given sentence, SMT-based approach retrieves a list of candidate sentences. The phrasal SMT does not take the whole context of the sentence into account. Thus, in order to find the correct sentence from the candidate list and obtain a better ranking, we define other features that indicate the affinity of each word in candidate sentences with the whole context. Both the sentence and the whole document are considered as the context of the candidate sentences. For example in the sentence: \"This cat is black.\", both \"cat\" and \"car\" could be meaningful. In this sentence, by considering just the sentence as context window, we cannot identify whether \"cat\" is correct or \"car\". Discourse analysis may help us to detect the best candidate. If we know the document is about automobile or animal, then we can have better reranking on candidates. In other word, considering whole document as the context window is more helpful than considering just whole sentence for reranking the candidate. Here, we get the benefit from discourse by capturing the relations among the words in a candidate sentence and with the keywords of whole document. In Subsection 5.1, we show that by selecting Point-wise Mutual Information (PMI) measure, we can find the long distance dependency between the words in a document. Table 1 : One erroneous sentence with 7 candidate sentences and their PMIs. Contextual Features We select some features that describe the information about the context of the sentences. PMI is used to measure the relation between candidate sentences and the document; and also to measure the co-occurrence among words of the sentence. Another feature that gives us useful information about fluency of candidate sentences is language model (LM) of sentence. A monolingual corpus is required to calculating PMI and LM. PMI of two words of A and B is calculated as follows: 𝑃𝑀𝐼 𝐴, 𝐵 = 𝐷𝑜𝑐 _𝐶𝑜𝑢𝑛𝑡 (𝐴,𝐵) 𝐷𝑜𝑐 _𝐶𝑜𝑢𝑛𝑡 (𝐴) × 𝐷𝑜𝑐 _𝐶𝑜𝑢𝑛𝑡 (𝐵) (5) In Equation ( 5 ), Doc_Count(A) is number of documents that contain word A. Doc_Count(A,B) is number of documents that contain both A, B. We formulate two criteria based on PMI for each candidate sentence PMI discourse and PMI sentence . PMI discourse is the PMI of the candidate sentence with its discourse while PMI sentence is the PMI of words candidate sentence. PMI for all words of the candidate sentence with the keywords of document is calculated as PMI discourse . For extracting the keywords, term frequency (TF) and inverse document frequency (IDF) measure is like (Li & Zhang, 2007) . For each sentence of the test data, 50 keywords are extracted from its discourse. To formulate this, consider W as a sentence in the test data and S j ={w j1 ,w j2 ,…,w jn } as j-th candidate sentence resulted from SMTbased approach. Let C w ={c 1 ,c 2 ,…,c 50 } is 50 keywords of the document containing W. PMI discourse for S j is calculated as follow: 𝑃𝑀𝐼 𝑑𝑖𝑠𝑐𝑜𝑢𝑟𝑠𝑒 𝑆 𝑗 = PMI 𝑤 𝑗𝑘 ;𝑐 𝑚 50 𝑚 =1 𝑛 𝑘 =1 𝑛 * 50 (6) In Equation ( 6 ), n is the number of sentence words. c m is the m-th keyword of discourse and w jk is k-th word of j-th candidate for W. Since PMI measures the co-occurrence of two different words, two identical words has maximum PMI in the sentence. In this case, if a word in the candidate is a keyword of the context, corresponding PMI discourse is increased. Consider S j ={This,cat,is,black} and S k ={This,car,is,black} are candidates of erroneous sentence of W. If discourse of W is about automobile then PMI discourse (S k ) > PMI discourse (S j ), because the cooccurrence of \"car\" with the keywords of automobile related document is greater than the co-occurrence of \"cat\" with that keywords. Second criterion is PMI sentence , which refers to co-occurrence of sentence words with each other. To calculate PMI sentence , the PMI of all words of the candidate sentence is calculated. To formulate this, consider S j ={w j1 ,w j2 ,…,w jn } is j-th candidate sentence for test sentence W. PMI sentence of S j is calculated as follow: 𝑃𝑀𝐼 𝑠𝑒𝑛𝑡𝑒𝑛𝑐𝑒 𝑆 𝑗 = PMI 𝑤 𝑗𝑘 ;𝑤 𝑗𝑚 n 𝑚 =k 𝑛 𝑘 =1 𝑛 * 𝑛 −1 2 (7) In Equation ( 7 ), n is number of words of the sentence and w jk is k-th word of j-th candidate of W. Table 1 shows an example of our Persian artificial test data in which PMI discourse and PMI sentence of correct candidate are more than that of SMT-based approach suggests. The input sentence is: ‫هيكل‬ ‫قوي‬ ‫دندان‬ ‫دو‬ ‫متر‬ ‫از‬ ‫راه‬ ‫ريل‬ ‫آهن‬ ‫دزديدند‬ ‫را‬ ‫اوكراين‬ dandaan-ghavi-hikal-dv-mtr-az-ril-raah-aahanavkraain-raa-dozdidand \"Robust teeth stole two meters of railway of Ukrainian\". There are two confusable words in the sentence, ‫دندان‬ dandaan \"teeth\" and ‫متر‬ metr \"meter\". SMT generate 7 candidate sentences in which the 5th candidate is the correct one. As shown in Table 1 , the first candidate, generated by SMT, has PMI discourse and PMI sentence score less than the correct sentence. By reranking SMT results using PMI discourse and PMI sentence , we can put the correct sentence at better rank or the top of the list. The third contextual feature is LM, which is used to score the fluency of the candidate. We consider surrounding words of suspicious word, whole sentence and whole document as the context, then, we use LM, PMI sentence and PMI sentence to extract information. After calculating PMI sentence , PMI discourse and LM for all candidate sentences, a log-linear model is used to rerank the N-best results. For reranking with log-linear model we need the weight of each feature. Support Vector Machine 1 (SVM) (Tsochantaridis, Joachims, Hofmann, Altun, & Singer, 2006 ) is used to weight each feature. SVM is a machine-learning algorithm based on statistical learning theory. It has been widely used, especially in function regression (Jeng, 2005) and pattern recognition (Tsai, 2005) , in recent years for its better generalization performance (Burges, 1998) . Feature Weighting Log linear model is used to rerank the N-best results of SMT. Like (Hayashi, Watanabe, Tsukada, & Isozaki, 2009) , we use SVM-rank to obtain the weight of each feature. A corpus contains erroneous and correct sentence is developed. For each sentence of the corpus, PMI sentence , PMI discourse and LM is calculated. We use the corpus a training data for SVM-rank to obtain the weight. In next section, the details of all data sets are described more precisely. Experiment Result We evaluate the accuracy of the approach by using the false positive and false negative rates as follows: False positive (FP) errors refer to real-word errors that were not identified by SMT-based system. False negative (FN) errors refer to appropriately written word that SMTbased approach detected as real-word error. True positive (TP) results are correct words that are considered as correct. True negative (TN) results refer to real-word errors that SMT-based approach detected and changed regardless of the correction. Finally True negative with correction (TNC) are real-word errors that SMT-based approach was able to replace them with the correct word. Evaluation metrics are computed as follows: In Equation ( 11 ), |Q| is the number of sentences of test data and 𝑟𝑎𝑛𝑘 𝑖 is the rank of correct sentence in 20-best result. We tested the SMT-based approach on two different languages, English and Persian. In the next subsections, we illustrate results on Persian and English languages. 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = # 𝑜𝑓 𝑇𝑁𝐶 # 𝑜𝑓 𝑇𝑁 (8) 𝐶𝑜𝑟𝑟𝑒𝑐𝑡𝑖𝑜𝑛 𝑅𝑒𝑐𝑎𝑙𝑙 = # 𝑜𝑓 𝑇𝑁𝐶 # 𝑜𝑓 𝐹𝑃 𝑎𝑛𝑑 𝑇𝑁 Results on Persian Language Our train data is generated from Peykareh (Bijankhan, 2004) , Hamshahri 2 and IRNA 3 data sets. Hamshahri and IRNA are collections of news documents of Persian language. These corpora contain 814, 166,774 and 179,574 documents of general texts respectively. They have 56,241, 576,137 and 332,343 types and 2,530,772, 78,841,045 and 64,085,181 tokens respectively. All three corpora contain 923,744 types. Our confusion set is generated from all mentioned data sets. It includes 5,000 headwords and each headword has about 4 confusable words in average. For our experiments on Persian, we have deployed two different test sets: an artificial and a real-world test sets. Our Persian real-world test data for contextsensitive spelling errors contains 1,100 sentences. The test set selected manually from the Internet mostly from Persian weblogs 4 . Each sentence contains 16.7 words in average and only one real-word error. The test set contains 27 insertion errors, 266 deletion errors, 527 substitution errors and 91 transpositions errors. Only 89 errors, 8% of whole errors, need more than one editing action. We also made an artificial test data for context-sensitive spelling errors. 1,500 sentences were selected randomly from Peykareh corpus. Length of each sentence is between 4 and 20 words. For each sentence in the artificial test set, one real-word error was inserted artificially, by replacing a random word with a word in its confusion set. Our training corpus contains 381,007 sentence pairs which are selected form mentioned corpora. After generating training data, Moses is used as our SMT system, GIZA++ (Och & Ney, 2003) is used for word alignment and SRILM (Stolcke, 2002) is used as LM toolkit. Our LM is created from Hamshahri and IRNA and contains 329, 607 unigrams, 4, 764, 131 bigrams and 6, 228, 300 trigrams. In order to develop training data for SVM, a confusion set is generated. The confusion set contains 26,891 headwords, which are selected from Hamshahri and Peykareh. Each headword has 4.6 confusable words. 5,000 sentences from Hamshahri and Peykareh are selected randomly. All sentences have at least one headword in the confusion set. For each sentence, one word of the sentence is selected and replaced with one of its headword. For each erroneous sentence maximum 20 candidates are generated by SMT. 56,320 sentences are generated and 3,728 of them are correct sentences. For each sentence of training data, PMI sentence , PMI discourse and LM are calculated and their values normalized. We used 56,320 sentences as training data for SVM-rank to obtain the weights. We generate a candidate list for each sentence of test sets by using the SMT and rerank the list in a post-processing step. In Table 2 , results of discourse-aware reranking on real-world and artificial test data are shown. We selected the work of Ehsan and Faili (2013) as a baseline. As it is shown in Table 2 , in both test sets, the proposed ranker retrieved a significant superior result over the baseline with respect to recall metric with a comparable precision. Since the principle of discourse-aware SMT is language independent, we tested it on English language too. Experiments on Persian Artificial Results on English Language The test sets for English language were drawn from two corpora: Wall Street Journal (WSJ) and Brown corpus. For WSJ test set, a confusion set is generated with 73,437 headwords and each headword has 5.9 confusable words in average. We extract confusable words from WSJ based on one editing action. 1,500 sentences are selected from WSJ randomly similar to the test sets developed in (Islam & Inkpen, 2009; Wilcox-O\"Hearn et al., 2008) . For each sentence, a realword error is inserted randomly. Rest of WSJ is considered as training data for SMT. Similar work of Golding and Roth (1999) ; Jones and Martin (1997) , we use 20% Brown corpus as test data and apply on 19 confusion sets. The test data contains 3015 erroneous sentences 1 . Train data for SMT, is generated from WSJ and rest of Brown corpus, 80%. We have tested SMT based approach on both artificial English test data, generated candidates and reranked them with discourse-aware features. Table 3 shows results of discourseaware. As shown in Table 3 , in WSJ and Brown test sets, our proposed system outperforms the baseline with respect to all metrics. We have a significant improvement over the baseline with respect to detection and correction recall. Experiments on Conclusion & Future work We improved SMT-based approach by extracting some contextual features and using a learning algorithm, SVM-rank, for getting weights of each feature and reranking the N-best results by a log-linear model. The proposed ranker retrieved a significant superior result over the baseline with respect to recall metric with a comparable precision. Real-word errors with two editing actions can be injected to training data. An ontology, named FarsNet (Shamsfard, 2008) , can be used as an external resource to identify Persian semantic relationships between words. We can use discourse-aware reranking as a Learning To Rank, and apply it on every method that generate N-best result. 1 The test set is available on: http://cogcomp.cs.illinois.edu/Data/Spell/ Stolcke, Andreas. (2002) . SRILM-an extensible language modeling toolkit. Paper presented at the Proceedings of the international conference on spoken language processing.",
         "1643900",
         "7ad183c2473b6dac96ba7d1d6af0ac95ff4b619b",
         "2",
         "https://aclanthology.org/R13-1062",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Mirzababaei, Behzad  and\nFaili, Heshaam  and\nEhsan, Nava",
         "Discourse-aware Statistical Machine Translation as a Context-sensitive Spell Checker",
         "475--482",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "mirzababaei-etal-2013-discourse",
         null,
         null
        ],
        [
         "20",
         "W14-0137",
         "There are more than 60 wordnets worldwide; the Romanian wordnet is among those that are maintained and further developed. Begun within the BalkaNet project and further enriched in various (application oriented) projects, it was used in word sense disambiguation, machine translation and question answering with promising results. We present here the latest qualitative and quantitative improvements of our lexical resource, special attention being paid to derivational relations, the latest statistics, as well as the development of an Application Programming Interface, meant to facilitate work with the wordnet, both for its further development purposes and for its use in applications. In the context of creating a common European research infrastructure network, our wordnet is licensed through META-SHARE, being freely available for scientific purposes.",
         "There are more than 60 wordnets worldwide; the Romanian wordnet is among those that are maintained and further developed. Begun within the BalkaNet project and further enriched in various (application oriented) projects, it was used in word sense disambiguation, machine translation and question answering with promising results. We present here the latest qualitative and quantitative improvements of our lexical resource, special attention being paid to derivational relations, the latest statistics, as well as the development of an Application Programming Interface, meant to facilitate work with the wordnet, both for its further development purposes and for its use in applications. In the context of creating a common European research infrastructure network, our wordnet is licensed through META-SHARE, being freely available for scientific purposes. Introduction The development of the Romanian wordnet (RoWN henceforth) started within BalkaNet project 1 . Afterwards, it has been developed and maintained within several projects by the Natural Language Processing (NLP) group of the Romanian Academy Research Institute for Artificial Intelligence (RACAI): ROTEL 2 , STAR 3 , SIR-1 http://www.dblab.upatras.gr/balkanet 2 http://www.ai.ici.ro/rotel_eng/index. htm 3 http://www.racai.ro/star RESDEC 4 , ACCURAT 5 , METANET4U 6 , the Romanian Academy research plan. Within BalkaNet a core of 18000 synsets was created. They were aligned to the Princeton WordNet (PWN) versions available throughout time, respectively version 2.0 at the end of the project. Among those synsets there were more than 400 that lexicalize concepts specific to the Balkan area. These were implemented in all six languages of the project (Bulgarian, Czech, Greek, Romanian, Serbian, Turkish) and were linked to hypernym synsets, already existing in PWN, so they were not left dangling in the network. RoWN contains words belonging both to the general vocabulary and to various domains of activity. Throughout time, we aimed at a complete coverage of the basic common sets from EuroWordNet 7 , of the 1984 corpus 8 , of the newspaper articles corpus NAACL2003 9 , of the Acquis Communautaire corpus and the Eurovoc thesaurus 10 , of VerbNet 3.1 11 , and as much as possible from the ROWikipedia lexical stock. Two basic development principles have always been followed: the Hierarchy Preservation Principle (HPP) (according to which the hierarchical structure of the concepts in a wordnet is the same irrespective of the natural language for which the wordnet is developed) and the Conceptual Density Principle (which ensures that once a concept is selected to be implemented, all its ancestors up to the unique beginners are also selected, thus preventing the existence of dangling nodes) (Tufiş et al., 2004) . The former principle was the assumption behind our development methodology, namely the expand method. The latter ensured the lack of dangling nodes in the nouns and verbs hierarchies. As a consequence of the way we chose to create our language resource, the lexical density has never been our preoccupation, thus there are many words that do not occur in as many synsets as how many meanings they have. Nevertheless, we do not exclude such an objective from our further developments. At present, RoWN is aligned to PWN version 3.0. Details about the way we performed the alignment from PWN 2.0 to PWN 3.0 and about the way we solved the encountered problems (the n:1 or 1:n matches between synsets in the two versions) are presented in Tufiş et al. (2013) . RoWN is licensed through META-SHARE 12 (). It is free for academic research, but restricted for commercial use. In this paper we present the latest qualitative and quantitative improvements of our lexical resource, the latest statistics (Section 3), special attention being paid to derivational relations (Section 4), as well as the development of an Application Programming Interface, meant to facilitate work with the wordnet, both for its further development purposes and for its use in applications (Section 5). Our intentions for further development are included in the Conclusions section. Before proceeding, we enumerate the applications in which our team used RoWN and which, throughout time, influenced our decisions about the concepts to be further implemented in the network. Ion and Tufiş (2009) and Ion and Ştefănescu (2011) describe word sense disambiguation (WSD) methods that make use of wordnets: the former is set in a multilingual environment and the WSD is done with the help of aligned wordbrowse/18 nets. The latter is set in a monolingual environment and the WSD is done with the help of the lexical chains established between the cooccurring words in the text, chains whose length is calculated in the wordnet. The assumption is that the shorter the lexical chain, the more similar the words. The length of the lexical chain depends on the number of relations marked in the network. The results in the multilingual environment are reported as better than those in the monolingual one. Uses of RoWN For a Question Answering (QA) system, RoWN was used for expanding the query introduced by the user (Ion et al., 2008) with words semantically related (i.e., synonyms, hypo-and hyperonyms) to the ones it contained. Moreover, RoWN was also used in the last phase, that of ranking the found results by calculating the semantic distance, again as a lexical chain, between the words introduced by the user and those occurring in the text. It was noticed that the relations with the greatest contribution at calculating the score are hyponymy and derivational relations. Aligned wordnets are valuable sources of cross-language equivalents, especially multiword terms, in machine translation. Latest Quantitative Developments Lately our efforts of implementing new synsets aimed at a complete coverage of VerbNet 3.1, with the prospect of creating a syntactic parser for Romanian. The up-to-date statistics about RoWN are presented in Table 1 and 2 below. In the former, PoS stands for part of speech, S for synset, L for literal, UL for unique literals and NL for nonlexicalized synsets. Obeying the HPP stated above implies the transfer of the hierarchies from PWN into RoWN. The lack of perfect equivalences among languages is widely known; nevertheless, we chose to disregard it. Moreover, there are lexical gaps in all languages. We call them nonlexicalized concepts and represent them as empty synsets. For example, for the PWN verbal synset {zip_up:1} (gloss: close with a zipper) there is no literal in the corresponding Romanian synset. However, such synsets do not lack relations: the corresponding ones from PWN are transferred into RoWN. It is worth noticing that antonymy, which is a lexical relation in PWN, is represented as a semantic one in RoWN. The conceptual opposition between the synsets is more useful in various applications than the mere antonymy between two literals. With the exception of attribute relation, all the others enumerated in Table 2 link synsets with literals of the same part of speech. A path between two words of a different part of speech, about which any speaker would say they are related, although not impossible to find, would be too long, thus providing wrong information about the similarity between those words. PoS Derivational Relations Using RoWN in applications, as presented above, showed unnatural lexical chains, such as one of the possible chains between inventator -inventor‖ and inventa -to invent‖: inventator(1.1) instance_hyponym James_Watt(x) James_Watt(x) instance_hypernym inginer(1.1) inginer(1.1) hyponym inginer_software(1) inginer_software(1) domain_member_TOPIC ştiinţa_calculatoarelor(x) ştiinţa_calculatoarelor(x) domain_TOPIC pro- grama(3) programa(3) hyponym crea_mental(1) crea_mental(1) hypernym inventa(1) The strangeness of this example results from the intricate path from inventator to inventa, uncommon for whatever speaker of Romanian: inventator -James Wattinginer -engineer‖inginer software -software engineer‖programa -to program‖crea mental -to create by mental act‖inventa. Faced with a number of such cases, we decided to implement derivational relations into our wordnet. This type of relations exists in other wordnets as well: the Turkish WordNet (Bilgin et al., 2004) , PWN (Fellbaum et al., 2007) , the Czech WordNet (Pala and Hlaváčková, 2007) , the Polish WordNet (Piasecki et al., 2012) , the Estonian one (Kahusk, et al., 2010) . Given the language-specific character of such relations, each team undertook their own strategy for finding the relations in their wordnet. However, there are teams that transferred the derivational relations in PWN and then validated them: this is the case for the Bulgarian WordNet (Koeva, 2008) , the Serbian (Koeva et al., 2008) and the Finnish one (Lindén and Niemi, 2013) . Whereas most of the undertakings above aimed at expanding the network with new synsets derivationally linked with the literals already in the wordnet, we were interested in adding such relations between literals that are in the synsets. No extension was intended, at least for the moment. We discuss below some theoretical aspects of derivational relations and the significance of their representation in a wordnet and then present the methodology we adopted for identifying and annotating them in RoWN. Pre-requisites Derivation is one means of creating new words in a language from existing morphemes, i.e. the smallest units of a language that have their own meaning. It ensures both formal and semantic relatedness between the root and the derived word: the formal relatedness is ensured by the fact that the root and the derived word contain (almost) the same string of letters that represent the root, while the semantic relatedness is ensured by the compositionality of meaning of the derived word: its meaning is a sum of the meaning of the root and the meaning of the affix(es). Thus, the Romanian words alerga -run‖ and alergător -runner‖ are derivationally related: the latter is obtained from the former by adding the suffix -ător (after removing -a, the infinitive suffix) and it means -the one who runs‖. However, derivational relations cannot be established for all meanings of these words: when considered with their proper meaning, they are related, but when alerga is considered with its figurative meaning -to try hard to get something‖, it does not establish a derivational relation with alergător, as it has not developed any related figurative meaning. In the derivation process only one affix of a type is added. So, a prefix and a suffix can be added to a root in the same derivation step, but never two suffixes or/and two prefixes. If a word contains more than two affixes of the same type, then they were attached in different steps in the derivation. Identifying derivational relations between literals in RoWN Having available a list of (492) Romanian affixes and the list of (31872) simple literals in RoWN, we searched for pairs of literals (literal 1 and literal 2 ) such that literal 1 +/-affix(es) = literal 2 . The -+‖ version covers progressive derivation, while the --‖ version covers backformation. We allowed for at most 2 affixes, but of different types, as discussed above. The results are presented in The percents are reasonable: it is a wellknown fact that prefixation is weakly productive in Romanian, unlike suffixation. We subjected the found pairs to an automatic and then a manual validation. For the former, we enriched the list of affixes with information about the part of speech of the words to which they can attach and of the words they help create. The list is available at www.racai.ro/~vergi under Research. For example, the suffix -a can be attached to nouns or to adjectives to create verbs: -a n>v a>v Examples include: buton (-button‖) + -a > butona (-to channel-surf‖), scurt (-short‖) + -a > scurta (-to shorten‖). Afterwards we proceeded to a manual validation of the whole number of pairs. The results are presented in Table 4 : for each type of derivation (DT) (prefixation P or suffixation S), from the found pairs (column 2) we present the number of those passing the automatic validation (AV) in column 3 and then of those that passed the manual validation (MV) in column 4; the last column presents the percent of manually validated pairs for each derivation type. Examples of pairs that passed the automatic validation but not the manual one include: prinde -to catch‖surprinde -to surprise‖, abate -to deviate‖abator -slaughter house‖. DT Sense level annotation Having already established that derivational relations need to be marked at the word sense level, not for all senses of the words in a pair, the next necessary step is to calculate the Cartesian product of the sets of synsets in which the members of the validated pairs occur. Thus, for the 10442 pairs of literals resulted after manual validation, we calculated the Cartesian product for each pair, obtaining a total of 101729 pairs of synsets. They display formal relatedness and, in order to mark a derivational relation for them, it is also necessary to subject them to a semantic evaluation. A linguist goes through them and whenever semantic similarity is noticed, the pair is labeled with one of the 57 semantic labels we established: 16 for prefixed words (together, subsumption, opposition, mero, eliminate, iterative, through, repeat, imply, similitude, instead, aug, before, anti, out, back) and 41 for suffixed ones (subsumption, member_holo, member_mero, substance_holo, substance_mero, ingredient_holo, holonym, part, agent, result, location, of_origin, job, state, period, undergoer, instrument, sound, cause, container, vehicle, body_part, material, destination, gender, wife, dim, aug, object_made_by, subject_to, by_means_of, clothes, event, abstract, colour, tax, make_become, make_acquire, manner, similitude, related) . The most frequently attached semantic labels are: for prefixed words: opposition (neesenţial -unessential‖essential -essential‖) (792), subsumption (subclasă -subclass‖ -clasă -class‖) (363), repeat (reaprinde -reignite‖aprinde -ignite‖) (305); for suffixed words: related (călduros -warm‖ -căldură -warmth‖) (1294), event (împărtăşanie -communion‖ -împărtăşi -commune‖) (699), abstract (cerinţă -require-ment‖cere -require‖) (490), manner (primejdios -dangerous‖ -primejdie -danger‖) (436), agent (linguşitor -adulator‖ -linguşi -ad-ulate‖) (394). At the end of the article, in the Annex, containing Table7 and Table 8 , we present the semantic labels and their frequencies for prefixed and, respectively, suffixed words, accompanied by examples. Statistics about derivational relations Going through 55849 such pairs of synsets, we obtained the results in Table 5 . The aim of marking these derivational relations was to increase the number of links between synsets, especially between synsets of different parts of speech. For the validated pairs we included in Table 6 Prefixed RoWordNetLib We have built an Application Programming Interface (API) for RoWN, called RoWordNetLib, meant as a tool to aid quick implementations of RoWN into both research-oriented and industry applications. When designing it, we envisaged a tool that should be easy to use, easy to extend and that would offer a sufficiently large array of functionalities. The chosen programming language is Java. The main functionalities that RoWordNetLib provides are:  Input/Output for working with XMLbased RoWN files;  Methods for working with the semantic network itself (RoWordNet objects containing RoWN);  Set operations for working with multiple RoWordNet objects (reunion, intersection, complement, difference, merge, etc.);  Basic Word Sense Disambiguation (WSD) algorithms;  Similarity Metrics (both distance-based and semantic). The API's uses can be classified as (1) internalit helps to facilitate the continuous work of enriching RoWN and (2) externalto quicken the development of Romanian-enabled smart applications. By providing set operations like difference, intersection or reunion on RoWordNet objects, more people can work in parallel on RoWN and then easily join their versions into a single wordnet, thus easing its development. Externally, wordnets are successfully used to perform word sense disambiguation, information retrieval, information extraction, machine translation, automatic text classification and summarization. RoWordNetLib is structured into several packages, each with its assigned functionality. The main packages are: 'data', 'io', 'op' and 'wsd'. The 'data' package contains the data structures RoWordNetLib uses internally. Its structure is simple, following the way the data is naturally structured in a wordnet: a RoWordNet object contains an array of Synset objects which are indexed by the synset ID for retrieval speed. Each Synset object contains a number of primitive types as well as an array of Literal objects and an array of Relation objects. A Literal object contains a word and an associated sense. A Relation object contains a relation (string) that points to a target synset (defined as an ID), as well as optionally having a source and target literal for cases where the relation is not between synsets but between two synsets' particular literals. The 'io' package provides input and output functions. The most important I/O function is reading and writing RoWordNet objects in their native XML format. The 'op' package provides different operational tools: (1) set operation methods for joining, intersecting, complementing, etc., multiple RoWordNet objects; (2) through the BFWalk class, the ability to perform a breadth-first walk through the RoWN semantic network; (3) a number of distance-based and semantic similarity measures (Resnik, 1995) for measuring the closeness of concepts (lexicalized by literals in synsets). The 'wsd' package implements two Word Sense Disambiguation algorithms: Lesk (1986) and an adapted version of Lesk. They are used to obtain information content values for synsets in RoWN given an arbitrary Romanian text as the input corpus, which is further used to enable the semantic similarity measures. Conclusions and Further Work RoWN is a valuable resource for the Romanian language and the NLP group of RACAI uses it in most of their applications. We presented here our latest qualitative and quantitative achievements. Further enrichment of RoWN is a constant preoccupation of our team. It follows all the time the other interests of the group. For instance, the last set of implemented synsets was made up of verbs exclusively, given our present interest to cover VerbNet 3.1, with the prospect of creating a parser for Romanian. Increasing the density of relations between synsets in order to make RoWN more effective in applications was obtained by adding derivational relations. Although they are relations between literals, the semantic labels we attached to them can be viewed as a link between the synsets to which the respective literals belong. After finishing the semantic annotation of the derivative pairs, we could try to expand the network with automatically derived words. For Romanian an experiment of automatically deriving words is reported by Petic (2011) , who used very productive and reliable affixes. With the list of affixes and their combination possibilities (available at www.racai.ro/~vergi under Research) that we have created, we can dare test new cases of automatic derivation for Romanian.",
         "15727844",
         "e6edfccbf4913e83caa8f9d08c828dede6fc1c67",
         "2",
         "https://aclanthology.org/W14-0137",
         "University of Tartu Press",
         "Tartu, Estonia",
         "2014",
         "January",
         "Proceedings of the Seventh Global {W}ordnet Conference",
         "Mititelu, Verginica Barbu  and\nDumitrescu, {\\textcommabelow{S}}tefan Daniel  and\nTufi{\\textcommabelow{s}}, Dan",
         "News about the {R}omanian {W}ordnet",
         "268--275",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "mititelu-etal-2014-news",
         null,
         null
        ],
        [
         "21",
         "P02-1040",
         "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1",
         "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1 Introduction Rationale Human evaluations of machine translation (MT) weigh many aspects of translation, including adequacy, fidelity , and fluency of the translation (Hovy,  1999; White and O'Connell, 1994). A comprehensive catalog of MT evaluation techniques and their rich literature is given by Reeder (2001). For the most part, these various human evaluation approaches are quite expensive (Hovy, 1999). Moreover, they can take weeks or months to finish. This is a big problem because developers of machine translation systems need to monitor the effect of daily changes to their systems in order to weed out bad ideas from good ideas. We believe that MT progress stems from evaluation and that there is a logjam of fruitful research ideas waiting to be released from the evaluation bottleneck. Developers would benefit from an inexpensive automatic evaluation that is quick, language-independent, and correlates highly with human evaluation. We propose such an evaluation method in this paper. Viewpoint How does one measure translation performance? The closer a machine translation is to a professional human translation, the better it is. This is the central idea behind our proposal. To judge the quality of a machine translation, one measures its closeness to one or more reference human translations according to a numerical metric. Thus, our MT evaluation system requires two ingredients: 1. a numerical \"translation closeness\" metric 2. a corpus of good quality human reference translations We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community, appropriately modified for multiple reference translations and allowing for legitimate differences in word choice and word order. The main idea is to use a weighted average of variable length phrase matches against the reference translations. This view gives rise to a family of metrics using various weighting schemes. We have selected a promising baseline metric from this family. In Section 2, we describe the baseline metric in detail. In Section 3, we evaluate the performance of BLEU. In Section 4, we describe a human evaluation experiment. In Section 5, we compare our baseline metric performance with human evaluations. The Baseline BLEU Metric Typically, there are many \"perfect\" translations of a given source sentence. These translations may vary in word choice or in word order even when they use the same words. And yet humans can clearly distinguish a good translation from a bad one. For example, consider these two candidate translations of a Chinese source sentence: Example 1. Candidate 1: It is a guide to action which ensures that the military always obeys the commands of the party. Candidate 2: It is to insure the troops forever hearing the activity guidebook that party direct. Although they appear to be on the same subject, they differ markedly in quality. For comparison, we provide three reference human translations of the same sentence below. Reference 1: It is a guide to action that ensures that the military will forever heed Party commands. Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party. Reference 3: It is the practical guide for the army always to heed the directions of the party. It is clear that the good translation, Candidate 1, shares many words and phrases with these three reference translations, while Candidate 2 does not. We will shortly quantify this notion of sharing in Section 2.1. But first observe that Candidate 1 shares \"It is a guide to action\" with Reference 1, \"which\" with Reference 2, \"ensures that the military\" with Reference 1, \"always\" with References 2 and 3, \"commands\" with Reference 1, and finally \"of the party\" with Reference 2 (all ignoring capitalization). In contrast, Candidate 2 exhibits far fewer matches, and their extent is less. It is clear that a program can rank Candidate 1 higher than Candidate 2 simply by comparing ngram matches between each candidate translation and the reference translations. Experiments over large collections of translations presented in Section 5 show that this ranking ability is a general phenomenon, and not an artifact of a few toy examples. The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are positionindependent. The more the matches, the better the candidate translation is. For simplicity, we first focus on computing unigram matches. Modified n-gram precision The cornerstone of our metric is the familiar precision measure. To compute precision, one simply counts up the number of candidate translation words (unigrams) which occur in any reference translation and then divides by the total number of words in the candidate translation. Unfortunately, MT systems can overgenerate \"reasonable\" words, resulting in improbable, but high-precision, translations like that of example 2 below. Intuitively the problem is clear: a reference word should be considered exhausted after a matching candidate word is identified. We formalize this intuition as the modified unigram precision. To compute this, one first counts the maximum number of times a word occurs in any single reference translation. Next, one clips the total count of each candidate word by its maximum reference count, 2 adds these clipped counts up, and divides by the total (unclipped) number of candidate words. Example 2. Candidate: the the the the the the the. Reference 1: The cat is on the mat. Reference 2: There is a cat on the mat. Modified Unigram Precision = 2/7. 3 In Example 1, Candidate 1 achieves a modified unigram precision of 17/18; whereas Candidate 2 achieves a modified unigram precision of 8/14. Similarly, the modified unigram precision in Example 2 is 2/7, even though its standard unigram precision is 7/7. Modified n-gram precision is computed similarly for any n: all candidate n-gram counts and their corresponding maximum reference counts are collected. The candidate counts are clipped by their corresponding reference maximum value, summed, and divided by the total number of candidate ngrams. In Example 1, Candidate 1 achieves a modified bigram precision of 10/17, whereas the lower quality Candidate 2 achieves a modified bigram precision of 1/13. In Example 2, the (implausible) candidate achieves a modified bigram precision of 0. This sort of modified n-gram precision scoring captures two aspects of translation: adequacy and fluency. A translation using the same words (1-grams) as in the references tends to satisfy adequacy. The longer n-gram matches account for fluency. 4 Modified n-gram precision on blocks of text How do we compute modified n-gram precision on a multi-sentence test set? Although one typically evaluates MT systems on a corpus of entire documents, our basic unit of evaluation is the sentence. A source sentence may translate to many target sentences, in which case we abuse terminology and refer to the corresponding target sentences as a \"sentence.\" We first compute the n-gram matches sentence by sentence. Next, we add the clipped n-gram counts for all the candidate sentences and divide by the number of candidate n-grams in the test corpus to compute a modified precision score, p n , for the entire test corpus. p n = ∑ C ∈{Candidates} ∑ n-gram ∈ C Count clip (n-gram) ∑ C ∈{Candidates} ∑ n-gram ∈ C Count(n-gram ) . 4 BLEU only needs to match human judgment when averaged over a test corpus; scores on individual sentences will often vary from human judgments. For example, a system which produces the fluent phrase \"East Asian economy\" is penalized heavily on the longer n-gram precisions if all the references happen to read \"economy of East Asia.\" The key to BLEU's success is that all systems are treated similarly and multiple human translators with different styles are used, so this effect cancels out in comparisons between systems. Ranking systems using only modified n-gram precision To verify that modified n-gram precision distinguishes between very good translations and bad translations, we computed the modified precision numbers on the output of a (good) human translator and a standard (poor) machine translation system using 4 reference translations for each of 127 source sentences. The average precision results are shown in Figure 1 . The strong signal differentiating human (high precision) from machine (low precision) is striking. The difference becomes stronger as we go from unigram precision to 4-gram precision. It appears that any single n-gram precision score can distinguish between a good translation and a bad translation. To be useful, however, the metric must also reliably distinguish between translations that do not differ so greatly in quality. Furthermore, it must distinguish between two human translations of differing quality. This latter requirement ensures the continued validity of the metric as MT approaches human translation quality. To this end, we obtained a human translation by someone lacking native proficiency in both the source (Chinese) and the target language (English). For comparison, we acquired human translations of the same documents by a native English speaker. We also obtained machine translations by three commercial systems. These five \"systems\" -two humans and three machines -are scored against two reference professional human translations. The average modified n-gram precision results are shown in Figure 2 . Each of these n-gram statistics implies the same ranking: H2 (Human-2) is better than H1 (Human-1), and there is a big drop in quality between H1 and S3 (Machine/System-3). S3 appears better than S2 which in turn appears better than S1. Remarkably, this is the same rank order assigned to these \"systems\" by human judges, as we discuss later. While there seems to be ample signal in any single n-gram precision, it is more robust to combine all these signals into a single number metric. Combining the modified n-gram precisions How should we combine the modified precisions for the various n-gram sizes? A weighted linear average of the modified precisions resulted in encouraging results for the 5 systems. However, as can be seen in Figure 2 , the modified n-gram precision decays roughly exponentially with n: the modified unigram precision is much larger than the modified bigram precision which in turn is much bigger than the modified trigram precision. A reasonable averaging scheme must take this exponential decay into account; a weighted average of the logarithm of modified precisions satisifies this requirement. BLEU uses the average logarithm with uniform weights, which is equivalent to using the geometric mean of the modified n-gram precisions. 5 ,6 Experimentally, we obtain the best correlation with mono- 5 The geometric average is harsh if any of the modified precisions vanish, but this should be an extremely rare event in test corpora of reasonable size (for N max ≤ 4). 6 Using the geometric average also yields slightly stronger correlation with human judgments than our best results using an arithmetic average. lingual human judgments using a maximum n-gram order of 4, although 3-grams and 5-grams give comparable results. Sentence length A candidate translation should be neither too long nor too short, and an evaluation metric should enforce this. To some extent, the n-gram precision already accomplishes this. N-gram precision penalizes spurious words in the candidate that do not appear in any of the reference translations. Additionally, modified precision is penalized if a word occurs more frequently in a candidate translation than its maximum reference count. This rewards using a word as many times as warranted and penalizes using a word more times than it occurs in any of the references. However, modified n-gram precision alone fails to enforce the proper translation length, as is illustrated in the short, absurd example below. Example 3: Candidate: of the Reference 1: It is a guide to action that ensures that the military will forever heed Party commands. Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party. Reference 3: It is the practical guide for the army always to heed the directions of the party. Because this candidate is so short compared to the proper length, one expects to find inflated precisions: the modified unigram precision is 2/2, and the modified bigram precision is 1/1. The trouble with recall Traditionally, precision has been paired with recall to overcome such length-related problems. However, BLEU considers multiple reference translations, each of which may use a different word choice to translate the same source word. Furthermore, a good candidate translation will only use (recall) one of these possible choices, but not all. Indeed, recalling all choices leads to a bad translation. Here is an example. The first candidate recalls more words from the references, but is obviously a poorer translation than the second candidate. Thus, naïve recall computed over the set of all reference words is not a good measure. Admittedly, one could align the reference translations to discover synonymous words and compute recall on concepts rather than words. But, given that reference translations vary in length and differ in word order and syntax, such a computation is complicated. Sentence brevity penalty Candidate translations longer than their references are already penalized by the modified n-gram precision measure: there is no need to penalize them again. Consequently, we introduce a multiplicative brevity penalty factor. With this brevity penalty in place, a high-scoring candidate translation must now match the reference translations in length, in word choice, and in word order. Note that neither this brevity penalty nor the modified n-gram precision length effect directly considers the source length; instead, they consider the range of reference translation lengths in the target language. We wish to make the brevity penalty 1.0 when the candidate's length is the same as any reference translation's length. For example, if there are three references with lengths 12, 15, and 17 words and the candidate translation is a terse 12 words, we want the brevity penalty to be 1. We call the closest reference sentence length the \"best match length.\" One consideration remains: if we computed the brevity penalty sentence by sentence and averaged the penalties, then length deviations on short sentences would be punished harshly. Instead, we compute the brevity penalty over the entire corpus to allow some freedom at the sentence level. We first compute the test corpus' effective reference length, r, by summing the best match lengths for each candidate sentence in the corpus. We choose the brevity penalty to be a decaying exponential in r/c, where c is the total length of the candidate translation corpus. BLEU details We take the geometric mean of the test corpus' modified precision scores and then multiply the result by an exponential brevity penalty factor. Currently, case folding is the only text normalization performed before computing the precision. We first compute the geometric average of the modified n-gram precisions, p n , using n-grams up to length N and positive weights w n summing to one. Next, let c be the length of the candidate translation and r be the effective reference corpus length. We compute the brevity penalty BP, BP = 1 if c > r e (1−r/c) if c ≤ r . Then, BLEU= BP • exp N ∑ n=1 w n log p n . The ranking behavior is more immediately apparent in the log domain, log BLEU = min(1 − r c , 0) + N ∑ n=1 w n log p n . In our baseline, we use N = 4 and uniform weights w n = 1/N. The BLEU Evaluation The BLEU metric ranges from 0 to 1. Few translations will attain a score of 1 unless they are identical to a reference translation. For this reason, even a human translator will not necessarily score 1. It is important to note that the more reference translations per sentence there are, the higher the score is. Thus, one must be cautious making even \"rough\" comparisons on evaluations with different numbers of reference translations: on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references. Table 1 shows the BLEU scores of the 5 systems against two references on this test corpus. The MT systems S2 and S3 are very close in this metric. Hence, several questions arise: • Is the difference in BLEU metric reliable? • What is the variance of the BLEU score? • If we were to pick another random set of 500 sentences, would we still judge S3 to be better than S2? To answer these questions, we divided the test corpus into 20 blocks of 25 sentences each, and computed the BLEU metric on these blocks individually. We thus have 20 samples of the BLEU metric for each system. We computed the means, variances, and paired t-statistics which are displayed in Table 2 . The t-statistic compares each system with its left neighbor in the table. For example, t = 6 for the pair S1 and S2. Note that the numbers in Table 1 are the BLEU metric on an aggregate of 500 sentences, but the means in Table 2 are averages of the BLEU metric on aggregates of 25 sentences. As expected, these two sets of results are close for each system and differ only by small finite block size effects. Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems' scores are statistically very significant. The reported variance on 25-sentence blocks serves as an upper bound to the variance of sizeable test sets like the 500 sentence corpus. How many reference translations do we need? We simulated a single-reference test corpus by randomly selecting one of the 4 reference translations as the single reference for each of the 40 stories. In this way, we ensured a degree of stylistic variation. The systems maintain the same rank order as with multiple references. This outcome suggests that we may use a big test corpus with a single reference translation, provided that the translations are not all from the same translator. The Human Evaluation We had two groups of human judges. The first group, called the monolingual group, consisted of 10 native speakers of English. The second group, called the bilingual group, consisted of 10 native speakers of Chinese who had lived in the United States for the past several years. None of the human judges was a professional translator. The humans judged our 5 standard systems on a Chinese sentence subset extracted at random from our 500 sentence test corpus. We paired each source sentence with each of its 5 translations, for a total of 250 pairs of Chinese source and English translations. We prepared a web page with these translation pairs randomly ordered to disperse the five translations of each source sentence. All judges used this same webpage and saw the sentence pairs in the same order. They rated each translation from 1 (very bad) to 5 (very good). The monolingual group made their judgments based only on the translations' readability and fluency. As must be expected, some judges were more liberal than others. And some sentences were easier to translate than others. To account for the intrinsic difference between judges and the sentences, we compared each judge's rating for a sentence across systems. We performed four pairwise t-test comparisons between adjacent systems as ordered by their aggregate average score. Monolingual group pairwise judgments Figure 3 shows the mean difference between the scores of two consecutive systems and the 95% confidence interval about the mean. We see that S2 is quite a bit better than S1 (by a mean opinion score difference of 0.326 on the 5-point scale), while S3 is judged a little better (by 0.114). Both differences are significant at the 95% level. 7 The human H1 is much better than the best system, though a bit worse than human H2. This is not surprising given that H1 is not a native speaker of either Chinese or English, whereas H2 is a native English speaker. Again, the difference between the human translators is significant beyond the 95% level. Bilingual group pairwise judgments Figure 4 shows the same results for the bilingual group. They also find that S3 is slightly better than S2 (at 95% confidence) though they judge that the human translations are much closer (indistinguishable at 95% confidence), suggesting that the bilinguals tended to focus more on adequacy than on fluency. Figure 4 : Bilingual Judgments -pairwise differential comparison BLEU vs The Human Evaluation Figure 5 shows a linear regression of the monolingual group scores as a function of the BLEU score over two reference translations for the 5 systems. The high correlation coefficient of 0.99 indicates that BLEU tracks human judgment well. Particularly interesting is how well BLEU distinguishes between S2 and S3 which are quite close. Figure 6 shows the comparable regression results for the bilingual group. The correlation coefficient is 0.96. We now take the worst system as a reference point and compare the BLEU scores with the human judg-ment scores of the remaining systems relative to the worst system. We took the BLEU, monolingual group, and bilingual group scores for the 5 systems and linearly normalized them by their corresponding range (the maximum and minimum score across the 5 systems). The normalized scores are shown in Figure 7 . This figure illustrates the high correlation between the BLEU score and the monolingual group. Of particular interest is the accuracy of BLEU's estimate of the small difference between S2 and S3 and the larger difference between S3 and H1. The figure also highlights the relatively large gap between MT systems and human translators. 8 In addition, we surmise that the bilingual group was very forgiving in judging H1 relative to H2 because the monolingual group found a rather large difference in the fluency of their translations. Conclusion We believe that BLEU will accelerate the MT R&D cycle by allowing researchers to rapidly home in on effective modeling ideas. Our belief is reinforced by a recent statistical analysis of BLEU's correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)! BLEU's strength is that it correlates highly with human judg-8 Crossing this chasm for Chinese-English translation appears to be a significant challenge for the current state-of-the-art systems. ments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality. Finally, since MT and summarization can both be viewed as natural language generation from a textual context, we believe BLEU could be adapted to evaluating summarization or similar NLG tasks. Acknowledgments This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No. N66001-99-2-8916. The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred. We gratefully acknowledge comments about the geometric mean by John Makhoul of BBN and discussions with George Doddington of NIST. We especially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of Chinese-English MT systems.",
         "11080756",
         "d7da009f457917aa381619facfa5ffae9329a6e9",
         "17139",
         "https://aclanthology.org/P02-1040",
         "Association for Computational Linguistics",
         "Philadelphia, Pennsylvania, USA",
         "2002",
         "July",
         "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
         "Papineni, Kishore  and\nRoukos, Salim  and\nWard, Todd  and\nZhu, Wei-Jing",
         "{B}leu: a Method for Automatic Evaluation of Machine Translation",
         "311--318",
         "10.3115/1073083.1073135",
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "papineni-etal-2002-bleu",
         null,
         null
        ],
        [
         "22",
         "2012.eamt-1.36",
         "Statistical-based methods are the prevalent approaches for implementing machine translation systems today. However the resulted translations are usually flawed to some degree. We assume that a statistical baseline system can be re-used to automatically learn how to (partially) correct translation errors, i.e. to turn a \"broken\" target translation into a better one. By training and testing on initial bilingual data, we constructed a system S1 which was used to translate the source language part of the training corpus. The new translated corpus and its reference translation are used to train and test another similar system S2. Without any additional data, the chain S1+S2 shows a sensible quality increase against S1 in terms of BLEU scores, for both translation directions (English to Romanian and Romanian to English).",
         "Statistical-based methods are the prevalent approaches for implementing machine translation systems today. However the resulted translations are usually flawed to some degree. We assume that a statistical baseline system can be re-used to automatically learn how to (partially) correct translation errors, i.e. to turn a \"broken\" target translation into a better one. By training and testing on initial bilingual data, we constructed a system S1 which was used to translate the source language part of the training corpus. The new translated corpus and its reference translation are used to train and test another similar system S2. Without any additional data, the chain S1+S2 shows a sensible quality increase against S1 in terms of BLEU scores, for both translation directions (English to Romanian and Romanian to English). Introduction The paper presents a cascaded phrase based translation system that obtains improved translation scores using no additional data compared to the standard single-step translation system. The first challenge of our research was to obtain the best standard translation system possible. We experimented with different factored models that include surface form, lemmas and different part of speech tag sets in various combinations to confirm the assumption that translation accuracy is improved over a surface form only baseline model. The second objective of our work was to validate our intuition that a statistical baseline system can be re-used (cascaded) to automatically learn how to (partially) correct its own translation errors, i.e. to turn an initially \"broken\" translation into a better one. The phrase-based translation approach has overcome several drawbacks of the word-based translation methods and proved to significantly improve the quality of translated output. The morphology of a highly inflected language permits a flexible word order, thus shifting the focus from long-range reordering to the correct selection of a morphological variant. Morphologically rich languages have a large number of surface forms in the lexicon to compensate for a flexible word order. Both Transfer and Interlingua MT employ a generation step to produce the surface form from a given context and a lemma of the word. In order to allow the same type of flexibility in using the morpho-syntactic information in translation, factored translation models (Koehn and Hoang, 2007) provide the possibility to integrate the linguistic information into the phrase-based translation model. Most of the statistical machine translation (SMT) approaches that have a morphologically rich language as target employ factored translation models. Our approach is similar to several other factored machine translation experiments such as adding the morphological features as factors (Avramidis and Koehn, 2008) . Our results confirm findings of other researchers, namely that when very large parallel corpora are available, minimal pre-processing is sufficient to get better results than the baseline (raw data); however, when only a limited amount of training data is available, better results are achieved with partof-speech tags and complex morphological analysis (Habash and Sadat, 2006) . Romanian is a morphologically rich language which needs more than 1200 lexical tags in order to be compliant with the Multext-East lexical specifications (Erjavec and Monachini, 1997) . Czech and Slovene require more than 2000 such morpho-lexical descriptors (MSDs). These descriptors encode detailed linguistic information (gender, case, modality, tense etc.) which can be extremely useful for an accurate translation based on factored models. The set of MSDs can be reduced without information loss by exploiting the redundancy between various featurevalue combinations in these descriptors. Yet, the resulting tagsets are too large and thus the datasparseness hampers the reliability of automatic assignment of MSDs to arbitrary new texts. Tiered tagging (Tufiș, 1999) is a two-stage technique addressing the issue of training data sparseness. It uses an automatically induced intermediary tag-set, named CTAG tagset, of a smaller size on the basis of which a common POS tagging technique can be used. In a second phase, it replaces the tags from the small tag-set with tags from the fully-specified morphosyntactic tag-set (MSD tag-set) also taking into consideration the context. The second phase of tiered tagging relies on a lexicon and a set of hand-written rules. The original idea of tiered tagging has been extended in (Ceaușu, 2006) , so that the second phase is replaced with a maximum entropy-based MSD recovery. In this approach, the rules for CTAG to MSD conversion are automatically learnt from the corpus. Therefore, even the CTAG labels assigned to unknown words can be converted into MSD tags. If an MSD-lexicon is available, replacing the CTAG label for the known words by the appropriate MSD tags is almost 100% accurate. System overview Factored translation models extend the phrasebased translation by taking into account not only the surface form of the phrase, but also additional information like the dictionary form (lemma), the part-of-speech tag or the morpho-syntactic specification. It also provides, on the target side, the possibility to add a generation step. All these new features accommodate well in the log-linear model employed by many decoders: ܲሺ݁|݂ሻ = ‫ݔ݁‬ ∑ ߣ ℎ ሺ݁, ݂ሻ ୀଵ (1) where h i (e,f) is a function associated with the pair e, f and λ i is the weight of the function. To improve the translation into morphologicallyrich languages, the multitude of options provided by the factored translation can help validate the following assumptions: a) Aligning and translating lemma could significantly reduce the number of translation equivalency classes, especially for languages with rich morphology; b) Part of speech affinities. In general, the translated words tend to preserve their part of speech and when this is not the case, the part-of-speech chosen is not random; c) The re-ordering of the target sentence words can be improved if language models over POS or MSD tags are used. In order to test the improvement of the factored model over the phrase-based approach, we built strong baseline systems for the RO-EN language pair (Ceaușu and Tufiș, 2011) . The intuition that motivated our experiments is that the same methodology used in translating from language A into language B could be applied for (partially) correcting the initial translation errors. We wanted to validate this idea without recourse to additional resources. To this end, we built a two -layered cascaded translation system. The first step was to create the best possible direct translation system S1 for A B. For this we started from a parallel corpus: {C A ,C B }. Using this corpus we trained a factored phrasedbased translation model. Having the A B system obtained (Ceaușu and Tufiș, 2011) , we prepared for the second system S2 by translating the entire training corpus C A into language B, obtaining T S1 (C A ). Using the new parallel corpus {T S1 (C A ),C B } we trained the second system S2. At this point we chained the two systems together: we give an input text I A (in language A), the first system translates I A to T S1 (I A ) which is the input for the second system. Thus, the chained system receiving the input I A produces the output O B :T S2 (T S1 (I A )). We further present the steps taken to build this cascaded system and compare the translation performance against the direct, S1 one-step system. Data Preparation The corpus used to train any SMT system has the biggest influence on translation quality, so special attention is given to its preparation. For the purposes of this paper we used the bilingual parallel corpus (Romanian-English) that had been developed during the ACCURAT FP-7 research project. We chose this resource because it is a reasonably large parallel corpus between a highly inflectional language (Romanian) and a less inflectional reference language (English). The content of the corpus is drawn from several other corpora: 1) DGT-TM 1 , law and juridical domain, approx. 650,000 sentences; 2) EMEA (Tiedemann, 2009) , medical corpus, approx. 994,000 sentences; 3) Romanian-English part of the multilingual thesaurus Eurovoc 2 , (1-5 words), approx. 6,500 bilingual terms, treated as short sentences; 4) PHP 3 , translation of the PHP software manual, approx. 30,000 sentences; 5) KDE 4 , translation of the Linux KDE interface, approx. 114,000 sentences; 6) SETIMES 5 , news corpus, approx. 170,000 sentences. In total, the source Romanian -English corpus has over 1,950,000 sentences. However, the corpus needed to be cleaned and annotated. This was performed in three steps: 1) Step 1 -Initial corpus cleaning -We created a cleaning application that removes duplicate lines (ex: the PHP corpus contains many identical lines), lines that contain only/mostly numbers (such as lines that consist only of telephone numbers), lines that contain no Latin characters, lines that contain less than 3 characters and other similar heuristics. Additionally, there are three specific types of text distortions occurring in Romanian texts: (i) missing diacritical characters, (ii) different encoding codes for the same diacritical characters, and (iii) different orthographic systems. When ignored, they have a negative impact on the quality of translation and language models and, thus, on the translation results. For details on the process of diacritics restoration, see (Tufiș and Ceaușu, 2008) . 2) Step 2 -Corpus annotation -The parallel corpus was annotated using our NLP tools (Tufiș et al., 2008) that tokenize, lemmatize and tag the input text. The tagger does its job both in terms of CTAG and MSD tagsets. This annotation was performed for both Romanian and English sides of the corpus. The annotation has the Moses (Koehn et al., 2007) input file structure. For example, the sentence: \"Store in the original package.\" has been annotated as shown in Table 1 , one token per line followed by three additional fields, separated by \"|\": English Romanian Store|store^Nc|NN|Ncns A|avea^Va|VA3S|Va-- 3s in|in^Sp|PREP|Sp se|sine^Px|PXA| Px3--a--------w the| the^Dd| DM|Dd pastra|pastra^Vm|V3| Vmii3s original|original^Af| în|în^Sp|S|Spsa package|package^Nc| NN|Ncns ambalajul|ambalaj^Nc| NSRY|Ncmsry original|original^Af| ASN| Afpms-n Table 1 . EN-RO annotated sentence pair 0 -surface form -the token itself; 1 -lemma of the token, trailed (^) with the grammar category; 2 -CTAG -tag from the reduced tagset; 3 -MSD -Morpho-Syntactic Annotation tag. 3) Step 3 -Final cleaning -The last step involved using the Moses cleaner, a Perl script that ensured that the corpus did not contain illegal characters, spaces, etc. and that the two corpus sides (Romanian -English) had an equal number of sentences. After these cleaning steps the RO-EN corpus was reduced to around 1,250,000 sentences. Finally, the corpus was randomized and 1200 sentence-pairs (T RO -T EN ) were extracted that represent the RO-EN test files. Translation experiments First layer translation system (S1) The first step was to decide on a model for the direct Romanian ↔ English translation. Several models have been proposed and tested. Using the Moses SMT software, we have created the following models (we have experimented with several more models, but kept here only the top performers for reference): Model # Details #1 t0-0 m0 #2 t1-1 g1-0 m0 #3 t1-1 g1-3 t3-3 g1,3-0 , m0m3 #4 t1-1 g1-3 t3-3 g1,3-0 , m0m3 r0 #5 t1-1 g1-3 t3-3 g1,3-0 , m0m3 r3 Table 2 . Models description for the first layer Notation: t = translation step, g = generation step, m = language model, r = reordering model. The first model (#1) simply translates surface forms in language A to surface forms in language B (t0-0). The second model (#2) first translates lemmas in language A to lemmas in language B (t1-1) and then employs a generation step to generate surface forms in language B from lemmas in language B (g1-0). The third, fourth and fifth models (#3, #4, #5) follow a more complex path. They first start with a lemma-lemma translation (t1-1), followed by a lemma to MSD generation in language B (g1-3), a translation of MSDs in language A to MSDs in language B (t3-3) and finally generating surface forms from the previously translated lemmas and MSDs in language B (g1,3-0). They use two language models. While models #1 and #2 use just a surface language model, models #3, #4 and #5 additionally use a MSD language model. The difference between models #3, #4 and #5 is that model #4 uses a reordering model based on surface forms while model #5 uses reordering based on MSDs. Table 3 presents the BLEU scores (Papineni et al., 2002) obtained testing the five proposed models. For the Romanian English direction, model #3 was the best performing of five, with a BLEU score of 57.01. For the English Romanian direction, scores were a bit lower, model #2 having the highest 53.94 BLEU points. Interestingly, the large size of the corpus shows its power, bringing the score of the unfactored model #1 very close to the factored models. The next step was to estimate the translation time of the corpus. This was necessary because of the size of the training corpus: approx. 1.25 million sentences. Moses offers two different translation options: the default translation search and the cube pruning search algorithm. There are two adjustable parameters: the stack size and beam search. These parameters have been manually specified to obtain insights about their influence on translation speed and quality. We present only model #3 for the RO EN direction. The translation time includes language model and translation/generation tables loading time. The test machine is a dedicated 16 core (8 physical + 8 virtual, running at 2.6GHz), 12 GM RAM server. Table 4 shows measurements for the translation times and BLEU scores (RO EN direction) of the test files (1200 sentences), for different settings of the Stack Size and Beam Search. Even though the best performing translation was achieved using the default parameters (BLEU score: 57.01), due to the very long translation time, we found that the best compromise was to use the cube pruning algorithm with the stack size 2000 that obtains a marginally lower BLEU score of 56.29. When using the cube pruning algorithm, we found that, for our test set, increasing the stack size to more than 2000 does not generate any noticeable score improvements. Based on these results, we have used the two best performing models (model #3 for the RO EN direction and model #2 for the EN RO direction) with the cube pruning search algorithm to translate both languages of the parallel corpus {C RO , C EN }. We obtained two new corpora: for the RO EN direction we obtained the {T S1 (C RO ),C EN } corpus, and for the EN RO direction we obtained the {C RO ,T S1 (C EN )} corpus. After the translation, the final phase of this step was to process the two newly obtained corpora. Using the same NLP tool we used to annotate the original corpus we annotated the translated corpora with lemma, CTAGs and MSDs. Finally, the annotated corpora were cleaned again, but using only step 3 (the Moses cleaning script) of the cleaning process described in section 3. The cleaning yielded for the RO EN direction a corpus of around 1,110,000 sentences (losing in this second cleaning process about 140,000 sentences -around 11% -from the initial 1,250,000), while for the EN RO direction the corpus lost almost 240,000 sentences resulting in a corpus of 1,010,000 sentences. Second layer translation system (S2) For this step, using the intermediary corpus, we trained 9 models to see which one would perform best. Table 5 shows the models chosen and table 6 shows the translation and BLEU scores using the cube pruning and default translation algorithms. The same models were used for both translation directions. Table 5. S2: Models description Translating was performed with both default parameters and using the cube pruning search with stack size 2000. The reordering model is the Moses default, with the only difference that in model 5 we have used MSDs as the reordering factor. For testing S2 we used the same test files as for S1, but translated with the best S1 models: the model #3 for RO EN direction and the model #2 for the EN RO direction (see Table 3 ). The reference translations for the two directions were T EN and T RO respectively (1200 sentences each). For the RO EN direction the BLEU translation score of the S1+S2 system has been improved from the best S1 model (57.01) to a new BLEU score of 60.90. The fact that S2 translation based on model #7 (surface form & lemma to surface form & lemma using only the surface language model) was the fastest and most accurate is not surprising: we \"translated\" from partly broken English into presumably better English. Generation steps were not necessary and the information on the lemma eliminated some candidates from the search space. Interestingly, the translation time the using default Moses parameters is very close to the cube pruning search (because the chosen model has just phrase translation and no generation component), but yields approximately 0.14 BLEU point increase. Table 7 shows that for the EN RO direction, the S2 system models #7 and #8 have a similar performance, increasing the BLEU score from the original 53.94 points to 54.44 (0.5 BLEU point net increase). As with the RO EN direction, the S2 models that employ generation steps actually slightly decrease the score. Evaluation procedure and discussion After the original corpus was annotated and cleaned, it was split into two separate files for each language: training set and test set. The test file T EN -T RO contains 1200 aligned sentences. Since the sentences were extracted from the randomized corpus after cleaning, the test files contain sentences from all genres that make up the original corpus, so they represent in-domain data. In Tables 6 and 7 we showed that the cascaded factored SMT (S1+S2) performs better than the baseline system (S1) for both translation directions, in terms of BLEU scores. We were inter- Model Details #1 t0-0 m0 #2 t1-1 g1-0 m0 #3 t1-1 g1-2 t2-2 g1,2-0 m0,m2 #4 t1-1 g1-3 t3-3 g1,3-0 m0,m3 #5 t1-1 g1-3 t3-3 g1,3-0 m0,m3 r3 #6 t1-1 g1-2 t2-2 g2-3 t3-3 g1,3-0 m0,m2,m3 #7 t0,1-0,1 m0 #8 t0,1,2-0,1,2 m0,m2 #9 t1,2-t1,2 m0,m2 ested to see which were the most distant translations from the reference, assuming that these were bad translations. We computed for each sentence I the similarity scores SIM between its translations and the reference translation. These scores were computed with the same BLEU-4 function used for bitexts. Similarly to the BLEU score applied to a bitext, 100 means perfect match and 0 means complete mismatch. Thus, we obtained 1200 pairs of scores ‫ܯܫܵ‬ ௌଵ ூ and ‫ܯܫܵ‬ ௌଵାௌଶ ூ . We also compute the average similarity scores as ଵ ଵଶ ∑ ‫ܯܫܵ‬ ௌఈ ூ ଵଶ ூୀଵ where S α is S1 or S1+S2. As expected, the average SIM scores make the same ranking as the BLEU scores, although they are a bit higher (ex: 61.18 for S1 and 63.58 for S1+S2 for the RO EN direction). We briefly comment on the results of this analysis for the Romanian-English translation direction. We manually analysed the test set translations. We identified 3 sentences with their translations having a zero SIM score for both systems. The explanation was that the reference translation was wrongly aligned to the source sentence. S1 produced 72 perfect translations (score 100) while S1+S2 produced 105. Only 57 perfect translations were common to S1 and S1+S2, meaning that S1+S2 actually deteriorated a few of the original perfect translations. By analyzing the 15 translations that were \"deteriorated\" we noticed that they were identical, except that unlike S1+S2, S1 and Reference translations either had a differently capitalized letter that marginally lowered the score or had multiword units joined by underscores (e.g. as well as vs. as_well_as). This was a small bug which has been removed and which, overall, brought a 0.05 increase in the BLEU score. One of the \"degraded\" translation pair is given below: RO: după examinarea problemelor și consecințelor posibile , Uniunea Democrată Croată a Primului Ministru Ivo Sanader și aliații săi parlamentari au decis să sprijine amânarea . S1: after examination problems and possible consequences , the Democratic Union of Croatian Prime_Minister Ivo Sanader and his allies lawmakers decided to support the postponement . (score 0.1794) S1+S2: after examination problems and possible consequences , the Croatian Democratic Union of Prime Minister Ivo Sanader and his allies lawmakers decided to support the postponement . (score 0.1695) EN REF : after considering possible issues and consequences , Prime_Minister Ivo Sanader 's Croatian Democratic Union and its parliamentary allies decided to support a delay . \" If one ignores the underscore issue in the S1+S2 translation, then this translation is better than the one of S1. A frequent translation difference with respect to the reference translations is illustrated by the example above: the Saxon genitive construction for noun phrases is replaced by a prepositional genitival construction (in this case the word order is closer to the Romanian word order). The capitalization and punctuation are other sources of lower scoring against the reference. All these examples show the sensitivity of the BLEU scoring method, especially for very short sentences. Another important variable to note is the amount of change from one layer to the other: out of all sentences, around 37% had a BLEU increase while around 20% had a BLEU decrease (but see the comment on the underscore difference), the rest 43% have not been changed in any way. Overall, we obtain a 3.89 BLEU point increase for the RO EN direction and a smaller 0.5 BLEU point increase for the more difficult EN RO direction using our cascaded system. Another interesting result was to evaluate the simple cascading systems without feature models, that is (S1=t0-0m0)+(S2=t0-0m0) and compare their performances with the direct translations and the best feature-models cascaded systems. The results are shown in Table 8 . The increased accuracy due to various feature combinations versus the baseline system has been apparent from Tables 6 and 7 compared to the results in Table 3 . Table 8 shows that the direct translations (S1 with any model) for both directions have BLEU scores lower than the cascaded system (S1+S2) even when feature models were not used (model #1+#1). Thus, we can support the statement that the morphological features and the cascading idea are beneficial to the overall accuracy of translations (at least between Romanian and English). the three not făcuseră any movement. the three not to make any movement. the three men never stirred. Table 9 . Out-of-domain text S1 / S1+S2 translation improvement / degradation examples for RO EN Given the corpus is almost entirely composed of juridical and medical texts, we were anxious to see how the second translation step would perform on out-of-domain texts. To make things even harder, we chose a different genre: literary fiction. We extracted 1000 sentences between 3 and 40 words long from Orwell's \"1984\" novel. This test text is challenging because it contains many out of vocabulary words, new senses, frequent subject-elided constructions (Romanian is a pro-drop language), verbal tenses specific to literary narratives which are practically absent from the training data. Another challenge was due to the Romanian translation of Orwell's original, which is not a wordfor-word translation, but a literary one. We tested only the RO EN direction with the following results: the first translation system (S1) obtained a score of 27.53 BLEU points (model #3), while the second system (S2) marginally improved the translation to 27.70. Out of the 1000 sentences, 69 have had their scores properly increased and 76 slightly \"decreased\". However, even if the overall BLEU score increase was minimal, we observed that the translation quality has improved from a human analysis point of view. The positive and negative examples (Table 9 ) show that even though the changes in SIM score are minimal, the text produced by S2 corrects some of the unknown words of S1 (by synonyms or paraphrases, not matching the reference) as well as phrase structure by better word choice and word reordering (corrections missed by the BLEU/SIM scores). Finally, we took the cascading idea one step further by repeating the entire train-translate process (step 2), obtaining S3(S2(S1(T source ))). We observed that the translation stabilized, with very few sentences being changed (around 1%), and with the changes being minor (increasing or even decreasing the BLEU score by less than ~0.05 points). We concluded that further cascading would not bring significant improvements. Conclusions and future work This article presented a simple but effective way of further improving the quality of a phrasedbased statistical machine translation system, by cascading translators. We are not aware of better translation scores for the Romanian-English pair of languages. The idea of post-processing the output of a SMT system is not new but, this step was most often than not based on hand-crafted rules or other knowledge intensive methods. A similar idea was recently reported in (Ehara, 2011) but, their EIWA ensemble is based on a commercial rule-based MT (specialized in patent translation) for the first step and a MOSES-based SMT for the second phase (named statistical post-editing). There are several other methodological differences between our system and the one described in (Ehara, 2011) . EIWA does not work in real time because before proper translation of a text T, the SMT post-editor is trained on a text similar to T. The similar text is constructed from a large patent parallel corpus (3,186,284 sentence pairs) by selecting for each sentence in T an average number of 127 similar sentences. We use the same SMT system trained on different parallel data. The first system S1, trained on parallel data {C A ,C B } learnt to produce draft translations from L A to L B . The second translation system S2, trained on the \"parallel\" data {S1(C A )-C B }, learnt how to improve the draft translations. Except for the training data and the different parameter settings, the two systems are incarnations of the same basic system. Contrary to Ehara (2011) , we found that setting the distortion parameter to a non-null value improves the translation quality. Translation of a new, unseen text is achieved in real time (no retraining at the translation time). While in (Ehara, 2011) improvements were reported for two language pairs (Japanese to English and Chinese to English), we showed that our approach, for the present moment, works only for one language pair (Romanian and English) but in both translation directions. We also showed that the cascaded approach improves the translation quality for both in-domain and out-of-domain texts, although not to the same degree. As future research, we are considering extending the factored experiment with comparable parallel data. The comparable data is available through the ACCURAT project. The aim of the ACCURAT project, to be finalized in June this year, is to research methods and techniques to overcome one of the central problems of machine translation (MT) -the lack of linguistic resources for under-resourced areas of machine translation. Within this context various narrow domain adaptation techniques will be evaluated and experiments will be conducted for several other language pairs. Acknowledgments. This work has been supported by the ACCURAT project (www.accuratproject.eu/) funded by the European Community's Seventh Framework Program (FP7/2007-2013)  under the Grant Agreement n° 248347.",
         "9885664",
         "ff5eb092464333ab1e5fdacdbb4b08d70f68fc7d",
         "6",
         "https://aclanthology.org/2012.eamt-1.36",
         "European Association for Machine Translation",
         "Trento, Italy",
         "2012",
         "May 28{--}30",
         "Proceedings of the 16th Annual conference of the European Association for Machine Translation",
         "Tufi{\\c{s}}, Dan  and\nDumitrescu, {\\textcommabelow{S}}tefan Daniel",
         "Cascaded Phrase-Based Statistical Machine Translation Systems",
         "129--136",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "tufis-dumitrescu-2012-cascaded",
         null,
         null
        ],
        [
         "23",
         "N03-4001",
         "Searching online information is increasingly a daily activity for many people. The multilinguality of online content is also increasing (e.g. the proportion of English web users, which has been decreasing as a fraction the increasing population of web users, dipped below 50% in the summer of 2001). To improve the ability of an English speaker to search mutlilingual content, we built a system that supports cross-lingual search of an Arabic newswire collection and provides on demand translation of Arabic web pages into English. The cross-lingual search engine supports a fast search capability (sub-second response for typical queries) and achieves state-of-the-art performance in the high precision region of the result list. The on demand statistical machine translation uses the Direct Translation model along with a novel statistical Arabic Morphological Analyzer to yield state-of-the-art translation quality. The on demand SMT uses an efficient dynamic programming decoder that achieves reasonable speed for translating web documents.",
         "Searching online information is increasingly a daily activity for many people. The multilinguality of online content is also increasing (e.g. the proportion of English web users, which has been decreasing as a fraction the increasing population of web users, dipped below 50% in the summer of 2001). To improve the ability of an English speaker to search mutlilingual content, we built a system that supports cross-lingual search of an Arabic newswire collection and provides on demand translation of Arabic web pages into English. The cross-lingual search engine supports a fast search capability (sub-second response for typical queries) and achieves state-of-the-art performance in the high precision region of the result list. The on demand statistical machine translation uses the Direct Translation model along with a novel statistical Arabic Morphological Analyzer to yield state-of-the-art translation quality. The on demand SMT uses an efficient dynamic programming decoder that achieves reasonable speed for translating web documents. Overview Morphologically rich languages like Arabic (Beesley, K. 1996 ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix) . By segmenting words into morphemes, we can improve the performance of natural language systems including machine translation (Brown et al. 1993 ) and information retrieval (Franz, M. and McCarley, S. 2002) . In this paper, we present a cross-lingual English-Arabic search engine combined with an on demand Arabic-English statistical machine translation system that relies on source language analysis for both improved search and translation. We developed novel statistical learning algorithms for performing Arabic word segmentation (Lee, Y. et al 2003) into morphemes and morphological source language (Arabic) analysis (Lee, Y. et al 2003b) . These components improve both monolingual (Arabic) search and cross-lingual (English-Arabic) search and machine translation. In addition, the system supports either document translation or convolutional models for cross-lingual search (Franz, M. and McCarley, S. 2002) . The overall demonstration has the following major components: 1. Mono-lingual search: uses Arabic word segmentation and an okapi-like search engine for document ranking. 2. Cross-lingual search: uses Arabic word segmentation and morphological analysis along with a statistical morpheme translation matrix in a convolutional model for document ranking. The search can also use document translation into English to rank the Arabic documents. Both approaches achieve similar precision in the high precision region of retrieval. The English query is also morphologically analyzed to improve performance. 3. OnDemand statistical machine translation: this component uses both analysis components along with a direct channel translation model with a fast dynamic programming decoder (Tillmann, C. 2003) . This system ) that we detect and highlight in Arabic text and provide the translation of these entities into English. The highlighted named entities help the user to quickly assess the relevance of a document. All of the above functionality is available through a web browser. We indexed the Arabic AFP corpus about 330k documents for the demonstration. The resulting search engine supports sub-second query response. We also provide an html detagging capability that allows the translation of Arabic web pages while trying to preserve the original layout as much as possible in the on demand SMT component. The Arabic Name Entity Tagger is currently run as an offline process but we expect to have it online by the demonstration time. We aslo include two screen shots of the demonstration system. Acknowledgments This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No. N66001-99-2-8916. The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.",
         "28059334",
         "0d41aa6d5777da66acd938ea08b4bd536e190fce",
         "2",
         "https://aclanthology.org/N03-4001",
         null,
         null,
         "2003",
         null,
         "Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations",
         "Al-Onaizan, Yaser  and\nFlorian, Radu  and\nFranz, Martin  and\nHassan, Hany  and\nLee, Young-Suk  and\nMcCarley, J. Scott  and\nPapineni, Kishore  and\nRoukos, Salim  and\nSorensen, Jeffrey  and\nTillmann, Christoph  and\nWard, Todd  and\nXia, Fei",
         "{TIPS}: A Translingual Information Processing System",
         "1--2",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "al-onaizan-etal-2003-tips",
         null,
         null
        ],
        [
         "24",
         "2008.eamt-1.4",
         "We compare three commercial Machine Translation (MT) systems, Power Translator Pro 1 , SYSTRAN 2 , and T1 Langenscheidt 3 , with the research hybrid, statistical and rule-based system, METIS-II, with respect to identification of idioms. Firstly, we make a distinction between continuous (adjacent constituents) and discontinuous idioms (non-adjacent constituents). Secondly, we describe our idiom resources within METIS-II, the system's identification process, and we evaluate the results with simple techniques. From the translation outputs of the commercial systems we deduce that they cannot identify discontinuous idioms. We prove that, within METIS-II, the identification of discontinuous idioms is feasible, even with low resources.",
         "We compare three commercial Machine Translation (MT) systems, Power Translator Pro 1 , SYSTRAN 2 , and T1 Langenscheidt 3 , with the research hybrid, statistical and rule-based system, METIS-II, with respect to identification of idioms. Firstly, we make a distinction between continuous (adjacent constituents) and discontinuous idioms (non-adjacent constituents). Secondly, we describe our idiom resources within METIS-II, the system's identification process, and we evaluate the results with simple techniques. From the translation outputs of the commercial systems we deduce that they cannot identify discontinuous idioms. We prove that, within METIS-II, the identification of discontinuous idioms is feasible, even with low resources. 1 Introduction Wehrli (1998) points out in speaking about some current commercial translation systems that \"a simple glance at some of the current commercial translation systems shows that none of them can be said to handle multi-word expressions in an appropriate fashion\". Furthermore, the manual of Power Translator Pro explicitly warns the users to avoid inputting sentences containing idioms if they expect high-quality translation. Section 2 discusses a basic distiction between continuous and discontinuous idioms on the basis of their syntactic realization. In section 3 we introduce the research system, METIS-II, and refer to its idiom identification process. Then, we evaluate this process arriving at the result that METIS-II can translate successfully both continuous and discontinuous idioms. In section 4 we provide some general information about the commercial systems and give their translation outputs of input sentences containing an idiom. Realization of Continuous and Discontinuous Idioms The distinction between continuous and discontinuous idioms is noteworthy. The idioms are called continuous when their constituents are adjacent to each other (see Example 1). By contrast, when an alien element is inserted between two idiom's constituents, the idioms are called discontinuous (see Example 2). The alien elements in Example 2 are an adverb, oft (often), and a prepositional phrase (PP), wegen Stress (due to stress). A subordinate clause could be inserted too. The kind of elements which can split the adjacency of the idiom's constituents is outside the scope of this paper. More information can be found in (Anastasiou & Carl, 2008) . As for the frequency of the two realizations of idioms, we measured their distribution in our German corpus of 486 sentences containing an idiom and we found that 73,8% is covered by continuous and 26,2% by discontinuous idioms. (1) Niemand will auf die Nase fallen. Nobody wants to come a cropper. (2) Er fällt oft wegen Stress auf die Nase. He often comes a cropper due to stress. Hybrid Statistical and Rule-based MT System METIS-II In the following sections we provide information about our idiom resources, describe the idiom identification and translation process of METIS-II, and evaluate the results. METIS-II Resources METIS-II is considered as a hybrid system since it combines statistical tools and linguistic rules. It has Dutch, German, Greek and Spanish as source languages (SL), and British English as target language (TL). It uses the British National Corpus (BNC) as well as resources, which are language-specific for both SL and TL, such as bilingual dictionaries, tokenizer, part-of-speech (PoS) tagger, chunker, lemmatizer, and manually constructed matching rules. The matching rules \"transfer\" the structures from the SL to TL. We experiment with the German-English language pair and focus on idiom processing. The low resources we use for the idiom processing within METIS-II are the following: 1) Bilingual (German-English) idiom dictionary of 871 entries; 2) Monolingual (German) corpus of 486 sentences containing an idiom; 3) Syntactic rules according to the German topological field model. Our dictionary and corpus are described in detail in (Anastasiou & Carl, 2008) . Idiom Dictionary As for the dictionary entries, 72% of the total amount are idiomatic verb phrases (iVPs). The German lemmas are verb-final (see Examples 3, 4). It is noteworthy that the English lemma-translation counterparts omit the infinitive particle to. It should also be noted that the language sides are independent, i.e. a German idiomatic multi-word expression (MWE) can have an English one-word counterpart (Example 4), or vice versa. Lemmas: PoS-tags (3) auf die Nase fallen iVP come a cropper iVP (4) ins Auge fassen iVP envisage verb Idiom Corpus As far as our German idiom corpus is concerned, it is assembled from four different resources: 1) A subset of the German-English Europarl corpus 4 (Koehn, 2005) ; 2) Manually constructed sentences; 3) Sentences extracted from the Web; 4) Sentences extracted from the digital dictionary of the German language in the 20 th century (DWDS 5 ); All sentences of the corpus contain an idiom. The corpus is manually annotated. There is one pair of angled brackets for continuous idioms (Example 5) and (at least) two pairs of brackets for discontinuous idioms (Example 6). In the first case, the brackets are placed at the beginning and the end of the whole continuous idiom (Example 5), whereas in the latter case, we set them at the beginning and the end of each idiom's constituent (Example 6). (5) Niemand will <MWE id=1> auf die Nase fallen </MWE id=1>. Nobody wants to come a cropper. (6) Er <MWE id=1> fällt </MWE id=1> oft wegen Stress <MWE id=1> auf die Nase </MWE id=1>. He often comes a cropper due to stress. Syntactic Rules As for the syntactic rules, they are based on the German topological field model. We now introduce the topological fields, into which the German main clause is divided: i) Pre-field (Pf); ii) Left bracket (Lb); iii) Middle field (Mf); iv) Post-field (Pf); v) Right bracket (Rb). The verb forms occur in the left and right bracket. The finite verb occurs always in the left bracket. When a modal/auxiliary verb is placed in the left bracket, the infinite verb/participle form occurs in the right bracket. The pre-field is occupied mostly by one constituent, whereas many constituents are present in the middle field, even in a relatively free word order. One or more subclauses often populate the right bracket. More information about the German topological field model can be found in Drach (1964) , DUDEN (1998), and Dürscheid (2000) . Our syntactic rules are based on this field model. Firstly, we observe sentences containing an idiom on the basis of the field model, i.e. we see where the idiom's verb and constituents can occur, and which alien elements can be inserted between them. The topological field model is actually taken into account for the discontinuous idioms (Examples 7a, 7b, 7c, 7d). Secondly, we interpret this topological field model into rules, in order to apply to sentences containing an idiom. Within METIS-II we load these rules, the idiom dictionary and corpus, and then the system reads the sentences of the corpus, trying to \"match\" the idiom contained in each sentence and translates it with reference to the dictionary. \"Match\" in this context means \"identify as an idiom\". The system starts by matching the first idiom's participant. Then, it continues matching each of the remaining idiom's participants, either when there are no gaps (continuous idioms) or with gaps (discontinuous idioms). In the latter case, we have to name the topological syntactic field in which the alien element occurs, in order for the system to know where it should ignore matching the alien element(s). There is only one rule needed for continuous idioms (see Rule 9) and four for discontinuous idioms. The rules for discontinuous idioms are as many as four, because the discontinuous patterns of an idiom in a sentence are four, shown in (7a, 7b,7c, and 7d). We now provide only one rule for discontinuous idioms (Rule 8) which identifies the discontinuous idioms in their most common 6 syntactic discontinuous pattern (7a): : identify as discontinuous iVP. The rule (8) contains three \"constraints\", (a), (b), (c), and one command (d). The constraints (a) and (c) have the attribute-value pair, match=yes, to make clear to the system that they are parts of the idiom which should be \"matched\". The elements of the middle field which do not belong to the idiom should be ignored (match=no). The rule interprets the following discontinuous syntactic structure: i) Constraint (a): The idiom's verb (PoS=verb) occurs in the left bracket (field=Lb); ii) Constraint (b): Arbitrarily many words (asterisk*) which do not belong to the idiom can follow the verb in the middle field (field =Mf); iii) Constraint (c): The idiom's constituent (PP, NP, or combination NP-PP) follows in the middle field (field =Mf) and is the last matched word (last word =yes). As for the continuous idioms, their constituents occur en bloc, thus the rule ( 9 ) is simple: (9) Continuous pattern en bloc = (a) match=yes, last word=no, (b) match=yes, last word =yes : identify as continuous iVP. The first constraint (a) shows that all idiom' s parts should be matched. The system keeps on matching every following idiom's constituent until it finds the last one (last word =yes) which should be matched (match=yes) (see Constraint b). In the following section we explain why these rules are helpful for the translation of idioms. METIS-II Translation of Idioms The translation process of idioms within METIS-II entails the three following steps: 1) SL analysis; 2) SL-to-TL matching; 3) TL generation. The first step of the translation process is the shallow analysis of SL sequences, sentences, or texts. Tokenization, part-of-speech (PoS) tagging, lemmatization, and chunking are the SL analysis stages. In these stages, idioms are processed in the same way as all lexical units. These stages are performed automatically by means of the appropriate morphological tools of the institute IAI. As far as the SL-to-TL matching is concerned, two resources are taken into consideration: i) The bilingual idiom dictionary; ii) The syntactic matching rules. A basic prerequisite for the matching is that the idiom is stored in the bilingual idiom dictionary. METIS-II then takes into account the syntactic rules which are based on the topological field model (shown in 3.1.3). METIS-II is able to identify the idiom only by means of these rules. As aforementioned, the system reads the corpus or other input sentences, to which it matches the idiom (continuous or discontinuous), which is then \"captured\" and considered as a unity. Also, two very important tools for SL-to-TL matching (not restricted to idiom matching) within METIS-II are Expander and Ranker (see Carl, 2007) . Expander is a rule-based software tool which reverses the allomorphy of the SL and TL at the lexical or structural level. For example, it adjusts the word order according to the grammatical rules of each language, e.g.: Ranker is a statistical tool which functions similarly as a decoder used in statistical MT (SMT). It computes the most likely target sentences in a log-linear fashion (Och & Ney, 2002) . In our case, the Ranker gives the following translation versions too, as there is another English lemma (fall flat on one's face) for the same German lemma (auf die Nase fallen): (13) Nobody wants to fall flat on their face. (14) He often falls flat on his face due to stress. 12th EAMT conference, 22-23 September 2008, Hamburg, Germany As for the TL generation, it is performed by using the BNC as a data set of examples. The BNC must be pre-processed at the same level as the input sentence. It is tokenized, tagged, lemmatized, and chunked (see Dirix et al., 2005) . The BNC helps in disambiguating between various translation possibilities and it is used to retrieve the TL word order (Vandeghinste et al., 2005) . The token generator has been described in Carl & Schütz (2005) . We do not focus much on TL idiom generation, but on the matching of the idiom to the input. Since the idiom is matched, then it is also correctly generated in the TL. METIS-II Evaluation We evaluate the matching/identifying of idioms with simple techniques. When the idiom is identified, this is called hit and when it is not, this is called miss. When the idiomatic phrase is used in its literal meaning 7 , but the MT system identifies it as an idiom, this is called noise. As for the evaluation techniques, we compute precision (Pr) as the ratio of the correct items, hits, over hits and noise items: noise hits hits   Pr and recall (Re) as the ratio of the correct items over hits and misses items: misses hits hits   Re The fscore is the result of the following formula: recall precision recall precision fscore     2 We provide two tables considering the evaluation figures of both continuous idioms (Table 1 ) and discontinuous idioms (Table 2 ) of the sentences which are included in the corpus data sets. From the tables it can be seen that the evaluation figures for continuous idioms of all techniques are higher than these for the discontinuous idioms. This is attributed to the fact that discontinuous idioms are more difficult to identify due to their spread compoments through the sentence. Recall Identification of Idioms by Three Commercial MT Systems In this section we provide some information about the companies and the language pairs of three commercial MT systems. Also, we present the same input examples, as within the METIS-II, as well as the commercial systems' translation outputs. We tested only a small sample of 50 sentences, because, even after adding more examples, recall and precision were still lower than 5% and 10% respectively; we also could not advance the translation quality by writing rules. Power Translator Pro It was Globalink Inc., then Lernout and Hauspie Speech Products and now Language Engineering Company (LEC) that publishes Power Translator Pro. Pro versions translate English texts into French, German, Italian, Portuguese, Spanish, and vice versa. The online translation software uses English as Interlingua and thus gives access to 21 languages and over 300 language pairs 8 . We tested the version Power Translator Pro 7.0 (Binder, 2000) . As far as the treatment of idioms is concerned, the shortcoming of Power Translator Pro is the attribution of the idiomatic phrases. There is not any attribution category for verb phrases, where most idioms belong to. The lemmas of the attribution category verb are limited to one word, thus users cannot attribute a multi-word idiomatic verb phrase (iVP) as verb and thus the system cannot identify any idiom (continuous or discontinuous) when the verb is inflected. In Pro 7.0, idiom pairs are included in the main dictionary, as there is not a separate idiom dictionary available. Also, users have to add the idiom pairs to both language sides, which is time-consuming. Below are the Pro's 7.0 translation outputs of two input sentences (15a, 15b), before and after adding the idiom entry, auf die Nase fallen -come a cropper, to the dictionary: (15a) Niemand will auf die Nase fallen. before: Nobody wants on the nose fall. after: Nobody wants come a cropper. (15b) Er fällt oft wegen Stress auf die Nase. before/after: It falls often because of stress on the nose. We see that, in both cases, when the idiom is not stored in the dictionary, Pro 7.0 literally translates the input sentences. Referring to (15a), after adding the idiom entry to the dictionary, the system identifies the idiom and translates it correctly. The output of the input sentence (15b), even after adding the idiom entry to the dictionary, is the same as before. Thus, Pro 7.0 cannot handle this case when the PP-component of the idiom, auf die Nase, is placed at the end of the sentence. We will briefly discuss the continuous idiom of (15a). Its identification is actually not such a difficult task, as the idiom is realized in the sentence in the same form as in the dictionary entry, auf die Nase fallen. However, the whole sentence is not correctly translated, as the English infinitive construction of the idiom does not contain the particle to. If we add the infinitive particle to to the English idiom in the dictionary, then the whole sentence would be correctly translated. METIS-II overcomes this \"problem\" without storing all English verbs with the particle to, which saves time. Also, there is the case when the idiom auf die Nase fallen occurs in German subordinate clause (Example 16), where the verb fallen is finite, 3 rd plural and not infinitive. Then the English translation with the infinitive construction is wrong. (16) Er sagt, dass sie immer auf die Nase fallen. He says that they always come a cropper. *He says that they always to come a cropper. The example (16) shows that the storing of the German idiomatic multi-word expression (MWE) with the English to-idiomatic MWE does not bring satisfying results. SYSTRAN The software of the company SYSTRAN is based on over four decades of expertise and is used by global corporations, Internet portals, and public agencies, such as the US Intelligence Community and the European Commission. The MT system SYSTRAN has over 35 available language pairs and 20 vertical domains. It contains many subject-specific dictionaries and preserves the original layout of the user's documents. We have tested a rather old version, the SYSTRAN Premium 4.0, so we are cautious about current advances of this system. As Power Translator Pro 7.0, it does not contain a separate idiom database. Below are Premium's 4.0 translation outputs of the same input sentences: (17a) Niemand will auf die Nase fallen. before: Nobody wants on the nose fall. after: Nobody wants come a cropper. The output of the sentence (17a), after adding the idiom to the dictionary, is the same as the corresponding output of Power Translator Pro 7.0. Also, if we add to the dictionary to come a cropper, the translation would be the following: Nobody wants to come a cropper. We also input the same German sentence with the finite verb, as shown in ( 16 ) above, and there was the same output as Pro 7.0. Moreover, we tested an input sentence, where the verb is inflected in second position (17b); the idiom is still continuous. (17b) Er fällt auf die Nase. It comes a cropper. Although the idiom is identified and correctly translated, there is a fundamental grammatical mistake, i.e. the German relative pronoun er (3 rd person singular, masculine) is translated as it. It is bizarre that in the case where the inflected verb (other than in 3 rd plural) follows the PP, for example in a German subordinate clause, the idiom could not be identified and was literally translated (auf die Nase fällt -on the nose falls). As for the example (17c) containing the discontinuous idiom, SYSTRAN 4.0, as Power Translator Pro 7.0, cannot identify the idiom, when alien elements occur between the idiom's parts. (17c) Er fällt oft wegen Stress auf die Nase before/after: It often falls because of stress on the nose T1 Langenscheidt Langenscheidt is a privately-held publishing company in the field of language resource literature. The first Langenscheidt software for PC, T1 Standard, came out on the market in 1984. We tested the version T1 Professional. It translates between English -German, Spanish -German, and French -German. Dictionary entries consist of a lexeme, its translation equivalents, and its PoS with corresponding information. T1 Professional includes a Translation Memory (TM) and thus identifies exact and fuzzy matches, and newly translated sentences by using different colors in the screen output. For every source sentence it can present a choice of up to 3 different target sentences from the TM, if that many are found. T1 Professional contains in its TM two external modules, 5,000 phrases/sentences for business letters and a huge idiom collection of 71,000 pairs, which is derived from Langenscheidt's Handwörterbuch Englisch. Some idioms are complete in themselves, but most idioms consist of sentence fragments. Again the same input examples and the system's outputs follow: As we more or less expected from the example (18b), the translation of the discontinuous idiom is not feasible; thus, T1 Professional translates it literally. What astonishes us is that the output of (18a), even after adding the idiom to the dictionary, does not change at all. The idiom has to appear with exactly the same context as it is stored in the idiom collection, in order to be correctly translated. Thus, since translations are always done on complete sentences and T1's idioms occur with specific context in the collection, T1's idiom collection is not meant for automatic translation, but only for manual look-up, as Volk (1998) emphasizes. We also tried to add idiomatic VPs to the main lexicon, whose entries are subject to MT analysis, transfer and generation, but it accepts only one-word verbs. Discussion Among the small number of examples tested, METIS-II performs better than the commercial MT systems for three reasons: 1) METIS-II identifies and translates the continuous idiom correctly, not only in the morphosyntactic form it is stored in the dictionary, as Pro and SYSTRAN do, but also in other forms, too. This is attained through the Expander tool, which takes into account the allomophy of the languages. T1 cannot identify any continuous idioms, as the idiom module is used only for manual look-up. 2) The processing of discontinuous idioms is not feasible at all by any of the three commercial MT systems. METIS-II achieves by means of the syntactic matching rules almost more than 90% recall and 80% precision. 3) METIS-II identifies and accordingly translates correctly the sentences containing idioms, continuous or discontinuous, even if their verb is inflected or the idiom's participants undergo syntactic transformations. Summary and Future Work In this paper, we described the hybrid statistical and rule-based research system METIS-II. We referred to our idiom resources, the system's idiom translation process, and evaluation of results. The evaluation of METIS-II by using simple techniques, gave (almost) always more than 80% recall, precision, and fscore, for both continuous and discontinuous idioms. We also had a short look at three commercial MT systems: Power Translator Pro, SYSTRAN, and T1 Langenscheidt, and how they translate sentences containing idioms. After adding the idiom entry to the dictionary, two of them gave satisfying translation outputs having identified the idiom. However, this is the case only for continuous idioms. In the case of the discontinuous idioms, all three commercial MT systems were uncapable of identifying and accordingly translating the idiom. In the future, we plan to increase the recall and precision within METIS-II, eliminating noise and misses. We will add more entries to the idiom dictionary and enrich our corpus with more sentences containing both continuous and mainly discontinuous idioms, in order to set high standards to face the difficult task of automated idiom matching and translation.",
         "856869",
         "05d094e0205e4696232dad41dabb0ace3024e1b1",
         "5",
         "https://aclanthology.org/2008.eamt-1.4",
         "European Association for Machine Translation",
         "Hamburg, Germany",
         "2008",
         "September 22-23",
         "Proceedings of the 12th Annual conference of the European Association for Machine Translation",
         "Anastasiou, Dimitra",
         "Identification of idioms by machine translation: a hybrid research system vs. three commercial systems",
         "12--20",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "anastasiou-2008-identification",
         null,
         null
        ],
        [
         "25",
         "P98-1023",
         "In this paper we present an algorithm to anchor floating quantifiers in Japanese, a language in which quantificational nouns and numeralclassifier combinations can appear separated from the noun phrase they quantify. The algorithm differentiates degree and event modifiers from nouns that quantify noun phrases. It then finds a suitable anchor for such floating quantifiers. To do this, the algorithm considers the part of speech of the quantifier and the target, the semantic relation between them, the case marker of the antecedent and the meaning of the verb that governs the two constituents. The algorithm has been implemented and tested in a rule-based Japanese-to-English machine translation system, with an accuracy of 76% and a recall of 97%.",
         "In this paper we present an algorithm to anchor floating quantifiers in Japanese, a language in which quantificational nouns and numeralclassifier combinations can appear separated from the noun phrase they quantify. The algorithm differentiates degree and event modifiers from nouns that quantify noun phrases. It then finds a suitable anchor for such floating quantifiers. To do this, the algorithm considers the part of speech of the quantifier and the target, the semantic relation between them, the case marker of the antecedent and the meaning of the verb that governs the two constituents. The algorithm has been implemented and tested in a rule-based Japanese-to-English machine translation system, with an accuracy of 76% and a recall of 97%. Introduction One interesting phenomenon in Japanese is the fact that quantifiers can appear in two main positions, as pre-modifier in a noun phrase (1), or 'floating' as adjuncts to the verb phrase, typically in pre-verbal position (2). 1,2 (1) watashi-wa 3-ko-no kgki-wo tabeta I-TOP 3-CL-ADN cake-ACC ate I ate three cakes (2) watashi-wa kgki-wo 3-ko tabeta I-TOP cake-ACC 3-CL ate I ate three cakes Quantifier 'float' of numeral-classifier combinations is widely discussed in the linguistic liter-1Quantifiers are shown in bold, the noun phrases they quantify are underlined. 2This phenomenon exists in other languages, such as Korean. We will, however, restrict our discussion to Japanese in this paper. ature. 3 Much of the discussion focuses on identifying the conditions under which a quantifier can appear in the adjunct position. The explanations range from configurational (Inoue, 1983; Miyagawa, 1989) to discourse based (Downing, 1996; Alam, 1997) , we shall discuss these further below. There has been almost no discussion of other floating quantifiers, such as quantificational nouns. We call the process of identifying the noun phrase being quantified by a floating quantifier 'anchoring' the quantifier. The necessity of anchoring floating quantifiers for many natural language processing tasks is widely recognized (Asahioka et al., 1990; Bond et al., 1996) , and is important not only for machine translation but for the interpretation of Japanese in general. However, although there are several NLP systems that incorporate some solution to the problem of floating quantifiers, to the best of our knowledge, no algorithm for anchoring floating quantifiers has been given. We propose such an algorithm in this paper. The algorithm uses information about case-marking, sentence structure, part-of-speech, noun and verb meaning. The algorithm has been implemented and tested within the Japanese-to-English machine translation system ALT-J/E (Ikehara et al., 1991). The next section describes the phenomenon of quantifier float in more detail. We then propose our algorithm to identify and anchor floating quantifiers in Section 3. The results of implementing the algorithm in ALT-J/E are dis-3The name 'float' comes from early transformational accounts, where the quantifier was said to 'float' out of the noun phrase. Although this analysis has largely been abandoned, and we disagree with it, we shall continue with accepted practice and call a quantifier in the adjunct position a floating quantifier. cussed in Section 4 and some remaining problems identified. The conclusion summarises the implementation of the algorithm and highlights some of its strengths. 2 Quantifier float in Japanese First we will give a definition of quantifiers. Semantically, quantifiers are elements that serve to quantify, or enumerate, some target. The target can be an entity, in which case the number of objects is quantified, or an action, in which case the number of events (i.e. iterations of the action) are quantified. The quantification can be by a cardinal number, or by a more vague expression, like several or many. In Japanese, quantifiers (Q) axe mainly realised in two ways: numeral-classifier combinations (XC) and quantificational nouns (N). Note that these nouns are often treated as adverbs, as they typically function as adjuncts that modify verbs, a function prototypically carried out by adverbs. They can however head noun phrases, and take some case-markers, so we classify them as nouns. Numeral classifiers form a closed class, although a large one. Japanese and Korean both have two or three hundred numeral classifiers (not counting units), although typically individual speakers use far less, between 30 and 80 (Downing, 1995, 346) . Syntactically, numeral classifiers are a subclass of nouns. The main property distinguishing them from prototypical nouns is that they cannot stand alone. Typically they postfix to numerals, forming a quantifier phrase, although they can also combine with the quantificational prefix s~ \"some\" or the interrogative nani \"what\": (3) 2-hiki \"2 animals\" (Numeral) (4) s~-hiki \"some animals\" (Quantifier) (5) nan-biki \"how many animals\" (Interrogative) Semantically, classifiers both classify and quantify the referent of the noun phrase they collocate with. Quantificational nouns, such as takusan \"much/many\", subete \"all\" and ichibu \"some\", only quantify their targets, there is no classification involved. Numeral classifier combinations appear in seven major patterns of use (following Asahioka et al. (1990) ) as shown below (T refers to the quantified target noun phrase, m is a casemarker): Type Form XC N pre-nominal Q-no T-m + + appositive TQ-m + - floating T-m Q + + Q T-m partitive T-no Q-m + + attributive QT-m + - anaphoric T-m ÷ - predicative T-wa Q-da ÷ - Table 1 : Types of quantifier constructions Noun quantifiers cannot appear in the appositive, attributive, anaphoric and predicative complement patterns. In the pre-nominal construction the relation between the target noun phrase and quantifier is explicit. For numeral-classifier combinations the quantification can be of the object denoted by the noun phrase itself as in (6); or of a subpart of it as in (7) (see Bond and Paik (1997) for a fuller discussion). For nouns, only the object denoted by the noun itself can be quantified. (6) 3-ts~-no tegami 3-CL-ADN letter 3 letters (7) 3-rnai-no tegami 3-CL-ADN letter a 3 page letter In the partitive construction the quantifier restricts a subset of a known amount: e.g., tegamino 3-ts~ \"three of the letters\". This is a very different construal to the pre-nominal construction. Only rational quantificational nouns can appear in the partitive construction. The floating construction, on the other hand, has the same quantificational meaning as the pre-nominal. Two studies indicate that there are pragmatic differences (Downing, 1996; Kim, 1995) . Pre-nominal constructions typically are used to introduce important referents, with nonexistential predicates, while floating constructions typically introduce new number information. In addition floating constructions are used when the nominal has other modifiers, and are more common in spoken text. We will restrict the following discussion to the difference between the pre-nominal and floating uses. Restrictions on quantifier float There have been many attempts to describe the situations under which the floating construction is possible, almost all of which only consider numeral-classifier constructions. The earliest generative approaches suggested that the target in the floating construction must be either subject or object. Inoue (1983) pointed out that quasi-objects, noun phrases marked with the accusative case-marker but failing other tests for objecthood, could also be targets. Miyagawa (1989) gives a comprehensive configurational explanation, where the target and quantifier must mutually c-command each other (that is, neither the target nor the quantifier dominates the other, and the first branching node that dominates either one, dominates the other). The restriction to nominative and accusative targets is explained by proposing a difference in structure. Verb arguments subcategorized for in the lexicon are noun phrases, where the case-marker is a clitic and thus can be c-commanded, whereas adjuncts are headed by their markers, to form post-positional phrases which are thus not available as targets. The c-command relation is applied to both the noun phrases themselves and traces. Quantifiers can be scrambled (moved from their base position after their target) leaving a trace if the target is an affected Theme NP, and the target and quantifier are governed by the verb that assigns this thematic role. Thus quantifiers associated with affected themes can move within the sentence. Affected themes are things that axe \"changed, created, converted, extinguished, consumed, destroyed or gotten-rid of\". Miyagawa (1989, 57) proposes a syntactic test for affectiveness: affected themes can occure in the intransitive resultative construction -te-aru. Alam (1997) looks at the problem from a different angle, and proposes that only quantifiers which are interpreted \"distributively or as a quantified event\" can float, as they take wide scope beyond the NP. A quantified noun phrase will also quantify the event if the noun phrase measures-out the event, where \"direct internal arguments undergoing change in the event described by the verb measure out the event\" a very similar description to that of affected theme. However, Jackendoff (1996) has shown that a wide variety of arguments can measure out processes, not just subjects and objects, but also the complements of prepositional phrases. Which case-roles measure out the process can be pragmatically determined as well as lexically stipulated, so it is not a simple matter to determine which arguments are relevent. The excellent distributional analysis of Downing (1996) shows that actual cases of floating tend to be absolutive, that is quantifiers largely float from intransitive subjects (67%) or direct objects of transitive verbs (24%) rather than from transitive subjects (4%) or indirect objects (1%). On the question of why quantifiers appear outside of the noun phrases they quantify, there have been two explanations: Discourse new information floats to the pre-verb focus position (Downing, 1996; Kim, 1995) , quantifiers float from noun phrases that 'measure out' an event (Alam, 1997) . We speculate that there may be a performance based reason. Hawkins (1994) has shown that many phenomena claimed to be discourse related are in fact largely due to performance. However we have not yet compiled sufficient empirical evidence to show this conclusively. 3 An algorithm to identify and anchor floating quantifiers The proposed algorithm is outlined in Figure 1 . In our implementation it is appplied to each of one or more candidate outputs of a Japanese dependency parser as part of the semantic ranking. Identify potential floating quantifiers The first step is to identify potential floating quantifiers. Every adjunct case element headed by a noun is checked. All numeral classifier combinations are potential candidates. An adjunct must must meet two conditions to be considered a floating quantificational nouns, one semantic and one syntactic. The semantic criterion is that one of the noun's senses must be The syntactic criterion is that the part of speech subcategory must be one of degree or quantifier adverbial. 4 We use the Goi-Taikei (Ikehara et al., 1997) to test for the senses and Miyazaki et al. (1995) for the syntactic classification. Identify potential anchors All noun phrases that matched a case-slot marked with -ga (nominative) or -o (accusative) are accepted as potential anchors. This is the traditional criterion given for potential anchors. Note even if the surface marker is different, for example when the case-marker is overwritten by a focus-marker such as -wa \"topic\", the 'canonical' case-marker will be found by our parser. Noun phrases marked with -hi (dative), have been shown to be permissible candidates, but we do not allow them. Such sentences are, however, rare outside linguistics papers. We found no such candidates in the sentences we examined, and Downing (1996, 239) found only one in ninety six examples. When we tried allowing dative noun phrases, it significantly reduced the performance of our algorithm: every dative noun phrase selected was wrong. If we could determine which noun phrases measure-out the action, then they should also be considered as 4This part of speech category actually includes both true adverbs and adverb-like nouns. candidates, but we have no way to identify them at present. Discard bad combinations Some combinations of anchor and quantifier can be ruled out. We have identified three cases: semantically anomalous cases; sentences where the quantifier modifies the verb as a degree modifier; and sentences where the quantifier modifies the verb as a frequency modifier. 3.3.1 Semantically anomalous cases Singular noun phrases In Japanese, pronouns and names are typically marked with a collectiviser (such as -tachi) if there are multiple referents (see e.g. Martin (1988, 143-154) ). A pronoun or name not so marked characteristically has a singular interpretation. For names this can be overridden by a numeral-classifier combination (8), although it is rare, but not by an quantificational noun (9). In all the texts we examined, we found no examples of names modified by floating numeralclassifier combinations. We therefore block all pronouns and names not modified by a collectiviser from serving as anchors to floating quantifiers. In Japanese, there is not a clear division between pronouns and common nouns, particularly kin-terms such as ojisan \"grandfather/old man\". Pronouns can be modified in the same way as common nouns, and kin-terms are often used to refer to non kin. Pronouns modified by quantifiers need to be translated by more general terms as in ( 10 ). (10) kanojo-tachi-ga 3-nin kita she-COL-NOM 3-CL came ? 3 she came The 3 girls came Classifier semantic restrictions For numeral classifiers, the selectional restrictions of the classifier can be used to disallow certain combinations. For example,-kai \"event\" can only be used to modify event-nouns such as shokuji \"meal\" or fishin \"earthquake\". However, the semantics are very complicated, and there is a great deal of variation, as a classifier can select not just for the object denoted by its target but also a sub-part of it. In addition, classifiers can be used to select meanings figuratively, coercing a new interpretation of their head. Bond and Paik (1997) suggest a way of dealing with this in the generative lexical framework of Pustejovsky (1995) but it requires more information about the conceptual structure of noun phrases than is currently available. For the time being, we use a simple table of forbidden combinations. For example pointo \"point\" will not be used to quantify nouns denoting agent, place or abstract noun. Degree modification Noun quantifiers can be used as degree modifiers as well as quantifying some referent. If the predicate is used to state a property of the potential anchor, then a noun quantifier will characteristically be a degree modifier. We use the verbal semantic attributes given in the Goi-Taikei (Ikehara et al., 1997) to test for this relationship. Anchoring will be blocked either if the potential anchor is nominative and the verbal semantic attribute is one of attribute transfer, existence, attribute or result or if the anchor is accusative and the verbal semantic attribute is physical/attribute transfer. Sentence (ii) shows this constraint in action: (11) kodomo-ga sukoshi samui child-NOM a little cold * A few children are cold The child is a little cold Event modification The final case we need to consider is where the noun quantifier can quantify the event or the affected theme of the event, such as (12). In Japanese, either reading is possible when the quantifier is in pre-verbal position. Anchoring the quantifier is equivalent to choosing the theme reading. (12) kare-wa k~ki-wo takusan tabeta he-TOP cake-NOM much ate He ate cake a lot He ate a lot of cake (event) (theme) Examining our corpus showed the theme reading to be the default. Of course, if the event is modified elsewhere, for example by a temporal modifier, then different readings are possible. The system in which our implementation was tested lacks a system for event quantification, so we were not able to implement any constraint for this phenomenon. We therefore implemented the theme reading as our default. Note that, for stative verbs with permanent readings such as shiru \"know\", there is almost no difference between the two readings (13). (13) watashi-wa ratengo-wo sukoshi Rank and select candidates If there are more than two combinations, the following heuristics are used to choose which one or ones to choose. Prefer accusative: A combination with an accusative anchor gets two points: This is to allow for the absolutive bias. Prefer left anchor: If the anchor is to the left of the quantifier score it with one point: Quantifiers tend to float to the right of their anchors. Prefer closest: Subtract one for each intervening quantifier: Closer targets are better. Finally select the highest scoring combination and eliminate any combinations that include the chosen quantifier and anchor. If there is still a combination left (e.g. there were two quantifiers and two targets) then select it as well. These heuristics rule out crossing combinations in the rare instances of two quantifiers and two candidates. Anchoring Once the best combinations are chosen, the quantifier can be anchored to its target. We consider the best way to represent this would be by showing the semantic relation in a separate level from the syntax, in a similar way to the architecture outlined by Jackendoff (1997) . Our implementation is in a machine translation system and we simply rewrite the sentence so that the floating quantifier becomes an prenominal modifier of its target, marked with the adnominal case-marker -no. The resulting modifier is labeled as 'anchored', to allow special processing during the transfer phase. Results and Discussion The algorithm was tested on a 3700 sentence machine translation test set of Japanese sentences with English translations, produced by a professional human translator. A description of the test set and its design is given in Ikehara et al. (1994) . Overall, 56 possible combinations were found and 37 anchored in 3700 sentences: Table 2 . Of these, 9 were anchored that should not have been, and 1 was not anchored that should have been. The accuracy (correctly anchored/anchored) was 76% (28/37), and the recall (correctly anchored/should be anchored) was 97% (28/29). The major source of errors was from parsing errors in the system as a whole. All of the badly anchored numeral-classifiers combinations were caused by this. In this case, the algorithm has not degraded the system performance, it would have been a bad result anyway. There were three problems with the algorithm itself. In one case an anaphoric quantifier was mistaken as a floating quantifier, in another the verbal semantic attribute check for degree modification gave a bad result. Finally there was one case where the default blocking for semantic anomalies blocked a good combination. Translation of floating quantifiers Note that anchoring a floating quantifier is only the first step toward translating it. Special handling is sometimes needed to translate the anchored quantifiers. For example, Japanese has some universal pronouns that can stand alone as full noun phrases ( 14 ) or act as floating quantifiers ( 15 ): e.g., minna \"everyone\", zen'in \"all members\". When they are anchored, the information about the denotation of the head carried by the pronoun is redundant, and should not be translated. A special rule is required for this. Further work The proposed algorithm forms a solid base for extensions in various ways. 1. Combine it with a fuller system of event semantics. 2. Make the treatment of classifier-target semantics more detailed, so that inbuilt semantic restrictions can be used instead of a table of forbidden combinations. 3. Use the results of the algorithm to help choose between candidate parses and integrate it with the resolution of zero pronouns. 4. Test the algorithm on other languages, for example Korean. Conclusion We have presented an algorithm to anchor floating quantifiers in Japanese. The algorithm proceeds as follows. First identify potential floating quantifiers: either numeral classifier combinations or quantificational nouns. Then identify potential anchors: all accusative or nominative noun phrases. Inappropriate combinations are deleted, either because of a semantic mismatch between the target and quantifier, or because the quantifier is interpreted as a degree or event modifier. Finally, possible combinations are ranked, with the accusative candidate being the best choice, then the closest and leftmost. The algorithm is robust and uses the full power of currently available detailed semantic dictionaries. Zusammenfassung In diesem Papier beschreiben wit einen Algorithmus zur Resolution von 'floating quantifiers' im Japanischen. Japanisch ist eine Sprache, in der quantifizierende Adverbien oder Kombinationen aus Numeral + Klassifikator yon der Nominalphrase, fiir die sie quantifizieren getrennt werden kSnnen, d.h. sie miissen nicht in unmittelbarer linearer Abfolge stehen. Der Algorithmus unterscheidet Grad-und Ereignismodifi_katoren yon Adverbialen, die fiir Nominalphrasen quantifizieren und resolviert den richtigen Antezedenten f'tir jeden 'floating quantifier'. Zur Anbindung an die richtige Nominalphrase finden die folgenden Parameter Beriicksichtigung: Wortart der Quantifikators und des Antezedenten, die semantische Relation zwischen diesen beiden, die Kasusmaxkierungen des Antezedenten und die Semantik des Verbs, das sowohl den Quantifikator als auch dessen Antezedenten regiert. Der Algorithmus wurde implementiert und in einem regel-basierten Japanisch/Englischem U'bersetzungssystem evaluiert. Acknowledgments The authors thank Tim Baldwin, Yukie Kuribayashi, Kyonghee Paik and the members of the NTT Machine Translation Research Group for their discussion and comments on this paper and earlier versions. The research was carried out while Daniela Kurz visited the NTT Communication Science Laboratories. Francis Bond is currently also enrolled part time as a doctoral candidate at the University of Queensland's Center for Language Teaching & Research.",
         "2370623",
         "ee1abe58d4a754c6ff46fe3349a631a33c4987b0",
         "5",
         "https://aclanthology.org/P98-1023",
         "Association for Computational Linguistics",
         "Montreal, Quebec, Canada",
         "1998",
         "August",
         "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",
         "Bond, Francis  and\nKurz, Daniela  and\nShirai, Satoshi",
         "Anchoring Floating Quantifiers in {J}apanese-to-{E}nglish Machine Translation",
         "152--159",
         "10.3115/980845.980870",
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "bond-etal-1998-anchoring-floating",
         null,
         null
        ],
        [
         "26",
         "C98-1023",
         "In this paper we present an algorithm to anchor floating quantifiers in Japanese, a language in which quantificational nouns and numeralclassifier combinations can appear separated from the noun phrase they quantify. The algorithm differentiates degree and event modifiers from nouns that quantify noun phrases. It then finds a suitable anchor for such floating quantifiers. To do this, the algorithm considers the part of speech of the quantifier and the target, the semantic relation between them, the case marker of the antecedent and the meaning of the verb that governs the two constituents. The algorithm has been implemented and tested in a rule-based Japanese-to-English machine translation system, with an accuracy of 76% and a recall of 97%.",
         "In this paper we present an algorithm to anchor floating quantifiers in Japanese, a language in which quantificational nouns and numeralclassifier combinations can appear separated from the noun phrase they quantify. The algorithm differentiates degree and event modifiers from nouns that quantify noun phrases. It then finds a suitable anchor for such floating quantifiers. To do this, the algorithm considers the part of speech of the quantifier and the target, the semantic relation between them, the case marker of the antecedent and the meaning of the verb that governs the two constituents. The algorithm has been implemented and tested in a rule-based Japanese-to-English machine translation system, with an accuracy of 76% and a recall of 97%. Introduction One interesting phenomenon in Japanese is the fact that quantifiers can appear in two main positions, as pre-modifier in a noun phrase (1), or 'floating' as adjuncts to the verb phrase, typically in pre-verbal position (2). 1'2 (1) watashi-wa 3-ko-no k~ki-w__p_o tabeta I-TOP 3-CL-ADN cake-Ace ate I ate three cakes (2) watashi-wa kgki-wo 3-ko tabeta I-TOP cake-ACe 3-CL ate I ate three cakes Quantifier 'float' of numeral-classifier combinations is widely discussed in the linguistic liter-1Quantifiers are shown in bold, the noun phrases they quantify are underlined. 2This phenomenon exists in other languages, such as Korean. We will, however, restrict our discussion to Japanese in this paper. ature. 3 Much of the discussion focuses on identifying the conditions under which a quantifier can appear in the adjunct position. The explanations range from configurational (Inoue, 1983; Miyagawa, 1989) to discourse based (Downing, 1996; Alam, 1997) , we shall discuss these further below. There has been almost no discussion of other floating quantifiers, such as quantiflcational nouns. We call the process of identifying the noun phrase being quantified by a floating quantifier 'anchoring' the quantifier. The necessity of anchoring floating quantifiers for many natural language processing tasks is widely recognized (Asahioka et al., 1990; Bond et al., 1996) , and is important not only for machine translation but for the interpretation of Japanese in general. However, although there are several NLP systems that incorporate some solution to the problem of floating quantifiers, to the best of our knowledge, no algorithm for anchoring floating quantifiers has been given. We propose such an algorithm in this paper. The algorithm uses information about case-marking, sentence structure, part-of-speech, noun and verb meaning. The algorithm has been implemented and tested within the Japanese-to-English machine translation system ALT-J/E (Ikehara et al., 1991) . The next section describes the phenomenon of quantifier float in more detail. We then propose our algorithm to identify and anchor floating quantifiers in Section 3. The results of implementing the algorithm in ALT-J/E are dis-aThe name 'float' comes from early transformational accounts, where the quantifier was said to 'float' out of the noun phrase. Although this analysis has largely been abandoned, and we disagree with it, we shall continue with accepted practice and call a quantifier in the adjunct position a floating quantifier. cussed in Section 4 and some remaining problems identified. The conclusion summarises the implementation of the algorithm and highlights some of its strengths. 2 Quantifier float in Japanese First we will give a definition of quantifiers. Semantically, quantifiers are elements that serve to quantify, or enumerate, some target. The target can be an entity, in which case the number of objects is quantified, or an action, in which case the nmnber of events (i.e. iterations of the action) are quantified. The quantification can be by a cardinal number, or by a more vague expression, like several or many. In Japanese, quantifiers (Q) are mainly realised in two ways: numeral-classifier combinations (XC) and quantificational nouns (N). Note that these nouns are often treated as adverbs, as they typically flmction as adjuncts that modify verbs, a function prototypically carried out by adverbs. They can however head noun phrases, and take some case-markers, so we classify them as nouns. Numeral classifiers form a closed class, although a large one. Japanese and Korean both have two or three hundred numeral classifiers (not counting units), although typically individual speakers use far less, between 30 and 80 (Downing, 1995, 346) . Syntactically, numeral classifiers are a sub-(:lass of nouns. The main property distinguishing them from prototypical nouns is that they cannot stand alone. Typically they postfix to numerals, forming a quantifier phrase, atthough they can also combine with the quantificational prefix s~2 \"some\" or the interrogative nani \"what\" : (3) 2-hiki \"2 animals\" (Numeral) (4) sg-hiki \"some animals\" (Quantifier) (5) nan-biki \"how many animals\" (Interrogative) Semantically, classifiers both classify and quantify the referent of the noun phrase they collocate with. Quantificational nouns, such as takusan \"much/many\", subete \"all\" and ichibu \"some\", only quantify their targets, there is no classification involved. Numeral classifier combinations appear in seven major patterns of use (following Asahioka et al. (1990) ) as shown below (T refers to the quantified target noun phrase, m is a casemarker): Type Form XC N pre-nominal Q-no T-m + + appositive TQ-m + - floating T-m Q + + Q T-m partitive T-no Q-m + + attributive QT-m + - anaphoric T-m + - predicative T-wa Q-da + - Table h Types of quantifier constructions Noun quantifiers cannot appear in the appositive, attributive, anaphoric and predicative complement patterns. In the pre-nominal construction the relation between the target noun phrase and quantifier is explicit. For numeral-classifier combinations the quantification can be of the object denoted by the noun phrase itself as in (6); or of a subpart of it as in (7) (see Bond and Paik (1997) for a fuller discussion). For nouns, only the object denoted by the noun itself can be quantified. (6) 3-tsg-no tegami 3-CL-ADN letter 3 letters (7) 3-mai-no tegami 3-CL-ADN letter a 3 page letter In the partitive construction the quantifier restricts a subset of a known amount: e.g., tegamino 3-ts(, \"three of the letters\". This is a very different construal to the pre-nominal construction. Only rational quantificational nouns can appear in the partitive construction. The floating construction, on the other hand, has the same quantificational meaning as the pre-nominal. Two studies indicate that there are pragmatic differences (Downing, 1996; Kim, 1995) . Pre-nominal constructions typically are used to introduce important referents, with nonexistential predicates, while floating constructions typically introduce new number information. In addition floating constructions are used when the nominal has other modifiers, and are more common in spokeu text. We will restrict the following discussion to the difference between the pre-nominat and floating USES. Restrictions on quantifier float There have been many attempts to describe the situations under which the floating construction is possible, almost all of which only consider numeral-classifier constructions. The earliest generative approaches suggested that the target in the floating construction must be either subject or object. Inoue (1983) pointed out that quasi-objects, noun phrases marked with the accusative case-marker but failing other tests for objecthood, could also be targets. Miyagawa (1989) gives a comprehensive configurational explanation, where the target and quantifier must mutually c-command each other (that is, neither the target nor the quantifier dominates the other, and the first branching node that dominates either one, dominates the other). The restriction to nominative and accusative targets is explained by proposing a difference in structure. Verb arguments subcategorized for in the lexicon are noun phrases, where the case-marker is a clitic and thus can be c-commanded, whereas adjuncts are headed by their markers, to ibrm post-positional phrases which are thus not available as targets. The c-command relation is applied to both the noun phrases themselves and traces. Quantifiers can be scrambled (moved from their base position after their target) leaving a trace if the target is an affected Theme NP, and the target and quantifier are governed by the verb that assigns this thematic role. Thus quantifiers associated with affected themes can move within the sentence. Affected themes are things that are \"changed, created, converted, extinguished, consumed, destroyed or gotten-rid of\"'. Miyagawa (1989, 57) proposes a syntactic test for affectiveness: affected themes can occure in the intransitive resultative construction -te-aru. Alam (1997) looks at the problem from a different angle, and proposes that only quantifiers which are interpreted \"distributively or as a quantified event\" can float, as they take wide scope beyond the NP. A quantified noun phrase will also quantify the event if the noun phrase measures-out the event, where \"direct internal arguments undergoing change in the event described by the verb measure out the event\" a very similar description to that of affected theme. However, Jackendoff (1996) has shown that a wide variety of arguments can measure out processes, not just subjects and objects, but also the complements of prepositional phrases. Which case.roles measure out the process can be pragmatically determined as well as lexically stipulated, so it is not a simple matter to determine which arguments are relevent. The excellent distributional analysis of Downing (1996) shows that actual cases of floating tend to be absolutive, that is quantifiers largely float from intransitive subjects (67%) or direct objects of transitive verbs (24%) rather than from transitive subjects (4%) or indirect objects (1%). On the question of why quantifiers appear outside of the noun phrases they quantify, there have been two explanations: Discourse new information floats to the pre-verb focus position (Downing, 1996; Kim, 1995) , quantifiers float from noun phrases that 'measure out' an event (Alam, 1997) . We speculate that there may be a performance based reason. Hawkins (1994) has shown that many phenomena claimed to be discourse related are in fact largely due to performance. However we have not yet compiled sufficient empirical evidence to show this conclusively. An algorithm to identify and anchor floating quantifiers The proposed algorithm is outlined in Figure 1 . In our implementation it is appplied to each of one or more candidate outputs of a Japanese dependency parser as part of the semantic ranking. Identify potential floating quantifiers The first step is to identify potential floating quantifiers. Every adjunct case element headed by a noun is checked. All numeral classifier combinations are potential candidates. An adjunct must must meet two conditions to be considered a floating quantificational nouns, one semantic and one syntactic. The semantic criterion is that one of the noun's senses must be  subsumed by quanta, few/some, all-part. The syntactic criterion is that the part of speech subcategory must be one of degree or quantifier adverbial. 4 We use the Goi-Taikei (Ikehara et al., 1997) to test for the senses and Miyazaki et al. (1995) for the syntactic classification. Identify potential anchors All noun phrases that matched a case-slot marked with -ga (nominative) or -o (accusative) are accepted as potential anchors. This is the traditional criterion given for potential anchors. Note even if the surface marker is different, for example when the ease-marker is overwritten by a focus-maa'ker such as -wa \"topic\", the 'canonical' case-marker will be found by our parser. Noun phrases marked with -ni (dative), have been shown to be permissible candidates, but we do not allow them. Such sentences are, however, rare outside linguistics papers. We found no such candidates in the sentences we examined, and Downing (1996, 239) found only one in ninety six examples. When we tried allowing dative noun phrases, it significantly reduced the performance of our algorithm: every dative noun phrase selected was wrong. If we could determine which noun phrases measure-out the action, then they should also be considered as 4This part of speech category actually includes both true adverbs and adverb-like nouns. candidates, but we have no way to identify them at present. Discard bad combinations Some combinations of anchor and quantifier can be ruled out. We have identified three cases: semantically anomalous cases; sentences where the quantifier modifies the verb as a degree modifier; and sentences where the quantifier modifies the verb as a frequency modifier. Semantically anomalous cases Singular noun phrases In Japanese, pronouns and names are typically marked with a collectiviser (such as -tachi) if there are multipie referents (see e.g. Martin (1988, 143-154) ). A pronoun or name not so marked characteristically has a singular interpretation. For names this can be overridden by a numeral-classifier combination (8), although it is rare, but not by an quantificational noun ( 9 ). In all the texts we examined, we found no examples of names modified by floating numeralclassifier combinations. We therefore block all pronouns and names not modified by a collectiviser from serving as anchors to floating quantifiers. In Japanese, there is not a cleaa\" division between pronouns and common nouns, particularly kin-terms such as ojisan \"grandfather/old man\". Pronouns can be modified in the same way as common nouns, and kin-terms are often used to refer to non kin. Pronouns modified by quantifiers need to be translated by more general terms as in ( 10 ). (10) kanojo-tachi-ga 3-nin kita she-COL-NOM 3-CL came ? 3 she came The 3 girls came Classifier semantic restrictions For numeral classifiers, the selectional restrictions of the classifier can be used to disallow certain combinations. For example,-kai \"event\" can only be used to modify event-nouns such as shokuji \"meal\" or jishin \"earthquake\". However, the semantics are very complicated, and there is a great deal of variation, as a classifier can select not just for the object denoted by its target but also a sub-part of it. In addition, classifiers can be used to select meanings figuratively, coercing a new interpretation of their head. Bond and Paik (1997) suggest a way of dealing with this in the generative lexical framework of Pustejovsky (1995) but it requires more information about the conceptual structure of noun phrases than is currently available. For the time being, we use a simple table of forbidden combinations. For example pointo \"point\" will not be used to quantify nouns denoting agent, place or abstract noun. 3.3.2 Degree modification Noun quantifiers can be used as degree modifiers as well as quantifying some referent. If the predicate is used to state a property of the potential anchor, then a noun quantifier will characteristically be a degree modifier. We use the verbal semantic attributes given in the Goi-Taikei (Ikehara et al., 1997) to test for this relationship. Anchoring will be blocked either if the potential anchor is nominative and the verbal semantic attribute is one of attribute transfer, existence, attribute or result or if the anchor is accusative and the verbal semantic attribute is physical/attribute transfer. Sentence (Ii) shows this constraint in action: (11) kodomo-ga sukoshi samui child-NOM a little cold * A few children are cold The child is a little cold Event modification The final case we need to consider is where the noun quantifier can quantify the event or the affected theme of the event, such as (12). In Japanese, either reading is possible when the quantifier is in pre-verbal position. Anchoring the quantifier is equivalent to choosing the theme reading. (12) kare-wa k~ki-wo takusan tabeta he-TOP cake-NOM much ate He ate cake a lot He ate a lot of cake (event) (theme) Examining our corpus showed the theme reading to be the default. Of course, if the event is modified elsewhere, for example by a temporal modifier, then different readings are possible. The system in which our implementation was tested lacks a system for event quantification, so we were not able to implement any constraint for this phenomenon. We therefore implemented the theme reading as our default. Note that, for stative verbs with permanent readings such as shiru \"know\", there is almost no difference between the two readings (13). (13) watashi-wa ratengo-wo sukoshi Rank and select candidates If there are more than two combinations, the following heuristics are used to choose which one or ones to choose. Prefer accusative: A combination with an accusative anchor gets two points: This is to allow for the absolutive bias. Prefer left anchor: If the anchor is to the left of the quantifier score it with one point: Quantifiers tend to float to the right of their anchors. Prefer closest: Subtract one for each intervening quantifier: Closer targets are better. Finally select the highest scoring combination and eliminate any combinations that include the chosen quantifier and anchor. If there is still a combination left (e.g. there were two quantifiers and two targets) then select it as well. These heuristics rule out crossing combinations in the rare instances of two quantifiers and two candidates. Anchoring Once the best combinations are chosen, the quantifier can be anchored to its target. We consider the best way to represent this would be by showing the semantic relation in a separate level fl'om the syntax, in a similar way to the architecture outlined by Jackendoff (1997) . Our implementation is in a machine translation system and we simply rewrite the sentence so that the floating quantifier becomes an prenominal modifier of its target, marked with the adnominal case-marker -no. The resulting modifier is labeled as 'anchored', to allow special processing during the transfer phase. 4 Results and Discussion The algorithm was tested on a 3700 sentence machine translation test set of Japanese sentences with English translations, produced by a professional hulnan translator. A description of the test set and its design is given in Ikehara et al. (1994) . Overall, 56 possible combinations were found and 37 anchored in 3700 sentences: Table 2 . Of these, 9 were anchored that should not have been, and 1 was not anchored that should have been. The accuracy (correctly anchored/anchored) was 76% (28/37), and the recall (correctly anchored/should be anchored) was 97% (28/29). The major source of errors was from parsing errors in the system as a whole. All of the badly anchored numeral-classifiers combinations were caused by this. In this case, the algorithm has not degraded the system performance, it would have been a bad result anyway. There were three problems with the algorithm itself. In one case an anaphoric quantifier was mistaken as a floating quantifier, in another the w~rbal semantic attribute check for degree modification gave a bad result. Finally there was one case where the default blocking for semantic anomalies blocked a good combination. Translation of floating quantifiers Note that anchoring a floating quantifier is only the first step toward translating it. Special handling is sometimes needed to translate the anchored quantifiers. For example, Japanese has some universal pronouns that can stand alone as full noun phrases ( 14 ) or act as floating quantifiers (15): e.g., minna \"everyone\", zen'in \"all members\". When they are anchored, the information about the denotation of the head carried by the pronoun is redundant, and should not be translated. A special rule is required for this. Further work The proposed algorithm forms a solid base for extensions in various ways. 1. Combine it with a fuller system of event semantics. 2. Make the treatment of classifier-target semantics more detailed, so that inbuilt semantic restrictions can be used instead of a table of forbidden combinations. 3. Use the results of the algorithm to help choose between candidate parses and integrate it with the resolution of zero pronouns. 4. Test the algorithm on other languages, for example Korean. 5 Conclusion We have presented an algorithm to anchor floating quantifiers in Japanese. The algorithm proceeds as follows. First identify potential floating quantifiers: either numeral classifier combinations or quantificational nouns. Then identify potential anchors: all accusative or nominative noun phrases. Inappropriate combinations are deleted, either because of a semantic mismatch between the target and quantifier, or because the quantifier is interpreted as a degree or event modifier. Finally, possible combinations are ranked, with the accusative candidate being the best choice, then the closest and leftmost. The algorithm is robust and uses the full power of currently available detailed semantic dictionaries. Acknowledgments The authors thank Tim Baldwin, Yukie Kuribayashi, Kyonghee Paik and the members of the NTT Machine Translation Research Group for their discussion and comments on this paper and earlier versions. The research was carried out while Daniela Kurz visited the NTT Communication Science Laboratories. Francis Bond is cm'rently also enrolled part time as a doctoral candidate at the University of Queensland's Center for Language Teaching & Research. Zusammenfassung In diesem Papier beschreiben wit einen Algorithmus zur Resolution von 'floating quantifiers' ira Japanisehen. Japanisch ist eine Sprache, in der quantifizierende Adverbien oder Kombinationen aus Numeral + Klassifikator voader Nominalphrase, fiir die sie quax~tifizieren getrennt werden kSnnen, d.h. sie miissen nicht in unmittelbarer linearer Abfolge stehen. Der Algorithmus unterscheidet Grad-und Ereignismodifikatoren yon Adverbialen, die fiir Nominalphrasen quan~ifizieren and resolviert den richtigen Antezedenten fiir jeden 'floating quantifier'. Zur Anbindtmg an die richtige Nominalphrase finden die folgenden Parameter Beriicksichtigung: Wortart der Quantifikators und des Antezedenten, die semantische Relation zwischen diesen beiden, die Kasusmarkierungen des Antezedenten und die Semaatik des Verbs, das sowohl den Quantifikator als aueh dessen Antezedenten regiert. Der Algorithmus wurde imp[ementiert und in einem regel-basierten Japaniseh/Englischem fd'bersetzungssystem evaluiert. a%?:o ~d~l ~d =--,'l~r-:t -. o1~ ol+-q~ ~dx._v_x-i~. =~,g-~",
         "219310218",
         "39f60cfc2431545160e98ac461d43222ee105488",
         "4",
         "https://aclanthology.org/C98-1023",
         null,
         null,
         "1998",
         null,
         "{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics",
         "Bond, Francis  and\nKurz, Daniela  and\nShirai, Satoshi",
         "Anchoring Floating Quantifiers in {J}apanese-to-{E}nglish Machine Translation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "bond-etal-1998-anchoring",
         null,
         null
        ],
        [
         "27",
         "W03-0310",
         "We present two methods for the automatic creation of parallel corpora. Whereas previous work into the automatic construction of parallel corpora has focused on harvesting them from the web, we examine the use of existing parallel corpora to bootstrap data for new language pairs. First, we extend existing parallel corpora using co-training, wherein machine translations are selectively added to training corpora with multiple source texts. Retraining translation models yields modest improvements. Second, we simulate the creation of training data for a language pair for which a parallel corpus is not available. Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. This suggests the method may be useful in the creation of parallel corpora for languages with scarce resources.",
         "We present two methods for the automatic creation of parallel corpora. Whereas previous work into the automatic construction of parallel corpora has focused on harvesting them from the web, we examine the use of existing parallel corpora to bootstrap data for new language pairs. First, we extend existing parallel corpora using co-training, wherein machine translations are selectively added to training corpora with multiple source texts. Retraining translation models yields modest improvements. Second, we simulate the creation of training data for a language pair for which a parallel corpus is not available. Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. This suggests the method may be useful in the creation of parallel corpora for languages with scarce resources. Introduction Statistical translation models (such as those formulated in Brown et al. (1993) ) are trained from bilingual sentencealigned texts. The bilingual data used for constructing translation models is often gathered from government documents produced in multiple languages. For example, the Candide system (Berger et al., 1994) was trained on ten years' worth of Canadian Parliament proceedings, which consists of 2.87 million parallel sentences in French and English. While the Candide system was widely regarded as successful, its success is not indicative of the potential for statistical translation between arbitrary language pairs. The reason for this is that collections of parallel texts as large as the Canadian Hansards are rare. Al-Onaizan et al. (2000) explains in simple terms the reasons that using large amounts of training data ensures translation quality: if a program sees a particular word or phrase one thousand times during training, it is more likely to learn a correct translation than if sees it ten times, or once, or never. Increasing the amount of training material therefore leads to improved quality. This is illustrated in Figure 1 , which plots translation accuracy (measured as 100 minus word error rate) for French⇒English, German⇒English, and Spanish⇒English translation models trained on incrementally larger parallel corpora. The quality of the translations produced by each system increases over the 100,000 training items, and the graph suggests the the trend would continue if more data were added. Notice that the rate of improvement is slow: after 90,000 manually provided training sentences pairs, we only see a 4-6% change in performance. Sufficient performance for statistical models may therefore only come when we have access to many millions of aligned sentences. One approach that has been proposed to address the problem of limited training data is to harvest the web for bilingual texts (Resnik, 1998) . The STRAND method automatically gathers web pages that are potential translations of each other by looking for documents in one language which have links whose text contains the name of another language. For example, if an English web page had a link with the text \"Español\" or \"en Español\" then the page linked to is treated as a candidate translation of the English page. Further checks verify the plausibility of its being a translation (Smith, 2002) . Instead of attempting to gather new translations from the web, we describe an alternate method for automatically creating parallel corpora. Specifically, we examine the use of existing translations as a resource to bootstrap more training data, and to create data for new language pairs. We generate translation models from existing data and use them to produce translations of new sen- (Blum and Mitchell, 1998; Abney, 2002) which is a weakly supervised learning technique that relies on having distinct views of the items being classified. The views that we employ for co-training are multiple source documents. Section 2 motivates the use of weakly supervised learning, and introduces co-training for machine translation. Section 3 reports our experimental results. One experiment shows that co-training can modestly benefit translation systems trained from similarly sized corpora. A second experiment shows that co-training can have a dramatic benefit when the size of initial training corpora are mismatched. This suggests that co-training for statistical machine translation is especially useful for languages with impoverished training corpora. Section 4 discusses the implications of our experiments, and discusses ways which our methods might be used more practically. Co-training for Statistical Machine Translation Most statistical natural language processing tasks use supervised machine learning, meaning that they require training data that contains examples that have been annotated with some sort of labels. Two conflicting factors make this reliance on annotated training data a problem: • The accuracy of machine learning improves as more data is available (as we have shown for statistical machine translation in Figure 1 ). • Annotated training data usually has some cost associated with its creation. This cost can often be sub-stantial, as with the Penn Treebank (Marcus et al., 1993) . There has recently been considerable interest in weakly supervised learning within the statistical NLP community. The goal of weakly supervised learning is to reduce the cost of creating new annotated corpora by (semi-) automating the process. Co-training is a weakly supervised learning techniques which uses an initially small amount of human labeled data to automatically bootstrap larger sets of machine labeled training data. In co-training implementations multiple learners are used to label new examples and retrained on some of each other's labeled examples. The use of multiple learners increases the chance that useful information will be added; an example which is easily labeled by one learner may be difficult for the other and therefore adding the confidently labeled example will provide information in the next round of training. Self-training is a weakly supervised method in which a single learner retrains on the labels that it applies to unlabeled data itself. We describe its application to machine translation in order to clarify how co-training would work. In self-training a translation model would be trained for a language pair, say German⇒English, from a German-English parallel corpus. It would then produce English translations for a set of German sentences. The machine translated German-English sentences would be added to the initial bilingual corpus, and the translation model would be retrained. Co-training for machine translation is slightly more complicated. Rather than using a single translation model to translate a monolingual corpus, it uses multiple translation models to translate a bi-or multilingual corpus. For example, translation models could be trained for German⇒English, French⇒English and Spanish⇒English from appropriate bilingual corpora, and then used to translate a German-French-Spanish parallel corpus into English. Since there are three candidate English translations for each sentence alignment, the best translation out of the three can be selected and used to retrain the models. The process is illustrated in Figure 2 . Co-training thus automatically increases the size of parallel corpora. There are a number of reasons why machine translated items added during co-training can be useful in the next round of training: 4 French some english sentence some french sentenc some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence German English some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence Spanish English some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence 1 English French some english sentence some french sentenc some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence some english sentence some french sentence • coping with morphology -The problem mentioned above is further exacerbated by the fact that most current statistical translation formulations have an incomplete treatment of morphology. This would be a problem if the training data for a Spanish translation model contained the masculine form of a adjective, but not the feminine. Because languages vary in how they use morphology (some languages have grammatical gender whereas others don't) one language's translation model might have the translation of a particular word form whereas another's would not. Thus co-training can increase the inventory of word forms and reduce the problem that morphology poses to simple statistical translation models. English • improved word order -A significant source of errors in statistical machine translation is the word reordering problem (Och et al., 1999) . The word order between related languages is often similar while word order between distant language may differ significantly. By including more examples through cotraining with related languages, the translation models for distant languages will better learn word order mappings to the target language. In all these cases the diversity afforded by multiple translation models increases the chances that the machine translated sentences added to the initial bilingual corpora will be accurate. Our co-training algorithm allows many source languages to be used. Experimental Results In order to conduct co-training experiments we first needed to assemble appropriate corpora. The corpus used in our experiments was assembled from the data used in the (Och and Ney, 2001 ) multiple source translation paper. The data was gathered from the Bulletin of the European Union which is published on the Internet in the eleven official languages of the European Union. We used a subset of the data to create a multi-lingual corpus, aligning sentences between French, Spanish, German, Italian and Portuguese (Simard, 1999) . Additionally we created bilingual corpora between English and each of the five languages using sentences that were not included in the multi-lingual corpus. Och and Ney (2001) used the data to find a translation that was most probable given multiple source strings. Och and Ney found that multi-source translations using two source languages reduced word error rate when compared to using source strings from a single language. For multi-source translations using source strings in six languages a greater reduction in word error rate was achieved. Our work is similar in spirit, although instead of using multi-source translation at the time of translation, we integrate it into the training stage. Whereas Och and Ney use multiple source strings to improve the quality of one translation only, our co-training method attempts to improve the accuracy of all translation models by bootstrapping more training data from multiple source documents. Software The software that we used to train the statistical models and to produce the translations was GIZA++ (Och and Ney, 2000) , the CMU-Cambridge Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) , and the ISI ReWrite Decoder. The sizes of the language models used in each experiment were fixed throughout, in order to ensure that any gains that were made were not due to the trivial reason of the language model improving (which could be done by building a larger monolingual corpus of the target language). The experiments that we conducted used GIZA++ to produce IBM Model 4 translation models. It should be observed, however, that our co-training algorithm is entirely general and may be applied to any formulation of statistical machine translation which relies on parallel Evaluation The performance of translation models was evaluated using a held-out set of 1,000 sentences in each language, with reference translations into English. Each translation model was used to produce translation of these sentences and the machine translations were compared to the reference human translations using word error rate (WER). The results are reported in terms of increasing accuracy, rather than decreasing error. We define accuracy as 100 minus WER. Other evaluation metrics such as position independent WER or the Bleu method (Papineni et al., 2001) could have been used. While WER may not be the best measure of translation quality, it is sufficient to track performance improvements in the following experiments. Co-training Table 1 gives the result of co-training using the most accurate translation from the candidate translations produced by five translation models. Each translation model was initially trained on bilingual corpora consisting of around 20,000 human translated sentences. These translation models were used to translate 63,000 sentences, of which the top 10,000 were selected for the first round. At the next round 53,000 sentences were translated and the top 10,000 sentences were selected for the second round. The final candidate pool contained 43,000 translations and again the top 10,000 were selected. The table indicates that gains may be had from co-training. Each of the translation models improves over its initial training size at some point in the co-training. The German to English translation model improves the most -exhibiting a 2.5% improvement in accuracy. The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001) : gains above the accuracy of the initial corpus are achieved, but decline as after a certain number of machine translations are added to the training set. This could be due in part to the manner in items are selected for each round. Because the best translations are transferred from the candidate pool to the   training pool at each round the number of \"easy\" translations diminishes over time. Because of this, the average accuracy of the training corpora decreased with each round, and the amount of noise being introduced increased. The accuracy gains from co-training might extend for additional rounds if the size of the candidate pool were increased, or if some method were employed to reduce the amount of noise being introduced. Coaching In order to simulate using co-training for language pairs without extensive parallel corpora, we experimented with a variation on co-training for machine translation that we call \"coaching\". It employs two translation models of vastly different size. In this case we used a French to English translation model built from 60,000 human translated sentences and a German to English translation model that contained no human translated sentences. The German-English translation model was meant to represent a language pair with extremely impoverished parallel corpus. Coaching is therefore a special case of cotraining in that one view (the superior one) never retrains upon material provided by the other (inferior) view. A German-English parallel corpus was created by taking a French-German parallel corpus, translating the French sentences into English and then aligning the translations with the German sentences. In this experiment the machine translations produced by the French⇒English translation model were always selected. Figure 3 shows the performance of the resulting German to English translation model for various sized machine produced parallel corpora. We explored this method further by translating 100,000 sentences with each of the non-German translation models from the co-training experiment in Section 3.3. The result was a German-English corpus containing 400,000 sentence pairs. The performance of the resulting model matches the initial accuracy of the model. Thus machinetranslated corpora achieved equivalent quality to humantranslated corpora after two orders of magnitude more data was added. The graphs illustrate that increasing the performance of translation models may be achievable using machine translations alone. Rather than the 2.5% improvement gained in co-training experiments wherein models of similar sizes were used, coaching achieves an 18%(+) improvement by pairing translation models of radically different sizes. Discussion and Future Work In this paper we presented two methods for the automatic creation of additional parallel corpora. Co-training uses a number of different human translated parallel corpora to create additional data for each of them, leading to modest increases in translation quality. Coaching uses existing resources to create a fully machine translated corporaessentially reverse engineering the knowledge present in the human translated corpora and transferring that to another language. This has significant implications for the feasibility of using statistical translation methods for language pairs for which extensive parallel corpora do not exist. A setting in which this would become extremely useful is if the European Union extends membership to a new country like Turkey, and wants develop translation resources for its language. One can imagine that sizable parallel corpora might be available between Turkish and a few EU languages like Greek and Italian. However, there may be no parallel corpora between Turkish and Finnish. Our methods could exploit existing parallel corpora between the current EU language and use machine translations from Greek and Italian in order to create a machine translation system between Turkish and Finnish. We plan to extend our work by moving from cotraining and its variants to another weakly supervised learning method, active learning. Active learning incorporates human translations along with machine translations, which should ensure better resulting quality than using machine translations alone. It will reduce the cost of creating a parallel corpus entirely by hand, by selectively and judiciously querying a human translator. In order to make the most effective use of the human translator's time we will be required to design an effective selection algorithm, which is something that was neglected in our current research. An effective selection algorithm for active learning will be one which chooses those examples which will add the most information to the machine translation system, and therefore minimizes the amount of time a human needs to spend translating sentences.",
         "5539819",
         "7d4047c02f346602f4a488b8fadd0b72f4786855",
         "23",
         "https://aclanthology.org/W03-0310",
         null,
         null,
         "2003",
         null,
         "Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond",
         "Callison-Burch, Chris  and\nOsborne, Miles",
         "Bootstrapping Parallel Corpora",
         "44--49",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "callison-burch-osborne-2003-bootstrapping",
         null,
         null
        ],
        [
         "28",
         "W03-0311",
         "Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness. It retrieves example sentences similar to the input and adjusts their translations to obtain the output. However, it has problems in that the performance degrades when input sentences are long and when the style of inputs and that of the example corpus are different. This paper proposes a method for retrieving \"meaning-equivalent sentences\" to overcome these two problems. A meaning-equivalent sentence shares the main meaning with an input despite lacking some unimportant information. The translations of meaning-equivalent sentences correspond to \"rough translations.\" The retrieval is based on content words, modality, and tense.",
         "Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness. It retrieves example sentences similar to the input and adjusts their translations to obtain the output. However, it has problems in that the performance degrades when input sentences are long and when the style of inputs and that of the example corpus are different. This paper proposes a method for retrieving \"meaning-equivalent sentences\" to overcome these two problems. A meaning-equivalent sentence shares the main meaning with an input despite lacking some unimportant information. The translations of meaning-equivalent sentences correspond to \"rough translations.\" The retrieval is based on content words, modality, and tense. Introduction Speech-to-speech translation (S2ST) technologies consist of speech recognition, machine translation (MT), and speech synthesis (Waibel, 1996; Wahlster, 2000; Yamamoto, 2000) . The MT part receives speech texts recognized by a speech recognizer. The nature of speech causes difficulty in translation since the styles of speech are different from those of written text and are sometimes ungrammatical (Lazzari, 2002) . Therefore, rule-based MT cannot translate speech accurately compared with its performance for written-style text . Example-based MT (EBMT) is one of the corpusbased machine translation methods. It retrieves examples similar to inputs and adjusts their translations to obtain the output (Nagao, 1981) . EBMT is a promising method for S2ST in that it performs robust translation of ungram-matical sentences and requires far less manual work than rule-based MT. However, there are two problems in applying EBMT to S2ST. One is that the translation accuracy drastically drops as input sentences become long. As the length of a sentence becomes long, the number of retrieved similar sentences greatly decreases. This often results in no output when translating long sentences. The other problem arises due to the differences in style between input sentences and the example corpus. It is difficult to acquire a large volume of natural speech data since it requires much time and cost. Therefore, we cannot avoid using a corpus with written-style text, which is different from that of natural speech. This style difference makes retrieval of similar sentences difficult and degrades the performance of EBMT. This paper proposes a method of retrieving sentences whose meaning is equivalent to input sentences to overcome the two problems. A meaning-equivalent sentence means a sentence having the main meaning of an input sentence despite lacking some unimportant information. Such a sentence can be more easily retrieved than a similar sentence, and its translation is useful enough in S2ST. We call this translation strategy example-based \"rough translation.\" Retrieval of meaning-equivalent sentences is based on content words, modality, and tense. This provides robustness against long inputs and in the differences in style between the input and the example corpus. This advantage distinguishes our method from other translation methods. We describe the difficulties in S2ST in Section 2. Then, we describe our purpose, features for retrieval, and retrieval method for meaning-equivalent sentences in Section 3. We report an experiment comparing our method with two other methods in Section 4. The experiment demonstrates the robustness of our method to length of input and the style differences between inputs and the example corpus. Translation Degradation by Input Length A major problem with machine translation, regardless of the translation method, is that performance drops rapidly as input sentences become longer. For EBMT, the longer input sentences become, the fewer similar example sentences exist in the example corpus. Figure 1 shows translation difficulty in long sentences in EBMT (Sumita, 2001) . The EBMT system is given 591 test sentences and returns translation result as translated/untranslated. Untranslated means that there exists no similar example sentences for the input. Although the EBMT is equipped with a large example corpus (about 170K sentences), it often failed to translate long inputs. Style Differences between Concise and Conversational The performance of example-based S2ST greatly depends on the example corpus. It is advantageous for an example corpus to have a large volume and the same style as the input sentences. A corpus of texts dictated from conversational speech is favorable for S2ST. Unfortunately, it is very difficult to prepare such an example corpus since this task requires laborious work such as speech recording and speech transcription. Therefore, we cannot avoid using a written-style corpus, such as phrasebooks, to prepare a sufficiently large volume of examples. Contained texts are almost grammatical and rarely contain unnecessary words. We call the style used in such a corpus \"concise\" and the style seen in conversational speech \"conversational.\" Table 1 shows the average numbers of words in concise (Takezawa et al., 2002) and conversational corpora (Takezawa, 1999) . Sentences in conversational style are about 2.5 words longer than those in concise style in both Language English Japanese Concise 5.4 6.2 Conversational 7.9 8.9 English and Japanese. This is because conversational style sentences contain unnecessary words or subordinate clauses, which have the effects of assisting the listener's comprehension and avoiding the possibility of giving the listener a curt impression. Table 2 shows cross perplexity between concise and conversational corpora (Takezawa et al., 2002) . Perplexity is used as a metric for how well a language model derived from a training set matches a test set (Jurafsky and Martin, 2000) . Cross perplexities between concise and conversational corpora are much higher than the selfperplexity of either of the two styles. This result also illustrates the great difference between the two styles. Meaning-equivalent Sentence Example-based S2ST has the difficulties described in Section 2 when it attempts to translate inputs exactly. Here, we set our translation goal to translating input sentences not exactly but roughly. We assume that a rough translation is useful enough for S2ST, since unimportant information rarely disturbs the progress of dialogs and can be recovered in the following dialog if needed. We call this translation strategy \"rough translation.\" We propose \"meaning-equivalent sentence\" to carry out rough translation. Meaning-equivalent sentences are defined as follows: meaning-equivalent sentence (to an input sentence) A sentence that shares the main meaning with the input sentence despite lacking some unimportant information. It does not contain information additional to that in the input sentence. Important information is subjectively recognized mainly due to one of two reasons: (1) It can be surmised from the general situation, or (2) It does not place a strong restriction on the main information. Figure 2 shows examples of unimportant/important information. Information to be examined is written in bold. The information \"of me\" in (1) and \"around here\" in (3) can be surmised from the general situation, while the information \"of this painting\" in (2) and \"Chinese\" would not be surmised since it denotes a special object. The subordinate sentences in (4) and ( 5 ) are regarded as unimportant since they have small significance and are omittable. Basic Idea of Retrieval The retrieval of meaning-equivalent sentence depends on content words and basically does not depend on functional words. Independence from functional words brings robustness to the difference in styles. However, functional words include important information for sentence meaning: the case relation of content words, modality, and tense. Lack of case relation information is compensated by the nature of the restricted domain. A restricted domain, as a domain of S2ST, has a relatively small lexicon and meaning variety. Therefore, if content words included in an input are given, their relation is almost determined in the domain. Information of modality and tense is extracted from functional words and utilized in classifying the meaning of a sentence (described in Section 3.2.2). This retrieval method is similar to information retrieval in that content words are used as clues for retrieval (Frakes and Baeza-Yates, 1992) . However, our task has two difficulties: (1) Retrieval is carried out not by documents but by single sentences. This reduces the effectiveness of word frequencies. (2) The differences in modality and tense in sentences have to be considered since they play an important role in determining a sentence's communicative meaning. Features for Retrieval Content Words Words categorized as either noun 1 , adjective, adverb, or verb are recognized as content words. Interrogatives  We utilize a thesaurus to expand the coverage of the example corpus. We call the relation of two words that are the same \"identical\" and words that are synonymous in the given thesaurus \"synonymous.\" Modality and Tense The meaning of a sentence is discriminated by its modality and tense, since these factors obviously determine meaning. We defined two modality groups and one tense group by examining our corpus. The modality groups are (\"request\", \"desire\", \"question\", \"confirmation\", \"others\",) and (\"negation\", \"others\".) The tense group is (\"past\", \"others\".) These modalities and tense are distinguished by surface clues, mainly by particles and auxiliary verbs. Table 3 shows a part of the clues used for discriminating modalities in Japanese. Sentences having no clues are classified as others. Figure 3 A speech act is a concept similar to modality in which speakers' intentions are represented. The two studies introduced information of the speech act in their S2ST systems (Wahlster, 2000; Tanaka and Yokoo, 1999) . The two studies and our method differ in the effect of speech act information. Their effect of speech act information is so small that it is limited to generating the translation text. Translation texts are refined by selecting proper expressions according to the detected speakers' intention. Retrieval and Ranking Sentences that satisfy the conditions below are recognized as meaning-equivalent sentences. 1. It is required to have the same modality and tense as the input sentence. 2. All content words are included (identical or synonymous) in the input sentence. This means that the set of content words of a meaning-equivalent sentence is a subset of the input. 3. At least one content word is included (identical) in the input sentence. If more than one sentence is retrieved, we must rank them to select the most similar one. We introduce \"focus area\" in the ranking process to select sentences that are meaning-equivalent to the main sentence in complex sentences. We set the focus area as the last N words from the word list of an input sentence. N denotes the number of content words in meaning-equivalent sentences. This is because main sentences in complex sentences tend to be placed at the end in Japanese. 3 Space characters are inserted into word boundaries in Japanese texts. 4 The value \"others\" in all modality/tense groups is omitted. Retrieved sentences are ranked by the conditions described below. Conditions are described in order of priority. If there is more than one sentence having the highest score under these conditions, the most similar sentence is selected randomly. C1: # of identical words in focus area. C2: # of synonymous words in focus area. C3: # of identical words in non-focus area. C4: # of synonymous words in non-focus area. C5: # of common functional words. C6: # of different functional words. (the fewer, the higher priority) Figure 4 shows an example of conditions for ranking. Content word in a focus area of input are underlined and functional words are written in italic. Experiment Test Data We used a bilingual corpus of travel conversation, which has Japanese sentences and their English translations (Takezawa et al., 2002) . This corpus was sentencealigned, and a morphological analysis was done on both languages by our morphological analysis tools. The bilingual corpus was divided into example data (Example) and test data (Concise) by extracting test data randomly from the whole set of data. In addition to this, we used a conversational speech corpus for another set of test data (Takezawa, 1999) . This corpus contains dialogs between a traveler and a hotel  We use sentences including more than one content word among the three corpora. The statistics of the three corpora are shown in Table 4 . The thesaurus used in the experiment was \"Kadokawa-Ruigo-Jisho\" (Ohno and Hamanishi, 1984) . Each word has semantic code consisting of three digits, that is, this thesaurus has three hierarchies. We defined \"synonymous\" words as sharing exact semantic codes. Compared Retrieval Methods We use two example-based retrieval methods to show the characteristic of the proposed method. The first method (Method-1) uses \"strict\" retrieval, which does not allow missing words in input. The method takes functional words into account on retrieval. This method corresponds to the conventional EBMT method. The second method (Method-2) uses \"rough\" retrieval, which does allow missing words in input, but still takes functional words into account. Evaluation Methodology Evaluation was carried out by judging whether retrieved sentences are meaning-equivalent to inputs. It must be noted that inputs and retrieved sentences are both in Japanese. We did not compare inputs and translations of retrieved sentences, since translation accuracy is a matter of the example corpus and does not concern our method. The sentence with the highest score among retrieved sentences was taken and evaluated. The sentences are marked manually as meaning-equivalent or not by a Japanese native. A meaning-equivalent sentence includes all important information in the input but may lack some unimportant information. Results Figure 5 shows the accuracy of the three methods with the concise and conversational style data. Accuracy is defined as the ratio of the number of correctly equivalent sentences to that of total inputs. Inputs are classified into four types by their word length. The performance of Method-1 reflects the narrow coverage and style-dependency of conventional EBMT. The longer input sentences become, the more steeply its performance degrades in both styles. The method can retrieve no similar sentence for inputs longer than eleven words in conversational style. Method-2 adopts a \"rough\" strategy in retrieval. It attains higher accuracy than Method-1, especially with longer inputs. This indicates the robustness of the rough retrieval strategy to longer inputs. However, the method still has an accuracy difference of about 15% between the two styles. The accuracy of the proposed method is better than that of Method-2, especially in conversational style. The accuracy difference in longer inputs becomes smaller (about 4%) than that of Method-2. This indicates the robustness of the proposed method to the differences between the two styles. Related Work EBMT The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Veale and Way, 1997; Carl, 1999; Brown, 2000) . The basic idea of EBMT is that sentences similar to the inputs are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functional-word part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the functional-word part, our method is characterized by disregarding functional words, while other EBMT methods still rely on them for the similarity measure. In our method, the lack of functional word information is compensated by the semantically narrow variety in S2ST domains and the use of information on modality and tense. Consequently, our method gains robustness to length and the style differences between inputs and the example corpus. Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992) , removing functional words (Kumano et al., 2002; Wakita et al., 2000) , and removing content words (Sumita and Tsutsumi, 1988) . In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant information. Translation of meaning-equivalent sentences corresponds to rough translations, which aim not at exact translation with narrow coverage but at rough translation with wide coverage. For S2ST, we assume that this translation strategy is sufficiently useful. Then, we described a method for retrieving meaningequivalent sentences from an example corpus. Retrieval is based on content words, modality, and tense. This strategy is feasible owing to the restricted domains, often adopted in S2ST, which have relatively small variety in lexicon and meaning. An experiment demonstrated the robustness of our method to input length and the style differences between inputs and the example corpus. Most MT systems aim to achieve exact translation, but unfortunately they often output bad or no translation for long conversational speeches. The rough translation proposed in this paper achieves robustness in translation for such inputs. This method compensates for the shortcomings of conventional MT and makes S2ST technology more practical. Acknowledgements The research reported here was supported in part by a contract with the Telecommunications Advancement Organization of Japan entitled, \"A study of speech dialogue translation technology based on a large corpus\".",
         "3104693",
         "65a08a5db810ffbb55ec8034e7e35a2d76e3ff03",
         "8",
         "https://aclanthology.org/W03-0311",
         null,
         null,
         "2003",
         null,
         "Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond",
         "Shimohata, Mitsuo  and\nSumita, Eiichiro  and\nMatsumoto, Yuji",
         "Retrieving Meaning-equivalent Sentences for Example-based Rough Translation",
         "50--56",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "shimohata-etal-2003-retrieving",
         null,
         null
        ],
        [
         "29",
         "W03-0312",
         "We propose a method of constructing an example-based machine translation (EBMT) system that exploits a content-aligned bilingual corpus. First, the sentences and phrases in the corpus are aligned across the two languages, and the pairs with high translation confidence are selected and stored in the translation memory. Then, for a given input sentences, the system searches for fitting examples based on both the monolingual similarity and the translation confidence of the pair, and the obtained results are then combined to generate the translation. Our experiments on translation selection showed the accuracy of 85% demonstrating the basic feasibility of our approach.",
         "We propose a method of constructing an example-based machine translation (EBMT) system that exploits a content-aligned bilingual corpus. First, the sentences and phrases in the corpus are aligned across the two languages, and the pairs with high translation confidence are selected and stored in the translation memory. Then, for a given input sentences, the system searches for fitting examples based on both the monolingual similarity and the translation confidence of the pair, and the obtained results are then combined to generate the translation. Our experiments on translation selection showed the accuracy of 85% demonstrating the basic feasibility of our approach. Introduction The basic idea of example-based machine translation, or EBMT, is that translation examples similar to a part of an input sentence are retrieved and combined to produce a translation (Nagao, 1984) . In order to make a practical MT system based on this approach, a large number of translation examples with structural correspondences are required. This naturally presupposes high-accuracy parsers and well-aligned large bilingual corpora. Over the last decade, the accuracy of the parsers improved significantly. The availability of well-aligned bilingual corpora, however, has not increased despite our expectations. In reality, the number of bilingual corpora that share the same content, such as newspapers and broadcast news, has increased steadily. We call this type of corpus a content-aligned corpus. With these observations, we started a research project that covered all aspects of constructing EBMT systems starting from using Building Translation Memory In EBMT, an input sentence can hardly be translated by a single translation example, except when an input is extremely short or is a typical domain-dependent sentence. Therefore, two or more translation examples are used to translate parts of the input and are then combined to generate a whole translation. Syntactic information is useful for composing example fragments. In this paper, we call a structurally aligned bilingual sentence pair a translation example or TE (Figure 1 ). This section presents our method for building TEs from a content-aligned corpus. Since the bilingual corpus used in our project does not contain literal translations, automatic parsing and alignment inevitably contain errors. Therefore, we selected highly likely TEs to make a translation memory. NHK News Corpus We used a bilingual news corpus compiled by the NHK broadcasting service (NHK News Corpus), which consists of about 40,000 Japanese-English article pairs covering a five-year period. The average number of Japanese sentences in an article is 5.2, and that of English sentence is 7.4. Table 2 shows an example of an article pair. As shown in Table 2 , an English article is not a literal translation of a Japanese article, although their contents are almost parallel. Sentence Alignment We used a DP matching for bilingual sentence alignment, where we allow the matching of 1-to-1, 1-to-2, 1-to-3, 2to-1 and 2-to-2 Japanese and English sentence pairs. This matching covered 84% of the following evaluation set. We selected 96 article pairs for the evaluation of sentence and phrase alignment, and we call this the evaluation set. We use the following score for matching, which is based on a ratio of corresponding content words (WCR: content Word Corresponding Ratio). WCR Ï Ï • Ï (1) where Ï is the number of Japanese content words in a unit, Ï is the number of English content words, and Ï is the number of content words whose translation is also in the unit, which is found by translation dictionaries ． We used the EDR electronic dictionary, EDICT, ENAMDICT, the ANCHOR translation dictionary, and  the EIJIRO translation dictionary. These dictionaries have about two million entries in total. On the evaluation data, the precision of the sentence alignment (defined as follows) was 60.7%. precision # of correct system outputs # of system outputs (2) Among types of a corresponding unit, the precision of 1-to-1 correspondence was the best, at 77.5%. Since a 1to-1 correspondence is suitable for the following phrase alignment, we decided to use only the 1-to-1 correspondence results. Phrase Alignment The 1-to-1 sentence pairs obtained in the previous section are then aligned at phrase level by the method based on (Aramaki et al., 2001) . The method consists of the following pre-process and two aligning steps. Pre-process: Conversion to phrasal dependency structures. First, the phrasal dependency structures of the sentence pair are estimated. The English parser returns a word-based phrase structure, which is merged into a phrase sequence by the following rules and converted into a dependency structure by lifting up head phrases. The Japanese parser outputs the phrasal dependency structure of an input, and that is used as is. We used The Japanese parser KNP (Kurohashi and Nagao, 1994) and The English nl-parser (Charniak, 2000) . Step 1: Estimation of basic phrasal correspondences. We started with the word-level alignment to get the basic phrasal alignment. We used translation dictionaries for this process. The word sense ambiguity in the dictionaries is resolved with a heuristics that the most plausible correspondence is near other correspondences. Step 2: Expansion of phrasal correspondences. Finally, the remaining phrases, which were not handled in the step 1, are merged into a neighboring phrase correspondence or are used to establish a new correspondence, depending on the surrounding existing correspondences. Figure 3 shows an example of a new correspondence established by a structural pattern. These procedures can detect the phrasal alignments in a pair of sentences as shown in Figure 1 . For phrase alignment evaluation, we selected all of the 145 sentence pairs that had 1-to-1 correspondences form the evaluation set and gave correct content word correspondences to these pairs. The phrase correspondences detected by the system were judged correct when the correspondences include the manually given content word correspondences. Based on this criterion, the precision of phrase alignment was 50%. Then, we found a correlation between the phrase alignment precision and WCR of parallel sentences as shown in Figure 4 . Furthermore, the precision of sentence alignment and WCR also have a correlation. Since their performances nearly reaches their limits when WCR is 0.3, we decided to use parallel sentences whose WCR is 0.3 or greater as TEs. Building Translation Memory As explained in the preceding sections, among sentencealigned and phrase-aligned NHK News articles, TEs with a 1-to-1 sentence correspondence and whose WCR is 0.3 or greater are registered in the translation memory. Table 1 shows the number of TEs for each WCR range. In addition, the Bilingual White Paper and Translation Memory of SENSEVAL2 (Kurohashi, 2001) were also phrase-aligned and registered in the translation memory. Sentence alignments are already given for these corpora. Since their parallelism are fairly high and the accuracies of their phrase alignments are more than 70%, we utilized all phrase-aligned sentence pairs as TEs (Table 1 ). EBMT System Our EBMT system translates a Japanese sentence into English. A Japanese input sentence is parsed and transformed into a phrase-based dependency structure. Then, for each phrase, an appropriate TE is retrieved from the translation memory that is most suitable for translating the phrase (and its neighboring phrases). Finally, the English expressions of the TEs are combined to produce the final English translation (Figure 5 ). This section describes our EBMT system, mainly the TE selection part. Basic Idea of TE Selection The basic idea of TE selection is shown in Figure 6 . When a part of the input sentence and a part of the TE source language sentence have an equal expression, the part of the input sentence is called I and the part of the TE source language sentence is called S. A part of the TE target language corresponding to S is called T. The pair S and T is called fragment of TE (FTE). I, S and T have to meet the following conditions, as a natural consequence of the fact that S-T is used for translating I. 1. I, S and T are each structurally connected phrases. 2. I is equal to S except for function words at the boundaries. 3. S corresponds to T completely, that is, all phrases in S and T are aligned. It might be the case that for an I, two or more FTEs that meet the above conditions exist in the translation memory. Our method takes into account the following relations among I-S-T to select the best FTE: 1. The largest pair of I and S. 2. The similarity between the surroundings of I and these of S. The confidence of alignment between S and T. The following sections concretely present how to calculate these criteria. For simplicity of explanation, we call a set of phrasal correspondences between S and T, EQ; that neighboring EQ, CONTEXT; that between S and T, ALIGN (Figure 6 ). Monolingual Similarity between Japanese Expressions The equality between I and S is a sum of the equality score of each phrase correspondence in EQ, which is calculated as follows: EQUAL´ µ È Ë ÓÒØ ¢ ¾ ÓÒØ • ¼ ¾ ¢ È Ë ÙÒ ¢ ¾ Ù Ò (3) where ÓÒØ is the number of content words in the phrase correspondence, ÙÒ is the number of function words, Ë ÓÒØ is the equality between content words, and Ë Ù Ò is the equality between function words. Ë ÓÒØ and Ë Ù Ò are given in Table 2 . 1  Usually, the equality score between I and S is equal to the number of phrases in I (the number of phrase correspondences in EQ), but sometimes these are slightly different, depending on the conjugation type and function words. On the other hand, the similarity between the surroundings of I and those of S is a sum of the similarity score of each phrase correspondence in CONTEXT, which is calculated as follows: SIM´ µ Ë ÓÒØ ¢ ¾ ÓÒØ •¼ ¾¢ È Ë ÙÒ ¢ ¾ ÙÒ ¢Ë ÓÒÒ Ø (4) Basically the calculation of SIM and EQUAL is the same, except that SIM considers the relation type between the phrase in I and its outer phrase by Ë ÓÒÒ Ø . When the relation is the same, the influence of the surrounding phrases must be large, so Ë ÓÒÒ Ø is set to 1.0; when the relation is not the same, Ë ÓÒÒ Ø is set to 0.5. The relations between phases are estimated by the function word or conjugation type of the dependent phrase. The monolingual similarity between Japanese expressions I and S is calculated as follows: ¾ É EQUAL´ µ • ¾ Ç AE Ì Ì SIM´ µ (5) Translation Confidence of Japanese-to-English Alignment The translation confidence of phrase alignment between S and T is the sum of the confidence score of each phrase correspondence in ALIGN, CONF( ) in Table 2 , and it is weighted by the WCR of the parallel sentences. As a final measure, the score of I-S-T is calculated as follows: Ò ¾ É EQUAL´ µ • ¾ Ç AE Ì Ì SIM´ µ Ó ¢ Ò ¾ ÄÁ AE CONF´ µ Ó ¢ WCR (6) Search Algorithm of FTE For each phrase (P) in an input sentence, the most plausible FTE is retrieved by the following algorithm: 1. FTEs are retrieved from the translation memory, in which a Japanese phrase matches P, and it is aligned to an English phrase. (that is, these are FTEs that meet the basic conditions for translation in Section 3.1). 2. For each FTE obtained in the previous step, it is checked whether the surrounding phrase of P and that of FTE are the same or similar, phrase by phrase, and the largest I-S-T that meets the basic conditions is detected. * ËÒØØ is a similarity calculated based on NTT thesaurus (Ikehara et al., 1997 ) (max = 1). exact match Ë Ù Ò 1.0 stem match 0 otherwise 1.0 all content words in alignment correspond to each other in dic CONF( ) 0.8 some content words in alignment correspond to each other in dic 0.5 otherwise 3. The score of each I-S-T is calculated, and the best I-S-T (S-T is the FTE) is selected as the FTE for P. As a result of detecting FTEs for phrases in the input, two FTEs starting from the different phrase might overlap each other. In such a case, we employed a greedy search algorithm that adopts the higher score FTE one by one; therefore, each previously adopted FTE is only partly used for translation. On the other hand, when no FTE is obtained for an input phrase, a translation dictionary is utilized (when the phrase contains two or more content words, the longest matching strategy is used for dictionary look-up). When two or more possible translations are given from the dictionary, the most frequent phrase/word in the NHK News Corpus is adopted. Figure 5 shows examples of FTEs detected by our method. 2 Generating a Target Sentence The English expressions in the selected FTEs are combined, and the English dependency structure is constructed. The dependency relations in FTEs are preserved, and the relation between the two FTEs is estimated based on the relation of the input sentences. Figure 5 shows an example of a combined English dependency structure. When a surface expression is generated from its dependency structure, its word order must be selected properly. This can be done by preserving the word order in FTEs and by ordering FTEs by a set of rules governing both the dependency relation and the word-order. The module for controlling conjugation, determiner, and singular/plural is not yet implemented in our current MT system. 2 As the bottom example in Figure 5 shows, EBMT can easily handle head-switching translation by using an FTE that contains all of the head-switching phenomena in it. Experiments For evaluation, we selected 50 sentence pairs from the NHK News Corpus that were not used for the translation memory. Their source (Japanese) sentences were translated by our EBMT system, and the selected FTEs were evaluated by hand, referring to the target (English) sentences. A phrase by phrase evaluation was done to judge whether the English expression of the selected FTE was good or bad. The accuracy was 85.0%. In order to investigate the effectiveness of each component of FTE selection, we compared the following four methods: 1. EQCONTEXTALIGN: The proposed method. 2. EQALIGN: FTE score is calculated as follows, without the CONTEXT similarity: ¾ É EQUAL( ) ¢ ¾ ÄÁ AE CONF( ) ¢ WCR (7) 3. EQCONTEXT: FTE score is calculated as follows, without the ALIGN confidence: ¾ É EQUAL( ) • ¾ Ç AE Ì Ì SIM( ) (8) 4. DICONLY: Word selection is based only on dictionaries and frequency in the corpus. The accuracy of each method is shown in Table 3 , and the results indicate that the proposed method, EQ-CONTEXTALIGN, is the best, that is, using context similarity and align confidence works effectively. Figure 7  shows examples of EQCONTEXTALIGN and DICONLY. EQCONTEXTALIGN usually selects appropriate words, compared to DICONLY. When there are no plausible translation examples in the translation memory, the system selects a low-similarity or low-confidence FTE. However we believe this problem will be resolved as the number of translation examples increases, since the News Corpus is increasing day by day. Related Work The idea of example based machine translation systems was first proposed by (Nagao, 1984) , and preliminary systems that appeared about ten years (Sato and Nagao, 1990; Sadler and Vendelmans, 1990; Maruyama and Watanabe, 1992; Furuse and Iida, 1994) showed the basic feasibility of the idea. Recent studies have focused on the practical aspects of EBMT, and this technology has even been applied to some restricted domains. The work in (Richardson et al., 2001; Menezes and Richardson, 2001) addressed the problem of technical manual translation in several languages, and the work of (Imamura, 2002) dealt with dialogues translation in the travel arrangement domain. These works select the translation example pairs based solely on the source language similarity. We believe this is partly due to the high parallelism found in their corpora. Our work targets a more general corpus of wider coverage, i.e., the broadcast news collection. Generally available corpora like the one we use tend to be more freely translated and suffer from lower parallelism. This compelled us to use the criterion of translation confidence, together with the criterion of monolingual similarity used in the previous works. As we showed in this paper, this metric succeeded in meeting our expectations. Conclusion In this paper, we described operations of the entire EBMT process while using a content-aligned corpus, i.e., the NHK Broadcast Corpus. In this process, one of the key problems is how to select plausible translation examples. We proposed a new method to select translation examples based on source language similarity and translation confidence. In the word selection task, the performance is highly accurate. Acknowledgements This work was supported in part by the 21st Century COE program \"Information Science and Technology Strategic Core\" at University of Tokyo and by a contract with the Telecommunications Advancement Organization of Japan, entitled \"A study of speech dialogue translation technology based on a large corpus\".",
         "1710453",
         "d15362913142ff6c3aff085492a5da8e3ca47cd6",
         "17",
         "https://aclanthology.org/W03-0312",
         null,
         null,
         "2003",
         null,
         "Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond",
         "Aramaki, Eiji  and\nKurohashi, Sadao  and\nKashioka, Hideki  and\nTanaka, Hideki",
         "Word Selection for {EBMT} based on Monolingual Similarity and Translation Confidence",
         "57--64",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "aramaki-etal-2003-word",
         null,
         null
        ],
        [
         "30",
         "2009.mtsummit-posters.3",
         "In this paper, we evaluate for the first time the use of Machine Translation technology to repair general errors in second language (L2) authoring. Contrary to previously evaluated approaches which rely exclusively on unilingual models of L2, this method takes into account both languages, and is thus able to model linguistic interference phenomena where the author produces an erroneous word for word translation of his L1 intent. We evaluate a simple roundtrip MT approach on a corpus of foreign-sounding errors produced in the context of French as a Second Language. We show that the roundtrip approach is better at repairing linguistic interference errors than non-interference ones, and that it is better at repairing errors which only involve function words. We also show that the first leg of the roundtrip (inferring the author's L1 intent) is more sensitive to error type and more error prone than the second leg (rendering a correct L1 intent back into L2).",
         "In this paper, we evaluate for the first time the use of Machine Translation technology to repair general errors in second language (L2) authoring. Contrary to previously evaluated approaches which rely exclusively on unilingual models of L2, this method takes into account both languages, and is thus able to model linguistic interference phenomena where the author produces an erroneous word for word translation of his L1 intent. We evaluate a simple roundtrip MT approach on a corpus of foreign-sounding errors produced in the context of French as a Second Language. We show that the roundtrip approach is better at repairing linguistic interference errors than non-interference ones, and that it is better at repairing errors which only involve function words. We also show that the first leg of the roundtrip (inferring the author's L1 intent) is more sensitive to error type and more error prone than the second leg (rendering a correct L1 intent back into L2). Introduction In this paper, we investigate a novel approach to correcting grammatical and lexical errors in texts written by second language learners or authors. Contrary to most previous approaches which tend to use unilingual models of the learner's second language (L2), this new approach uses a bilingual translation model based on both the learner's first (L1) and second languages. It has the advantage of being able to model linguistic interference phe-nomena, that is, errors which take their root in literal translation from the author's first language. Although we apply this method in the context of French-as-a-Second-Language, its principles are largely independent of language. While there currently exist many Editing Aids which can assist a user in producing written compositions, very few of them target specifically the type of errors made by second language authors. These tools typically use rules for grammar checking as well as lexical heuristics to suggest stylistic tips, synonyms or fallacious collocations. Advanced examples of such tools include Antidote for French 1 and StyleWriter 2 for English. Text Editors like MS Word and Word Perfect also include grammar checkers, but their style checking capabilities tend to be limited. Most of the above tools were designed with native authors in mind, and do not deal well with errors found in foreign sounding sentences often produced by second language authors. Recent work in the field of error correction, especially as applied to English in the context of English as a Second Language (ESL) and Computer Assisted Language Learning (CALL), show an increasing use of corpora and language models. The work presented in this paper is in a similar vein, as it is based on Statistical Machine Translation (MT) systems which are corpus-based. The remainder of this paper is organized as follows. Section 2 discusses the problem of linguistic interference and its influence on second language writing. Section 3 presents a review of related work. Section 4 discusses different ways in which MT could be used to help repair second language errors, and explains the simple roundtrip translation approach used in this paper. Sections 5 and 6 respectively describe the corpus and method used to evaluate performance of this roundtrip approach. Section 7 presents the results of this evaluation, while conclusions and directions for future work are discussed in Section 8. The linguistic interference problem It is widely accepted among linguists (Selinker, 1969; Sheen, 1980; Cowan, 1983 ) that a significant portion of the errors committed by Second Language students are caused by linguistic interference from the student's first language (L1). Some researchers have even observed empirically that the majority of errors made by second language students fall in that category (Wang and Garigliano, 1992, Cowan, 1983, p 109) . Therefore, in the context of automatic correction of L2 errors, it is worth paying special attention to these so-called transfer errors. This type of error is also particularly interesting and challenging, because they often result in sentences which are perfectly grammatical, yet clearly sound foreign. As pointed out by by Wang and Garigliano (1992) , there are many different types of transfer errors. A lexical transfer error is when the author improperly renders a content L2 word using one of its many possible translations in L1, but this turns out to be an inappropriate choice in the particular context of the L1 sentence he is writing. For example, a French as a Second Language (FSL) author might write \"je vais commencer une famille\" (\"I will start a family\"), where \"fonder\" (\"to found\") should have been used instead of \"commencer\". A syntactic transfer error is when the author writes an L1 sentence which borrows improperly from a L2 grammatical or syntactic structure. For example, a FSL author might write \"si j'avais eu donné du temps\", which has a one-to-one mapping with \"if I had been given time\", when in fact, the proper French formulation would be \"si on m'avait donné du temps\". An idiomatic transfer error is when the author literally translates an idiom or collocation, resulting in a sentence which is either nonsensical or for-eign sounding in L2. For example, a FSL author might write \"regarder pour une maison\" (\"look for a house\"), when in French, the collocation \"look for\" should be rendered as \"chercher\" (\"search\", without a preposition). To these three categories, we add a fourth which was not mentioned by Wang and Garigliano: orthographic transfer errors. This is when the author \"invents\" a word which does not actually exist in L2, by using an L2-ish orthography for a L1 word. For example, an FSL author might write \"constructer\" in an attempt to render \"to construct\", when in fact the proper French word is \"construire\". Note that the boundaries between these different categories are not crisply defined, and it is not always easy to decide which category a particular error fits in. For example, \"commencer une famille\" could be construed either as a lexical transfer error (bad choice for \"start\"), or an idiomatic transfer error (bad rendering of the collocation \"start a family\"). However, it is generally easy to determine whether or not an error is a transfer error (whatever its sub-category) or not. In this paper, we considered an error to be a transfer error if we could come up with a native sounding English sentence which had the correct meaning, and translated word for word to the L1 error. Given that transfer errors are caused by literal translation of a L1 thought, it seems reasonable to try and leverage knowledge of the authors' L1 to repair them. In this paper, we evaluate for the first time how Statistical Machine Translation might be used to do this. Related Work Historically, grammatical error correction has been done through parsing-based techniques such as syntactic constraint-relaxation (L'haire & Vandeventer-Feltin, 2003) , or mal-rules modeling (Schneider and McCoy, 1998) . But generating the rule-bases needed by these types of approaches involves a lot of manual work, and may still in the end be too imprecise to convey information on the nature and solution of an error. Recently, more effort has been put in methods that rely on automatically built language models. This is particularly true of work done in the context of Second Language learning or authoring, and Computer Assisted Language Learning (CALL). Typically, this kind of work will either focus on controlled inputs (ex: a small number of predetermined sentences used as exercises in a CALL context), on a specific domain (ex: flight reservation), or on a specific class of errors which usually involves function words only (ex: determiners or prepositions). Shei and Pain (2001) propose a unilingual method for correcting L2 collocation errors, where words in the collocation are substituted by synonyms taken from a dictionary, and the likelihood of these combinations is evaluated by looking up in library of collocations pre-compiled from a corpus. The paper does not actually evaluate the approach. Lee and Seneff (2006) propose a two-phased generation-based framework where a n-gram model re-ranked by a stochastic context-free-grammar model is used to correct sentence-level errors in the language domain of flight reservation. Brockett et al. (2006) used a Brown noise channel translation model to record patterns of determiner error correction on a small set of mass-nouns, and reducing the error spectrum in both class and semantic domain, but adding detection capabilities. Note that although they use a translation model, it processes only text that is in one language. More specifically, the system learned to \"translate\" from poorly written English into correctly written English. Chodorow et al. (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. They use lemmatization as a means of generalization and trained their model over 7 million prepositional contexts, achieving results of 84% precision and 19% recall in preposition error detection in the best of the system's configurations. Gamon et al. (2008) worked on a similar approach using only tagged trigram left and right contexts: a model of prepositions uses serves to identify preposition errors and the Web provides examples of correct form. They evaluate their framework on the task of preposition identification and report results ranging from 74 to 45% precision on a set of 13 prepositions. Yi et al. (2008) use the Web as corpus and send segments of sentences of varying length as bag-ofconstituents queries to retrieve occurrence contexts. The number of the queried segments is a PoS condition of \"check-points\" sensitive to typical errors made by L2 authors. The contexts retrieved are in turn analyzed for correspondence with the original input. The detection and correction methods differ according to the class of the error. Determiner errors call for distinct detection and correction procedures while collocation errors use the same procedure for both. Determiner errors are discovered by thresholds ratios on search hits statistics, taking into account probable ambiguities, since multiple forms of determiners can be valid in a single context. Collocation errors on the other hand, are assessed only by a threshold on absolute counts, that is, a form different from the input automatically signals an error and provides its correction. This suggests that detection and correction procedures coincide when the error ceases to bear on a function word. Similarly, Hermet et al. (2008) use a Web as corpus approach to address the correction of preposition errors in a French-as-a-Second-Language (FSL) context. Candidate prepositions are substituted for erroneous ones following a taxonomy of semantic classes, which produces a set of alternate sentences for each error. The main interest of their study is the use of a syntax-based sentence generalization method to maximize the likelihood that at least one of the alternatives will yield at least one hit on the Web. They achieve accuracy of 69% in error repair (no error detection), on a small set of clauses written by FSL Learners. There has also been some work on bilingual approaches, or use of MT in error correction settings. Several authors (Wang and Garigliano, 1992 , Anderson, 1995 , La Torre, 1999 , Somers, 2001) have suggested that students may learn by analyzing erroneous sentences produced by a MT system and reflecting on the probable cause of errors, especially in terms of interference between the two languages. In this context however, the MT system is used only to generate exercises, as opposed to helping students find and correct errors in texts that they produce. Schuster (1986) describes a system which uses L1 information to correct L2 errors in verb-particle constructs (ex: \"go over\", \"put on\"). The system is designed to work only for controlled example sentences in a CALL context. It uses hand-crafted L2 grammar to identify where student's translation of those examples differ from a correct translation, with respect to verb-particle constructs. Directtranslation tables of the verbs and prepositions are then used to find the likely L1 construct, and information about the L1 verb is used to explain to the student, the difference between that construct in L1 and L2. Performance of the system was not evaluated. Although it is not based on an MT model, Wang and Garigliano (1992) propose an algorithm which uses a hand-crafted, domain-specific, mixed L1 and L2 grammar, in order to identify L1 interference errors in L2 sentences. L2 sentences are parsed with this mixed grammar, giving priority to L2 rules, and only employing L1 rules as a last resort. Parts of the sentence which required the use of L1 rules are labeled as errors caused by L1 interference. The paper does not present an actual evaluation of the algorithm. In a broad patent, Dymetman and Isabelle (2007) propose a range of methods for correcting single-word errors. The method computes the probability of different corruption paths, starting backward from a potentially incorrect L2 word written by the author, going to various L1 words which might have erroneously been rendered as that L2 word. It then goes back to L2, generating alternative renderings of those L1 words, and those with highest probability are suggested as potential repairs. To the best of our knowledge, no evaluation has been published for any embodiment of this general method. Hermet and Désilets (2009) compare the performance of a web-as-corpus with shallow syntactical pruning, against that of a roundtrip MT approach, for repairing preposition errors. They also evaluate a hybrid approach where the roundtrip approach is used as a fallback for cases where the web-as-corpus approach produces no suggestions at all. While they found no significant difference between the repair rates of the first two approaches, they found the hybrid method to perform significantly better than either approach in isolation. None of the work cited above on bilingual models tried to evaluate this type of approach in situations with open-ended input, domain, and error type all at once. To the best of our knowledge, this is a unique feature of the present paper. Roundtrip Translation as a Means for Second Language Error Repair There are many ways in which MT could be used to correct L2 errors, many of which were first proposed in Dymetman and Isabelle (2007) . For this very first evaluation of this kind of approach, we choose the simplest possible implementation of the concept (roundtrip translation), and only apply it to error repair (but see section 7 for a discussion of future research on error detection). Given an L2 sentence S2, for which we already know that specific words are erroneous, we carry out an automatic roundtrip translation to generate an alternative L2 sentence S2*. The roundtrip is carried out in two separate steps, first producing the most probable L1 translation S1 of S2, then producing the most probable L2 translation S2* of S1. We then follow the word alignments in the two legs of this path, in order to identify the words in S2* which align with the erroneous words in S2. Those aligned words in the roundtrip sentence are presented as the correction. One can think of this approach as trying to reverse engineer the correct L1 intent behind the L2 error, and then trying to produce a better rendering of that intent into L2. Note that one limitation of this simple approach is that it carries out the roundtrip in two steps, and may not return a path which maximizes the overall probability of S2* given S2. This choice was made because of its ease of implementation. In particular, it allowed us to carry out the two legs of the roundtrip using the Google Translate service. One drawback of using such an online service is that it is essentially a closed box, and we therefore have little control over the translation process, and no access to lower level data generated by the system in the course of translation (e.g. phrase alignments between source and target sentences). In particular, this means that we have no way of assessing which parts of S2* have a high probability of being better than their corresponding parts in the original S2. This is the main reason why we focus first on error repair and leave error detection as future work. We have found this simple approach to be unexpectedly effective, often resulting in a S2* which addresses the erroneous parts of S2. This is somewhat surprising, since one would expect the roundtrip sentence to be worse than the original, on account of the \"Chinese Whisper\" effect 3 . Our current theory for why this is not the case in practice goes as follows. In the course of translating the original L2 sentence to L1, when the MT system encounters a part that is ill-formed, it will tend to use single word entries from its phrase table, because longer phrases will not have been represented in the well-formed L2 training data. In other words, the system tends to generate a word for word translation of ill-formed parts, and this turns out to mirror exactly what L2 authors do when they write poorly formed L2 sentences by translating too literally from their L1 intent. As a result, that part of the L1 sentence produced by the MT system is often well formed for that language. Subsequently, when the MT system tries to translate that well-formed L1 part back to L2, it is therefore able to use longer entries from its phrase table, and hence produce a better L2 translation than what the author originally produced. While this theory sounds plausible, we have not been able to verify it in this work, as we did not have access to the phrases used by Google Translate in the course of roundtrip translation. Evaluation Corpus In order to evaluate the roundtrip approach, we collected a total of 829 erroneous sentences from a corpus of texts written by 30 students, during one semester in an advanced-intermediate French as a Second Language (FSL) course given to university students. The class included native English students, as well as allophones for whom English was a second language and French was a third. In pulling out errors from this corpus, we focused only on errors which were clearly unlikely for a native speaker. Most sentences presented several errors, including some that could have been made by native speakers (ex: spelling mistakes, agreement). All sentences were fed as is to the roundtrip translation procedure, without any pre-massaging. In particular, we did not run them through standard spelling and grammar checkers. Each error was classified along two axes. Table 1 shows different examples of how this was done. The first axis looked at whether the error was due to linguistic interference from English or not. For an error to be categorized as such, we had to be able to think of a proper native-sounding English sentence which had the same meaning as the French one, and for which the faulty part of the French was a word for word translation from the English. The reason for this classification was to assess the degree to which the roundtrip approach might work better on interference errors, since it is essentially trying to model the process that produces them. The second axis of classification looked at whether the error could be fixed simply by substituting a contiguous sequence of function words (prepositions, determiners, conjunctions, auxiliaries, etc.) Not just function words. Although the error concerns placement of determiner \"le\", fixing the error requires that it be swapped with content word \"trouver\". Table 1 : Examples of errors with their classifications in the two axes. cation was to assess the degree to which the roundtrip approach could deal with \"difficult\" errors which involve more than changing a few contiguous function words. Selection of the erroneous phrases and their classification into the two axes was done in two phases. For each of those tasks, one of the authors of the present paper did a first pass, and his choices were subsequently validated by the other author. All differences of opinion were resolved through discussion. Evaluation procedure For each error in our corpus, we carried out roundtrip translation from French to English, and then back to French, using the Google Translate web service. We then looked at both legs of this trip, and assessed whether or not the error had been repaired. The reason for assessing both legs was to evaluate the degree to which the system was actually able to infer the the author's L1 intent (S1) based on the erroneous L1 sentence (S2). Regarding the first leg, we should point out that we were always able to manually infer the author's intended meaning beyond doubt. This is because we had access to the full length of the text in which the sentence appeared, and the intended meaning was always clear from this context, even if the sentence was severely garbled. As we pointed out earlier, this simple roundtrip approach cannot distinguish between false positive and true positives. Therefore, we focused only on error correction, that is, given that we know a particular part of the original S2 sentence to be erroneous, we looked at whether the roundtrip S2* reformulated that part in a way that fixed the error or not. In particular, this means that if S2* introduced new errors in parts of S2 that were not considered for repair, the roundtrip could still be considered a success if it did repair the part of S2 which had been flagged for repair. An erroneous part of the original S2 was deemed to have been repaired if its corresponding part in S2*: • was grammatically correct This is a girl of twenty years which <makes walking> dogs from its neighbors. Not corrected. Failed to infer the proper English collocation \"walk the dogs\". Il s'agit d'une jeune fille de vingt ans qui <fait marcher> les chiens de ses voisins. Corrected. Eventhough \"Il s'agit\" may have slightly changed the meaning of that part of the sentence, the roundtrip did fix the part which was flagged for repair, by using a proper French collocation \"faire marcher les chiens\". J'ai deux frères et beaucoup <des> oncles. I have two brothers and many <> uncles. Corrected. J'ai deux frères et <de> nombreux oncles. Corrected. Although the roundtrip unnecessarily changed content word \"beaucoup\" to \"nombreux\" (which is still appropriate), the sentence is grammatical, native sounding, and preserves the author's original meaning. La vielle dame a <formé instantanément une amitié> avec les huit garçons The old lady has <formed an instant friend-ship> with the eight boys. Corrected. Although the choice of tense is dubious, the intermediate translation does address the error which was flagged for repair, namely, failure to use a proper collocation for \"form an instant friendship\". La vieille dame a <formé un instant d'amitié> avec les huit garçons. Not corrected. The meaning has been changed to \"had a moment of friendship\". Je donnerais de l'argent pour construire des maisons pour ces qui n'en ont pas <une>. I would give the money to build houses for those who do not<>. Not corrected. Meaning has changed from \"those who do not have a house\" to \"those who do not build a house\". Je souhaite donner de l'argent pour construire des maisons pour <ceux qui ne le font pas>. Not corrected. Meaning has changed from \"those who do not have a house\" to \"those who do not build a house\". Table 2 : Examples of roundtrip translations from original L2 sentence (S2), through L1 sentence (S1) and back to L1 (S2*). • was native sounding (Google counts used in case of doubt) • preserved the meaning intended by the author in the corresponding part of S2 The last point means that, as long as the first two conditions were met, S2* was allowed to make larger reformulations than were strictly necessary. For example, if it changed an erroneous preposition, but also changed its support verb for a synonym, the change could still be considered a success event though changing the support verb was not absolutely necessary. Table 2 provides examples of various sentences and how the success of the roundtrip was evaluated. Results and Discussion Table 3 summarizes the results of the evaluation. For the various types of errors, it provides the probability that the error was repaired in the overall roundtrip (P(S2*) column) as well as in each leg of the roundtrip (P(S1) and P(S2*|S1) columns). The probabilities for P(S2*|S1) were estimated by taking the sample mean for the cases where S1 repaired the error. Note that P(S2) is not necessarily equalt to the product P(S1) and P(S2*| S1) because there were cases where S2* was correct, even though S1 was incorrect. A first observation is that in our data, only 60% of the foreign-sounding errors were caused by linguistic interference. Although we did not collect native-sounding errors in our corpus of texts, we noticed that there was a large number of them, none of which can (by definition) be transfer errors. Therefore, it is unlikely that the majority of errors present in our texts were caused by linguistic interference. This contradicts what was previously reported (Selinker, 1969; Sheen, 1980; Cowan, 1983; Wang and Garigliano, 1992) . We can offer two possible explanations for this. Firstly, it could be that previous authors used \"transfer errors\" in the sense of \"an error which a native speaker would not make\". In contrast, we adopted a stricter definition where there had to be a one-to-one mapping between a correct L1 sentence with the appropriate sense, and the incorrect L2 sentence. Another hypothesis is that, because our corpus was partly produced by learners whose first language was neither English or French, it may contain a large number of errors which involve linguistic interference through a language other than English. More research needs to be done to clarify this. Looking at the P(S2*) column, we see that roundtrip translation repaired 55.6% of all errors. If we compare the Interf and Non-Interf rows, we see that the approach is better at repairing interference errors, and that this holds whether we look at the overall roundtrip or each leg separately. This was to be expected, since the error repair strategy specifically models the process through which L2 authors produce transfer errors. All differences were found to be statistically significant (p < 0.05), except for the difference for the second leg of the trip P(S2*|S1). Comparing the Fct Wrds and Non Fct rows, we see that the approach is better at repairing function word errors, and that this holds whether we look at the overall roundtrip or each leg separately. In this case, all three differences were found to be statistically significant. This indicates that, although roundtrip translation is able to deal with errors that involve more than just function words, it still performs better on errors of that type. Comparing the P(S1) and P(S2*|S1) columns, we see that the first leg of the trip exhibits more variance than the second one, meaning that reverse engineering the author's L1 intent is more sensitive to the type of error than rendering a correct L1 intent back into L2. We also see that with the exception of interference errors, the first leg tends to be more successful than the second leg of the roundtrip. All differences (with the exception of the interference row) were found to be statistically significant. This is was to be expected since MT systems are trained mostly on well formed sentences. Therefore, they are less likely to produce a correct target sentence when the source sentence is ill-formed. The one exception to this seems to be cases where the ill-formed source sentence is a word for word translation of a well formed target sentence, in which case, performance seems to be as good as with well-formed source sentences. Conclusions and Future Work We have presented a very first attempt at evaluating the use of MT technology for repairing errors in second language authoring. While performances may seem moderate (55.6% repair rate), one must remember that this simple generic approach was used to tackle a very wide range of errors, including choice of preposition and determiners, choice of verb tense, lexical choice for content words, syntactic word order and collocations. Some of those (ex: lexical choice of content words) are particularly challenging. More work needs to be done to compare the performance of MT based correction on such wide range of errors, to that of unilingual approaches such as the ones cited in section 3. It would be interesting to test the approach using other MT systems. In particular, given that many of the errors in our corpus involved improper use of L1 grammar and syntax, it would be interesting to see if a hybrid SMT system which takes syntax and grammar into account (see Eisele 2008 for a survey) would perform better than a purely phrasebased system like the one we used in this study (Google Translate). Certainly, the fact that even this simplest possible use of MT in a L2 correction context was able to correctly infer the L1 intent behind 74.6% of interference errors, is quite encouraging. Future work should also be done in order to evaluate more complex algorithms based on MT. For example, instead of choosing S1 and S2* which respectively maximize likelihood for each leg of the roundtrip, one might instead choose the S2* which maximizes likelihood of the complete path. This would involve the generation of a combined phrase lattice for the overall roundtrip, instead of separate lattices for each leg. More sophisticated approaches are also needed to deal with error detection, as opposed to repair. For example, one might analyze the combined phrase lattice to identify S2* segments whose probability given S1 are sufficiently high to be considered better than their counterpart in S1. These S1 segments would then be flagged as errors. Acknowledgments The authors are indebted to the following people (all from NRC) for helpful advice on how to best exploit MT for second language correction: Pierre Isabelle, George Foster and Eric Joanis. Also to Howard Johnson, for help with statistical analysis of the results.",
         "28893172",
         "b976c166f88d03d4c7c77405ba41a2013fb88537",
         "4",
         "https://aclanthology.org/2009.mtsummit-posters.3",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "D{\\'e}silets, Alain  and\nHermet, Matthieu",
         "Using Automatic Roundtrip Translation to Repair General Errors in Second Language Writing",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "desilets-hermet-2009-using",
         null,
         null
        ],
        [
         "31",
         "2022.eamt-1.54",
         "This paper describes a multilingual chatbot developed for public administration within the ENRICH4ALL project. We argue for multilingual chatbots powered through machine translation (MT) and discuss the integration of the eTranslation service in a chatbot solution. Related Work The benefits of having e-government chatbots are several: they can process service requests in huge",
         "This paper describes a multilingual chatbot developed for public administration within the ENRICH4ALL project. We argue for multilingual chatbots powered through machine translation (MT) and discuss the integration of the eTranslation service in a chatbot solution. Related Work The benefits of having e-government chatbots are several: they can process service requests in huge Introduction In this paper, we introduce the Action ENRICH4ALL (E-goverNment [RI] CHatbot for ALL) which is about the development of a multilingual chatbot service to be deployed in public administration in Luxembourg, Denmark, and Romania. ENRICH4ALL is funded by the Connecting Europe Facility and its duration is from June 2021 to May 2023. The partners are Luxembourg Institute of Science and Technology, BEIA Consulting Romania, Romanian Academy Institute for AI and SupWiz, Denmark. In this paper, we refer to the benefits and challenges of egovernment chatbots and to the integration of eTranslation with the chatbot platform. numbers, work 24/7, provide up-to-date information and consequently reduce operational costs. In some European countries, such as Denmark, Estonia, and Latvia, there are chatbots used in many public authorities, whereas in other countries, such as Romania or Luxembourg, there are not. Some of the challenges of using chatbots in public administration are the large number of relevant services, the complexity of administrative services, the context-dependent relevance of user questions, the differences in expert-language and user-language as well as the necessity of providing highly reliable answers for all questions (Lommatzsch, 2018) . To these challenges, we should add the language diversity in Europe. The consequence of language diversity is that each EU country and each administration uses its own initiative to deploy a chatbot (often monolingual) resulting in a scenario where the interaction with e-government through virtual assistants is scarce and fragmented. A multilingual chatbot in public administration Particularly for administrative procedures, there are many requests from expatriates, who enter a new country. Application for residence, importing a car, starting-up a new business, and building a house are some of such requests. Public administration was also burdened with many questions related to the pandemic, which gave rise to COVID-19 chatbots in Europe. We created (and actively develop) three datasets: • COVID-19 (RO 2 ) • Construction permits (RO) • Administrative questions (LTZ-FR-DE-EN) The datasets are available at the project's website and will soon be available at the European Language Grid. As for BERT language models, we use already existing ones for RO, FR, DE, EN, and we have developed and trained one for Luxembourgish 3 to use for detecting question similarity and classification with user intent labels, but this is outside the scope of this paper. BotStudio BotStudio is the AI-powered chatbot developed by the Danish partner SupWiz, where the eTranslation API is now integrated. BotStudio can use fine-tuned BERT-based models built with HuggingFace APIs to appropriately map user intents to chat nodes in specific domains. Integration of eTranslation eTranslation 4 is both a stand-alone MT tool and an API that can be integrated into various systems to facilitate multilingual services. The tool translates from and to 27 languages in different domains, including Russian, simplified Chinese, and recently Ukrainian. eTranslation is the neural MT tool provided by the European Commission to all EU bodies but also public services and SMEs across Europe. The latency of the service is low for small input texts, which makes it usable for real-time applications. Three arguments for using eTranslation compared to other translation services are: i) privacy is a priority; all data resides in Europe 5 ; ii) it is free for SMEs; iii) it supports niche domains for formal language. Figure 1 presents the eTranslation integration. One of the challenges is language identification. In our chatbot, we added a language identification service based on the PyPI langdetect package. For LTZ, a new language profile was added, while for DE, FR, EN, RO, and DA 6 , existing language profiles are used. For all languages, the language of the input question is automatically detected and Conclusion and Future Prospects Our chatbot is an AI-based, MT-powered service, which proves available information to citizens 24/7 and reduces the administrative burden from public authorities. After the chatbot deployment, there will be additional data created and shared with the EC. Through data creation and training, eTranslation will be trained for other domains and maybe extended for LTZ, which is not supported in eTranslation. Generally, it would be interesting to integrate MT in open-domain conversational QA, e.g. ORConvQA (Qu et al., 2020) . Acknowledgement The Action 2020-EU-IA-0088 has received funding from the EU's 2014-2020 -CEF Telecom, under Grant Agreement No. INEA/CEF/ICT/A2020/2278547.",
         "249204460",
         "d58a549d6cc59b3aba066d0f86c430412dab1831",
         "0",
         "https://aclanthology.org/2022.eamt-1.54",
         "European Association for Machine Translation",
         "Ghent, Belgium",
         "2022",
         "June",
         "Proceedings of the 23rd Annual Conference of the European Association for Machine Translation",
         "Anastasiou, Dimitra  and\nRuge, Anders  and\nIon, Radu  and\nSeg{\\u{a}}rceanu, Svetlana  and\nSuciu, George  and\nPedretti, Olivier  and\nGratz, Patrick  and\nAfkari, Hoorieh",
         "A Machine Translation-Powered Chatbot for Public Administration",
         "329--330",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "anastasiou-etal-2022-machine",
         null,
         null
        ],
        [
         "32",
         "2020.lrec-1.337",
         "We present the Romanian legislative corpus which is a valuable linguistic asset for the development of machine translation systems, especially for under-resourced languages. The knowledge that can be extracted from this resource is necessary for a deeper understanding of how law terminology is used and how it can be made more consistent. At this moment, the corpus contains more than 144k documents representing the legislative body of Romania. This corpus is processed and annotated at different levels: linguistically (tokenized, lemmatized and POS-tagged), dependency parsed, chunked, named entities identified and labeled with IATE terms and EUROVOC descriptors. Each annotated document has a CONLL-U Plus format consisting of 14 columns; in addition to the standard 10-column format, four other types of annotations were added. Moreover the repository will be periodically updated as new legislative texts are published. These will be automatically collected and transmitted to the processing and annotation pipeline. The access to the corpus is provided through ELRC infrastructure.",
         "We present the Romanian legislative corpus which is a valuable linguistic asset for the development of machine translation systems, especially for under-resourced languages. The knowledge that can be extracted from this resource is necessary for a deeper understanding of how law terminology is used and how it can be made more consistent. At this moment, the corpus contains more than 144k documents representing the legislative body of Romania. This corpus is processed and annotated at different levels: linguistically (tokenized, lemmatized and POS-tagged), dependency parsed, chunked, named entities identified and labeled with IATE terms and EUROVOC descriptors. Each annotated document has a CONLL-U Plus format consisting of 14 columns; in addition to the standard 10-column format, four other types of annotations were added. Moreover the repository will be periodically updated as new legislative texts are published. These will be automatically collected and transmitted to the processing and annotation pipeline. The access to the corpus is provided through ELRC infrastructure. Introduction In this paper we review the first results of the new project, \"Multilingual Resources for CEF.AT in the legal domain\" (MARCELL) action 1 whose final goal is to enable enhancement of the automatic translation in CEF.AT 2 on the body of national legislation in seven EU official languages. For this task, all the seven teams cooperated in order to produce a comparable corpus aligned at the top-level domains identified by EUROVOC descriptors 3 . EUROVOC is a multilingual thesaurus maintained by the Publications Office of the European Union. It exists in the 24 official languages of the European Union. It is used by many major institutions in Europe which include: several governmental departments, regional and national parliaments in the continent, the Council of the European Union and the European Parliament. It contains keywords organized in 21 fields and 127 micro-thesauri and it serves as the basis for the domain names used in the European Union's terminology database: Interactive Terminology for Europe (IATE) 4 . IATE is the EU's terminology database and it is used for dissemination and management of EU-specific terminology. One of its main aims is to facilitate the task of translators working for the EU. Currently, it has over 8 million terms and uses the EUROVOC thesaurus as a domain classification system. In the following we describe the activities and the results of the Romanian team. A general view of the project activities is given in another article (Váradi et al., 2020) . For the Romanian language, the current legal database created includes more than 144k legislative processed documents, issued starting from 1881. Since the last round of document The first part of the paper presents the main goals of this project together with the process of collecting the corpus. The second part details the statistics of the corpus. The third part presents the annotation process of the corpus: part-of-speech tagging, dependency parsing, nominal phrases identification, named entity recognition and IATE and EUROVOC terms identification. Collecting The Romanian Legal Corpus Goals of the MARCELL Project Since the techniques for processing and exploiting corpora have been developed and are not dependent on features of specific languages, text corpora have become the main source of research data in computer linguistics. Lately, it has become a common practice to use the web for corpus acquisition, but in general most of the texts gathered in a corpus have restrictions regarding the laws of intellectual property and licensing. However, there is a type of text data that is exempted from copyright protection (unlike web-based texts) -the law body, which is often specifically excluded from copyright laws. In law texts including constitution, acts, public notices and court judgements the used sub-language is partially restricted and also different from general language. Therefore, within this project, seven monolingual corpora of national legislation documents will be created in order to provide automatic translation on the body of national legislation (laws, decrees, regulations, etc) in seven countries: Bulgaria, Croatia, Hungary, Poland, Romania, Slovakia and Slovenia. A related project was, years ago, JRC-Acquis, concerned with compiling a parallel corpus of the total body of European Union (EU) law applicable in the the EU Member States (Steinberger et al., 2006) . Unlike JRC-Acquis corpus, the MARCELL comparable corpus addresses national laws in only 7 EU countries and is supposed to be a very good companion thematic data to JRC-Acquis with little (if any) duplication. The main goals of the first phase of this action are: to produce a pre-processed (tokenized and morphologically tagged) monolingual corpus of national legislative texts for each of the seven languages; to identify the IATE and EU-ROVOC terms in texts and to classify the documents according to the 21 EUROVOC fields (top-level domains). The corpus building process The acquisition of the texts included in the Romanian legislative corpus was done via crawling. In order to collect the corpus, we used the Romanian legislative portal 5 , which provides free access to all the legal documents issued since 1881. During this step, all the HTML-tags were eliminated together with style sheets, objects, tables, figures etc. From each file we collected only raw texts and the information needed to create metadata such as: document type, issuer, date, title and URL. Corpus Statistics The corpus contains more than 144k files ranging from 1881 to 2018. In There are five main types of Romanian legal documents: governmental decisions (25%), ministerial orders (18%), decisions (16%), decrees (16%) and laws (6%). After the statistics were calculated, we found that there are six main issuers of the documents: Government (28%), Ministers (19%), President (14%), Constitutional Court (12%), Parliament (6%) and National Authorities (4%). Concerning the time-stamp, most of the published documents were issued after year 2000. Before 1990, almost 4,000 documents were issued and between 1990 and 2000 around 21,000 legal documents were published. After year 2000, the number of issued documents has increased and, on average, more than 6,000 documents were issued every year, reaching a total of 120,000 until 2018, in 19 years. In terms of document length, there are around 6,000 short documents (less than 100 words per document, most of them being updates to other previously published legal documents), 70,000 documents contain between 100 and 500 words per document, more than 18,000 documents have around 1000 words per document and 52,000 contain more than 1000 words. Corpus Annotation Linguistic Annotation The corpus is annotated in batches, as new documents are collected. The processing flow is part of the RELATE por- The preprocessing pipeline, excluding IATE and EU-ROVOC annotations, is done using the TEPROLIN text preprocessing platform (Ion, 2018) , which was integrated into RELATE such that its output is as visually descriptive as possible. TEPROLIN can be easily configured to use different algorithms to do different parts of the text preprocessing pipeline and it only needs a list of desired text annotations to infer and construct the pipeline getting these annotations out (see Figure 1 ). Annotation with NER Part of the overall annotations required within the MAR-CELL project is named entity recognition. In this context, we used the module integrated in the RELATE platform (Pȃis , et al., 2019) . This is a general named entity recognizer for Romanian language implemented using Conditional Random Fields (CRF), based on the Stanford NER (Finkel et al., 2005) software package. It is enhanced with the Romanian word embeddings (Pȃis , and Tufis , , 2018) learned from the CoRoLa corpus (Tufis , et al., 2019) . Furthermore, it uses the embeddings web service from the RELATE platform in order to obtain at runtime representations for previously unknown Romanian words. In total, a number of four entity classes can be identified: person (PER), organization (ORG), location (LOC) and time (TIME). In accordance with the MARCELL project specifications, the resulting annotation is added in IOB format (Sang and Veenstra, 1999) , in a dedicated column in the resulting annotated file. The final algorithm was based on the Aho-Corasick (Aho and Corasick, 1975) data structure and the previously defined Compression Function. It also introduced a processing separation between short terms (having at most 4 consonants) and long terms (all the other terms) in order to increase identification accuracy. Short terms were directly inserted into an Aho-Corasick structure, through which the corpus was also directly passed in order to identify the matches. On the other hand, long terms were first passed through the Compression Function (pseudolemmatized), then inserted into a different Aho-Corasick structure, through which we passed the image of the corpus through the Compression Function (Algorithm 2). It is worth mentioning that, unlike other string-matching algorithms like Levenshtein Automata, Aho-Corasick does not impose the need to process multi-word terms separately. Each term is passed through the Compression Function regardless of its structure, then the corpus is \"fed\" character by character to the structure. The identified terms from those two Aho-Corasick structures (represented by Short-Terms/LongTerms in the previous pseudocode) were then merged and inserted in the legal corpus. Overall, there were 51,517,877 matches, with an average of 347 IATE terms matched per document. In order to compute the accuracy of the algorithm, we took into consideration both the fraction of positive-matches which we identified and the fraction of matches which are false-positives. Thus, an evaluation over a testing sample yielded an accuracy of 98%-99%. Due to the size of the In each document, the legal terms identified from IATE and EUROVOC can be found on columns 13 and 14 respectively. The IATE and EUROVOC labels are prefixed with a number counting the terms in the current document. For multi-word terms, this counter allows correct term identification. In figure 2 , MONITORUL (English \"the instructor\") is the first term in the current document, identified by the IATE code 1394636 and EUROVOC descriptor 3206 (Education and Communication). However, MONI-TORUL OFICIAL (English \"the official monitor\") is a different term (the second) with IATE code 3522817 for which three EUROVOC descriptors applies: 3221, 7206, 7231. File structure and metadata In order to enable further analysis for all seven action languages, the format of the processed documents is the same, irrespective of the language. Each document has a CoNLL-U Plus 10 format and begins with a line describing the columns followed by a newdoc marker holding the file id (# newdoc id = ro.legal). Each sentence in a document is labelled by a unique ID (# sent id = ro legal.4), followed by the text of the respective sentence (# text = . . . ) and then the vertical analysis, CoNLL-U Plus with 14 columns, of the tokens occurring in the sentence. Each file also contains the corresponding in-line metadata: the title of the document, date of issue, document type and URL. For the purpose of local corpus management, we also created standoff metadata to be used in the KORAP platform we use for the exploitation of the CoRoLa national corpus (Tufis , et al., 2019) . The structure of a line is the following, the line fields being tab-separated (see Figure 2 ): ID, FORM, LEMMA, UDPOS, XPOS, FEATS, HEAD, DEPREL, , , NER, CHUNK, IATE, EUROVOC. After the language specific processing the documents are archived and sent to the next processing hub: the multilingual clustering and comparable documents semantic align-10 https://universaldependencies.org/ ext-format.html ment phase begins. Availability The corpus is stored in a uniform representation format and is already made available to ELRC 11 . Using the ELRC infrastructure and protocols, the corpus can be accessed in two forms: raw legislative documents or linguistically annotated legislative documents. Moreover, periodically new documents will be added to the corpus, pre-processed, annotated and classified. This process will assure that language-specific features and changes will be captured. Conclusions In this paper, we described the process of creating a largescale monolingual corpus of national legislation documents enhanced with different types of annotations. Identifying the terms from both IATE and EUROVOC makes this resource very useful in the development of machine translation systems. Moreover, the work presented in this paper emphasizes the fact that the construction of domain-specific corpora also involves putting work and effort into developing domain-specific annotation tools. We are planning to classify all the documents according to the 21 top-level EUROVOC categories. Several approaches will be used in order to determine the optimal one. Currently, the classification of the documents is done based on the most frequent EUROVOC category in each document, but we are also working on a classification based on word embeddings and on another one using the JRC Eurovoc Indexer JEX 12 , which is pre-trained for all EU official languages. One of the main goals of the MARCELL project is to ensure sustainability by continuous feeding of the repository with new incoming data and ensuring time-persistence and low maintenance times of the processing pipelines against the OS updates and other changes between hosts and environments. In this context, the size of the Romanian legal corpus is expected to increase in both raw and annotated data. Furthermore, the Romanian language-specific processing flow, as all language-specific flows will be containerized, using Docker or similar technologies. Acknowledgements This research was supported by the EC grant no. INEA/CEF/ICT/A2017/1565710 for the Action no. 2017-EU-IA-0136 entitled \"Multilingual Resources for CEF.AT in the legal domain\" (MARCELL).",
         "218974546",
         "6638d1f4b481d12cec2d888e6442545a7dc7fe0f",
         "2",
         "https://aclanthology.org/2020.lrec-1.337",
         "European Language Resources Association",
         "Marseille, France",
         "2020",
         "May",
         "Proceedings of the 12th Language Resources and Evaluation Conference",
         "Tufi{\\textcommabelow{s}}, Dan  and\nMitrofan, Maria  and\nP{\\u{a}}i{\\textcommabelow{s}}, Vasile  and\nIon, Radu  and\nComan, Andrei",
         "Collection and Annotation of the {R}omanian Legal Corpus",
         "2773--2777",
         null,
         null,
         null,
         null,
         null,
         "979-10-95546-34-4",
         "inproceedings",
         "tufis-etal-2020-collection",
         "English",
         null
        ],
        [
         "33",
         "R13-1091",
         "This article reports on mass experiments supporting the idea that data extracted from strongly comparable corpora may successfully be used to build statistical machine translation systems of reasonable translation quality for in-domain new texts. The experiments were performed for three language pairs: Spanish-English, German-English and Romanian-English, based on large bilingual corpora of similar sentence pairs extracted from the entire dumps of Wikipedia as of June 2012. Our experiments and comparison with similar work show that adding indiscriminately more data to a training corpus is not necessarily a good thing in SMT.",
         "This article reports on mass experiments supporting the idea that data extracted from strongly comparable corpora may successfully be used to build statistical machine translation systems of reasonable translation quality for in-domain new texts. The experiments were performed for three language pairs: Spanish-English, German-English and Romanian-English, based on large bilingual corpora of similar sentence pairs extracted from the entire dumps of Wikipedia as of June 2012. Our experiments and comparison with similar work show that adding indiscriminately more data to a training corpus is not necessarily a good thing in SMT. Introduction Wikipedia is one of the most accessed websites of the Internet according to Alexa.com with a global rank of 6 (being outrun only by major search engines such as Google, Yahoo and Baidu and by Face-book and YouTube). Approximately 14% of all Internet users use it on a daily basis and out of these, more than 50% browse through the English version of Wikipedia which is the most comprehensive one, judged by the number of articles. Wikipedia is not a real parallel corpus, although many documents in different languages are translations from English. Many documents in one language are shortened or adapted translations 1 of documents from other (not always the same) languages and this property of Wikipedia together with its size makes it the ideal candidate of a strongly comparable corpus from which parallel sentences can be mined. In the following, we use the term MT useful data to denote sets of bilingual sentences/phrases with a high level of cross-lingual similarity, out of which a word/phrase aligner can extract translation lexicons relevant for the SMT task. SMT engines like Moses (Koehn et al., 2007) produce better translations when presented with larger and larger training parallel corpora. For a given training corpus, it is also known that Moses produces better translations when presented with indomain new texts (texts from the same domain as the training data, e.g. news, laws, medicine, etc.). Collecting parallel data from a given domain, in sufficiently large quantities to be of use for statistical translation, is not an easy task. To date, OPUS 2 (Tiedemann, 2012) is the largest online collection of parallel corpora, comprising of juridical texts (EUROPARL and EUconst) 3 , medical texts (EMEA), technical texts (e.g. software KDE manuals, PHP manuals), movie subtitles corpora (e.g. OpenSubs) or news (SETIMES) but these corpora are not available for all language pairs nor their sizes are similar with respect to the domain. In a previous paper (Ștefănescu et al., 2012) we described in details an open-source parallel data extractor from comparable corpora, developed within the ACCURAT EU-project 4 . Essentially, this extractor allows for identifying similar (translation-wise) sentences in a bilingual comparable corpus. A multi-variable function scores the similarity of each candidate pair, and depending on the level of similarity score (ranging between 0 and 1), one could compile different MT useful data sets. We showed elsewhere (Ion et al., 2011) that with the similarity threshold above 0.7, for all the languages we experimented with, our extracted data, human validated, is really parallel. However, depending on the comparability level of the extraction corpus, the quantity of parallel data extracted may range from 0.1% (weakly comparable corpora) to 29% (strongly comparable corpora) of the entire corpus (Ion et al., 2011) . Setting a high similarity threshold has the disadvantage that a significant part of the MT useful data contained in the comparable corpora is lost. The experiments we report in this article had multiple purposes: a) to assess the usefulness of extracted data for SMT by investigating the contribution of less than parallel extracted data to the quality of the translations produced by a baseline SMT; this investigation was driven by iteratively lowering the similarity threshold for the extracted data and evaluating the translation quality for the system trained on the resulted MT useful data. b) to assess the feasibility of better translating English documents absent from a foreign Wikipedia version; currently, Wikipedia does not offer an integrated translation engine to assist the translation task but this could be a worthy option to consider. With respect to this aim, all our experiments were conducted on in-domain (but unseen during the training) test sets. c) to add a new domain (for many language pairs) -the encyclopedic domain -to the list of already existing domains for which MT useful data exists (e.g. Tiedemann's OPUS collection multilingual corpora). In the rest of this paper, after reviewing the related research (Section 2), we provide some statistics on three large sets of similar sentence-pairs extracted from Wikipedia for the English-Spanish, English-German and English-Romanian language pairs (Section 3). In Section 4 we describe the Moses-based experiments with the extracted MT useful data and compare the results with those obtained in a similar scale experiment on Wikipedia. Section 5 describes the follow-up of the previously described experiments with even better results. We conclude with Section 6. Related work Due to its structure with linked articles on the same subject and because, frequently, articles in foreign languages contain adapted versions of the translations (or just the translation) of the English or other languages counterparts, Wikipedia is arguably the largest strongly comparable corpus available online. It has been the test bed of many attempts at parallel sentence mining. Adafre and Rijke (2006) were among the first to attempt extraction of parallel sentences from Wikipedia. Their approach consists of two experiments: 1) the use of a MT system (Babelfish) to translate from English to Dutch and then, by word overlapping, to measure the similarity between the translated sentences and the original sentences and 2) with an automatically induced (phrase) translation lexicon from the titles of the linked articles, they measure the similarity of source (English) and target (Dutch) sentences by mapping them to (multiple) entries in the lexicon and computing lexicon entry overlap. Experiments were performed on 30 randomly selected English-Dutch document pairs yielding a few hundred parallel sentence pairs. Mohammadi and GhasemAghaee (2010) continue the work of Adafre and Rijke ( 2006 ) by imposing certain limits on the sentence pairs that can be formed from a Wikipedia document pair: the length of the parallel sentence candidates must correlate and the Jaccard similarity of the lexicon entries (seen as IDs) mapped to source (Persian) and target (English) must be as high as possible. As with Adafre and Rijke, the work performed by Mohammadi and GhasemAghaee does not actually generate a parallel corpus but only a couple of hundred parallel sentences intended as a proof of concept. Another experiment, due to Smith et al. (2010) , addressed large-scale parallel sentence mining from Wikipedia. Based on binary Maximum Entropy classifiers, in the spirit of Munteanu and Marcu (2005) , they automatically extracted large volumes of parallel sentences for English-Spanish (almost 2M pairs), English-German (almost 1.7M pairs) and English-Bulgarian (more than 145K pairs). According to Munteanu and Marcu (2005) , a binary classifier can be trained to distinguish between parallel sentences and non-parallel sentences using features such as: word alignment log probability, number of aligned/unaligned words, longest sequence of aligned words, etc. To enrich the feature set, Smith et al. proposed to automatically extract a bilingual dictionary from the Wikipedia document pairs and use this dictionary to supplement the word alignment lexicon derived from existing parallel corpora. Since the work of Smith et al. (2010) is the only one we know of that extracted parallel corpora of similar sizes to ours, we will reserve a detailed comparison with their work in the evaluation section (Section 4.4). Furthermore, they released their English-Spanish and English-German Wikipedia test sets and so, a direct comparison is made possible. Unfortunately, the large amounts of extracted parallel corpora are not available online for the SMT research community. The Extracted Wiki Datasets Using LEXACC (Ștefănescu et al., 2012) we mined (Ștefănescu and Ion, 2013) for parallel sentence pairs from selected documents belonging to full dumps of English, Romanian, Spanish and German Wikipedias as of December 2012. Table 1 lists, for different similarity scores (Sim) as extraction thresholds, the number of MT useful sentence pairs (P) found in each language pair dataset, as well as the number of words (ignoring punctuation) per language (EnW, DeW, RoW, EsW) in the respective sets of sentence pairs. Data extracted with a given similarity score threshold is a proper sub-set of any data extracted with a lower similarity score threshold. Sim En-De En-Ro En-Es 1 : Number of parallel sentences and words extracted for each language pair, for a given threshold (Ștefănescu and Ion, 2013) From Table 1 , one could easily calculate the average word length for the extracted sentences for each language and each threshold value. It is not surprising that longer the sentences their similarity scores get lower. For the En-De language pair, the sentence word length varied for En from 28.98 to 18.11 while for De it varied from 24.65 to 14.43. A similar variation may be noticed for En-Es pair: from 24.42 to 12.28 (En) and from 25.49 to 12.63 (Es). For En-Ro the average sentence word length varied less: from 23.82 to 19.27 (En) and from 24. 03 to 19.63 (Ro) . By random manual inspection of the generated sentence pairs, we confirmed earlier evaluations (Ion et al., 2011) that, in general, irrespective of the language pair, sentence pairs with a translation similarity measure of at least 0.7 are entirely parallel (e.g. \"In 2003, Africa 2 Africa was merged with SABC Africa.\" \"En 2003, Africa 2 Africa fue fusionada con SABC Africa.\", score 0.97), those with a translation similarity measure of at least 0.5 have extended parallel fragments which an accurate word or phrase aligner easily detects (e.g. \"Besides regular repairs of the existing runways, Prague Airport (Letiště Praha s.p.\" \"Además de las habituales refacciones de las pistas, Letiště Praha s.p.\", score 0.59). Below 0.5, sentences usually become strongly comparable. Further down the threshold scale, below 0.3, we usually find sentences that roughly speak of the same event but are not actual translations of each other (e.g. \"Slaves were previously introduced by the British and French who colonized the island in the 18th century.\" \"Los esclavos ya habían sido introducidos un siglo antes por los británicos y franceses que trataron de conquistar la isla.\", score 0.29). The noisiest data sets were extracted for the 0.1 similarity threshold and we drop them from further experiments. SMT experiments with Wiki datasets There is a strong opinion, empirically supported, that parallel data extracted from comparable corpora leads to improvements of the translation quality of a baseline MT system when it incorporates this data. This has been exemplified by showing that a baseline MT system trained on data covering one or more domains, when tested on texts out of the respective domain(s), performed significantly worse. Translation models adaptation with data extracted from comparable corpora from the test domain improved the translation quality, but in general not reaching the same quality as in the baseline MT translation of the in-domain texts. One can naturally raise the following question: given a large and continuously growing multilingual collection of documents (such as Wikipedia) what would be a good approach for enhancing a SMT trained to translate Wikipedia-like documents (let's call it Wikitranslator)? The question calls to the limited available in-domain parallel data for any language pair (the sizes of pair-wise parallel Wikipedias are limited, even for the best represented languages) but suggest the benefits of indomain adaptation by using comparable data extracted from Wikipedia. This issue is placed into operational terms, by asking the question: what level of sentences comparability is useful for improving the quality of Wiki-translator's output? The experiments described in this section try to provide some hints to the questions above. We argued that with a high value (0.7) for the similarity threshold, the extracted sentence-pairs can safely be considered truly parallel. However, in Table 1 , we showed that the number of sentences pairs with a similarity score of at least 0.7 represents a small portion (ranging from 14% to a maximum of 24%) of the potentially MT useful sentence pairs (corresponding to the threshold 0.1) from the interlinked documents. In what follows, we give experimental insights by observing how translation improves/degrades when training on parallel sentences with different translation similarity thresholds. Experimental setup As mentioned in Section 4, the English, German and Spanish Wikipedias are the largest ones with substantial cross-lingual coverage. Romanian Wikipedia is medium-sized but containing many translations or adaptations of articles from other languages (mainly English). Consequently, we could find in En-De, En-Es and En-Ro Wikipedias a number of parallel sentences (190, 135 for En-De with more than 6.86 million words, 142,512 for En-Ro with more than 6 million words and 1,219,866 for En-Es with almost 50 million words) allowing for building baseline Wiki-translators for these language pairs. The large sets of comparable sentences allowed us to conduct experiments on assessing the translation quality improvement/degradation when the parallel core training corpora were gradually extended with comparable but less and less parallel sentence pairs. As the standard SMT system we chose Moses 5 with the default parameters for factorial optimization. We used it with the following parameters: • surface-to-surface translation; • phrase length of maximum 4 words; • lexical reordering model with parameters wbe-msd-bidirectional-fe. The language model (LM) for all experiments was trained on entire monolingual, sentence-split English Wikipedia, after removing the administrative articles as described in Section 4. The language model was limited to 5-grams and the counts were smoothed with the interpolated Knesser-Ney method. The test sets for the three language pairs were created by concatenating randomly extracted 2500 sentence pairs from each similarity interval ensuring parallelism ([0.6, 1], [0.7, 1], [0.8, 1] and [0.9, 1]). The sentence pairs extracted from each similarity interval were manually checked for parallelism. Thus we obtained 10,000 parallel sentence pairs for each language pair. These sentences were removed from the training data. In compiling the test sets, we were careful to observe the Moses' filtering constraints: both the source and target sentences must have at least 4 words and at most 60 words and the ratio of the longer sentence (in tokens) of the pair over the shorter one must not exceed 2. Once the test sets were ready, we further trained eight translation models (TM), for each language pair, over cumulative threshold intervals beginning with 0.2: TM [0.2, 1] for [0.2, 1], TM [0.3, 1] for [0.3, 1] …, TM [0.9, 1] for [0.9, 1]. The training data for TM [0.2, 1] was the largest but the noisiest, while the training data for TM [0.9, 1] was the smallest but fully parallel. The resulting eight training corpora have been filtered with Moses' cleaning script with the same restrictions mentioned above. For every language, both the training corpora and the test set have been tokenized using Moses' tokenizer script and true-cased. We are interested in finding out if the quality of the translation system based on the translation model TM i were significantly different from the quality of the translation system based on the translation model TM i+1 , where TM i and TM i+1 are translation models built as described in the previous sub-section. The quality of the translation systems was measured as usual in terms of their BLEU score (Papineni et al., 2002) on the same test data (10,000 parallel sentence pairs). SMT results for Spanish-English and German-English SMT results for Romanian-English Translation for Romanian-English language pair has also been studied in Dumitrescu et al. (2013) with explicit interest for the indomain/out-of-domain test/train data, using Moses in various configurations for surface-tosurface and factored translation. Out of the seven domain specific corpora (legal, transcribed speech, parliamentary debates, literature, medi-cine, news and encyclopedic) the encyclopedic corpus was based on Wikipedia. They have experimented with English-Romanian parallel sentence pairs extracted from Wikipedia using LEXACC at a fixed threshold: 0.5 (called \"WIKI5\"). A random selection of unseen 1000 Wikipedia Romanian test sentences has been translated into English using combinations of: • a WIKI5-based translation model (240K sentence pairs)/WIKI5-based language model; • a global translation model (1.7M sentence pairs)/global language model named \"ALL\", made by concatenating all specific corpora. Table 4 gives the details, giving the BLEU scores for the Moses configuration similar to ours: surface-to-surface translation, with the language/translation model combinations described above. Dumitrescu et al.'s results confirm the conclusion we claimed earlier: the ALL system performs worse than the in-domain WIKI5 system. WIKI5 TM ALL Our present results show the same characteristics as those of the Spanish-English and German-English experiments presented earlier. They are summarized in Table 5 .  The almost eight BLEU points difference between our results and those in (Dumitrescu et al., 2013) may be explained by: 1) our language model was entirely in-domain for the test data and much larger: our language model was built from entire Romanian Wikipedia (more than 220,000 documents) while the language model in (Dumitrescu et al., 2013) was built only from the Romanian doc-ument paired to English documents (less than 100,000 documents); 2) different Moses filtering parameters (e.g. the length filtering parameters), 3) different test sets. TM BLEU SCORE Comparison with Smith et al. (2010) As mentioned in Section 2, Smith et al. (2010) mined for parallel sentences from Wikipedia producing parallel corpora of sizes similar to ours Furthermore, they have made their Wikipedia test set available for Spanish-English and German-English (500 sentence pairs per language pair). We have translated these test sets (after being true-cased) with our best translation models (0.3 for Spanish-English and 0.5 for German-English) and also with Google Translate (as of mid-February 2012).  It is thus empirically supported the finding that indiscriminately adding more out-of domain data, when large enough in-domain data already exists (as in these compared experiments), produces worse results. Bootstrapping experiments The astute reader may have noticed that the dictionaries used by LEXACC for mining MT useful data were extracted by GIZA++ from out-ofdomain corpora (JRC-Acquis and Europarl). After obtaining the sets of in-domain MT useful data for the three language pairs discussed above, it was a natural decision to go one step further: compute new translation dictionaries by merging the old ones with the dictionaries generated by GIZA++ from in-domain data (extracted as described in Section 4) and re-do the SMT experiments described in Section 5. Since the full chain of experiments for the three language pairs is extremely time consuming, at the time of this writing we have the new results only for En-Ro language pair, which has the smallest datasets. English-Romanian new extracted data The earlier experiments empirically showed that the Similarity Score below 0.2 produced too much noisy data to be useful in SMT experiments. Therefore, we proceed with the LEXACC extraction process considering Similarity Score (Sim) higher or equal to 0.2. Table 7 shows a significant increase of the number of extracted bilingual sentence pairs when the out-of-domain translation dictionary is extended by the in-domain translation lexicon. Sim Initial En-Ro Boosted En-Ro The new extracted corpus was used for the similar SMT experiments as described in Section 5. The test set was selected from completely parallel documents, not contained into the data extraction space. We changed the test set construction strategy using entire parallel documents and not sentence pairs from the parallel documents. The first strategy could be suspected of biasing, since the contexts of the tested sentences (the documents from where the test sentence-pairs were extracted) were used for training. The test set contains 1,000 Ro-En parallel sentences. Table 8 shows the results. Again, we outline the differences in BLEU scores for the initial SMT experiments and the boosted ones.  We made also translation experiments for the other direction, Ro-En, and as expected the translation accuracy (in terms of BLEU scores) was significantly lower. The best BLEU score for En-Ro translation direction was 44.09, but this time for the translation model trained on the bilingual corpus with the similarity score equal or higher than 0.5 (TM [0.6, 1] ). TM The last step in our experimental chain was to optimize the translation parameters using the usual MERT procedure. The development set used to tune the translation parameters had 1,000 parallel sentences, not used in the training or test sets. Not surprisingly, the BLEU scores further improve. Table 9 So far, we obtained our best result of 51.05 BLEU for the Ro-En direction, using the MERTenhanced Boosted method. Conclusions We have shown that Wikipedia is a rich resource for parallel sentence mining in Statistical Machine Translation. Comparing different translation models containing MT useful data ranging from comparable, through strongly comparable, to parallel, we concluded that there is sufficient empirical evidence not to dismiss sentence pairs that are not fully parallel on the suspicion that because of the inherent noise they might be detrimental to the translation quality. On the contrary, our experiments demonstrated that in-domain comparable data are strongly preferable to outof-domain parallel data. However, there is an optimum level of similarity between the comparable sentences, which according to our similarity metrics (for the language pairs we worked with) is around 0.4 or 0.5. Additionally, the two step procedure we presented, demonstrated that an initial in-domain translation dictionary is not necessary, it can be constructed subsequently, starting with a dictionary extracted from whatever out-of-domain data. The parallel Wiki corpora (before and after the boosting step), including the two test sets (containing 10,000 and respectively 1,000 sentences) are freely available on-line 6 . We want to clarify one aspect though: it is not the case that our extracted data is the maximally MT useful data. We evaluated and extracted only full sentences. A finer-grained (sub-sentential) extractor would likely generate more MT useful data. Acknowledgments This work has been supported by the EU under the Grant Agreements no. 248347 and no. 270893.",
         "16682338",
         "8d8fb4de0ade09bd3754aa06f4086f79c72f4805",
         "16",
         "https://aclanthology.org/R13-1091",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Tufi{\\textcommabelow{s}}, Dan  and\nIon, Radu  and\nDumitrescu, {\\textcommabelow{S}}tefan  and\n{\\textcommabelow{S}}tef{\\u{a}}nescu, Dan",
         "{W}ikipedia as an {SMT} Training Corpus",
         "702--709",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "tufis-etal-2013-wikipedia",
         null,
         null
        ],
        [
         "34",
         "2009.mtsummit-posters.1",
         "The paper explores a way to learn post-editing fixes of raw MT outputs automatically by combining two different types of statistical machine translation (SMT) systems in a linear fashion. Our proposed system (which we call a chained system) consists of two SMT systems: (i) a syntax-based SMT system and (ii) a phrase-based SMT system (Koehn, 2004). We first translate source sentences of the bitext training data into a target language, using the syntax-based SMT. This provides us the monolingual parallel data that consist of the raw MT outputs and their corresponding human translations. We then build a phrasebased SMT system, using the monolingual parallel corpus. Our system is thus a chain of a syntax-based SMT system and a phrase-based SMT system. The benefit of the chained system is to learn post-editing fixes automatically via a phrase-based SMT system (Simard, et al.,  2007a/b). We investigated the impact from the chained system on the initial SMT system in terms of BLEU, using typologically different language pairs. The results of our experiments strongly indicate that the second part of the chained system can compensate the weaknesses of the initial SMT system in a robust way by providing human-like fixes.",
         "The paper explores a way to learn post-editing fixes of raw MT outputs automatically by combining two different types of statistical machine translation (SMT) systems in a linear fashion. Our proposed system (which we call a chained system) consists of two SMT systems: (i) a syntax-based SMT system and (ii) a phrase-based SMT system (Koehn, 2004). We first translate source sentences of the bitext training data into a target language, using the syntax-based SMT. This provides us the monolingual parallel data that consist of the raw MT outputs and their corresponding human translations. We then build a phrasebased SMT system, using the monolingual parallel corpus. Our system is thus a chain of a syntax-based SMT system and a phrase-based SMT system. The benefit of the chained system is to learn post-editing fixes automatically via a phrase-based SMT system (Simard, et al.,  2007a/b). We investigated the impact from the chained system on the initial SMT system in terms of BLEU, using typologically different language pairs. The results of our experiments strongly indicate that the second part of the chained system can compensate the weaknesses of the initial SMT system in a robust way by providing human-like fixes. Introduction The quality of an SMT system has improved quite a lot since late 90\"s and different types of SMT systems have been proposed over the last decade. The quality of SMT, however, is still not sufficient for actual use. For instance, we have been using a syntax-based SMT system for the last several years to localize technical manuals or documents. However, most of our clients end up handing off raw MT output to human post-editors due to their concerns about the quality of the MT output. Human post-editing of raw MT output is as costly as human translation from scratch. This in fact devalues the use of MT for localization. When comparing raw MT outputs and their human post-edited translations, we often find repetitious changes of wrongly translated phrases to correct ones. This motivates a so-called \"automatic post-editing\" (APE) proposed by Simard et. al., (2007 a/b) . In their view, the task of postediting is considered as the task of finding mappings between raw MT output and human postedited translation. They used a phrase-based SMT system to learn such mappings and apply it to the output of a rule-based MT system. They show impressive results by adding this phrasal system to their rule-based MT system. We took their insight and applied it to our syntax-based SMT. Our system consists of two SMT systems: (i) a syntax-based SMT (called \"treelet\") system (Quirk, et al., 2005) and (ii) a phrase-based SMT system modelled on Pharaoh (Koehn, 2004) . We call it \"the chained system\" throughout the paper. We compare the baseline treelet SMT and the chained system in terms of BLEU (Papineni, et al., 2002) , using typologically different language pairs (English->Spanish, German, and Japanese). The results from our experiments show that the idea of APE discussed in Simard et al., 2007(a/b) works for an SMT system and that it can provide humanlike post-editing fixes across different language pairs automatically. The organization of the paper is as follows. Section 2 provides an overall architecture of our chained system. Section 3 provides the design of our experiments and their results. In Section 4, we provide the linguistic error analyses of the results from our experiments. Section 5 provides our concluding remarks and future work. Architecture Training Time Overview The overall architecture of the training of the chained system (using the English -> Spanish language pair as an example) is provided in Figure 1 . In the following subsections, we walk through the training process step-by-step while providing brief descriptions of the two SMT systems. First Path: Creating Monolingual Parallel Corpus Our baseline system (the treelet system) is a syntactically informed SMT and it requires bilingual parallel corpus data as its training data (see Quirk, et al., 2005 for technical details). We use this treelet system as the first path of the chained system. To illustrate the training process of the chained system step-by-step, let us assume that we are going to create an English-to-Spanish chained system. As the first path of the chained system, we train the treelet system on the bilingual (English-Spanish (ES)) parallel corpus and then translate all the source English sentences of the training data into Spanish (see 1 st Path in Figure 1 ). This gives us a new set of parallel data consisting of: (i) the Spanish MT outputs and (ii) their corresponding human translations (from the ES training data). The first phase of the chain system can be considered to be a process to create the monolingual parallel corpus via an existing SMT system, which will be used to train the second phase of the chained system. Second Path: Training a Phrase-based System As the second path of the chain, we train a phrasebased SMT system using the monolingual parallel corpus mentioned above. This second path is expected to learn the post-editing fixes for the raw MT output of the initial SMT. The phrase-based system we used in this paper is a reimplementation of the Pharaoh system (Koehn, 2004) . The word alignment of our phrase-based system is done by an HMM-based word alignment algorithm (He, 2007) . As in Koehn (2004) , word alignment is performed bi-directionally; (i) from the source (the raw SMT output) to the target (the human translation of the target side of the training data) and (ii) from the target to the source. These two alignments are combined to form the final word alignment with the heuristics described in Och and Ney (2000) . From this, we extract phrasal translation pairs that are consistent with the word alignment. For our experiments, we set the maximum phrase length to 4, and the maximum reordering limit for decoding to 2. Run-time Overview The overview of the run-time process of the chained system is provided in Figure 2 . The run-time process is simply the concatenation of the baseline syntax-based SMT trained on the bilingual parallel corpus (see Section 2.2) and the phrase-based system trained on the monolingual parallel data (see Section 2.3). That is, first translate an input English sentence into Spanish using the treelet system and then \"re-translate\" the output into Spanish using the monolingual parallel data trained phrase-based SMT system. Experiments We trained chained systems for three typologically different language pairs; (i) English->Spanish (ES), (ii) English->German (EG) and (iii) English->Japanese (EJ). For the ES system, we used data from the publicly available Europarl corpus v2 (Koehn, 2005) . 1 For the EG and the EJ systems, we used technical domain data sets. For each of these data sets we trained a baseline treelet system and evaluated the baseline system on the test set using BLEU. 2 For the evaluation of the chained system, we first translated the training and dev sets using the baseline system. With these translated sets, we trained the phrase-based system. In testing, we translated the test data using the chained system (i.e., first, the baseline treelet system and then, the phrase-based system) and measured its quality using BLEU. 3  The results are shown in Table 2 . For comparison purposes we also trained a baseline phrasebased system with the same configuration as the chained phrase-based system and the bilingual dataset of the baseline system. All BLEU scores are reported in percentage. Table 2 : BLEU scores of baseline treelet, chained phrasal and baseline phrasal (the delta indicated above is the comparison between the baseline SMT system and the chained system) As shown, all the chained systems show BLEU gains over the baseline treelet and the baseline phrase-based systems. 4 This shows that the chained system works across different language pairs. It is not yet clear to us, however, why we gained only 1 point for the ES system whereas for the EG and the EJ systems, we gained more than 3 points. One speculation for this is that the chained system works better when the domain is specified or narrower. This is a question that we plan to address in future work. To investigate the impact of the chained system from linguistic perspectives, we conducted error analyses on the results of the chained systems mentioned in Section 3. To this end, we first calculated sentence-level BLEU and character edit rate (Levenshtein, 1965) over the two sets of results: (a) the outputs from the treelet system and (b) those from the chained system. Second, we calculated the differences in the two metrics between (a) and (b). 5 We assumed that the differences in the scores reflect the magnitude of the positive/negative impact from the chained system on the baseline system. We extracted top 100 positive examples (i.e., top 100 examples that have a positive value for the chained system) and bottom 100 negative ones (i.e., bottom 100 examples that have a negative value against the chained system) for each of the three language pairs. We asked the speakers of these languages to analyze types of linguistic fixes/errors that the chained system has made on these 200 examples. In the following subsections, we describe some details of our error analyses. Positive Impact Table 3 provides categorical descriptions of the positive changes from the chained system for all the three language pairs. Categorical Descriptions ES inflections; agreements; pronouns; negations; better lexical choices; etc. EJ inflections; case-markers; etc. EG compound nouns; better fluency; inflections; etc. Table 3 : Categorical Descriptions of the Positive Changes English->Spanish Examples (1) and (2) illustrate some of the fixes that the chained system provided for the baseline ES system. ( The contrast between (1a) and (1b) describes the better handling of Spanish pronouns and lexical choices. Spanish is a pro-drop language and the subject pronoun can be dropped freely. The presence of the overt pronoun nos (\"us\") in (1a) gives the wrong pronominal interpretation of \"This is why (they) are criticizing us...\", whereas the absence of an overt pronoun in (1b) gives us the correct pronominal interpretation of \"This is why (we) are criticizing....\". The contrast in the lexical choices between que están bajo (\"that be below\") in (1a) and que sufren (\"that suffer\") in (1b) indicates that the chained system provided the better lexical choice in this context as well. The contrast between (2a) and ( 2b ) illustrates the improvements brought by the chained system in terms of agreement and negation. In (2a), there is no agreement between the noun religiones \"religions\" (which has +feminine and +plural) and its modifying adjective proscrita \"proscribed\" (which has +feminine and +singular), resulting in the mismatch in number. In (2b), on the other hand, the noun religiones agrees with its modifying adjective prohibidas \"prohibited\" both in gender and number. (2a) is worse than (2b) in terms of the handling of negation as well: in (2a), the so-called indefinite negative adjective ningún \"(not) any\" occurs, which requires a (true) negation marker. But (2a) does not have it, resulting in ungrammaticality. In (2b), by contrast, the chained system provided no \"no\" and hence, the negation is nicely recovered. English -> Japanese Example (3) below illustrates the inflection fix provided by the chained system. In (3a), the part 記載 修正 プログラム\"(lit) the hotfix describe\" is missing the light verb する \"to do\", which leads to the wrong interpretation. The chained system nicely supplied the missing predicate (i.e., the underline part in (3b)), resulting in the correct translation of \"the hotfix described in this article\". Other prototypical fixes that the chained system provided for the baseline EJ system are those for case-markers. Japanese is a word order-free language. To specify the argument structure of a predicate (e.g., the subject, the object, etc.), it requires a case-marker for nouns. In (4a), the noun, Windows XP, serves as the object of the verb start and it requires the object case-marker を. The lack of the case-marker in (4a) makes the translation ungrammatical. In (4b), by contrast, the chained system nicely supplies the object marker を, resulting in the correct translation. English -> German As for the EG chained system, it provided many nice fixes for the treatment of German compound nouns as shown in ( 5 )-( 6 ). ( Another type of fix that the EG chained system provided is to improve the fluency by changing the word order or by correcting phrasal expressions. For instance, (7b) below is much more fluent than (7a). ( 7 Negative Impact We also examined the negative cases to investigate what types of damage the chained system has done to the output of the baseline system. One of the most common errors that we found in the chained system output across all these language pairs is that the chained system sometimes deletes content words.  The underlined parts of the chained system outputs in Table 5 are the extra information that does not exist in the source English sentences. Although the number of such negative cases (e.g., those in Table 4 and Table 5 ) is not large, these types of errors should not be introduced by the chained system. Concluding Remarks/Future Work The proposed system is a chain of two types of SMT systems. In this paper, we used the syntaxbased system as our initial SMT system and trained a phrase-based SMT system based on the monolingual parallel corpus data created by the initial system. The results from our experiments strongly indicate that the chained system works for typologically different languages and it can provide a big boost over the overall quality of the initial SMT system. The paper also gives strong support for the idea of creating an APE system using a phrasebased system, which is entertained by Simard et al. (2007a/b) . There are several things that we would like to investigate in the future. First, as mentioned in Section 4, the chained system sometimes deletes content words from the raw SMT output. Also, it adds additional word(s) to the raw SMT output. We would like to investigate these cases further, so that we can prevent such errors automatically by blocking certain phrasal mappings from our phrase-based SMT system. Second, we would like to investigate if translating the training data with an SMT system that was trained on the same data leads to overfitting problems for our chained system. Third, we would like to see whether the proposed approach works for an SMT system other than a syntax-based SMT (e.g., a treelet system). For instance, we can build easily a chained system that consists of two phrase-based systems. We would like see whether such a chained system would give a similar boost as the proposed chained system did. Last, we would like to examine further what decoder settings (e.g., maximum phrase length, distortion length, etc. of a phrase-based SMT system) would work best for a chained system.",
         "32757145",
         "1fc8795bdd6649a97c4745c1d11d6b8ba9878521",
         "7",
         "https://aclanthology.org/2009.mtsummit-posters.1",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Aikawa, Takako  and\nRuopp, Achim",
         "Chained System: A Linear Combination of Different Types of Statistical Machine Translation Systems",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "aikawa-ruopp-2009-chained",
         null,
         null
        ],
        [
         "35",
         "2012.iwslt-evaluation.3",
         "In this paper, we present the KIT systems participating in the English-French TED Translation tasks in the framework of the IWSLT 2012 machine translation evaluation. We also present several additional experiments on the English-German, English-Chinese and English-Arabic translation pairs. Our system is a phrase-based statistical machine translation system, extended with many additional models which were proven to enhance the translation quality. For instance, it uses the part-of-speech (POS)-based reordering, translation and language model adaptation, bilingual language model, word-cluster language model, discriminative word lexica (DWL), and continuous space language model. In addition to this, the system incorporates special steps in the preprocessing and in the post-processing step. In the preprocessing the noisy corpora are filtered by removing the noisy sentence pairs, whereas in the postprocessing the agreement between a noun and its surrounding words in the French translation is corrected based on POS tags with morphological information. Our system deals with speech transcription input by removing case information and punctuation except periods from the text translation model.",
         "In this paper, we present the KIT systems participating in the English-French TED Translation tasks in the framework of the IWSLT 2012 machine translation evaluation. We also present several additional experiments on the English-German, English-Chinese and English-Arabic translation pairs. Our system is a phrase-based statistical machine translation system, extended with many additional models which were proven to enhance the translation quality. For instance, it uses the part-of-speech (POS)-based reordering, translation and language model adaptation, bilingual language model, word-cluster language model, discriminative word lexica (DWL), and continuous space language model. In addition to this, the system incorporates special steps in the preprocessing and in the post-processing step. In the preprocessing the noisy corpora are filtered by removing the noisy sentence pairs, whereas in the postprocessing the agreement between a noun and its surrounding words in the French translation is corrected based on POS tags with morphological information. Our system deals with speech transcription input by removing case information and punctuation except periods from the text translation model. Introduction In the IWSLT 2012 Evaluation campaign [1] , we participated in the tasks for text and speech translation for the English-French language pair. The TED tasks consist of automatic translation of both the manual transcripts and transcripts generated by automatic speech recognizers for talks held at the TED conferences 1 . The TED talks are given in English in a large number of different domains. Some of these talks are manually transcribed and translated by volunteers over the globe [2] . Given these manual transcripts and a large amount of outof-domain data (mainly news), our ambition is to perform optimal translation on the untranslated lectures which are more likely from different domains. Furthermore, we strive for performing as well as possible on the automatically transcribed lectures. The contribution of this work is twofold: on the one hand, it demonstrates how the complementary manipulation of indomain and out-of-domain data is gainful in building more accurate translation models. It will be shown that while the large amount of out-of-domain data ensures wider coverage, the limited in-domain data indeed helps to model better the style and the genre. On the other hand, we show that using a text translation system with a proper processing of punctuation can handle the translation of automatic transcriptions to some extent. Compared to our last year's system, three new components are introduced: adaptation of the candidate selection in the translation model (Section 5), continuous space language model (Section 8), and part-of-speech (POS)-based agreement correction (Section 9). The next section briefly describes our baseline, while Sections 3 through 9 present the different components and extentions used by our phrase-based translation system. These include the special preprocessing of the spoken language translation (SLT) system, POS-based reordering, translation and language model adaptation, the cluster language model, the descriminative word lexica (DWL), the continuous space language model, and the POS-based agreement correction. After that, the results of the different experiments (official and additional language pair systems) are presented and finally a conclusion ends the paper. Baseline System For the corresponding tasks, the provided parallel data consist of the EPPS, NC, UN, TED and Giga corpora, whereas the monolingual data consist of the monolingual version of the News Commentary and the News Shuffled corpora. In addition, the use of the Google Books Ngrams 2 was allowed. We did not use the UN data and Google Books Ngrams this year. The reason was that in several previous experiments (not reported in this paper), they consistently had a negative impact on the performance. A common preprocessing is applied to the raw data before performing any model training. This includes removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized. The first letter of every sentence is smart-cased. Furthermore, an SVM classifier was used to filter out the noisy sentences pairs in the Giga English-French corpus as described in [3] . The baseline system was trained on the EPPS, TED, and NC corpora. In addition to the French side of these corpora, we used the provided monolingual data and the French side of the parallel Giga corpus, for language model training. Systems were tuned and tested against the provided Dev 2010 and Test 2010 sets. All language models used are 4-gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [4] . The word alignment of the parallel corpora was generated using the GIZA++ Toolkit [5] for both directions. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. The phrases were extracted using the Moses toolkit [6] and then scored by our in-house parallel phrase scorer [7] . Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [8] . Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The POS tags for the reordering model are obtained using the TreeTagger [9] . Tuning is performed using Minimum Error Rate Training (MERT) against the BLEU score as described in [10] . All translations are generated using our in-house phrase-based decoder [11] . Preprocessing for Speech Translation The system translating automatic transcripts needs some special preprocessing on the data, since generally there is no or not reliable case information and punctuation in the automatically generated transcripts. We have tried two ways to deal with the difference on casing and punctuation between a machine translation (MT) system and a SLT system. In addition, we also optimize the system with different development data: simulated ASR output and original automatic speech recognition (ASR) output. In order to make the system translate the automatically generated transcripts, the first method we have used is to lowercase the source side of the training corpora and remove the punctuation except periods from the source language. On these modified source sentences and untouched target sentences, all models are re-trained, including alignments, phrase tables, reordering rules, bilingual language model and DWL model. Therefore, we can avoid having to build a whole MT system for the SLT task. In order to simplify the procedure, we tried a second method where we directly modify the source phrases in the phrase tables. We lowercase the source phrases and remove the punctuation except periods from the source phrases. Though there could be duplicated phrase pairs with different scores in the phrase ta-ble due to this modification, during the decoding the phrase with the best scores will be selected according to the weights. Two ways to optimize the system are possible. The first one is to use the manual transcripts but it requires lower casing and removal of punctuation marks. The other one is to use the ASR single-best output released by the SLT task. The advantage of optimizing with the manual transcripts is that the system will be adjusted with higher quality sentences. On the other side, optimization using ASR output makes the system more consistent with the evaluation test data. We have tested both methods in our experiments. Word Reordering Model Our word reordering model relies on POS tags as introduced by [12] . Rule extraction is based on two types of input: the Giza alignment of the parallel corpus and its corresponding POS tags generated by the TreeTagger for the source side. For each sequence of POS tags, where a reordering between source and target sentences is detected, a rule is generated. Its head consists of sequential source tags and its body is the permuted POS tags of the head which match the order of the corresponding aligned target words. After that, the rules are scored according to their occurrence and pruned according to a given threshold. In our system, the reordering is performed as a preprocessing step. Rules are applied to the test set and possible reorderings are encoded in a word lattice, where the edges are weighted according to the rule's probability. Finally, the decoding is performed on the resulted word lattice. During decoding, the distance-based phrase reordering could also be applied additionally. Adaptation To achieve the best performance on the target domain, we performed adaptation for translation models as well as language models. Translation Model Adaptation In a phrase-based translation system, building the translation consists of two steps. First, we select a set of candidate translations from the phrase table (candidate selection). In our system, we normally take the top 10 translations for every source phrase according to initially predefined weights. In the second step, the best translation is built from these candidates using the scores from the translation model (phrase scoring) as well as other models. In some of our systems we also adapted the first step, while the second step was adapted in all of our systems by using additional scores for the phrase table. To adapt the translation model towards the target domain, first, a large translation model is trained on all the available data. Then, a separate in-domain model is trained on the in-domain data only, reusing the alignment from the large model. The alignment is trained on the large data, because it seems to be more important for the alignment to be trained on bigger corpora than being based on only in-domain data. When we do not adapt the candidate selection, the best translations from the general phrase table is used and only the scores from the in-domain phrase table are taken into account. In the other case, we take the union of the phrase pairs collected from both phrase tables. We will refer to this adaptation method as CSUnion in the description of the results. The scores of the translation model are adapted to the target domain by combining the in-domain and out-of-domain scores in a log-linear combination. The adapted translation model uses the four scores (phrase-pair probabilities and lexical scores for both directions) from the general model as well as the two probabilities of both directions from the small in-domain model. If the phrase pair does not occur in the indomain part, a default score is used instead of a relative frequency. In our case, we use the lowest probability that occurs in the phrase table. Language Model Adaptation For the language model, it is also important to perform an adaptation towards the target domain. There are several word sequences, which are quite uncommon in general, but may be used often in the target domain. As it is done for the translation model, the adaptation of the language model is also achieved by a log-linear combination of different models. This also fits well into the global log-linear model used in the translation system. Therefore, we train a separate language model using only the in-domain data from the TED corpus. Then it is used as an additional language model during decoding. Optimal weights are set during tuning by MERT. Cluster Language Model In addition to the word-based language model, we also use a cluster language model in the log-linear combination. The motivation is to make use of larger context information, since there is less data sparsity when we substitute words by word classes. First, we cluster the words in the corpus using the MK-CLS algorithm [13] given a number of classes. Second, we replace the words in the corpus by their cluster IDs. Finally, we train an n-gram language model on this corpus consisting of cluster IDs. Because the TED corpus is small and important for this translation task and it exactly matches the target genre, we trained the cluster language model only on TED corpus in our experiments. The TED corpus is characterized by a huge variety of topics, but the style of the different talks of the corpus is quite similar. When translating a new talk from the same domain, we may not find a good translation in the TED corpus for many topic specific words. What TED corpus can help with, however, is to generate sentences in the same style. During decoding the cluster-based language model works as an additional model in the log-linear combination. Discriminative Word Lexica Mauser et al. [14] have shown that the use of DWL can improve the translation quality. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per one source word. One specialty of this task is that we have a lot of parallel data we can train our models on, but only a quite small portion of these data, the TED corpus, is very important to the translation quality. Since building the classifiers on the whole corpus is quite time consuming, we try to train them on the TED corpus only. When applying DWL in our experiments, we would like to have the same conditions for the training and test case. For this we would need to change the score of the feature only if a new word is added to the hypothesis. If a word is added the second time, we do not want to change the feature value. In order to keep track of this, additional bookkeeping would be required. Also the other models in our translation system will prevent us from using a word too often. Therefore, we ignore this problem and can calculate the score for every phrase pair before starting with the translation. This leads to the following definition of the model: p(e|f ) = J j=1 p(e j |f ) (1) In this definition, p(e j |f ) is calculated using a maximum likelihood classifier. Each classifier is trained independently on the parallel training data. All sentence pairs where the target word e j occurs in the target sentence are used as positive examples. We could now use all other sentences as negative examples. But in many of these sentences, we would anyway not generate the target word, since there is no phrase pair that translates any of the source words into the target word. Therefore, we build a target vocabulary for every training sentence. This vocabulary consists of all target side words of phrase pairs matching a source phrase in the source part of the training sentence. Then we use all sentence pairs where e j is in the target vocabulary but not in the target sentences as negative examples. This has shown to have a postive influence on the translation quality [3] and also reduces training time. Continuous Space Language Model In recent years, different approaches to integrate a continuous space models have shown significant improvements in the translation quality of machine translation systems, e.g. [15] . Since the long training time is the main disadvantage of this model, we only trained it on the small, but very domainrelevant TED corpus. In contrast to most other approaches, we did not use a feed-forward neural network, but used a Restricted Bolzmann Machine (RBM). The main advantage of this approach is that we can calculate the free energy of the model, which is proportional to the language model probability, very fast. Therefore, we are able to use the RBM-based language model during decoding and not only in the rescoring phase. The model is described in detail in [16] . The RBM used for the language model consists of two layers, which are fully connected. In the input layer, for every word position there are as many nodes as words in the vocabulary. Since we used an 8-gram language model, there are 8 word positions in the input layer. These nodes are connected to the 32 hidden units in the hidden layer. During decoding, we calculate the free energy of the RBM for a given n-gram. The product of this values is then used as an additional feature in the log-linear model of the decoder. Postprocessing for Agreement Correction The agreement in gender and number is one of the challenging problems encountered when translating from English into a morphologically richer language such as French. Consequently, a special postprocessing was designed in order to remedy the case where disagreements between nouns and related surrounding words exist. This post-processing is based on the POS tags generated by LIA tagger 3 . In order to improve the agreement features, several post-processing heuristics are applied on a sentence basis, which include the correction of the grammatical number and gender of adjective, article, possessive determiner, forms of quelque and past participles based on their corresponding nouns. In order to minimize spurious assignments when finding instances of these parts of speech related to a specific noun, strict heuristics are used: Adjectives must appear straight before or after the noun. Articles, possessive determiners and forms of quelque have to directly precede nouns or have at most one adjective in between. Past participles must stand after (possibly reflexive) inflected forms of être that immediately follow nouns. Results In this section, we present a summary of our experiments for all tasks we have carried out for the IWSLT 2012 evaluation. It includes the official systems for the MT and SLT translation tasks and additional systems for other language pairs: English-German, English-Chinese and English-Arabic translations. All the reported scores are the case-sensitive BLEU, and calculated based on the provided Dev and Test sets. MT Task Table 1 summarises how our MT system evolved. The baseline translation model was trained on EPPS, TED, NC, and Giga corpora. This big model was adapted with a smaller one trained on TED data only as described in Section 5. The language model is a log-linear combination of three language models trained on different data sets: the French side of the EPPS, TED, and NC corpora, the provided monolingual news data (Monolingual EPPS, NC and News Shuffled), and a smaller in-domain language model trained on TED data. The reordering in this system was handeled as a preprocessing step using POS-based rules as described in Section 4. The result of this setting was 28.5 BLEU points on Dev and 31.73 on Test. The performance could be improved by around 0.4 on Dev and 0.2 on Test by using a bilingual language model (details about bilingual language model computation can be found in [17] ). An additional 0.2 on both Dev and test could be gained by using a cluster language model where the clusters were trained on the in-domain TED data. After that, changing the adaptation strategy by the union selection discussed in Section 5 shows slight improvement of 0.1 on both Dev and Test. The effect of the DWL trained on only the TED corpus was rather dissimilar on Dev and Test. While it slightly improved the score on Dev (0.1) it has a much greater effect on Test (0.5). Further small improvement could be observed by using a continuous space language model: around 0.09 on both Dev and Test. Finally, by using the POS-based post-processing correction of the agreement on the target side the score on Test could be improved by an additional 0.06, resulting in 32.84 BLEU points on Test. We submitted the translations of Test2011 and Test2012 generated by this final system as primary; the translations generated by the second best system (same as the final but without agreement corrections) as contrastive. The baseline system of the speech translation task used almost the same configuration as the one for the MT task, for which the POS-based reordering and the adaptation for both translation and language model with TED data were added to the baseline. The special processing we have done for SLT task lie in the following aspects. System In order to simplify building the system, we did not retrain a new alignment for the SLT task, but modify the phrase tables from the MT task to make it suitable for the SLT task. Casing information and punctuation except periods has been removed from the source side of the phrase table. Then we feed this new phrase table with possibly duplicate phrase pairs into the SLT system and let the decoder select the best ones for a translation. For the purpose of comparison, we also rebuild a whole new SLT system, in which the alignment, the phrase table and all other models are newly generated with the training data without punctuation and casing information. However, the newly trained system is not better than the MT system with the modified phrase table. The experimental results are presented in Table 2 . large-retrain-PT are with the newly trained phrase table on the same corpora. large-modify-PT is the system with the modified phrase table trained on bilingual corpora TED, NC, EPPS and Giga corpus. We can see that the completely retraining the system does not improve the result. It is very surprising that the retrained system hurts the result much. One possible explanation could be punctuations are very help to generate good alignments. In order to know the reasons more clearly, more experiments should be done in the future. Another difference to the MT system is the the data used to build translation model does not include the Giga corpus. It includes only TED, NC and EPPS, since including the Giga corpus could not improve the translation results in the SLT task, as it does in the MT task. The intermediate experiments of comparing these two training data sets are shown in Table 2 . small-modify-PT is the system trained only on TED, NC and EPPS. The systems trained on TED, NC, EPPS and Giga are called large. Our SLT system is optimized on the modified Dev text data by removing the punctuation except periods and lowercasing. And we have tested the system both on modified text test data which is with the same processing as the Dev text data and on the ASR output of the test data. Table 3 presents the results optimized on modified Text and ASR output, respectively. The two columns marked with Test(ASR) are comparable scores. There is no convinced evidence that on which condition the optimization is better. In the settings of \"Baseline\", \"Adaptation\" and \"Bilingual LM\" optimizing on ASR output gets better results. After applying all models, the system optimized on the modified text data wins about 0.5 BLEU points. Considering the final result after adding all models is better and the test data from modified Text if more reliable than the ASR output, we have chosen the system optimized on the modified text data as our primary system. We present our system for the SLT task step by step in Table 3 . The bilingual language model was trained on the EPPS corpus and all other available parallel data, whose punctuation marks on the source side are all removed. The cluster language model is trained on the TED corpus, where the words are classified into 50 classes. The DWL model is also trained on the TED corpus, but the punctuation and casing information have been removed from the source side of the training data. System Compared to the baseline the SLT system has improved about 1.1 BLEU on both text and ASR test data by adding all the models. The largest gain is about 0.5 by adding the cluster-based language model. The domain adaptation model has improved all scores on Dev, text Test and ASR Test. It especially improves the text Test by 0.5 BLEU. The bilingual language model does not seem to contribute much to the results, except a little improvement of 0.2 on the ASR test data. Then we add the DWL model which also improves the test data by about 0.2 BLEU points. Finally we have carried out the morphology agreement correction as described in Section 9, which improves around 0.1 on the test data. This system was the system we used to translate the SLT evaluation set for our submission. We have submitted one primary system and three contrastive systems. The primary system is the translation of the ASR output system1 with all models presented in Table 3 . And the contrastive systems are the translations of the ASR outputs system1 -system3 excluding the Agreement Correction model. Additional Language Pairs English-German Several experiments were conducted for the English-German MT track on the TED corpus. They are summarized in Table 4. The baseline system is essentially a phrase-based translation system with some preprocessing steps on both source and target sides. Adapting huge parallel data from EPPS and NC to TED translation model helps us gain 0.71 BLEU scores on the test set. Short-range reordering based on POS information yields reasonable improvements on both development and test sets by about 0.5 BLEU points. In the language modeling aspect, different factors were experimented with, and 4-gram POS language model using RF-Tagger 4 slightly improves our system over the development set by 0.22 BLEU points but considerably shows its impact on test set with an improvement of 1 BLEU point. We approach our best system by adding a 9-gram cluster-based language model where the German side corpus is grouped into 50 classes, yielding 22.61 and 22.93 BLEU points on development and test sets, respectively. In this English-German translation system, we have also tried some other models such as using DWL, long-range reordering, bilingual language model as well as external monolingual language models but we do not gain noticeable improvements. Moreover, some experiments on tree-based reordering, which we believe helpful in this language pair, has been reserved for further considerations due to the limited time. English-Chinese With the bilingual data released by the TED Task of IWSLT 2012 we have developed an English-Chinese translation system. As it is an initial system for this new translation direction, we have made the main effort on data processing and preprocessing. There are three corpora that could be used: the TED bilingual sentence-aligned corpus, the UN bilingual document-aligned corpus and the monolingual Google Ngrams corpus. In our system we have used the TED corpus to train the translation model and trained a language model on TED, UN and Google Ngrams. In addition we classify the Google Ngram corpus with its year information, such as google1980 is the ngrams from 1980-1989, and train a language model separately on each class. Our experience has shown that google1980 has contributed the most to the improvement, even more than the whole Google Ngram corpus. In constrast to European languages, there are no spaces between Chinese words. Therefore, in the preprocessing of English-Chinese translation we need to decide on whether to segment Chinese into words, or to segment it into characters. We have tried both in our experiments. For the Chi-nese word segmentation we have made use of the Stanford Chinese word segmenter 5 . For the Chinese character segmentation we have simply inserted a space between neighbor Chinese characters. Then we have trained two systems: one based on Chinese words, the other based on Chinese characters. Table 5 shows the results from the two systems. Since the evaluation scores on Chinese words (Test(Word)) and on Chinese character (Test(Cha.)) are not comparable to each other, we segment the translation hypothesis on words into Chinese characters. Then the scores at the two columns Test(Cha.) are comparable. We can see that the system trained on characters is usually better than the system on words. In Table 5 we present the steps which achieve improvement. The baseline system is trained only on the TED corpus (both for translation model and language model). By adding all possible language models and a reordering model, the BLEU score on test data has gained 0.2 points in total. Most improvements come from the larger language model. It seems that the current reordering model does not work quite well for the English-Chinese translation. Further analysis and work need to be done on the reordering model. The other models that we have tried, but have not given improvement to the system, include sentence-aligned extraction from the UN corpus and long-range reordering as described in [18] . System English-Arabic The parallel data provided for this direction was from TED and UN. As for the English-Chinese direction (presented in Section 10.3.2), greater effort was devoted to the data preprocessing. The preprocessing for the English side is identical to the one used in the English-French system of the MT Task. Some of these preprocessing operations, such as long pair removal, were also applied to the Arabic side. In addition to that, the Arabic side was further orthographically transliterated using Buckwalter transliteration [19] . Tokenization and POS tagging were performed by the AMIRA toolkit [20] . The resulting translation is converted back to Arabic scripting before evaluation. Table 6 presents some initial experiments for the English-Arabic pair. The baseline system uses only TED data for translation and language modeling. This gave a score of 13.12 on Dev and 8.05 on Test. This system was remarkably enhanced by introducing the short range reordering rules. The scores were improved by about 0.3 on Dev and 0.2 on Test. Adding monolingual data from the UN corpus had a great impact on the score on Dev (improved by 0.6), whereas it has a much lower effect on Test (improves by 0.1 only). In this last setting, three language models were log-linearly combined: one trained on TED data, one trained on UN data, and another one trained on both. Since the UN corpus was provided as raw data (no sentence alignment was performed before), we selected a sub-corpus of documents consisting of exactly the same number of sentences. This resulted in around 500K additional parallel sentences. The line SubUN parallel in Table 6 shows that these data had almost no effect on the system's performance. It increased the score on Dev by 0.02 and by 0.07 on Test. However, using the first translation model (trained on TED only) as indomain model to adapt the last setting shows slightly better improvements (around 0.1 on Dev and Test). Using a bilingual language model rather harmed the system on Dev by around -0.1 but improved the score on Test by 0.06. We choose to include this model because combined with the cluster language model it could improve our system by around 0.2 on Dev and Test wheras none of these models alone could outperform this score (some of these experiments are not reported here). System Conclusions In this paper, we presented the systems with which we participated in the TED tasks in both speech translation and text translation from English into French in the IWSLT 2012 Evaluation campaign. Our phrase-based machine translation system was extended with different models. For the official language pair, even though we were authorized to use the UN parallel corpus and the monolingual Google Books Ngrams, these data had always a negative impact on our system's quality. More experiments should be carried out to extract some useful parts of these large data. The successful application of different supplementary models trained exclusively on TED data (cluster language model, DWL, and continuous space language model) shows the usefulness and importance of in-domain data for such tasks, regardless of their small size. The large amount of data used to train the different models integrated in our statistical system could not compensate for the ambiguity of translating into a morphologically richer language. Therefore, applying very simple and limited heuristics based on the target language grammar gave small but consistent improvments using the POS-based agreement correction. We also presented experiments with several additional pairs. Namely, from English into one of the languages German, Chinese, or Arabic. The use of additional bilingual corpora on adapting translation models as well as more complicated features from different language models led to expected performance in the English-German translation system. The effects of other techniques, e.g. long-range reordering or discriminative word alignment (DWA), were less obvious, mainly coming from the characteristics of the TED data. In case of English-Chinese, we have found that the system based on Chinese characters works better than the system based on Chinese words. The BLEU score calculated on Chinese characters and Chinese words are also different: the BLEU score on character is about 17 while evaluation on the words the score is around 10. In addition we found that the current reordering model does not help much on this language pair. Further work needs to be done in this field in the future. Due to the limited amount of data, the English-Arabic system performed relatively poorly. Furthermore, it showed eventual discrepency between Dev and Test data. Here again, as mentioned before, the UN data were not helpful. Acknowledgements This work was partly achieved as part of the Quaero Programme, funded by OSEO, French State agency for innovation. The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n • 287658.",
         "237558772",
         "607d0629f6cca1debb7285b3d92dbe3f042f958b",
         "1",
         "https://aclanthology.org/2012.iwslt-evaluation.3",
         null,
         "Hong Kong, Table of contents",
         "2012",
         "December 6-7",
         "Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign",
         "Mediani, Mohammed  and\nZhang, Yuqi  and\nHa, Thanh-Le  and\nNiehues, Jan  and\nCho, Eunach  and\nHerrmann, Teresa  and\nK{\\\"a}rgel, Rainer  and\nWaibel, Alexander",
         "The {KIT} translation systems for {IWSLT} 2012",
         "38--45",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "mediani-etal-2012-kit",
         null,
         null
        ],
        [
         "36",
         "2012.iwslt-evaluation.5",
         "This paper describes the NAIST statistical machine translation system for the IWSLT2012 Evaluation Campaign. We participated in all TED Talk tasks, for a total of 11 languagepairs. For all tasks, we use the Moses phrase-based decoder and its experiment management system as a common base for building translation systems. The focus of our work is on performing a comprehensive comparison of a multitude of existing techniques for the TED task, exploring issues such as out-of-domain data filtering, minimum Bayes risk decoding, MERT vs. PRO tuning, word alignment combination, and morphology.",
         "This paper describes the NAIST statistical machine translation system for the IWSLT2012 Evaluation Campaign. We participated in all TED Talk tasks, for a total of 11 languagepairs. For all tasks, we use the Moses phrase-based decoder and its experiment management system as a common base for building translation systems. The focus of our work is on performing a comprehensive comparison of a multitude of existing techniques for the TED task, exploring issues such as out-of-domain data filtering, minimum Bayes risk decoding, MERT vs. PRO tuning, word alignment combination, and morphology. Introduction This paper describes the NAIST participation in the IWSLT 2012 evaluation campaign [1] . We participated in all 11 TED tasks, dividing our efforts in half between the official English-French track and the 10 other unofficial Foreign-English tracks. For all tracks we used the Moses decoder [2] and its experiment management system to run a large number of experiments with different settings over many language pairs. For the English-French system we experimented with a number of techniques, settling on a combination that provided significant accuracy improvements without introducing unnecessary complexity into the system. In the end, we chose a four-pronged approach consisting of using the web data with filtering to remove noisy sentences, phrase table smoothing, language model interpolation, and minimum Bayes risk decoding. This led to a score of 31.81 BLEU on the tst2010 data set, a significant increase over 29.75 BLEU of a comparable system without these improvements. In Section 2 we describe each of the methods in more detail and examine their contribution to the accuracy of the system. For reference purposes, in Section 3, we also present additional experiments that gave negative results, which were not included in our official submission. For the 10 translation tasks into English, we focused on techniques that could be used widely across all languages. In particular, we experimented with unsupervised approaches to handling source-side morphology, minimum Bayes risk decoding, and large language models. In the end, most of our systems used a combination of unsupervised morphol- ogy processing and large language models, which resulted in an average gain of 1.18 BLEU points over all languages. Section 4 describes these results in further detail. English-French System The NAIST English-French translation system for IWSLT 2012 was based on phrase-based statistical machine translation [3] using the Moses decoder [2] and its corresponding training regimen. Overall, we made four enhancements over the standard Moses setup to improve the translation accuracy: Large-scale Data with Filtering: In order to use the large, but noisy parallel training data in the English-French Giga Corpus, we implemented a technique to filter out noisy translated text. Phrase Table Smoothing: We performed phrase table smoothing to improve the probability estimates of low-frequency phrases. Language Model Interpolation: In order to adapt to the domain of the task, we interpolated language models trained using text from several domains. Minimum Bayes-Risk Decoding: We used lattice-based minimum Bayes risk decoding to select hypotheses that are supported by other hypotheses in the n-best list, and calibrated the probability distribution to further improve performance. We demonstrate our results (in BLEU score) before and after these techniques are added in Table 1 . It can be seen that the combination of these 4 improvements leads to a 2.06 point gain in BLEU score on tst2010 over the baseline system. We will explain each of the techniques in detail as follows.  The number of words in each corpus. Corpus Data The first step of building our system was preparing the data. Table 2 shows the size and genre of each of the corpora available for the task. From these corpora, we used TED, NC, EuroParl, UN, and Giga for training the language model, and TED, NC, EuroParl, and filtered Giga (explained below) for training the translation model. 1 Tuning was performed on dev2010, and testing was performed on tst2010. In particular, the English-French Giga-word corpus is from the web and thus covers a wide variety of diverse topics, making it a strong ally for the construction of a general domain machine translation system. However, as the sentences were automatically extracted, they contain a significant number of errors where the content of the parallel sentences actually do not match, or only match partially. In order to filter out some of this noise, we re-implemented a variant of the sentence filtering method of [4] . The method works by using a clean corpus to train a classifier that can detect mis-aligned sentences. Because the clean corpus only contains correctly aligned sentences, we create pseudo-negative examples by traversing the corpus and randomly swapping two consecutive sentences with some set probability. These swapped sentences are labeled as \"negative,\" and the remainder of the unswapped samples are labeled as positive. In this application, the feature set chosen for the classifier must satisfy two desiderata. First, as with all machine learning applications, the features must be sufficient to discriminate between the classes that we are interested in: properly or improperly aligned sentences. Second, as our training data (a clean corpus) and testing data (a noisy corpus) will necessarily be drawn from different domains, we would like to use a small, highly generalizable feature set that will work on both domains. In order to achieve both of these objectives, we take hints from [4] and [5] to define the following features, where f J 1 and e I 1 are the source and target sentences, and J and I are their respective lengths: Length Ratio features capture the fact that properly aligned sentences should be approximately the same length. Two continuous features max(J, I)/min(J, I), J/I, 1 We also attempted to use the UN corpus for training the translation model, but found that it provided no gain, likely because of the specialized writing style of UN documents. and three indicator features J > I, I > J, I = J. Model One Probability features capture the fact that an unsupervised alignment model (in this case, the efficiently calculable IBM Model One [6] ) should assign higher probability to well-aligned sentences. In this category, we use two continuous features log P M 1 (e I 1 |f J 1 ) and log P M 1 (f J 1 |e I 1 ). Alignment features use Viterbi word alignments and capture certain patterns that should occur in properly aligned sentences. Word alignments are calculated using IBM Model One, and symmetrized using the \"intersection\" criterion [7] . If the number of aligned words is K, our features include aligned word ratio K/min(I, J), total number of aligned words K, number of alignments that are monotonic, monotonic alignment ratio, and the average length of gaps between words (similar to \"distortion\" used in phrasebased MT [3] ). Same Word features count the number of times that a word of length n is exactly equal to a word in the opposite sentence. This is useful for noticing when proper names, numbers, or words with a shared linguistic origin occur in both sentences. In our system we use separate features for n = 1, n = 2, n = 3, and n ≥ 4. To train the non-parallel sentence identifier, we use data from the TED, NC, and EuroParl corpora swapping sentences with a probability of 0.3 to create pseudo-negative examples. We use this as training data for a support vector machine (SVM) classifier, which we train using LIBLINEAR [8] . In order to get an estimate of the accuracy of sentence filtering, we perform 8-fold cross validation on the training data, and achieve a classification accuracy of 98.0%. 2  Next, we run the trained classifier on the entirety of the Giga corpus and remove the examples labeled as nonparallel. As a result of filtering with the classifier, a total of 485M English and 565M French words remained, a total of 84.3% of the original corpus. Finally, using no Giga data, the unfiltered Giga data, and the filtered Giga data (in addition to all other data sets), we measured the final accuracy of the translation system. results are shown in Table 3 . As a result, we can see that using the data from the Giga corpus has a positive effect on the results, but filtering does not have a clear significant effect on the results. Phrase Table Smoothing We also performed experiments that used smoothing of the statistics used in calculating translation model probabilities [9] . The motivation behind this method is that the statistics used to train the phrase table are generally sparse, and tend to over-estimate the probabilities of rare events. In the submitted system we used Good-Turing smoothing for the phrase table probabilities. Results comparing a system with smoothing and without smoothing can be found in Figure 4 . It can be seen that Good-Turing smoothing of the phrase table improves results by a significant amount. Language Model Interpolation One of the characteristics of the IWSLT TED task is that, as shown in Table 2 , we have several heterogeneous corpora. In addition, the in-domain TED data is relatively small, so it can be expected that we will benefit from using data outside of the TED domain. In order to effectively utilize out-ofdomain data in language modeling, we build one language model for each domain and interpolate the language models to minimize perplexity on the TED dev2010 set using the method described by [10] and implemented in the SRILM toolkit [11] . To measure the effectiveness of this technique, we also measure the accuracy without any data other than TED, and when the data from all domains was simply concatenated together for LM learning. The results can be found in Table 5 . We can see that adding the larger non-TED data to the language model is essential, and using linear interpolation to adjust the language model weights can also provide large further gains. Minimum Bayes Risk Decoding Finally, we experimented with improved decoding strategies for translation, particularly using minimum Bayes risk decoding (MBR, [12] ). In normal translation, the decoder attempts to simply find the answer with the highest probability among the translation candidates Ê = argmax E P (E|F ) (1) in a process called Viterbi decoding. As an alternative to this, MBR attempts to find the hypothesis that minimizes risk Ê = argmin E E ∈E P (E |F )L(E , E) (2) considering the posterior probability P (E |F ) of hypotheses E in the space of all possible hypotheses E, as well as a loss L(E , E) which determines how bad a translation E is if the true translation is E . In this work (as with most others on MBR in MT) we use one minus sentence-wise BLEU+1 score [13] as our loss function L(E , E) = 1 − BLEU+1(E , E). (3) In initial research on MBR, the space of possible hypotheses E was defined as the n-best list output by the decoder. This was further expanded by [14] , who defined MBR over lattices. We tested both of these approaches (as implemented in the Moses decoder). Finally, one fine point about MBR is that it requires a good estimate of the probability P (E |F ) of hypotheses. In the discriminative training framework of [15] , which is used in most modern SMT systems, scores of machine translation hypotheses are generally defined as a log-linear combination of feature functions such as language model or translation model probabilities P (E |F ) = 1 Z e i wiφi(E ,F ) (4) where φ i indicates feature functions such as the language model, translation model, and reordering model log probabilities, w i is the weight measuring the relative importance of this feature, and Z is a partition function that ensures that the probabilities add to 1. Choosing the weights w i for each feature such that the answer with highest probability Ê = argmax E P (E|F ) (5) is the best possible translation is a process called \"tuning,\" and essential to modern SMT systems. However, in most tuning methods, including the standard minimum error rate training [16] that was used in the proposed system, while the relative weight of each feature w i is adjusted, the overall sum of the weights i w i is generally set fixed at 1. While this is not a problem when finding the highest probability hypothesis in 5, it will affect the probability estimates larger s assigning a larger probability to the most probable hypothesis, and a smaller s spreading the probability mass more evenly across all hypotheses. In order to improve the calibration of our probability estimates, and thus improve the performance of MBR, we introduce an addition scaling factor λ into the calculation of our probability P (E |F ) = 1 Z e λ i wiφi(E ,F . ( 6 ) Using this lambda, we tried every value in 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, and 10.0, and finally chose λ = 5.0, which gave the best performance on tst2010. The final results of our system with Viterbi decoding (no MBR), regular MBR over n-best lists, and lattice MBR with the scaling factors of 1 and 5, are shown in Table 6 . It can be seen that both MBR and lattice-based MBR give small improvements over the baseline without tuning λ, while tuning λ gives a large improvement. 3 The reason why MBR reduces the accuracy on dev2010 is because dev2010 was used in tuning the parameters during MERT, so the one-best answers tend to be better on average than they would be on a held-out test set. Additional Results on English-French This section presents additional results obtained on the English-French track. The results here, for the most part, did not obtain worthwhile BLEU improvements in preliminary experiments, so we did not include them in the official system as described in Section 2. Although the systems reported in this section use the same dev and test set as that of Section 2, the training conditions and system configurations have slight differences, so the results should not be directly compared. We include these (negative) results for reference purposes, in order to aid understanding of the English-French TED task. Exploiting Out-of-domain Data We experimented with the simplest approach to exploiting out-of-domain bitext in translation models: data concatenation. This can be seen as adaptation at the earliest stage of the translation pipeline, and has achieved competitive results on TED En-Fr [17] . Three conditions were tried: (1) TED-only data, (2) TED + News (NC), (3) TED + NC + EuroParl (EP). Results are shown in Table 7 . First, we observe that adding data gives slight improvements (29.32 to 29.57). To analyze the potential for improvement, we also measured BLEU using \"CheatLM\" decoding [18] . \"CheatLM\" is an analysis technique for TM adaptation where the language model is trained on the reference; this gives a optimistic estimate on what can be achieved by the translation model, if other components are tuned almost perfectly. Here we see that TED+NC+EP (59.93 BLEU) can achieve large improvements over TEDonly (55.10 BLEU), indicating the potential value of out-of-domain bitext. However, note that the corresponding OOV rate reduction is relatively small (1.2% to 0.52%). We hypothesize that out-ofdomain probably is not helping because of improved word coverage, but rather because of improved word alignment estimation. In any case, the improvements are slight so we do not attempt to draw any further conclusions. The \"standard\" and \"CheatLM\" columns show the BLEU scores on tst2012, using standard Moses decoding and \"CheatLM\" decoding. The column \"force\" shows the percentage of tst2010 sentences that can be translated into the reference using forced decoding. OOV indicates the token out-of-vocabulary rate. Data Word Alignment & Phrase Table Combination We investigated different alignment tools and ways to combine them, as shown in Table 8 . Observations are as follows: • GIZA++ and BerkeleyAligner achieve similar BLEU on this task. • Concatenating GIZA++ and BerkeleyAligner word alignment results, prior to phrase extraction, achieves a small boost (29.57 to 29.89 BLEU). • We also experimented with pilaign [19] , a Bayesian phrasal alignment toolkit. This tool directly extracts phrases without resorting to the preliminary step of word alignments, and achieves extremely compact phrase table sizes (0.8M entries) without significantly sacrificing BLEU (29.24). • Combining the GIZA++ and pialign phrase tables by Moses' multiple decoding paths feature did not improve results. Overall, we did not find much differ-ence among these various approaches so we used the standard GIZA++ tool chain in the official submission. Tool Lexical Reordering Models Several reordering models available in the Moses decoder were tried. In general, we found the full \"msd-bidir-fe\" option to perform best, despite the small number of word order differences between English and French. Results are shown in Table 9 . Reordering model BLEU msd-bidir-fe 29.57 msd-bidir-f 29.43 monotonicity-bidir-fe 29.29 msd-backward-fe 29.22 distance 28.99 msd-bidir-fe-collapse 28.86 Table 9 : Comparison of Reordering models on tst2010. MERT vs. PRO tuning We compared two tuning methods: MERT and PRO [20] . We used the implementations distributed with Moses. For both MERT and PRO, we set the size of k-best list to k = 100, used 14 standard features, and removed duplicates in k-best lists when merging previously generated k-best lists. We ran MERT in multi-threaded setting until convergence. Since the number of random restarts in MERT greatly affects on the translation accuracy [21] , we tried various number of random restarts for 1, 10, 20, and 50. 4 For PRO, we used MegaM 5 as a binary classifier with the default setting. We ran PRO for 25 iterations. We tried two kinds of PRO: [20] interpolated the weights with previously learned weights to improve the stability (henceforth \"PRO-interpolated\") 6 , and 4 Currently, Moses's default setting is 20. 5 http://www.cs.utah.edu/ ˜hal/megam/ 6 We set the same interpolation coefficient value of 0.1 as [20] noted. the version that do not use such a interpolation (henceforth \"PRO-basic\"). We first investigate the effect of the number of random restarts in MERT on BLEU score and run-time for each iteration. Table 10 shows the result. As the number of random restarts increases, BLEU score improves. However, the run-time increases as well. We used 20 random restarts to compare to PRO. Table 11 shows the results of MERT and PRO. As can be seen in Figure 11 , MERT exceeds PRO-basic by 1.3 points and PRO-interpolated by 1.18 points. As a result, we used MERT for tuning in Sections 2 and 4. Systems for Translation into English We participated in the translation of all 10 additional language-pairs of the TED Talk track. The source languages are Arabic (ar), German (de), Dutch (nl), Polish (pl), Brazilian-Portuguese (pt), Romanian (ro), Russian (ru), Slovak (sk), Turkish (tr), and Chinese (zh). The target language for all tasks is English (en). Since all tasks translate into the same language, we are able to share the language model as well as many of the configurations for the Experimental Management System (EMS). This setup provides an invaluable chance to compare the same techniques across structurally-different languages, and is the focus of our work. Rather than optimizing for specific languages, we concentrate on building common systems under the same EMS framework and on comparing the performance of existing techniques cross-lingually. It is interesting to note that the 10 language-pairs cover a diverse range of linguistic phenomenon. In terms of historical relationships, the Italic family (pt,ro) and Germanic family (de, nl) are expected to be closer to the target language of English. The Slavic family (pl,ru,sk), Arabic, and Turkish languages exhibit rich morphology (fusional, non-catenative, or agglutinative). Additionally, the Germanic family may show word order differences (V2 and SOV) and Chinese requires word segmentation. Experiments Table 12 summarizes all the results (BLEU scores) for translation into English. In all language pairs, the baseline consists of a standard phrase-based Moses system (GIZA++ alignment, grow-diag-final-and heuristic, lexical ordering, 4-gram language model) trained on the TED Talks portion of the training data. MERT tuning is performed on the \"dev2010\" portion of the data and Table 12 shows test results on \"tst2010.\" 7 While it is not possible to directly compare BLEU across languages, we do observe that the Italic and Germanic languages fare better on this TED task (> 25 BLEU), while Chinese, Turkish, and the Slavic languages perform poorly at 10 − 17 BLEU. We then proceeded to improve on these baseline results. First, adding additional out-of-domain data (nc=News Commentary, ep=Europarl, un=UN Multitext) to the language model increased results uniformly for all language pairs (line (b) of Table 12 ). We used an interpolated language model, trained in the same fashion as in our English-French system. Next, we tried two strategies for handling rich morphology in the input. The \"CompoundSplit\" program in the Moses package was developed for languages with extensive noun compounding, e.g. German, and breaks apart words if sub-parts are seen in the training data over a certain frequency [22] . The alternate \"Morfessor\" program [23] is an unsupervised morphological analyzer based on the Minimum Description Length principle -it tries to find the the smallest set of morphemes that parsimoniously cover the training set. Morfessor is expected to segment more aggressively than CompoundSplit, especially because it can find both bound and free morphemes. However, we empirically found that Morfessor segments too aggressively for unknown words (i.e. each character becomes a morpheme), so we do not segment OOV words in dev/test. 8 The results in line (c) of Table 12 shows that German benefit most from CompoundSplit, while Arabic, Russian, and Turkish benefit from Morfessor. The remaining languages perform approximately equal or slightly better with these morphology enhancements, so in further experiments we keep the morphology pre-processing (de & ro uses CompoundSplit; others use Morfessor). In line (d) of Table 12 , we further added the Giga corpus to the interpolated language model. For some languages, this gave a large improvement (ar, de, pl, sk), while for other languages the results remain similar. Some of these results represent our official submission. In line (e), adding Lattice MBR decoding uniformly degraded results, so we chose not to include it. This is in contrast with our English-French results. We suspect that in this case uniformity of the training data and lack of diversity in the n-best list may have damaged MBR; the resulting translations appear similar in structure, but many have extraneous articles and determiners, which hurts BLEU. It should also be noted that unlike English-French, we did not calibrate the probability distribution by adjusting λ, which might also had a significant effect on the results. Finally in line (f), we added additional outof-domain bitext for Translation Model training. This only helped slightly for pl and tr, while degrading other language pairs: we conclude that more advanced TM adaptation methods is necessary, and simply concatenating the bitext does not help. Finally, we note that our submitted systems for each language achieve a 0.7-2.5 BLEU improvement over the respective baselines. We also achieve slight improvements in METEOR, despite not tuning for it. While the feature that helped most depends on language, we observe that morphological pre-processing and larger language models are generally worthwhile efforts. Conclusion This paper described our experiments with a number of existing machine translation techniques for the IWSLT 2012 TED task. Some of these techniques, such as minimum Bayes risk decoding with calibrated probabilities, language model interpolation, unsupervised morphology processing, translation model smoothing, and the use of large data proved to be effective. We also found that a number of techniques, including tuning using PRO, alignment combination, and data filtering had less of a positive effect. SYSTEM",
         "2966935",
         "a0aa860836de4860450d4a56350f61feaef8a296",
         "3",
         "https://aclanthology.org/2012.iwslt-evaluation.5",
         null,
         "Hong Kong, Table of contents",
         "2012",
         "December 6-7",
         "Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign",
         "Neubig, Graham  and\nDuh, Kevin  and\nOgushi, Masaya  and\nKano, Takamoto  and\nKiso, Tetsuo  and\nSakti, Sakriani  and\nToda, Tomoki  and\nNakamura, Satoshi",
         "The {NAIST} machine translation system for {IWSLT}2012",
         "54--60",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "neubig-etal-2012-naist",
         null,
         null
        ],
        [
         "37",
         "2012.iwslt-evaluation.6",
         "This paper reports on FBK's Machine Translation (MT) submissions at the IWSLT 2012 Evaluation on the TED talk translation tasks. We participated in the English-French and the Arabic-, Dutch-, German-, and Turkish-English translation tasks. Several improvements are reported over our last year baselines. In addition to using fill-up combinations of phrase-tables for domain adaptation, we explore the use of corpora filtering based on cross-entropy to produce concise and accurate translation and language models. We describe challenges encountered in under-resourced languages (Turkish) and language-specific preprocessing needs.",
         "This paper reports on FBK's Machine Translation (MT) submissions at the IWSLT 2012 Evaluation on the TED talk translation tasks. We participated in the English-French and the Arabic-, Dutch-, German-, and Turkish-English translation tasks. Several improvements are reported over our last year baselines. In addition to using fill-up combinations of phrase-tables for domain adaptation, we explore the use of corpora filtering based on cross-entropy to produce concise and accurate translation and language models. We describe challenges encountered in under-resourced languages (Turkish) and language-specific preprocessing needs. Introduction FBK's machine translation activities in the IWSLT 2012 Evaluation Campaign [1] focused on the speech recognition and translation of TED Talks 1 , a collection of public speeches on a variety of topics and with transcriptions available in multiple languages. In this paper, we discuss our involvement in the official Arabic-English and English-French Machine Translation tasks, as well as the auxillary German-English, Dutch-English, and Turkish-English Machine Translation tasks. We begin with an overview of the research procedure in common with all of language pair experiments in Section 2: namely, data filtering, phrase and reordering table fill-up, and mixture language modeling. In Section 4 we discuss our Arabic-English and Turkish-English MT systems. In Section 3 we discuss our English-French submissions. In Section 6 we discuss our German-and Dutch-English systems. Finally, in Section 8 we summarize our findings. TED Machine Translation Overview For all systems except for our Turkish-English system, we set up a standard phrase-based system using the Moses toolkit [2] . We construct a statistical log-linear model including a filled-up phrase translation and hierarchical reordering models [3, 4, 5] , a primary mixture target language model (LM), as well as distortion, word, and phrase penalties. The distortion limit is set to the default value of 6, except for 1 http://www.ted.com/talks Arabic-and Turkish-English (see respective sections). As proposed by [6] , statistically improbable phrase pairs are removed from our phrase tables. For each target language, we train 5-gram mixture language models from the available corpora, as described in Section 2.3. The language models are trained with IRSTLM [7] with improved Kneser-Ney smoothing and no pruning. Additional experiments on hybrid word/class language models are performed in the Arabic-English task. The weights of the log-linear combination are optimized via minimum error rate training (MERT) [8] . In the following sections, we discuss the data selection, phrase and reordering table fill-up, and mixture language modeling used by each of our systems. We follow the discussion with our language-specific submissions. Data selection Each out-of-domain corpus was domain-adapted by filtering aggressively using a cross-entropy difference scoring technique described by [9] on the target side and optimizing the perplexity against the (target language) TED training data by incrementally adding sentences. The idea of data selection is to find the subset of sentences within an out-of-domain corpus that better fits with a given in-domain corpus. Each sentence of the out-of-domain corpus is evaluated by comparing its likelihood (in terms of cross-entropy) to appear in the out-of-domain corpus against its likelihood to compare in the in-domain corpus. In order to decide how many sentences to keep, we build an out-ofdomain language model incrementally and measure its perplexity on the in-domain TED data. The two language models we compare are built from the same dictionary, namely the in-domain words occurring more than a specified frequency. All other words in the in-domain and out-of-domain corpora are taken as out-of-vocabulary words. For this kind of problem it is generally sufficient to work with 3-grams language models estimated on words occurring at least twice in the in-domain set. Figure 1 shows the effects of data selection on the four out-of-domain corpora used for language modeling in all of our foreign-to-English MT submissions. Three of the corpora are subcorpora drawn from seven available news text sources in the LDC English Gigaword (Fifth Edition) corpus. Phrase table fill-up As we did last year, we combine phrase tables via fill-up [10, 11] . Using the recommendations of [11] , we add k-1 binary provenance features for each of the k phrase tables to combine. Treating the TED phrase table as in-domain, we merge out-of-domain phrase pairs that do not appear in the in-domain TED table, along with their scores. Moreover, out-of-domain phrase pairs with more than four source tokens are pruned. The fill-up process is performed in a cascaded order, first filling in missing phrases from the corpora that are closest in domain to TED. Mixture language model adaptation After performing data selection and cross-entropy filtering on the provided monolingual corpora, we perform LM domain adaptation via mixture modeling [12] . For our foreign-to-English MT submissions, we construct a common 5-gram mixture LM consisting of TED data, a subset of corpora from the LDC Gigaword fifth edition corpus, and the WMT News Commentary. From the Gigaword corpus, we select the articles from the Los Angeles Times/Washington Post, New York Times, and Washington Post/Bloomberg subcorpora. After performing cross-entropy filtering on each subcorpus, we perform mixture model adaptation with the TED corpus as the in-domain background. French language model statistics are reported in Section 3.3. English-French More monolingual and parallel data were available in the English-French translation task. Several of the corpora were too large and noisy to use efficiency, which underscored the necessity of data selection and filtering. In the following sections we discuss the data selection, phrase and reordering table fill-up, and mixture language modeling approaches used for our English-French MT systems and report results on the official test sets. Data selection We perform data selection using the cross-entropy filtering technique described above, both for language and for translation modeling. In order to filter parallel corpora, we apply the cross-entropy filtering technique on the French (targetside) texts and prune the corresponding English segments. Table 2 provides statistics on the preprocessed monolingual and parallel corpora used by our systems, before and after filtering. In both monolingual and parallel corpora we observe over a 85% reduction in the number of words by filtering. Phrase table More parallel data was available in the English-French translation task than the other MT tracks. In particular, the Mul-tiUN and Giga French corpora were too large and noisy to use reliably for translation modeling without filtering. Table 2 shows that the size of these corpora were reduced by over 95% using cross-entropy filtering. We use the filtered TED, Europarl, MultiUN, and Giga French parallel corpora for translation model training. Our experiments from last year showed little improvement from using the parallel WMT News Commentary corpus. In order to reduce the size of the translation models and to stabilize MERT behavior, we independently train phrase and reordering tables on each corpus and experiment with several fill-up configurations with the TED as the in-domain corpus. Table 3 lists BLEU and TER evaluation results 2 on the IWSLT 2010 TED test set, three independent MERT runs for each fill-up combination. Each system uses the mixture LM described later in Section 3.3. In particular, we do not see any significant improvements filling up with using Europarl or MultiUN, but rather with the Giga French corpus. In order to improve the coverage of the TED and Giga fill-up models, we cascaded fill-up with Europarl and MultiUN respectively. While we do not observe significant improvement with the cascaded fill-up from Table 3 Language modeling In order to determine which monolingual data to use for language modeling, we trained 5-gram language models on each unfiltered corpus and evaluated their perplexity scores on the in-domain TED development data. From our experiments last year, the monolingual WMT News Commentary corpus yielded well-performing LMs. The Gigaword corpus consisted of articles from the Agence France-Presse (AFP) and Associated Press Worldstream (APW) newswires. Our perplexity analyses showed that APW did not model the TED domain well; thus, we opt to omit it. To our surprise, the French side of the parallel Giga French corpus modeled the TED domain well after filtering -even better than the TED training data! Rather than log-linearly combining four distinct LMs and optimizing four feature weights, we combine the LMs with mixture modeling and evaluate their cumulative effects on the IWSLT 2010 development set in Table 4 . After confirming that the four LMs in combination improve perplexity, we construct a 5-gram mixture model. Table 5 suggests that the mixture LM alone is responsible for a 2.7 BLEU improvement over a TED-only 5-gram baseline. 2 Evaluation results were performed with MultEval v0.3 [13] . Corpora Submitted runs Our primary (P) and constrastive (C) results are reported in Table 6 and are compared to a simple TED baseline (B), consisting of TED-only phrase and reordering tables. All systems use the mixture LM described in the previous section. Each system's feature weights are averaged over three MERT optimizations. The fill-up model with Europarl yielded higher BLEU and NIST scores on both the 2010 development and test sets; thus by providing additional phrase coverage we opted to submit it as our primary system. Our TED+Giga fill-up system served as our contrastive baseline. Each system performed similarly on the official test sets, though the MultiUN filled-up model was not consistent across the different test sets. Our primary system performed equally with our contrastive baseline on the 2011 test set in terms of BLEU, but performed slightly (though not significantly) worse in terms of NIST, while on the 2012 test set we observe a 0. Arabic-English The Arabic-English language pair is characterized by notable differences in morphological richness and word order. We follow last year's experience to deal with morphology and address word reordering by using an improved version of the distortion penalty that was proposed by [14] . In addition to that, we integrate a hybrid class language model [15] that proved to improve our system of last year. Preprocessing For Arabic we use our in-house tokenizer that also removes diacritics and normalizes special characters and digits. Then, segmentation is performed by the AMIRA toolkit [16] Phrase table While word alignment is obtained on the union of all available data, the translation model is built by filling up a TEDonly phrase table with a MultiUN-only phrase table. As previously said, out-of-domain (MultiUN) phrase pairs with more than four source words are filtered out. The lexicalized reordering table is obtained with the same procedure. Early distortion cost Moore and Quirk [14] proposed an improvement to the distortion penalty used in Moses, which consists in \"incorporating an estimate of the distortion penalty yet to be incurred into the estimated score for the portion of the source sentence remaining to be translated.\" The new distortion penalty has the same value as the usual one over a complete translation hypothesis (provided that the jump from the last translated word to the end of the sentence is taken into account). As a difference, though, it anticipates the gradual accumulation of the total distortion cost making partial translation hypotheses with the same number of covered words more comparable with one another. We have implemented this 'early distortion cost' option in the Moses platform and used it in our systems. As shown in Table 8 , increasing the distortion limit from the default value of 6 to 8 has normally a negative impact because standard distortion does not properly control long jumps. On the contrary, when early distortion cost is used, a slightly higher distortion limit is preferable, yield-ing an improvement of +0. Hybrid language modeling In addition to the mixture model, we use an in-domain hybrid word/class LM that was proposed by [15] to address style adaptation when out-of-domain data is likely to bias the system towards an unsuitable language style (e.g. news versus talks). Following the paper, we train a high order (10-gram) LM on TED data where infrequent words were mapped to their most likely Part-of-Speech tags, and frequent words to their lemma. We set the frequency threshold so that 25% of the tokens -corresponding to about 2% of the types -are replaced by part-of-speech (POS) tags. Adding this model to the log-linear combination yields a gain of +0.3 BLEU and +0.04 NIST (see Table 9 ). Submitted runs Turkish-English The additional training data provided for this language pair was limited to the South European Times news corpus. In our experiments we found that this data was not helpful for translation modeling and decided to use it only for word alignment 3 . A reason for this could be the size of this corpusonly slightly larger than the TED data -that is enough to bring noise into the system but not enough to improve its coverage in a significant way. We then focus on preprocessing techniques to address the agglutinative Turkish morphology and evaluate the performance of phrase-based against hierarchical systems. Morphological segmentation Turkish preprocessing involves supervised morphological analysis [17] and disambiguation [18] , followed by selective morpheme segmentation as described in [19] . We compare two of the segmentation schemes that were proposed and tested on the BTEC task by [19] and [20] : • 'MS6' deals only with nominal suffixes (case and possessive), • 'MS15' deals with nominal suffixes and verbal suffixes (copula, person subject, negation, ability, passive and causative suffixes). The latter segmentation scheme is more aggressive, which is good for model coverage but can make the translation harder (especially the reordering problem, due to the larger number of possible input permutations). To evaluate the actual importance of supervised methods, we also build a contrastive system using a fully data-driven segmentation approach proposed by [21] and implemented in the Morfessor Categories-MAP software. We train Morfessor on the TED training corpus, and obtain a unique segmentation of each word type into a sequence of morpheme-like units (morphs). As an intermediate solution between words and morphs -which are typically rather short -we concatenate the sequence of non-initial morphs to form so-called word endings 4 . In this way, each word can be segmented into at most two parts.  Turkish training data statistics in different segmentation settings are given in Table 11 , while the effect on translation quality is shown in Table 12 . Notice the very high distortion limit chosen because of the important order differences between English and Turkish, a head-final SOV language. In this set of experiments we use a 4-gram mixture LM trained on unfiltered data. The results show that supervised segmentation (MS15) can noticeably outperform the unsupervised one (Morfessor word endings), but they also show that the choice of a particular segmentation scheme is very important. In fact, the supervised MS6 scheme does no better than the unsupervised. We decide to use MS15 for the rest of the evaluation, however it is possible that the unsupervised approach may be improved by devising other ways to recombine the morphs. Translation model: phrase-based vs. hierarchical As we only use TED training data, no adaptation technique is required for translation modeling. Given the global and hierarchical nature of word reordering patterns in this language pair, we thought that a hierarchical translation system [23] could work better than a regular phrase-based one. We then construct a rule table with maximum rules span 15 and Good Turing score smoothing, and switch to chart decoding (all within the Moses platform). The hierarchical system strongly outperforms the phrasebased one, with a +1.7 BLEU and 0.25 NIST gain (see Table 13 ) proving the complexity of the word reordering problem in Turkish-English. Submitted runs We submitted two systems: the hierarchical as primary and the phrase-based with early distortion cost and a high distortion limit (15) as contrastive. Both of our official systems include a 6-gram mixture LM trained on the filtered data described in Section 2.1. System German-English Translating German compound words (also known as \"compounds\") is a challenge for Machine Translation: the first subsection focuses on the experiments we performed on compounds splitting. We subsequently report on the translation and language models used in our submissions and present our system results on the official test sets. Word splitting In order to choose the best splitter sub-system, we performed some preliminary experiments. We use the splitting tool provided in Moses (see [24] ), which is based on a trainable model. We test several splitter configurations with models trained on all the German data available for the MT track of the TED Task, but with different filtering techniques and parameter settings, inspired by [25] ). For the sake of efficiency, we perform the experiments on the TED corpora (namely the provided training and 2010 development and test sets). After applying a standard tokenization step, different groups of data sets are obtained, one for each splitting configuration. We conduct two sets of experiments; in the first we compute the perplexity and OOV-rate on the dev and test sets using the LM learned on the training set, while in the second we build SMT systems for each splitting configuration and evaluate their translations. It is worth noting that the splitters work only on the source language and do not affect the target language (English). Table 14 lists the outcomes of the first set of experiments: the normal splitter utilizes the default parameter setting of the tool, while in the aggressive splitter we change the parameters to allow decomposition into short words (minimum 2 characters). The best performance in terms of perplexity and OOV-rate reduction is exhibited by the aggressive splitter. There are no statistically significant differences among the translations provided by the three systems (unsplitted, normal-and aggressive-splitting). This can be explained mainly by the limited size of the training set. In the same  experiments performed with all the available German data, we observe a marginal but statistically significant improvement on translation scores when performing both normal and aggressive splitting. Phrase table For translation modeling we use the four provided data sets.  While word alignment is obtained on the union of all available data, the translation model is built by filling up a TED-only phrase table with two other phrase tables: the former obtained from WMT News Commentary v7 corpus and the latter from the union of MultiUN and Europarl v7 corpora. This partition has been chosen to maximize domain homogeneity in the three sub-corpora. The lexicalized reordering table is obtained with the same procedure. Submitted runs Table 16 presents results of our primary (P) and contrastive (C) systems on the IWSLT 2010, 2011 and 2012 TED test sets. Both systems use the English 5-gram mixture LM previously described in section 2.3 and differ only on the word splitting technique. Evaluation scores are rather close; the aggressive splitter appears to exhibit slightly better (although not statistically significant) performance. Dutch-English In the following sections we present the systems developed for the Dutch-English MT track of the TED task.  Word alignment is obtained on the concatenation of both corpora. The translation model is built by filling up the TEDonly phrase table with the out-of-domain Europarl phrase table. The same procedure is applied for the lexicalized reordering table. Submitted runs Table 18 presents results of our primary (P) and contrastive (C 1 and C 2 ) systems on the IWSLT 2010, 2011 and 2012 TED test sets. The three systems differ in the splitters (normal for P and C 1 , aggressive for C 2 ) and language models: all of them use the English mixture LM previously described in section 2.3, but differ in length (4-gram for P, 5-gram for C 1 , 6-gram for C 2 ). The evaluation scores do not highlight a single outperforming system. Conclusions We presented our submission runs to the IWSLT 2012 Evaluation Campaign for the TED MT tracks. Our MT systems benefited most from data filtering techniques and mixture language modeling. In particular, we observed significant BLEU improvements using mixture modeling over TEDonly baselines. We also took advantage of phrase and reordering table fill-up models for further domain adaptation that additionally compresses the size of the translation system. In Arabic-English, we used early distortion cost and incorporated a hybrid word/class language model to adapt to the style of talks, while for Germanic languages, we explored the effects of various compound splitting techniques. For Turkish-English, we compared several approaches to morphological segmentation and used a hierarchical SMT system. Acknowledgements This work was partially supported by TOSCA-MP project (IST-287532) and the EU-BRIDGE project (IST-287658), which are both funded by the European Commission under the Seventh Framework Programme for Research and Technological Development.",
         "759377",
         "b3472a5dd38cd2a40fb7c9acf207f35a6ea8c717",
         "11",
         "https://aclanthology.org/2012.iwslt-evaluation.6",
         null,
         "Hong Kong, Table of contents",
         "2012",
         "December 6-7",
         "Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign",
         "Ruiz, N.  and\nBisazza, A.  and\nCattoni, R.  and\nFederico, M.",
         "{FBK}{'}s machine translation systems for {IWSLT} 2012{'}s {TED} lectures",
         "61--68",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "ruiz-etal-2012-fbks",
         null,
         null
        ],
        [
         "38",
         "R13-1007",
         "Sentiment analysis is currently a very dynamic field in Computational Linguistics. Research herein has concentrated on the development of methods and resources for different types of texts and various languages. Nonetheless, the implementation of a multilingual system that is able to classify sentiment expressed in various languages has not been approached so far. The main challenge this paper addresses is sentiment analysis from tweets in a multilingual setting. We first build a simple sentiment analysis system for tweets in English. Subsequently, we translate the data from English to four other languages -Italian, Spanish, French and German -using a standard machine translation system. Further on, we manually correct the test data and create Gold Standards for each of the target languages. Finally, we test the performance of the sentiment analysis classifiers for the different languages concerned and show that the joint use of training data from multiple languages (especially those pertaining to the same family of languages) significantly improves the results of the sentiment classification.",
         "Sentiment analysis is currently a very dynamic field in Computational Linguistics. Research herein has concentrated on the development of methods and resources for different types of texts and various languages. Nonetheless, the implementation of a multilingual system that is able to classify sentiment expressed in various languages has not been approached so far. The main challenge this paper addresses is sentiment analysis from tweets in a multilingual setting. We first build a simple sentiment analysis system for tweets in English. Subsequently, we translate the data from English to four other languages -Italian, Spanish, French and German -using a standard machine translation system. Further on, we manually correct the test data and create Gold Standards for each of the target languages. Finally, we test the performance of the sentiment analysis classifiers for the different languages concerned and show that the joint use of training data from multiple languages (especially those pertaining to the same family of languages) significantly improves the results of the sentiment classification. Introduction Sentiment analysis is a task in Natural Language Processing whose aim is to automatically detect and classify sentiments in texts. Generally, the \"positive\", \"negative\" and \"neutral\" classes are considered, although other scales have also been used (e.g. from 1 to 5 \"stars\" -according to the reviewing systems put at the disposal of clients or users by amazon.com, booking.com, etc.; adding the \"very positive\" and \"very negative\" classes, scales from 1 to 10, etc.). In this article, we deal with the issue of sentiment analysis in tweets, in a multilingual setting. We employ machine translation -which was shown to be at a sufficiently high level of performance (Balahur and Turchi, 2012) -to obtain data in four languages. Our goal is to test if the use of multilingual data can help to improve sentiment classification in tweets (as shown to be the case in formal texts - (Banea et al., 2010) ) and if the joint use of data coming from similar languages or languages that are different in structure can influence on the final result. The main problem when designing automatic methods for the treatment of tweets is that they are highly informal texts, i.e. they contain slang, emoticons, repetitions of letters or punctuation signs, misspellings (done on purpose or due to writing them from mobile devices), entire words in capital letters, etc. In order to test our hypotheses, we first design a simple tweet sentiment analysis system for English, taking into account the specificity of expressions employed, but without using languagespecific text processing tools. The motivation is related to the fact that: a) such a distinction would require the use of language identifiers and would need the data from the different languages to be separated; b) We would like to apply the same techniques for as many languages as possible and for some of these languages, no freely-available language processing tools exist. We test this system on the SemEval 2013 Task 2 -Sentiment Analysis in Twitter (Wilson et al., 2013) -training data and test on the development data. The choice of this test set was motivated by the fact that it contains approximately 1000 tweets, being large enough to be able to draw relevant conclusions and at the same time small enough to allow manual correction of the translations, to eliminate incorrect translations being present in both training and test data. Subsequently, we employ the Google machine translation system 1 to translate the SemEval 2013 training and development tweets in Italian, Spanish, German and French. We manually correct the translated development data (which we use for testing, not for parameter tuning) to produce a reliable Gold Standard. Finally, we apply the same sentiment classification system to each of these languages and test the manner in which the combined datasets (from pairs of two languages, families of languages and all the languages together) perform. We conclude that the joint use of training data from different languages improves the classification of sentiment and that the use of training data from languages that are similar in structure helps to achieve statistically significant improvements over the results obtained on individual languages and all languages together. The remainder of this article is structured as follows: Section 2 gives an overview of the related work. In Section 3, we present the motivations and describe the contributions of this work. In the following section, we describe in detail the process followed to pre-process the tweets, build the classification models and obtain tweets for four other languages. In Section 5, we present the results obtained on different languages and combinations thereof. Finally, Section 6 summarizes the main findings of this work and sketches the lines for future work. Related Work The work described herein is related to the development of multilingual sentiment analysis systems and sentiment classification from tweets. Methods for Multilingual Sentiment Analysis In order to produce multilingual resources for subjectivity analysis, Banea et al. (Banea et al., 2008) apply bootstrapping to build a subjectivity lexicon for Romanian, starting with a set of 60 words which they translate and subsequently filter using a measure of similarity to the original words, based on Latent Semantic Analysis (LSA) (Deerwester et al., 1990) scores. Another approach to mapping subjectivity lexica to other languages is proposed by Wan (2009) , who uses co-training to classify un-annotated Chinese reviews using a corpus of annotated English reviews. (Kim et al., 2010) create a number of systems consisting of different subsystems, each classifying the subjectivity of texts in a different language. They translate a corpus annotated for subjectivity analysis (MPQA), the subjectivity clues (Opinion Finder) lexicon and re-train a Naive Bayes classifier that is implemented in the Opinion Finder system using the newly generated resources for all the languages considered. (Banea et al., 2010) translate the MPQA corpus into five other languages (some with a similar ethimology, others with a very different structure). Subsequently, they expand the feature space used in a Naive Bayes classifier using the same data translated to 2 or 3 other languages. Finally, (Steinberger et al., 2011a; Steinberger et al., 2011b) create sentiment dictionaries in other languages using a method called \"triangulation\". They translate the data, in parallel, from English and Spanish to other languages and obtain dictionaries from the intersection of these two translations. Sentiment Classification from Tweets One of the first studies on the classification of polarity in tweets was (Go et al., 2009) . The authors conducted a supervised classification study on tweets in English, using the emoticons (e.g. \":)\", \":(\", etc.) as markers of positive and negative tweets. (Read, 2005) employed this method to generate a corpus of positive tweets, with positive emoticons \":)\", and negative tweets with negative emoticons \":(\". Subsequently, they employ different supervised approaches (SVM, Naïve Bayes and Maximum Entropy) and various sets of features and conclude that the simple use of unigrams leads to good results, but it can be slightly improved by the combination of unigrams and bigrams. In the same line of thinking, (Pak and Paroubek, 2010) also generated a corpus of tweets for sentiment analysis, by selecting positive and negative tweets based on the presence of specific emoticons. Subsequently, they compare different supervised approaches with n-gram features and obtain the best results using Naïve Bayes with unigrams and part-of-speech tags. Another approach on sentiment analysis in tweet is that of (Zhang et al., 2011) . Here, the authors employ a hybrid approach, combining super-vised learning with the knowledge on sentimentbearing words, which they extract from the DAL sentiment dictionary (Whissell, 1989) . Their preprocessing stage includes the removal of retweets, translation of abbreviations into original terms and deleting of links, a tokenization process, and partof-speech tagging. They employ various supervised learning algorithms to classify tweets into positive and negative, using n-gram features with SVM and syntactic features with Partial Tree Kernels, combined with the knowledge on the polarity of the words appearing in the tweets. The authors conclude that the most important features are those corresponding to sentiment-bearing words. Finally, (Jiang et al., 2011) classify sentiment expressed on previously-given \"targets\" in tweets. They add information on the context of the tweet to its text (e.g. the event that it is related to). Subsequently, they employ SVM and General Inquirer and perform a three-way classification (positive, negative, neutral). Motivation and Contribution The work presented herein is mainly motivated by the need to: a) develop sentiment analysis tools for a high number of languages, while minimizing the effort to create linguistic resources for each of these languages in part; b) study the manner in which the use of machine translation systems to produce multilingual data performs in the context of informal texts such as tweets; and c) evaluate the performance of sentiment classification when data from different languages is combined in the training phase. We would especially like to study the effect of using data from similar languages versus the use of data from structurally and lexicallydifferent languages. The advantage of such an approach would be that if combined classifiers perform better, then the effort of separating tweets in different languages at the time of analysis (which in the case of streaming data is not negligeable) can be reduced or eliminated entirely. Unlike approaches we presented in Related Work section, we employ fully-formed machine translation systems. Bearing this in mind, the main contributions we bring in this paper are: 1. The creation of a simple tweet sentiment analysis system, that employs a preprocessing stage to normalize the language and generalize the vocabulary employed to express sentiment. At this stage, we take into account the linguistic peculiarities of tweets, regarding spelling, use of slang, punctuation, etc., and also replace the sentiment-bearing words from the training data with a unique label. In this way, the sentence \"I love roses.\" will be equivalent to the sentence \"I like roses.\", because \"like\" and \"love\" are both positive words according to the GI dictionary. If example 1 is contained in the training data and example 2 is contained in the test data, replacing the sentiment-bearing word with a general label increases the chance to have example 2 classified correctly. In the same line of thought, we also replaced modifiers with unique corresponding labels. 2. The use of minimal linguistic processing, which makes the approach easily portable to other languages. We employ only tokenization and do not process texts any further. The reason behind this choice is that we would like the final system to work in a similar fashion for as many languages as possible and for some of them, little or no tools are available. 3. The use of a standard news translation system to obtain data in four other languages -Italian, Spanish, German and French; 4. The evaluation of different combinations of languages in the training phase and the effect of using languages from the same family versus the use of individual or all languages in the training phase on the overall performance of the sentiment classification performance. We show that using the training models generated with the method described we can improve the sentiment classification performance, irrespective of the domain and distribution of the test sets. Sentiment Analysis in Tweets Our sentiment analysis system is based on a hybrid approach, which employs supervised learning with the Weka (Weka Machine Learning Project, 2008) implementation of the Support Vector Machines Sequential Minimal Optimization (Platt, 1998) linear kernel, on unigram and bigram features, but exploiting as features sentiment dictionaries, emoticon lists, slang lists and other social media-specific features. We do not employ any specific language analysis software. The aim is to be able to apply, in a straightforward manner, the same approach to as many languages as possible. The approach can be extended to other languages by using similar dictionaries that have been created in our team. They were built using the same dictionaries we employ in this work and their corrected translation to Spanish. The new sentiment dictionaries were created by simultaneously translating from these two languages to a third one and considering the intersection of the translations as correct terms. Currently, new such dictionaries have been created for 15 other languages. The sentiment analysis process contains two stages: pre-processing and sentiment classification. Tweet Pre-processing The language employed in Social Media sites is different from the one found in mainstream media and the form of the words employed is sometimes not the one we may find in a dictionary. Further on, users of Social Media platforms employ a special \"slang\" (i.e. informal language, with special expressions, such as \"lol\", \"omg\"), emoticons, and often emphasize words by repeating some of their letters. Additionally, the language employed in Twitter has specific characteristics, such as the markup of tweets that were reposted by other users with \"RT\", the markup of topics using the \"#\" (hash sign) and of the users using the \"@\" sign. All these aspects must be considered at the time of processing tweets. As such, before applying supervised learning to classify the sentiment of the tweets, we preprocess them, to normalize the language they contain. The pre-processing stage contains the following steps: In the first step of the pre-processing, we detect repetitions of punctuation signs (\".\", \"!\" and \"?\"). Multiple consecutive punctuation signs are replaced with the labels \"multistop\", for the fullstops, \"multiexclamation\" in the case of exclamation sign and \"multiquestion\" for the question mark and spaces before and after. In the second step of the pre-processing, we employ the annotated list of emoticons from Sen-tiStrength 2 (Thelwall et al., 2010) and match the content of the tweets against this list. The emoticons found are replaced with their polarity (\"positive\" or \"negative\") and the \"neutral\" ones are deleted. Subsequently, the tweets are lower cased and split into tokens, based on spaces and punctuation signs. The next step involves the normalization of the language employed. In order to be able to include the semantics of the expressions frequently used in Social Media, we employed the list of slang from a specialized site 3 . At this stage, the tokens are compared to entries in Rogets Thesaurus. If no match is found, repeated letters are sequentially reduced to two or one until a match is found in the dictionary (e.g. \"perrrrrrrrrrrrrrrrrrfeeect\" becomes \"perrfeect\", \"perfeect\", \"perrfect\" and subsequently \"perfect\"). The words used in this form are maked as \"stressed\". Further on, the tokens in the tweet are matched against three different sentiment lexicons: GI, LIWC and MicroWNOp, which were previously split into four different categories (\"positive\", \"high positive\", \"negative\" and \"high negative\"). Matched words are replaced with their sentiment label -i.e. \"positive\", \"negative\", \"hpositive\" and \"hnegative\". A version of the data without these replacements is also maintained, for comparison purposes. Similar to the previous step, we employ a list of expressions that negate, intensify or diminish the intensity of the sentiment expressed to detect such words in the tweets. If such a word is matched, it is replaced with \"negator\", \"intensifier\" or \"diminisher\", respectively. As in the case of affective words, a version of the data without these replacements is also maintained, for comparison purposes. Finally, the users mentioned in the tweet, which are marked with \"@\", are replaced with \"PER-SON\" and the topics which the tweet refers to (marked with \"#\") are replaced with \"TOPIC\". Sentiment Classification of Tweets Once the tweets are pre-processed, they are passed on to the sentiment classification module. We employed supervised learning using SVM SMO with a linear kernel, based on boolean features -the presence or absence of n-grams (unigrams, bigrams and unigrams plus bigrams) determined from the training data (tweets that were previousely pre-processed as described above). Bigrams are used specifically to spot the influence of modifiers (negations, intensifiers, diminishers) on the polarity of the sentiment-bearing words. Obtaining Multilingual Data for Sentiment Analysis in Tweets Subsequent to the tweet normalization, we translate the Twitter data (the training and development data in the SemEval Task 2 campaign) using the Google machine translation system to four languages -Italian, Spanish, French and German. The reason for choosing the development dataset for testing is that this set is smaller and allows us to manually check and correct it, to obtain a Gold Standard (and ensure that performance results are not biased by the incorrect translation in both the training, as well as the development data). Further on, we extract the same features as in the case of the system working for English -unigrams and bigrams -from these obtained datasets. We employ the features to train an SVM SMO classifier, in the same manner as we did for English. Evaluation and Discussion Although the different steps included to eliminate the noise in the data and the choice of features have been refined using our in-house gathered Twitter data, in order to evaluate our approach and make it comparable to other methods, we employ the data used in an established competition, allowing subsequent comparisons to be made. Data Set The characteristics of the training (T*) and development (test in our case) -t*-datasets employed are described in Table 1 . On the last column, we also include the baseline in terms of accuracy, which is computed as the number of examples of the majoritary class over the total number of examples: Data #Tweet # Evaluation and Results In order to test our sentiment analysis approach, we employed the datasets described above, for each of the languages individually, all the twolanguages combinations, combinations of languages from the same linguistic family and all languages together. The results are presented in Table 2 . We consider the measure of accuracy and do not compare to the SemEval official results, because in the competition, the results did not take into account the \"neutral\" class. Discussion From the results obtained, we can draw several conclusions. First of all, we can see that using tweet normalization and employing machine translation, we can obtain high quality training data for sentiment analysis in many languages. The machinetranslated data thus obtained can be reliably employed to build classifiers for sentiment, reaching a performance level that is similar to the results obtained for English and significatly above the baseline. Secondly, seeing the performance of the different pairs of languages compared to individual results, we can: a) on the one hand, see that combining languages with a comparatively high difference in performance results in an increase of the lower-performing one and b) on the other hand, in some cases, the overall performance is improved on both systems, which shows that combining this data helps to disambiguate the contextual use of specific words. Finally, the results show that the use of all the languages together improves the overall classification of sentiment in the data. This shows that a multilingual system can simply employ joint training data from different languages in a single classifier, thus making the sentiment classification straightforward, not needing any language detection software or training different classifiers. By manually inspecting some of the examples in the datasets, we could see that the most important causes of incorrect classification were the word orders and faulty translations in context. Another reason for incorrect sentiment classification was the different manner in which negation is constructed in the different languages considered. In order to improve on this aspect, we will include language-specific rules by adding skip-bigrams (bigrams made up of non-consecutive tokens) features in the languages where the place of the negators can vary. Conclusions and Future Work In this article, we presented a method to create a simple sentiment analysis system for English and extend it to the multilingual setting, by employing a standard news machine translation system. We showed that using twitter language normalization, we can obtain good results in target languages and that the joint use of training data from different languages helps to increase the overall performance of the classification. Finally, we showed that the joint training using translated data from languages that are similar yield significantly improved results. In future work, we plan to evaluate the use of higher-order n-grams (3-grams) and skip-grams to extract more complex patterns of sentiment expressions and be able to identify more precisely the scope of the negation. In this sense, we plan to take into account the modifier/negation schemes typical of each of the languages, to consider (further to translation) language-specific schemes of n-grams. We also plan to test the performance of sentiment classification using translations *to* English and employing classifiers trained on English data. In order to do this, we require lists of slang and digital dictionaries to perform normalization. We would like to study the performance of our approach in the context of tweets related to specific news, in which case these short texts can be contextualized by adding further content from other information sources. In this way, it would be interesting to make a comparative analysis of the tweets written in different languages (from the same or different regions of the globe), on the same topics.",
         "6789627",
         "2c2728a4333aad7b26bc84f291da10db77b94bf4",
         "38",
         "https://aclanthology.org/R13-1007",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Balahur, Alexandra  and\nTurchi, Marco",
         "Improving Sentiment Analysis in {T}witter Using Multilingual Machine Translated Data",
         "49--55",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "balahur-turchi-2013-improving",
         null,
         null
        ],
        [
         "39",
         "2012.iwslt-evaluation.7",
         "In this paper, the automatic speech recognition (ASR) and statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2012 are presented. We participated in the ASR (English), MT (English-French, Arabic-English, Chinese-English, German-English) and SLT (English-French) tracks. For the MT track both hierarchical and phrase-based SMT decoders are applied. A number of different techniques are evaluated in the MT and SLT tracks, including domain adaptation via data selection, translation model interpolation, phrase training for hierarchical and phrase-based systems, additional reordering model, word class language model, various Arabic and Chinese segmentation methods, postprocessing of speech recognition output with an SMT system, and system combination. By application of these methods we can show considerable improvements over the respective baseline systems.",
         "In this paper, the automatic speech recognition (ASR) and statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2012 are presented. We participated in the ASR (English), MT (English-French, Arabic-English, Chinese-English, German-English) and SLT (English-French) tracks. For the MT track both hierarchical and phrase-based SMT decoders are applied. A number of different techniques are evaluated in the MT and SLT tracks, including domain adaptation via data selection, translation model interpolation, phrase training for hierarchical and phrase-based systems, additional reordering model, word class language model, various Arabic and Chinese segmentation methods, postprocessing of speech recognition output with an SMT system, and system combination. By application of these methods we can show considerable improvements over the respective baseline systems. Introduction This work describes the automatic speech recognition (ASR) and statistical machine translation (SMT) systems developed by RWTH Aachen University for the evaluation campaign of IWSLT 2012 [1] . We participated in the ASR track, machine translation (MT) track for the language pairs English-French, Arabic-English, Chinese-English, German-English and the spoken language translation (SLT) track. State-ofthe-art ASR, phrase-based and hierarchical machine translation systems serve as baseline systems. To improve the MT baselines, we evaluated several different methods in terms of translation performance. We show that phrase training for the phrase-based (forced alignment) as well as for hierarchical approach (forced derivation) can reduce the phrase table size while even improving translation quality. In addition, different word segmentation methods are tested for both Arabic and Chinese as source language. For English as source language, we perform a part-of-speech-based adjective reorder-ing as preprocessing step. System combination is employed in three language pairs of the MT track to improve the translation quality further. Moreover, we investigate the use of the Google Books n-grams. For the SLT track, an SMT system is applied to perform a postprocessing of the given ASR output. This paper is organized as follows. In Section 2 and 3 we describe our ASR system and baseline translation systems. Sections 4 and 5 give an account of the phrase training procedure for the hierarchical phrase-based system and the system combination applied in several MT tasks. Our experiments for each track are summarized in Section 6. We conclude in Section 7. ASR System The ASR system is based on our English speech recognition system that we also successfully applied in Quaero evaluations [2] . In the acoustic feature extraction, the system computes Mel-frequency cepstral coefficients (MFCC) from the audio signal, which are transformed with a vocal tract length normalization (VTLN). In addition, a voicedness feature is computed. Acoustic context is incorporated by concatenating nine feature vectors in a sliding window. The resulting feature vector is reduced to 45 dimensions by means of a linear discriminant analysis (LDA). Furthermore, bottleneck features derived from a multilayer perceptron (MLP) are concatenated with the feature vector. The acoustic model is based on hidden Markov models (HMMs) with Gaussian mixture models (GMMs) as emission probabilities. The GMM has a pooled, diagonal covariance matrix. It models 4500 generalized triphones which are derived by a hierarchical clustering procedure (CART). The parameters of the GMM are estimated with the expectationmaximization (EM) algorithm with a splitting procedure according to the maximum likelihood criterion. The language model is a Kneser-Ney smoothed 4-gram. Several language models are trained on different datasets. The final language model is obtained by linear interpolation. The vocabulary of the recognition lexicon is obtained by applying a count-cut-off on the language model data. Each word in the lexicon can have multiple pronunciations. Missing pronunciations are derived with a grapheme-to-phoneme tool. The recognition is structured in three passes, In the first pass, a speaker independent model is used. The recognition result of the first pass is used for estimating feature transformations for speaker adaptation (CMLLR). The second pass uses the CMLLR transformed features. Finally, a confusion network decoding is performed on the word lattices obtained from the second pass. The acoustic model of the ASR system is trained on 793 hours of transcribed acoustic data in total, see Table 1 . The acoustic training data consists of American broadcast news data (hub4+tdt4), European parliament speeches (epps), and British broadcast conversations (quaero). The MLP is trained on the 268 hours of the quaero corpus only. We use 4500 triphone states and perform eight EM splits, resulting in a GMM with roughly 1.1 million mixture components. The language model is trained on a large amount of news data (Gigaword), the transcriptions of the audio training data, and a small amount of in-domain data (TED), see Table 2 . The recognition lexicon consists of 150k words. Baseline SMT Systems For the IWSLT 2012 evaluation RWTH utilized state-of-theart phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA++ [3] was employed to train word alignments, all LMs were created with the SRILM toolkit [4] and are standard 4-gram LMs with interpolated modified Kneser-Ney smoothing, unless stated otherwise. We evaluate in truecase, using the BLEU [5] and TER [6] measures. Phrase-based Systems For the phrase-based SMT systems, we used in this work both an in-house implementation of the state-of-the-art MT decoder (PBT) described in [7] and the implementation of the decoder based on [8] (SCSS) which is part of RWTH's open-source SMT toolkit Jane 2.1 1 . We use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distance-based reordering model, an n-gram target language model and three binary count features. The parameter weights are optimized with MERT [9] (SCSS, HPBT) or the downhill simplex algorithm [10] (PBT). Hierarchical Phrase-based System For our hierarchical setups, we employed the open source translation toolkit Jane [11] , which has been developed at RWTH and is freely available for non-commercial use. In hierarchical phrase-based translation [12] , a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane systems are: phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, four binary count features, phrase length ratios and an n-gram language model. Optional additional models are IBM model 1 [13] , discriminative word lexicon (DWL) models, triplet lexicon models [14] , a discriminative reordering model [15] and several syntactic enhancements like preference grammars and string-todependency features [16] . We utilize the cube pruning algorithm [17] for decoding and optimize the model weights with standard MERT [9] on 100-best lists. Forced Derivation As proposed in [18] , an alternative to the heuristic phrase extraction from word-aligned data is to train the phrase table with an EM-inspired algorithm. Since in [18] a phrase table for a phrase-based system was learned, we employed the idea of force-aligning the training data on a hierarchical phrase-based setup [19] . Instead of applying a modified version of the decoder, a synchronous parsing algorithm based on two successive monolingual parses is performed. The idea of the two-parse algorithm is to first parse the source sentence. Then, phrases extracted from the source parse tree are used to parse the target sentence. After parsing, we apply the inside-outside algorithm on the generated target parse tree to compute expected counts for each applied phrase. Using the expected counts, we update the phrase probabilities and apply a threshold pruning on the phrase table. Leave-one-out is applied to counteract over-fitting effects. We tested this procedure on the English-French MT task. The results are shown in Table 3 . The phrase table size was reduced by 88% without hurting performance. System Combination System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines. System combination can be divided into two steps. The first step produces a word to word alignment for the given single system hypotheses. In a second step a confusion network is constructed. Then, the hypothesis with the highest probability is extracted from this confusion network. For the alignment procedure, we have to choose one of the given single system hypotheses as primary system. To this primary system all other hypotheses are aligned and thus the primary system defines the word order. In Figure 1 a system combination of four different system is shown. We select the bold hypothesis as primary hypothesis. The other hypotheses are aligned to the primary using the METEOR [20] alignment. The resulting hypotheses have different word lengths and thus it is possible to align a word to an empty word marked as $. Once the alignment is given, we are able to built a confusion network. As the hypotheses consist of different words and may have different sentence length, the unaligned words could produce incorrect arcs. To fix the incorrect arcs, we introduce a reordering model based on the language model scores of the given adjacent incorrect arcs. For unaligned parts, we take the hypothesis with the highest language model score and align the unaligned parts of all hypotheses to that one. As result we get a more meaningful confusion network. In Figure 1 different confusion networks with and without the reordering model are shown. A more compact representation of the confusion network is given in Figure 2 . As choosing a primary hypothesis is a hard decision, we build for each hypothesis as primary system one confusion network. To combine these different networks, we just use the Union operation from the automata theory. The next step is to extract the most probably translation from the confusion network. Each arc in the confusion network is rescored with different statistical models as word or phrase counts of the single systems, a language model score, a word penalty and a binary feature which marks the primary system of the partial confusion network. We give each model a weight and  Table 4 : System combination results for the MT tasks English-French (en-fr), Arabic-English (ar-en) and Chinese-English (zh-en). system tst2010 BLEU TER en-fr best single system 32.0 50.1 system combination 32.9 42.9 ar-en best single system 27.1 54.4 system combination 28.0 53.4 zh-en best single system 14.7 74.5 system combination 15.4 74.1 combine them in a log-linear model. The weights can be optimized with MERT and the translation with the best score within the lattice is the consensus translation. By applying system combination in the English-French, Arabic-English and Chinese-English MT task, we achieve improvements of up to +0.9 points in BLEU and up to -1.0 points in TER. Experimental Evaluation Automatic Speech Recognition In Table 5 we compare the word error rate (WER) of the three different passes. A lower WER indicates a better recognition quality. We achieve an improvement of 2.5 points in WER by applying the second pass. Furthermore, the confusion network decoding improves the recognition by 0.2 points. English-French For the English-French task, RWTH employed both phrasebased decoders (SCSS, PBT), different hierarchical phrasebased systems (HPBT) and a system combination of the best setups. All experimental results are given in Table 6 . The SCSS baseline system is trained on the in-domain data (TED) [21] . For this baseline, we achieve the biggest improvement by training an additional translation model on the available out-of-domain data (+1.1% BLEU). The system is further improved by applying part-of-speech-based adjective reordering rules as preprocessing step [22] (+0.3% BLEU) and a 7-gram word class language model (+0.3% BLEU). For the PBT setups, the baseline is a system trained on all available data (allData). By adding phrase-level discriminative word lexicons [14] (DWL) and a reordering model, which distinguishes monotone, swap, and discontinuous phrase orientations [23, 24] (MSD-RO), the baseline system is improved by 0.9 points in BLEU and 0.7 points in TER. The HPBT baseline is trained on the in-domain data. By limiting the recursion depth for the hierarchical rules with a shallow-1 grammar [25] , we achieve an improvement of 0.6 points in BLEU. The bigger language model is trained on the target part of the bilingual corpus, the Shuffled News data and the 10 9 and French Gigaword corpora. As for the SCSS system, we trained an additional phrase table on the out-ofdomain data. All in all, we are able to improve the HPBT baseline by +2.3% BLEU and -1.8% TER. To increase the translation quality further, we employed system combination as described in Section 5 on several systems including the last year's primary submission (HPBT.2011). We gain an enhancement of 0.9 points in BLEU and 0.7 points in TER compared to the best single system. Compared to the last year's submission on the 2011 evaluation set, we could improve our best single system by 1.6 points in BLEU and 1.8 points in TER and further 1.0% BLEU with system combination (Table 7 ). Google Books n-grams For the English-French translation task we also investigated upon using the Google Books n-grams [26] which is a collection of n-gram counts extracted from digitized books. These counts are categorized by language and publication year of the books containing the n-grams. Selecting a range of years Table 6 : Results for the English-French MT task. The open-source phrase-based decoder (SCSS) is incrementally augmented with a second translation model trained on outof-domain data (oodDataTM), adjective-reordering as preprocessing step (adj-reordering) and a word class language model (WordClassLM). The in-house phrase-based decoder (PBT) is trained on all available bilingual data (allData) and incrementally augmented with a discriminative word lexicon (DWL) and an additional reordering model (MSD-RO). The hierarchical phrase-based decoder (HPBT) is incrementally augmented with a shallow-1 grammar (shallow), a bigger language model (bigLM), an alternative lexical smoothing (IBM-1), forced derivation (FD) and a second translation model trained on out-of-domain data (oodDataTM). The primary submission is a system combination of all systems marked with *. and using the vanilla n-grams resulted in language models with very high perplexities: The preprocessing steps applied to the underlying corpus do not match the preprocessing used in our system. By adapting the vanilla n-grams reasonable perplexities were obtained. We could further improve the language model by selecting only n-grams from books published in the last few years. Our final language model uses 4-grams obtained from the Google Books n-grams which are mixed with our previously described language model. The resulting language model has a perplexity of 81.4 on our development set which compares to a perplexity of 85.0 of the original language model. However, we did not use the improved language model in our final system since very small to no increase in translation quality was observed whereas the language model size was increased. We believe that the combination of mismatch in preprocessing, OCR errors and the very broad domain of the Google Books n-grams lead to the rather small improvements. It should be noted that a newer version of the Google Books n-grams [27] is available that was not available during the time of work. system Arabic-English RWTH participated last year in the Arabic-English TED task, achieving the best automatic results in the evaluation. This year, the architecture of the Arabic-English system is similar to last year, where a system combination is performed over different systems with differing Arabic segmentation methods. The differences from last year include: larger bilingual in-domain training data (130K versus 90K last year), the inclusion of the English Gigaword for language-modeling, and phrase table interpolation. We experimented with linear phrase table interpolation, where the phrase probabilities in both directions are interpolated linearly with a fixed weight optimized on the development set. We created two phrase tables, one using the TED in-domain and the other using the UN corpus, and interpolated them with a weight of 0.9 for the TED phrase table. The interpolation resulted in 1% BLEU improvement over a system using a phrase table trained over the full data. The different segmentation methods are similar to last year, and include: FST A finite state transducer-based approach introduced and implemented by [28] . The segmentation rules are encoded within an FST framework. SVM A reimplementation of [29] , where an SVM framework is used to classify each character whether it marks the beginning of a new segment or not. CRF An implementation of a CRF classifier similar to the SVM counterpart. We use CRF++ 2 to implement the method. MorphTagger An HMM-based Part-Of-Speech (POS) tagger implemented upon the SRILM toolkit [30] . MADA v3.1 An off-the-shelf tool for Arabic segmentation [31] . We use the following schemes: D1,D2,D3 and ATB (TB), which differ by the granularity of the segmentation. As in last year, adaptation using filtering is done for both LM training and TM training. To build the LM, we use a mixture of all available English corpora, where News Shuffle, giga-fren.en and the English Gigaword are filtered. For translation model filtering, we use the combined IBM-1 and LM cross-entropy scores. We perform filtering for the Mul-tiUN corpus, selecting 1  16 of the sentences (400K). Due to the different Arabic segmentations we utilize, we performed the sentence selection only once over the MADA-TB method, and used the same selection for all other setups. We trained phrase-based systems for all different segmentation schemes using the interpolation of TED and the 400K selected portion of the UN corpus. Additionally, one system was trained on all available data, preprocessed with MADA-TB. The results are summarized in Table 8 . The table includes a comparison between the 2011 and 2012 systems on the test set. This year systems clearly improves over last year, with improvements ranging from 1% up-to 1.7% BLEU. The single system MADA-TB ALL of 2012 performs similarly to the system-combination submission of 2011. The final system combination improves over last year submission with +1% BLEU and -1.3% TER. Chinese-English Results of Chinese-English systems are given in Table 9 . The system combination in Table 9 is RWTH's primary submission. The system combination was done as follows. We use both a phrase-based decoder [7] and a hierarchical phrasebased decoder Jane [11] . For each of the two decoders we do a bi-directional translation, which means the system performs standard direction decoding (left-to-right) and reverse direction decoding (right-to-left). We thereby obtain a total of four different translations. To build the reverse direction system, we used exactly the same data as the standard direction system and simply reversed the word order of the bilingual corpora. For example, the bilingual sentence pair \"今 天 是 星 期 天 。||Today is Sunday .\" is now transformed to \"。 星 期天 是 今天||. Sunday is Today\". With the reversed corpora, we then trained the alignment, the language model and our translation systems in the exactly same way as the normal direction system. For decoding, the test corpus is also reversed. The idea of utilizing right-to-left decoding has been proposed by [32] and [33] where they try to combine the advantages of both of the left-to-right and right-to-left decoding with a bidirectional decoding method. We also try to gain benefits from two-direction decoding, however, we use a system combination to achieve this goal. In Table 9 , first four systems do not use UN data. For HPBT-withUN-a and HPBT-withUN-b we additionally select 800k bilingual sentences from UN. HPBT-withUN-a and HPBT-withUN-b are built using the same setup but with differently optimized feature weights. PBT-reverse is the reverse system of PBT. HPBT-reverse is the reverse system of HPBT. HPBT-withUN-a and HPBT-withUN-b are trained with normal the left-to-right direction. From the results we draw the conclusions: HPBT performs better than PBT; UN data does not help; system combination of the six systems gets the best result. German-English For the German-English task, RWTH submitted a phrasebased system which is extended by several state-of-the-art improvements. In a preprocessing step, the German source is decompounded [34] and part-of-speech-based long-range verb reordering rules [22] are applied. The baseline uses a 4-gram language model trained on the target side of the bilingual data. When using additional monolingual data, we perform data selection as described in [35] . The results are given in Table 10 . We created two baselines, one trained on all available bilingual data, one trained on the in-domain TED data only. The pure in-domain system clearly outperforms the general system on the TED data sets. This baseline is improved by forced-alignment phrase training (+0.1% BLEU) [18] , adding 1  4 of the Shuffled News data (+0.7% BLEU), a 7-gram word class language model (+0.6% BLEU), a second translation model trained on all available out-of-domain data (+0.5% BLEU) and finally by adding 1   8   of each of the 10 9 and Gigaword corpora to the LM training data (+0.5% BLEU). Spoken Language Translation (SLT) The input for the translation systems in the SLT track is the automatic transcription provided by the automatic speech recognition track. In this work, we used the recognitions of our ASR system described in Section 2. Due to the fact that the output of the ASR system does not provide punctuation marks or case information and contains recognition errors, we have to adapt the standard text translation system used in the English-French MT track. Firstly, as described in [36] , we trained a translation system on data without punctuation marks and case information in the source language, but including punctuation and casing in the target language. By translating ASR output with such a system, punctuation and case information are predicted during the translation process. We denote this as IMPLICIT. As a second approach an SMT system was trained on a corpus with ASR output as source language data and the corresponding manual transcription as target language data, i.e. we interpret the postprocessing of the ASR output as machine translation [37] . We denote this as POSTPROCESS-ING. In order to built such a corpus we recognized the provided talks with our ASR system. On this corpus a standard phrase-based SMT was trained. During the translation of the ASR output punctuation and case information are restored. The output of this SMT system is the input of a standard text translation system. In Table 11 , we compare the IMPLICIT method with our second approach (POSTPROCESSING). Note, for the experiments we utilized the best single system of the MT English-French track. POSTPROCESSING outperforms IMPLICIT and we achieve an improvement of 0.9 points in BLEU and 0.9 points in TER. Conclusion RWTH participated in ASR, MT (English-French, Arabic-English, Chinese-English, German-English) and SLT tracks of the IWSLT 2012 evaluation campaign. Considerable improvements over respective baseline systems were achieved by applying several different techniques. For the MT track, among these are phrase training for the phrase-based as well as for the hierarchical system, an additional reordering model, word class language model, data filtering techniques, phrase table interpolation, and different Arabic and Chinese segmentation tools. To improve the SLT system, postprocessing of the ASR output is modelled as machine translation. By system combination, additional improvements of the best single system were achieved. Acknowledgements This work was partly achieved as part of the Quaero Programme, funded by OSEO, French State agency for innovation. The research leading to these results has also received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n o 287658. References",
         "7927647",
         "27d286e89a71fdf6465b1cc8d11689bf5561d7e7",
         "10",
         "https://aclanthology.org/2012.iwslt-evaluation.7",
         null,
         "Hong Kong, Table of contents",
         "2012",
         "December 6-7",
         "Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign",
         "Peitz, Stephan  and\nMansour, Saab  and\nFreitag, Markus  and\nFeng, Minwei  and\nHuck, Matthias  and\nWuebker, Joern  and\nNuhn, Malte  and\nNu{\\ss}baum-Thom, Markus  and\nNey, Hermann",
         "The {RWTH} {A}achen speech recognition and machine translation system for {IWSLT} 2012",
         "69--76",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "peitz-etal-2012-rwth",
         null,
         null
        ],
        [
         "40",
         "2012.iwslt-evaluation.8",
         "In this paper, we describe HIT-LTRC's participation in the IWSLT 2012 evaluation campaign. In this year, we took part in the Olympics Task which required the participants to translate Chinese to English with limited data. Our system is based on Moses [1] , which is an open source machine translation system. We mainly used the phrase-based models to carry out our experiments, and factored-based models were also performed in comparison. All the involved tools are freely available. In the evaluation campaign, we focus on data selection, phrase extraction method comparison and phrase table combination.",
         "In this paper, we describe HIT-LTRC's participation in the IWSLT 2012 evaluation campaign. In this year, we took part in the Olympics Task which required the participants to translate Chinese to English with limited data. Our system is based on Moses [1] , which is an open source machine translation system. We mainly used the phrase-based models to carry out our experiments, and factored-based models were also performed in comparison. All the involved tools are freely available. In the evaluation campaign, we focus on data selection, phrase extraction method comparison and phrase table combination. Introduction This paper describes the Statistical Machine Translation (SMT) system explored by the Language Technology Research Center of Harbin Institute of Technology (HIT-LTRC) for IWSLT 2012. Generally, our system was based on Moses, and phrase-based models were used. In Olympics shared task, the training data was limited to the supplied data including HIT Olympic Bilingual Corpus (HIT) [2] and Basic Travel Expression Corpus (BTEC) [3] . Although the two corpora are both oral corpus, there are still some differences between them. For example, the BTEC corpus is travel-related, and the HIT corpus is mainly about the Olympic Games. Besides this, the organizer of IWSLT 2012 also provided two development sets which are selected from the HIT and BTEC corpus respectively. Because the training data is limited by the above corpus, in order to get a better performance, we need to excavate all the potential of the two corpora, including the development sets. One key problem of the SMT system is how to extract the phrase. Giza++ [4] is a popular word alignment tool which can produce word alignment information with parallel corpus. By using heuristic phrase extraction method, we can extract phrases with the alignment. Compared with heuristic phrase extraction method, Pialign [5] is an unsupervised model for joint phrase alignment and extraction using nonparametric Bayesian methods and inversion transduction grammars (ITGs). We compared the phrase table extracted by the two phrase extraction methods in many ways, such as the size, the quality, and the differences of two methods. System combination has been approved to improve machine translation performance significantly. With several machine translation systems' outputs, researchers can get a better translation by combining the outputs. But in this paper, we didn't combine the outputs; instead we combine the models generated by Giza++ and Pialign. It is shown that we can get a better performance by model combination. The following of the paper is organized as follows. Section 2 describes a phrase-based machine translation system which was used in our work. In section 3, we compared differences of two corpora. The result and phrase extraction are discussed in section 4. And in the last section, we give a conclusion and discuss the future work. Phrase-based System Our primary system is based on Moses with a phrase-based model. Under the log-linear framework [6] , when given a source sentence f , we can get a translation e as follows: exp( ( , )) ( | ; ) ( ) h f e p e f Z < O O O with ( ) exp( ( , )) Z h f e ¦ < O O where ( , ) h f e denotes the feature vector of the pair ( , ) f e , and O is its corresponding weight vector. ( , ) h f e contains 14 features and they are divided into following categories: x Bidirectional translation probabilities; x Bidirectional lexical translation probabilities; x MSD-reordering model; x Distortion model; x Language model; x Word penalty; x Phrase penalty. Pre-processing The Chinese sentences supplied by the organizer were not segmented, so we used the Stanford Word Segmenter [7] to segment the Chinese sentences with the PKU model. The English sentences were not tokenized, thus we used the open source tools supplied by Moses to tokenize them. We also lowercased all the English data for training. There are many English punctuation characters in Chinese sentences (and vice versa), so we wrote some scripts to change all the punctuation characters in order. Training In the training step, we used Giza++ to get alignments and combined the alignments with grow-diag-final-and method. With the alignments, we can extract phrases with heuristic phrase extraction method and generate the translation model. Besides, we also used Pialign to generate the alignments and phrases. The language model was built with SRILM toolkit [8] . A 5gram language model was used for decoding. The corpus we used to build the language model is all the supplied data, including training data and development data. Decoder The decoder used in our system is Moses. Tuning The parameters were tuned on the development set with standard trainer MERT [9] . When running MERT, the k-bestlist-size was set as 100 and BLEU4 [10] was selected as the evaluation metric. Post-processing The translations were post-processed after decoding. x All the Chinese words in output were deleted. Because there are many names in the test set, and most of them can't be translated, so we deleted them; x The English sentences were de-tokenized ; x The English sentences were re-cased by the recaser tools provided by Moses. Corpus The IWSLT organizer provided two training corpus, including HIT corpus and BTEC corpus. HIT corpus is a multilingual oral corpus developed for the Beijing 2008 Olympic Games. There are five domains in HIT corpus, including traveling, dining, sports, traffic and business. The BTEC corpus is also an oral corpus containing tourismrelated sentences. Besides the training corpus, they also provided two development corpus, which were extracted from the HIT corpus and BTEC corpus. In the following paper, we use HIT_train, HIT_dev, BTEC_train, BTEC_dev to denote four corpora respectively. In our system, we used HIT_train, BTEC_train, BTEC_dev, HIT_dev as our training data. And HIT_dev was also used as our development set. We also random sampled 1000 sentences from HIT corpus as our test set. The detail of the corpus is presented in Table 1 . We combined the four corpora as training data, and the new generated corpus is shown in Table 2 . Experiments and Results The comparison of Giza++ and Pialign We first trained six models with Giza++ alignments and Pialign alignments. A comparison between the phrase table generated from Giza++ and Pialign is shown in Table 3 . Table 4 shows the covering of the six phrase tables of the test set. In Table 3 , we showed the total number of phrase pairs, the common phrase pairs of Giza++ and Pialign, the different phrase pairs of Giza++ and Pialign. In Table 4 , we show the covering capacity of the phrase table. The covering capacity c is defined as follows: # of phrases both in test set and in phrase table # of phrases in test set c To note that, the test set was divided into unigram to 5-gram phrases. From Table 3 we can find that the phrase table generated by Pialign is a little bigger than Giza++. Because we usesamps parameters to sample the bilingual parser tree repeatedly. In this experiment, we tuned this parameters from 1(default) to 80. At first, with the increment of the phrase table size, the performance grows at the same time. But after 20 th sampling, the bias of sampling adds too many noise phrase pairs. Finally, we set this value to 20. With default value, Pialign only generated 389,982 phrase pairs (32.28% as the Giza++ did), but the performances are still comparable. With the covering capacity, we can estimate the performance of the model. The result is the same with the translation result, which shows that Pialign is better than Giza++ in phrase extraction. Results of translation The result of translation outputs are shown in Table 5 and Table 6 . The result is confusing. After we tuned the parameters with HIT_dev, the result became worse. This may be caused by the mismatch between HIT_dev and HIT_train. The result also shows that although we continue to enlarge the size of training data, the BLEU score may reduce on the contrary. These remind us that the model is also important. Combination of two phrase table We explored Giza++ and Pialign to extract phrases. In this section, we want to combine the two methods by merging two phrase tables using a linear interpolation method. For Giza++, the best result was achieved when we used Corpus1. For Pialign, the best result was achieved when we used Corpus2. So we combined the two phrase tables. The result without tuning is shown in Table 7 . The parameter means the weight of Pialign. Compared with Table 7 and Table 5 , we can draw a conclusion that phrase table combination can improve the performance of machine translation systems a little. Maybe due to the size of the training data, the result is not very clear to see the increment. And our combination method is only a linear interpolation method, which is naive for phrase table combination. We believe that a more complex strategy, such as some machine learning algorithms can improve the phrase table combination results. Linguistic knowledge In recently years, many researchers have focused on how to integrate linguistic knowledge into machine translation systems. In this work, part of speech was introduced to improve the machine translation systems. We used Stanford Log-linear Part-Of-Speech Tagger [11] to get the POS tag. Factored-based model of Moses was used to train a translation model. The result is shown in Table 8 . As we can see that the result with POS tag is also not better than the baseline. We think that linguistic knowledge is a good research field to improve machine translation performance. Official Results We took part in the Olympics task(OLY) [12] , and the final translations we submitted was generated by Pialign with corpus 2. And because of the bad performance of tuning, we submit out results without tuning. The final result was shown in Table 9 . Conclusions and Future Work In this paper, we explained our work in the IWSLT 2012 evaluation campaign. We compared two phrase extraction methods and tried to combine the two methods. The results show that the combination method can improve the result of MT systems. In future, we will still try to study some other advanced combination methods to modify our system. Acknowledgements The",
         "5150207",
         "98a492663f246b50e766b278b5e436d25b1f637a",
         "1",
         "https://aclanthology.org/2012.iwslt-evaluation.8",
         null,
         "Hong Kong, Table of contents",
         "2012",
         "December 6-7",
         "Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign",
         "Zhu, Xiaoning  and\nCui, Yiming  and\nZhu, Conghui  and\nZhao, Tiejun  and\nCao, Hailong",
         "The {HIT}-{LTRC} machine translation system for {IWSLT} 2012",
         "77--80",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "zhu-etal-2012-hit",
         null,
         null
        ],
        [
         "41",
         "2020.iwltp-1.7",
         "This paper presents our progress towards deploying a versatile communication platform in the task of highly multilingual live speech translation for conferences and remote meetings live subtitling. The platform has been designed with a focus on very low latency and high flexibility while allowing research prototypes of speech and text processing tools to be easily connected, regardless of where they physically run. We outline our architecture solution and also briefly compare it with the ELG platform. Technical details are provided on the most important components and we summarize the test deployment events we ran so far.",
         "This paper presents our progress towards deploying a versatile communication platform in the task of highly multilingual live speech translation for conferences and remote meetings live subtitling. The platform has been designed with a focus on very low latency and high flexibility while allowing research prototypes of speech and text processing tools to be easily connected, regardless of where they physically run. We outline our architecture solution and also briefly compare it with the ELG platform. Technical details are provided on the most important components and we summarize the test deployment events we ran so far. Introduction While natural language processing (NLP) technologies like automatic speech recognition (ASR), machine translation (MT), spoken language translation 1 (SLT), natural language understanding (NLU), or automatic text summarization have recently seen tremendous improvements, and are provided to end users as services by large companies like Google, Microsoft or Facebook, 2 the output quality of applications is still insufficient for practical use in daily communication. The goal of the ELITR (European Live Translator) project 3 is to advance and combine different types of NLP technologies to create end-to-end systems that are usable in serious business communication. Specifically, the ELITR project targets the advancement and application of ASR and SLT in two challenging settings: • Face-to-face conferences (interpreting official speeches and workshop-style discussions) • Remote conferences (interpreting discussions held over a on-line platform) In addition to addressing technological challenges in ASR, SLT, and MT, the project covers a large number of languages: ELITR tests its ASR technology in 6 EU languages. The subsequent MT technology is currently able to translate among all 24 official EU languages but 1 We interpret this term in the narrow sense: speech in one language to text in another language 2 Microsoft Translator translates between 62 languages, with 22 handled by the novel neural approach, and recognizes speech in 11 languages. Two variants of Chinese and English can be included in a customized component. 3 http://elitr.eu/ aims at supporting a larger set of language relevant for our user partner, the languages of members of European Organisation of Supreme Audit Institutions, EUROSAI. 4   The paper is structured as follows: In Section 2., we describe the core of our systems, the processing platform, which is used in both face-to-face and remote meetings settings. In Section 3. we go through some of the differences between ELITR platform and the ELG Grid. In Section 4., we summarize the design decisions and status of the technologies connected to the platform. Section 5. describes our field tests and our first experience. Processing Platform Metadata: Fingerprint and Types The first problem addressed by the PerVoice Service Architecture is the declaration of Services and service requests descriptions. For this purpose, so called fingerprints and types are used to specify the exact language and genre of a data stream. Fingerprints consist of a two-letter language code (ISO369-1) followed by an optional two-letter country code (ISO3166) and an optional additional string specifying other properties such as domain, type, version, or dialect (ll[-LL [-dddd]] ). Types are: audio (audio containing speech), text (properly formatted textual data), unseg-text (unsegmented textual data such as ASR hypotheses). Service descriptions and service requests are fully specified by their input and output fingerprints and types. For example, the ASR service which takes English audio as input and provides English unsegmented text adapted on news domain will be defined by \"en-GB-news:audio\" input fingerprint and \"en-GB-news:unseg-text\" output fingerprint. The service request of German translation of English audio will be defined by \"de-DE-news:audio\" input fingerprint and \"en-GB-news:text\" output fingerprint. Workflow When a Worker (the encapsulation of a service) connects to the Mediator (orchestration service) on a pre-shared IP address and port, it declares its list of service descriptions, i.e. the list of services it offers. As soon as the connection is established, the Worker waits until a new service request is received. Several Workers can connect to the Mediator and offer the same service, which allows for a simple scaling of the system. As soon as the new service request has been accepted, the Worker waits for incoming packets from the Client's data stream to process, and performs specific actions depending on the message types (data to be processed, errors, reset of the connection). When the Client has sent all the data, the worker waits until all pending packets have been processed, terminates the connection with the Client and waits for a new Client to connect. From the Client perspective, when a Client connects to the Mediator, it declares its service request by specifying which kind of data it will provide (output fingerprint and type) and which kind of data it would like to receive (input fingerprint and type). If the Mediator confirms that the mediation between output type and input request is possible, the Client starts sending and receiving data. When all data has been sent, the Client notifies it to the Mediator and waits until all the data has been processed by the Workers involved in its request. The Client can then disconnect from the Mediator. Mediation In order to accomplish a Client's request, a collection of Workers able to convert from the Client's output fingerprint and type to the requested input fingerprint and type must be present. For example, if a Client is sending an audio stream with the fingerprint en-GB-news:audio and requests en-GB-news:unseg-text, the Mediator must find one Worker or a concatenation of multiple Workers that are able to convert audio containing English into unsegmented English text, i.e. a speech recognition Worker in the example. The Mediator searches for the optimal path to provide a service using a best path algorithm that works on fingerprint names and types match. In order to make sure that a mediation is still possible even if there are no workers available matching the requested stream types and fingerprints, back-up strategies have been implemented, which relax the perfect match on country and domain fingerprint's section. MCloud Library Through its light-weight API MCloud, the PerVoice Service Architecture defines a standard for services integration, allowing different partners integration and a flexible usage for different use cases. The Mediator supports parallel processing of service requests in a distributed architecture. MCloud is a C library which implements the raw XML protocol used by the PerVoice Service Architecture and exposes a simplified API for the development of Clients and Workers. For convenience, the library integrates some high-level features like audio-encoding support and data package management. A .NET and a Java wrapper of the MCloud API are available in order to support the development of client desktop applications for the PerVoice Service Architecture. Comparison of ELITR and ELG Platforms Another EU project, European Language Grid (ELG) 5 also develops a common platform for natural language processing. While starting from similar intentions, ELITR and ELG focus on different use cases. ELITR targets real-time business use cases-like face-to-face and remote video conferencing for selected events-ELG focuses on the creation of a shared European Language Technologies catalogue and marketplace for self-service usage of provided technologies. Both purposes and intentions are valuable but result in different technological approaches. ELITR use cases include live video streaming and automatically transcribed and translated subtitles. For this reason the project preferred the low-latency solution provided by the PerVoice Service Architecture, which works in real-time and also enables the transparent concatenation of services (e.g., ASR output passed as input to translation Worker) based on \"on-air\" services. Real-time communication is provided by a fast protocol working over TCP/IP sockets which ensures smaller latencies in contrast to approaches relying on external message brokers that introduce asynchronous interaction and delays. The decentralized approach of the PerVoice Service Architecture allows companies to avoid sharing proprietary technologies. Furthermore, the actual service provider of a Worker component is secondary to the actual functionality being provided. ELG instead prefers the service categorization approach, creating a catalogue of services deployed in its infrastructure. The ELITR solution could be deployed offline, should the use case require special security and data privacy measures-assuming that there are sufficient hardware resources and a partner agreement. The ELG grid instead is deployed only in cloud. In general, we highlight the fact that language technologies can rely on different software architectures, and not all of them are suitable to be containerized. For example, a complex language processing solutions could run more than one process, making it harder to manage the container and debug problems, or they could have high resource requirements. Large virtual machine images become an issue when thousands of containers need to be deployed across a cluster. The PerVoice Service Architecture instead delegates service management to individual parties contributing services to the infrastructure, in order to exploit their specific training and knowledge of the technologies and sysfor a better resource allocation and usage. ELITR Technologies With respect to the core language processing technology needed to realize the simultaneous translation service presented here we face several research questions that need to be addressed. Besides the obvious challenge of providing speech translation with sufficient performance, the special case of simultaneous speech translation for conferences, talks and lectures brings specific challenges with it. Two foremost challenges are a) that speech translation has to happen in real-time and with low latency in order to be simultaneous, and b) to cover and adapt to a large variety of domains as the topics of talks and conferences can be virtually arbitrary; therefore systems need to be either domainindependent (a still unsolved research question) or need to be able to adapt to the current domain, autonomously or with as little human supervision as possible. Currently the systems for speech translation also undergo an architecture transformation from statistical models based on Bayes' rule towards all neural models that give better performance. In our scenario this transformation has to be done under the aspects of the need for low latency translation which leads to task specific considerations. Architecture Consideration Over the last years the basic technology of the components for speech translation has undergone radical transformations. While for decades systems for speech recognition and machine translation where based on Bayes' rule and made use of statistical methods such as Hidden Markov Models, Gaussian Mixture Models, N-Gram Models, and Phrase Based Translation Models, lately the use of neural networks has led to significantly improved performance. While first individual components, such as the acoustic model or the language models, of the systems were replaced, the latest improvements were gained from end-toend systems that solve the problem of automatic speech recognition, machine translation etc. with a single neural network architecture, instead of solving the problem with several models given by Bayes' rule. This single network architecture can go to the extreme of solving the whole problem of speech translation with one single neural network architecture. Current SLT Architecture in ELITR At this time, end-to-end speech translation systems do not yet outperform cascaded systems consisting of several components (Niehues et al., 2019) . End-to-end speech recognition models (Nguyen et al., 2019; Pham et al., 2019) have been showing promising performance but have limit when being used in online conditions. Therefore, in ELITR we use a cascaded speech translation system consisting of: • Automatic Speech Recognition System (ASR) • Punctuation System (PUNCT) • Machine Translation System (MT) Automatic Speech Recognition In our system, the ASR component is in charge of processing the audio stream sent from recording clients and output a stream of text transcript to the next component in the pipeline. We currently follow the HMM/ANN hybrid approach (Fügen et al., 2008; Niehues et al., 2018) to build up the ASR model. In this approach, ASR modeling is handled by two separate components: acoustic model (AM) and language model (LM). The task of AM is to model acoustic observations with regard to the labels of context dependent phonemes. As recent advances in the field of ASR, deep neural networks are used to leverage the modeling capacity of the AM on many hours of speech training data. Separately from AM, LM is trained solely on text data and it is used to provide the probabilities of word sequences. The AM and LM are then used in a dynamic decoding framework that is capable of online and low-latency inference. As one of the most important advantages of the hybrid approach, both AM and LM can be easily adapted for better performance if in-domain data is available for a particular application setup. Punctuation System The hypotheses from speech recognition contain no punctuation. As our machine translation system is trained on well-structured, written sentence-level texts, we use a separate component to insert punctuation and sentence boundaries into the ASR output. This component also adds correct capitalization to the otherwise lowercased hypotheses. Essentially, the punctuation system is a monolingual translation system, which translates the lower-cased, unsegmented outputs from the ASR components into wellformed texts prior to the translation system (Cho et al., 2015) . We can employ any kind of translation approach and it is only required to train on a small amount of monolingual data. In our current punctuation system, for each language, we train a neural model on spoken texts, e.g the transcripts of TED talks. Using our compact representation described by Cho et al. (2017) , we are able to add punctuation and correct capitalization in one go. Furthermore, this compact representation helps to reduce the vocabulary size of our neural-based monolingual system, thus, reducing the model size and making the training of such system faster. Machine Translation System With the ultimate goal of featuring a translation system for all EUROSAI languages, we opt for the multilingual approach (Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches' transcription (Europarl) (Koehn, 2005) , the collection of legislative texts of the European Union (JRC-Acquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT 3 corpus extracted from TED talks (TED) (Cettolo et al., 2012) . • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of data. • In practice, having a small number of multilingual systems to cover all language pairs significantly reduces the development and deployment efforts compared with having one system for each pair. Our multilingual systems are based on the neural sequenceto-sequence with attention framework (Bahdanau et al., 2014) and shares the internal representation across languages (Pham et al., 2017) . At present, we have one manyto-many Transformer model (Vaswani et al., 2017) providing translation between all pairings of 36 languages, along with several specialized models focused on subsets of languages, in particular the project's primary languages of English, Czech, and German, see i.a. (Popel and Bojar, 2018; Popel et al., 2019) . The resulting multilingual models after training can be used immediately in deployment or can go through a language adaptation step. This language adaptation is simply continuing training the multilingual model on the data of a specific language pair for a few epochs in order to improve the individual translation performance. While we need to do this language adaptation for every single language pair in our system, it is a trivial job since we could automate the process with the same settings and it takes only a little of time and computing resources to reach decent performances. Low-Latency Speech Translation In order to realize low latency in automatic speech recognition we work with speculative output. The decoder in our speech recognition system realizes a Viterbi beam search. Due to the beam, partial hypotheses often have a stable part in which all alternative hypotheses have been pruned away by the beam further ahead in the search, and an unstable part that contains several competing hypotheses that fall within the beam. Therefore it is possible to output the stable part, knowing that it will never change again as the search progresses. Previous experiments have shown that such a strategy would lead to a latency of about 6-8 seconds. A user study had shown that this considered too high a latency by the users. We therefore lowered the latency further by using speculative output, always putting out the current best hypothesis. Often this hypothesis will stay the most likely hypothesis, as the search progresses. In case it changes, we make use of an update mechanism that allows us to update the recent part of the hypothesis as necessary. The punctuation component is set to generate the segmented, well-formed text whenever it receives any output, either unstable or stable, from the speech recognition system. And it passes its outputs along with the information of stability to the machine translation component. Normally the machine translation component waits for the whole sentence before conducting the translation process. To reduce the latency, we force the component to directly and constantly produce outputs right after it receives outputs of the punctuation component. It might then fix the generated translation to be stable by its best hypothesis. This brings down the average word-based latency, i.e. the time from which the last word of the sentence was spoken until the translation of that sentence is displayed and never changed again by the update mechanism, to under 5 seconds. Practical Tests While each of the components (ASR, punctuation, MT) are tested and evaluated on their own, on their respective test sets, the whole complex setup also has to be evaluated. We are still working on a tool which would allow for a rigorous evaluation of the performance considering multiple aspects like translation quality, delay or text updates which may damage the end user experience. For the time being, we focus on running many 'field tests', deploying the technology at various occasions. Our experience in the two intended settings (face-to-face multilingual conferences and remote conferencing) is described in the respective sections below. Tests of Multi-Target Conference Speech Translation Since the ELITR kick-off in January 2019, we carried out several tests and dry-runs to present our live-subtitling system. It first started with a Students Firms Fair in March 2019. During this event, we provided live subtitles on different languages that were spoken on the presentation stage, and we also collected a rather challenging speech test set (Macháček et al., 2019) which serves in the Non-native SLT task at IWSLT 2020. 6 Next, we had two officially planned events organized by the Supreme Audit Office of the Czech Republic (SAO) that were held in June 2019 and October 2019. In these events, the subtitles were delivered live to the participants through the presentation platform on their laptops. Apart from this, we also tested the input from interpreters into Czech and English respeakers. We also tried to show the live translation of the speaker in Czech, Hungarian, Spanish, German and Dutch from English. These translations were, however, unstable and inconvenient for users to interpret the context of the discussion. This event highlighted the required scope for improvement both in service functionalities and user experience. We made many critical observations from these two events and we gradually improved several aspects of the system for another dry-run in February 2020. Apart from the usual two-line subtitle view, we now present also a paragraph view of the transcript which contains more text in a history-style view. The subtitles were presented in English and translated into German, Czech, Russian, French, Hungarian, Polish, and Dutch. Tests of Remote Conferencing The functionality of live transcription has been succesfully tested in the field of labour market training by alfatraining, an educational provider using alfaview R . 7 A remote call participant with hearing impairment used the live transcript to follow the lessons and participate in discussions with a lecturer and other participants. In another test, CUNI organized a call between two persons. One person followed only the transcript or translation, without listening. The second person was describing a word without saying it explicitly. We showed on multiple person pairs and languages that it is possible to guess the explained word both from transcripts and automatic translations of natural, spontaneous speech. Conclusion The PerVoice Service Architecture decouples clients and service providers by providing a simple protocol and an integration library, available for the major platforms, to connect both end-user application and service engines to it. It simplifies the creation of workflows among different service providers by providing automatic workflow creation solution. Populated with state-of-the-art systems for automatic speech recognition and machine translation developed at KIT, UEDIN and CUNI, the architecture proves its applicability in challenging settings, as needed by the EU project ELITR. Tests showed practical usability of our systems for face-toface and remote conferences in real conditions. They also showed that the current and future main challenge is to improve speech recognition, especially for non-native dialects and out-of-vocabulary words. Acknowledgement This project has received funding from the European Union's Horizon 2020 Research and Innovation Programme under Grant Agreement No. 825460 (ELITR). 7 https://alfaview.com",
         "218974259",
         "d8604dfd4ce1f119e707afc32c668e053ad1cae0",
         "7",
         "https://aclanthology.org/2020.iwltp-1.7",
         "European Language Resources Association",
         "Marseille, France",
         "2020",
         "May",
         "Proceedings of the 1st International Workshop on Language Technology Platforms",
         "Franceschini, Dario  and\nCanton, Chiara  and\nSimonini, Ivan  and\nSchweinfurth, Armin  and\nGlott, Adelheid  and\nSt{\\\"u}ker, Sebastian  and\nNguyen, Thai-Son  and\nSchneider, Felix  and\nHa, Thanh-Le  and\nWaibel, Alex  and\nHaddow, Barry  and\nWilliams, Philip  and\nSennrich, Rico  and\nBojar, Ond{\\v{r}}ej  and\nSagar, Sangeet  and\nMach{\\'a}{\\v{c}}ek, Dominik  and\nSmr{\\v{z}}, Otakar",
         "Removing {E}uropean Language Barriers with Innovative Machine Translation Technology",
         "44--49",
         null,
         null,
         null,
         null,
         null,
         "979-10-95546-64-1",
         "inproceedings",
         "franceschini-etal-2020-removing",
         "English",
         null
        ],
        [
         "42",
         "I17-1038",
         "The recent technological shift in machine translation from statistical machine translation (SMT) to neural machine translation (NMT) raises the question of the strengths and weaknesses of NMT. In this paper, we present an analysis of NMT and SMT systems' outputs from narrow domain English-Latvian MT systems that were trained on a rather small amount of data. We analyze post-edits produced by professional translators and manually annotated errors in these outputs. Analysis of post-edits allowed us to conclude that both approaches are comparably successful, allowing for an increase in translators' productivity, with the NMT system showing slightly worse results. Through the analysis of annotated errors, we found that NMT translations are more fluent than SMT translations. However, errors related to accuracy, especially, mistranslation and omission errors, occur more often in NMT outputs. The word form errors, that characterize the morphological richness of Latvian, are frequent for both systems, but slightly fewer in NMT outputs.",
         "The recent technological shift in machine translation from statistical machine translation (SMT) to neural machine translation (NMT) raises the question of the strengths and weaknesses of NMT. In this paper, we present an analysis of NMT and SMT systems' outputs from narrow domain English-Latvian MT systems that were trained on a rather small amount of data. We analyze post-edits produced by professional translators and manually annotated errors in these outputs. Analysis of post-edits allowed us to conclude that both approaches are comparably successful, allowing for an increase in translators' productivity, with the NMT system showing slightly worse results. Through the analysis of annotated errors, we found that NMT translations are more fluent than SMT translations. However, errors related to accuracy, especially, mistranslation and omission errors, occur more often in NMT outputs. The word form errors, that characterize the morphological richness of Latvian, are frequent for both systems, but slightly fewer in NMT outputs. Introduction For many years, the central problem in machine translation (MT) has been the quality. MT quality has been recognized as a complicated research question when translation is performed into a morphologically rich (and also under-resourced) language with a relatively free word order, e.g., Bulgarian, Croatian, Estonian, Finnish, Greek or Latvian. Possible solutions for widely used statistical machine translation have been studied for many years (e.g., Koehn and Hoang 2007; Tamchyna and Bojar 2013; Burlot and Yvon 2015) . Today machine translation is experiencing a paradigm shift from (phrase-based) statistical machine translation (SMT) to neural machine translation (NMT). The first results obtained in recent years are promising, as it can be seen from the results of WMT 2016 (Bojar et al., 2016) and WMT 2017 (Bojar et al., 2017) . As NMT becomes more and more popular, the question of what can we expect from NMT in terms of quality becomes very important. Recent analysis of English to German SMT and NMT outputs of manual transcripts of short speeches showed that NMT can decrease the post-editing effort (Bentivogli et al., 2016) . A comparison of NMT and SMT systems for nine language directions (English to and from Czech, German, Romanian, Russian, and English to Finnish) on news stories made by Toral and Sánchez-Cartagena (2017) showed that translations produced by NMT systems are more fluent and more accurate in terms of word order compared to translations produced by SMT systems. By analyzing of manually error-annotated outputs of generic English-Croatian MT systems, Klubička et al. (2017) found that NMT handles all types of agreement better than SMT (including factored models). In this paper, we delve further into analyzing the strengths and weaknesses of NMT from the perspective of translation quality and the needs of the localization industry. We analyze translations of good quality domain-specific (medicine related) English-Latvian SMT and NMT systems that were trained on a rather small (ca. 325K sentences) data set. The target language -Latvian -is a morphologically rich under-resourced language (about 1.5 million speakers). As it is a synthetically inflected language, words change their form according to their grammatical function. In Latvian only half of the word endings are unambiguous, while for the rest, multiple base forms may be derived from the inflected form (Skadin ¸a et al., 2012) . We analyze outputs of NMT and SMT systems in a post-editing (PE) scenario. Data on PE time, keystrokes, and typical operations were collected during the PE process. Analysis of these data allowed us to conclude that both approaches (SMT and NMT) are comparably successful allowing to increase translator productivity, with the NMT system showing slightly worse results. We believe that the reason translations from the SMT system are better in our case, is that from the small amount of data, SMT learns better terminology and phrases which are specific for the particular narrow domain. The situation could be different for broad domain MT systems, as it can be seen from recent WMT 2017 English-Latvian news domain results, where NMT and hybrid approaches were better (Bojar et al., 2017; Pinnis et al., 2017) . In addition, for a small sub-set of the MT system translations, manual error annotation was performed. This allowed us to identify the main error categories for each MT system. Through analysis of annotated errors, we found that NMT translations are more fluent than SMT translations, NMT produces significantly fewer typography errors than SMT. At the same time errors related to accuracy, especially, mistranslation and omission errors, occur more often in NMT outputs. The word form errors, which characterize the morphological richness of Latvian, are slightly fewer in NMT outputs. Related work Questions on how to evaluate the quality and usefulness of machine translation have been studied for several decades. For localization industry needs, MT quality and PE productivity have been analyzed by Flournoy and Duran (2009) ; Groves and Schmidtke (2009) ; Plitt and Masselot (2010) ; Skadin ¸š et al. ( 2011 ); Pinnis et al. (2016) and others. These studies report significant productivity increase when good quality SMT systems are used. Recently, for English-Spanish Sanchez-Torron and Koehn (2016) reported that \"for 1point increase in BLEU, there is a PE time decrease of 0.16 seconds per word, about 3-4%\". Several studies have recently compared SMT and NMT systems. Bentivogli et al. (2016) conducted a detailed analysis of SMT and NMT output for the English-German language pair on translations of manual transcripts of TED talks 1 . They found that NMT decreases post-editing effort, but degrades faster than SMT for longer sentences. They also found that NMT output contains fewer morphology errors, lexical errors and substantially fewer word order errors. Toral and Sánchez-Cartagena (2017) compared NMT and SMT systems submitted to WMT16 news translation task for nine translation directions (English to and from Czech, German, Romanian, Russian, and English to Finnish). The authors found that the translations produced by NMT systems were more fluent and more accurate in terms of word order compared to translations produced by SMT systems. They observed that NMT systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences. However, when Farajian et al. ( 2017 ) compared the performance of generic English-French NMT and SMT systems, that were trained on a generic parallel corpus composed of data from different domains, they found that on such multidomain data SMT outperforms its neural counterpart. Moreover, Castilho et al. (2017) in their study, in which human evaluators compared NMT and SMT output for a range of language pairs, reported mixed results from the human evaluation. Similarly to the previous authors, they reported an increase in fluency, but inconsistent results for adequacy (the neural model showed a greater number of errors of omission, addition, and mistranslation) for NMT when compared to SMT. They argue that, although \"NMT shows significant improvements for some language pairs and specific domains, there is still much room for research and improvement before broad generalizations can be made.\" Analysis of NMT and SMT errors was recently made by Klubička et al. (2017) for English-Croatian MT systems. The authors analyzed manual error annotations of SMT and NMT system translations in the news domain and concluded that the NMT system reduces the errors produced by the SMT system by 54%. Data and MT Systems The SMT and NMT systems were trained on the parallel corpus from the European Medicines Agency (EMEA), which is a part of the OPUS cor- (Tiedemann, 2009) , and the latest documents from the EMEA website (years 2009-2014) 2 . Prior to the training of the MT systems, we preprocessed the training data using tools for corpora cleaning, filtering, non-translatable token (e.g., URL, e-mail address, different code, etc.) identification, tokenization, and true-casing. The statistics of the training corpora before and after preprocessing are given in Table 1 . Statistical Machine Translation System The SMT system is a standard phrase-based system that was trained on the Tilde MT platform (Vasil ¸jevs et al., 2012) with Moses (Koehn et al., 2007) . The system features a 7-gram translation model and a 5-gram language model. The language model was trained with KenLM (Heafield, 2011) . The system was tuned with MERT (Bertoldi et al., 2009) using a held-out set of 2,000 sentence pairs. Neural Machine Translation System We used the sub-word neural machine translation toolkit Nematus (Sennrich et al., 2017) for training the NMT system. The toolkit allows training attention-based encoder-decoder models with gated recurrent units in the recurrent layers. For word splitting in sub-word units, we use the byte pair encoding tools from the subword-nmt toolkit (Sennrich et al., 2015) . The NMT system was trained using a vocabulary of 40,000 word parts (39,500 for byte pair encoding), a projection (embedding) layer of 500 dimensions, recurrent units of 1024 dimensions, a batch size of 20 and dropout enabled. All other parameters were set to the default parameters as used by the developers of Nematus for their WMT 2016 submissions (Sennrich et al., 2016) . MT System Evaluation SMT and NMT systems were evaluated on a heldout set of 1000 randomly selected sentence pairs. The automatic evaluation results are given in Table 2. The results show that the SMT system achieves better results than the NMT system. This could be explained by the relatively small size of the parallel corpus and a very narrow domain, i.e., from the small amount of data, SMT learns better terminology and phrases which are specific for the particular narrow domain. When translation is performed into a morphologically rich language, such as Latvian, automatic metrics (e.g. BLEU score) are not always good indicators of translation quality. Table 3 illustrates a case, where both translations have the same quality, but because of different word order the SMT translation received 41.38 BLEU points, while the NMT translation -only 24.42 points. To validate the automatic evaluation results, we performed a small blind comparative evaluation task. The task was performed by 5 professional translators who evaluated 198 segments in total. The results of the comparative evaluation show that the translations of the SMT system are preferred more often by evaluators than the translations of the NMT system (see Figure 1 ). However, the difference is not statistically significant according to the methodology by Skadin ¸š et al. ( 2010 ). Therefore, both systems were further used in the post-editing and error annotation experiments. 4 What Can Be Learned from Post-edits? Post-editing process For post-editing, we compiled a list of 22,500 segments (360,000 words) from EMEA documents. Then, we split the list into documents consisting of 100 segments so that the original sequence of sentences is preserved, and translated the documents At first, translators were asked to post-edit SMT translations. Then, three months later, they were asked to post-edit NMT translations. For the NMT post-editing task, the documents were redistributed to translators, to ensure that each translator has different set of documents in SMT and NMT post-editing tasks. We asked translators to post-edit translated segments with the post-editing tool PET (Aziz et al. 2012) . It allowed us to track the time spent on each segment and to log all keystrokes that the translator performed while post-editing each segment. Translators were asked not to spend excessive amounts of time on each segment because the quality expectations were not \"human translation quality\", but rather \"post-editing quality\". To assist post-editing, translators were provided with an automatically extracted in-domain term collection that was integrated into PET and provided translation suggestions for known terms. After post-editing each segment, translators were asked to evaluate the quality of the MT translation, marking it as one of the following: \"near perfect\", \"very good\", \"poor\", and \"very poor\". If the translator did not apply any changes, the system automatically assigned the highest quality rating -\"Unchanged\". Five professional translators were involved in the SMT post-editing task and seven in the NMT post-editing task. Finally, we asked the translators who participated in both tasks (4 in total) to translate two documents without pre-translated segments in order to measure each translator's pure translation productivity.  To perform a fair comparison between SMT and NMT post-editing tasks, we limit our analysis to the first 20 documents post-edited by each translator participating in both post-editing tasks. We perform the analysis only on segments that were not found in the MT system training data (approximately 36% of segments were discarded). The statistics of the post-edited data that are used for the further analysis is given in Table 4 . Post-editing Results Most We start the analysis by examining the MT quality assessments produced by translators during post-editing. The Figure 2 summarizes the distribution of rankings showing that the SMT system produced a larger proportion of near perfect and perfect translations than the NMT system -50.2% compared to just 39.3%. The detailed logs of each translators work allowed to measure the time spent on post-editing   in three distinct intervals: the amount of time that elapsed between the appearance of an MT segment and the first click, or \"reading time\"; the amount of time between the first edit and approval of the segment, or \"editing time\"; and the amount of time spent between approval of the segment and completion of the quality assessment, referred to as \"assessment time\". The results of the log data analysis in Figure 3 show that on average it takes 30% more time for translators to start editing SMT translations. It is also obvious that editing of good, very good and near perfect SMT translations requires 16-62% more time than for NMT translations. However, the situation is opposite for poor and very poor translations -it requires 3-25% more time to post-edit NMT translations. This difference is more noticeable in Figure 4 , which shows that post-editing poor and very poor NMT translations (24% of all post-edited NMT translations) required more than half of the editing time (55.1%). In comparison post-editing of poor SMT translations (16.8% of all post-edited SMT translations)  In terms of productivity (see Figure 5 ), it is evident that both tasks (SMT and NMT postediting) obtain higher productivity than pure translation. However, the productivity is higher for post-editing SMT translations (104% compared to 94%). When analyzing the effect of the length of segments on productivity (tokens translated/postedited per hour), the results in Figure 6 showed that there is an obvious decrease in post-editing productivity for longer segments, with the NMT post-editing productivity decreasing faster than for SMT post-editing. It is interesting that there is almost no change in productivity when translating without MT support. The information on the time spent on each segment allows us to analyze the relationship between the post-editing productivity and the postediting effort that is expressed with the help of the Human-targeted Translation Edit Rate (HTER; Snover et al. 2006 ). Figure 7 depicts the aver- age productivity for different MT translation quality intervals. It shows that we can identify average system quality thresholds, at which postediting becomes productive (HTER of 0.4 or less) and at which it stops being productive (HTER of 0.7 or higher). The average HTER scores of the SMT and NMT systems are 0.22 and 0.31 respectively. The figure also shows that there is little difference between SMT and NMT post-editing, with the NMT post-editing being faster at individual quality levels. Still, because the NMT system produced more poor translations, the overall postediting productivity is higher for the SMT postediting task. To validate, whether the post-edits are of good quality, we performed quality assessment of the post-edits according to the LISA Quality Assurance model 3 . The quality assessment was performed by professional editors from our localization department. The results in Figure 8 show that even though the task for translators was to perform light post-editing, the quality of the post-edited translations is rated as excellent (i.e., the average error score for both SMT and NMT post-edits is below 10 per 1000 words). MT Error Annotation The aim of the error annotation task was to identify common and specific errors for both MT architectures and their influence on the overall quality of MT output. Error Annotation Task For error annotation (EA), 1800 English segments and their translations into Latvian by SMT and NMT systems were selected. Only translations that were marked as \"Very good\" during postediting for both MT systems were included. The main reason for including only segments that have good translations was the necessity to avoid wrong annotations due to very bad input. The error classification used, in this task, is based on Multidimensional Quality Metrics (MQM; Lommel et al. 2014) . More specifically, the subset that is defined by Burchardt and Lommel (2014) was used. In this classification, errors are divided into three top categories: accuracy, fluency, and terminology. These top level categories then include more detailed categories from the MQM issue type hierarchy. The EA was performed four months after finishing both post-editing tasks. Two translators, who participated in both post-editing tasks, were involved to ensure consistency between post-editing and error annotation tasks and to avoid a situation when translators annotate errors, which were not requested to be corrected during post-editing. The error annotation was performed in the Translate5 4 platform. Before translators started the error annotation, they were introduced to a video tutorial, written guidelines, and the decision process. During annotation, translators saw the source segment, MT output, and post-edited MT output. Each translator annotated 1000 segments translated by the SMT system and the same 1000 segments translated by the NMT system. Although inter-annotator agreement was not our main interest, 200 translations from each system were annotated by both translators. Table 5 : Summary of error annotation task (count -number of errors for particular category; total -sum of errors, including Observations from the Error Annotation Task The overall results of the error annotation task are summarized in Table 5 . Results show that although the segments were ranked as good, most of them contain more than one error per segment. The total number of errors is higher for SMT. There are twice as many errors related to fluency (77%) as to accuracy (28%) for SMT, while for NMT the fluency errors comprise 55% of errors, but accuracy errors -44%. The complexity of Latvian morphology is a reason why more than 1/4 of errors are grammar errors (35% for SMT and 27% for NMT), from which almost 1/5 of errors are word form errors (SMT 21%, NMT -19%). For instance, both MT systems generate the wrong form for the word \"aerosols (spray)\" when translating the sentence \"How to use the nasal spray\": the SMT system generates the singular nominative form aerosols (spray), while the NMT system generates singular genitive form aerosola (spray). A significant difference between SMT and 4 http://translate5-metashare.dfki.de NMT outputs has been observed for three error subcategories -typography (the subcategory of fluency), mistranslation (the subcategory of accuracy) and omission (the subcategory of accuracy). Typography errors are much more widespread in SMT (21.70%) than in NMT (11%). Usually these are cases where spaces are wrongly used (e.g. \"beta -2 -agonisti\" instead of \"beta-2-agonisti\" (beta-2-agonists), or wrong separators appear in numbers (e.g. \"3,644\" instead of \"3644\", or \"0.5\" instead of \"0,5\"). These errors, especially wrong separators, are not frequent in NMT translations. The Latvian language has a very rich, morphology-based word-building potential (words are usually built by adding affixes to the stem). This feature resulted in a high number (19%) of mistranslations from the NMT system. Typical cases of mistranslation from the NMT system include the incorrect translation of numbers (e.g., 30 July 2012 is translated as 2008. gada 30. jūlijs), terms (e.g., drop (piliens) is translated as injekcija (injection)) and named entities (e.g., Naglazyme (Naglazyme) is translated as MabCampath). Latvian also has a relatively free word order. In the case of a formal, narrow domain, where usually the word order is strict, it has a rather small influence even for the SMT system (9% of errors), while in the case of more general systems this could have much greater impact. Errors of omission are much more frequent for NMT (15%) than for SMT outputs (10%). NMT also produces fewer (4%) word order errors than SMT (9%), while SMT has fewer (8%) spelling errors than NMT (11%). Inter-annotator Agreement Although the aim of this research was not to study consistency between annotations, but to identify and analyze the main error categories, 200 segments translated by SMT and NMT systems were annotated by two translators. The reason for having only two annotators was seriously debated in the consortium of the QT21 project 5 by a number of leading MT researchers. It was agreed that, to show inconsistencies/issues, common understanding of the annotation task, it is enough to have two annotators. The inter-annotator agreement is more like a sanity check for the fine-grained annotation levels (whether annotators have common understanding or not). Table 6 presents the summary on errors annotated in these segments. Similarly to the whole error annotation task, slightly more errors are found in the SMT system's output. Table 6 also confirms the finding from the overall error annotation task, that NMT produces less typography and word order errors than SMT, but it produces more mistranslation and omission errors. There are several error categories where translators have different opinions about the applicability of the particular categories. The table clearly demonstrates that the most complicated case was the identification of a correct subcategory for wrong word form errors. The annotator A1 mostly assigned the top category \"word form\" for such errors, while the annotator A2 marked them as agreement errors. Another case of significant disagreement between annotators can be observed for fluency errors in the NMT post-editing task. As there was no consistent correspondence between an error category assigned by annotator A2 for cases where an- notator A1 marked fluency errors, we asked annotator A1 to explain her reasoning. She told us that she marked fluency errors where a post-editor during post-editing applied just stylistic corrections. After inspecting these cases, we agreed with her explanation. For inter-annotator agreement, we calculated free-marginal kappa under three different conditions (see Table 7 ): perfect match analysis (i.e., by taking the precise positions and (sub)categories of errors into account), error count analysis (i.e., by ignoring error positions), and error presence analysis (i.e., by just looking at whether both annotators identified that a segment contains a certain (sub)category of errors) 6 . The results show that when taking positions into account, there is just slight agreement between the annotators. This is explained by the different understanding of where errors need to be marked: one translator annotated errors at the character level, while the other -at the token level. For instance, in the case of wrong The inter-annotator agreement scores highlight the necessity for improvements in the general guidelines to mitigate the potential for disagreement. That being said, the inter-annotator agreement in the higher error levels (i.e., if we do not split errors up in 4 levels of sub-categories, but analyze only the top 2 levels) is good (over 0.6) for SMT and moderate (over 0.4) for NMT. Conclusion In this paper, we presented an analysis of narrow domain English-Latvian SMT and NMT systems, that were trained on a rather small in-domain corpus. Translations of both systems were post-edited by professional translators and ranked depending on the complexity of editing. 83% of SMT translations and 73% of NMT translations were ranked as perfect, near perfect or very good, thus confirming the fact that in-domain MT systems can produce good quality translations even when the amount of training data is limited. The analysis of post-edited data allowed us to conclude that both approaches allow for an increase in translator productivity, with the NMT system showing slightly worse results in general, but better for good quality MT output. We believe that the lower results for the NMT system are linked to the relatively small size of the parallel corpus and the narrow domain. By analysis of the manually annotated errors, we found that the SMT system produced twice as many errors related to fluency (77%) in comparison to those related to accuracy (28%), while for the NMT system the fluency errors comprise 55% of all errors, but accuracy errors -44%. In terms of error subcategories, widespread errors for both systems are grammar errors (35% for SMT and 27% for NMT), especially wrong word form errors (21% for SMT and 19% for NMT), indicating that morphologically rich languages, e.g., Latvian, are problematic for both MT systems, while improving with NMT. A significant difference between SMT and NMT outputs has been observed for three error subcategories -typography (22% for SMT and 11% for NMT), mistranslation (7% for SMT and 19% for NMT) and omission (10% for SMT and 15% for NMT). The obtained results show that in the case of a narrow domain, if MT systems are trained on a small amount of data, the SMT system performs better than the NMT system. The reason why the SMT system in our case is better, is that from the small amount of data, SMT learns better terminology and phrases which are specific for the particular narrow domain. The situation differs for broad domain MT systems, as it has been demonstrated by recent WMT 2017 English-Latvian news domain results, where NMT and hybrid approaches were better. Acknowledgments We would like to thank Tilde's Localization Department for the hard work they did to prepare material for the analysis presented in this paper. The work within the QT21 project has received funding from the European Union under grant agreement n • 645452. The research has been supported by the ICT Competence Centre (www.itkc.lv) within the project \"2.2. Prototype of a Software and Hardware Platform for Integration of Machine Translation in Corporate Infrastructure\" of EU Structural funds, ID n • 1.2.1.1/16/A/007.",
         "28739432",
         "11114d345ca4e3d1af04b1f59b655cb56e75cd0a",
         "7",
         "https://aclanthology.org/I17-1038",
         "Asian Federation of Natural Language Processing",
         "Taipei, Taiwan",
         "2017",
         "November",
         "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
         "Skadi{\\c{n}}a, Inguna  and\nPinnis, M{\\=a}rcis",
         "{NMT} or {SMT}: Case Study of a Narrow-domain {E}nglish-{L}atvian Post-editing Project",
         "373--383",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "skadina-pinnis-2017-nmt",
         null,
         null
        ],
        [
         "43",
         "2009.mtsummit-posters.18",
         "Statistical Machine Translation (SMT) systems rely heavily on the quality of the phrase pairs induced from large amounts of training data. Apart from the widely used method of heuristic learning of n-gram phrase translations from word alignments, there are numerous methods for extracting these phrase pairs. One such class of approaches uses translation information encoded in parallel treebanks to extract phrase pairs. Work to date has demonstrated the usefulness of translation models induced from both constituency structure trees and dependency structure trees. Both syntactic annotations rely on the existence of natural language parsers for both the source and target languages. We depart from the norm by directly obtaining dependency parses from constituency structures using head percolation tables. The paper investigates the use of aligned chunks induced from percolated dependencies in French-English SMT and contrasts it with the aforementioned extracted phrases. We observe that adding phrase pairs from any other method improves translation performance over the baseline n-gram-based system, percolated dependencies are a good substitute for parsed dependencies, and that supplementing with our novel head percolation-induced chunks shows a general trend toward improving all system types across two data sets up to a 5.26% relative increase in BLEU.",
         "Statistical Machine Translation (SMT) systems rely heavily on the quality of the phrase pairs induced from large amounts of training data. Apart from the widely used method of heuristic learning of n-gram phrase translations from word alignments, there are numerous methods for extracting these phrase pairs. One such class of approaches uses translation information encoded in parallel treebanks to extract phrase pairs. Work to date has demonstrated the usefulness of translation models induced from both constituency structure trees and dependency structure trees. Both syntactic annotations rely on the existence of natural language parsers for both the source and target languages. We depart from the norm by directly obtaining dependency parses from constituency structures using head percolation tables. The paper investigates the use of aligned chunks induced from percolated dependencies in French-English SMT and contrasts it with the aforementioned extracted phrases. We observe that adding phrase pairs from any other method improves translation performance over the baseline n-gram-based system, percolated dependencies are a good substitute for parsed dependencies, and that supplementing with our novel head percolation-induced chunks shows a general trend toward improving all system types across two data sets up to a 5.26% relative increase in BLEU. Introduction The phrase-based statistical machine translation (PB-SMT) (Koehn et al., 2003) model is the most widely researched paradigm in MT today. The standard method of extracting phrase-pairs from parallel data involves using union and intersection heuristics on both source-to-target and target-to-source word alignments, in the Moses system (Koehn et al., 2007) . This string-based extraction methodology gives rise to 'non-linguistic' chunk pairs, henceforth known as STR. 1  In this paper, we seek to investigate performance of the baseline Moses MT system by changing one step only, namely the phrase extraction process. Specifically, this entails using three sets of syntactically motivated phrase pairs such as those extracted from node-aligned parallel treebanks. Tinsley et al. (2007) and Hearne et al. (2008) extracted phrasepairs from constituency-aligned and dependencyaligned data, giving rise to two types of linguistic chunk pairs: CON and DEP respectively. Both these data sets were obtained by monolingual parsing of training sentences, subtree-aligning the parsed trees, and extracting word and phrase alignments . A prerequisite for this approach is the existence of constituency and dependency parsers for both the source and target languages. Hearne et al. (2008) demonstrated on a very small set of training data that combining string-based extraction (baseline Moses) with either of the syntaxinduced phrase extractions resulted in improved translation accuracy with a general trend toward preferring dependency-based over constituency-based phrases. However, there exist more robust and accu-rate phrase structure parsers than dependency structure parsers for most languages in NLP applications, which has led to alternate measures of automatically generating dependencies from phrase structure parses, as shown on pages 129-131 of Nivre (2006) . In this paper, we heuristically obtain dependency parses by using lexical head information in constituency parse trees. While the head percolation tables themselves are nothing new, the use of phrase pairs induced from them as a separate knowledge source in PB-SMT phrase tables is novel, to the best of our knowledge. This method of annotating and subsequently aligning percolated dependency parses gives rise to another set of aligned chunks: PERC. We then evaluate the uniqueness and usefulness of these alignments against STR, CON, and DEP alignments, and combinations thereof. The rest of the paper is organised as follows. After a review of related work in Section 2, we briefly describe the MT system setup and phrase extraction methodologies used to obtain the four types of chunk alignments in Section 3. The experiments and analysis of the results are detailed in Section 4, followed by our concluding remarks together with avenues for further research in Section 5. Background and Related Work We have taken a technique from statistical parsing and introduced its output as another knowledge source in the framework of syntax-aware PB-SMT. In what follows, we present our novel amalgamation of two pre-existing techniques, namely syntaxaware PB-SMT and generation of dependency structures from phrase-stucture parse trees. Syntax-aware PB-SMT Incorporation of linguistic knowledge into the phrase extraction process has shown mixed results in recent years. For instance, Koehn et al. (2003) , demonstrated that using syntax to constrain their phrase-based system actually harmed its quality. In contrast, all of the following approaches have shown that augmenting the baseline string-based translation model with syntax-aware word and phrase alignments causes translation performance to improve. Groves and Way (2005) extract EBMT phrase pairs by monolingually chunking both the source and target sides using closed-class marker words (Green, 1979) and then aligning the resulting chunks using mutual information techniques. Tinsley et al. (2007) extract phrase pairs by obtaining phrase structure parses for both the source and target sides using monolingual parsers and then aligning the subtrees using a statistical tree aligner. Hearne et al. (2008) go a step further by building on the work of (Tinsley et al., 2007) and adding phrase pairs induced from dependency parse trees. Note that all these approaches work on string-based translation models, i.e. syntactic knowledge is merely used to extract linguistically motivated phrase pairs. The phrase tables still contain translations of strings, just like in Moses. There also exist a number of other approaches (Chiang, 2005; Quirk et al., 2005; Galley et al., 2006; Hassan et al., 2008) which have developed different models where the incorporation of syntax has shown itself to be beneficial. However such models are not restricted to the string-based translation modeling and are thus somewhat out of the scope of this paper. In this paper, we extend the experiments of Hearne et al. (2008) by adding another syntax-aware phrase extraction methodology, namely percolated dependencies. We also scale up the volume of the training data, and compare and contrast the resultant phrase tables (cf. section 4). Head Percolation It is possible to obtain a dependency parse for a sentence from its constituency parse (Gaifman, 1965 ) by exploiting lexicalized heads, i.e. head words of each phrase or constituent. In the absence of this information, a head percolation table is used to select the head node in each constituent structure. For example, the head of a phrase (NP (DET The) (N box)) is the node (N box). This implies an entry in the head percolation table specifying the node N as a head child of the node NP. Head percolation tables were first introduced in Magerman (1995) and implemented in Collins (1997) . Head percolation tables are so called because, to extract headdependent information from a constituency parsed treebank, the lexical items are percolated like features from the heads to their parent projections. A head percolation table consists of hand-coded rules identifying the head-child of each node. We imple- (2001) to obtain head-dependent relations between words of a sentence. The head percolation algorithm will output the head or governor for each word in the sentence. In case the word is the head word of the sentence, it will be assigned a default value as its head. We used this method to obtain dependency parse structures from constituency parse structures for both the source and target languages. We distinguish these structures from the dependency structures obtained directly from a dependency parser by labelling the former as percolated dependencies. Theoretically, these percolated dependencies are induced from constituency parses and structurally equivalent to unlabelled dependency parses (Nivre, 2006) . However, experimentation in section 4 showed the percolated dependencies to be another source of information different from both constituency and dependency parses. System Specifics Before evaluating the impact of phrase pairs extracted from percolated dependencies, we describe the machine translation system and data used in our experiments followed by a brief description of the four phrase extraction methodologies. Tools and Resources As described in the previous section, we develop four French-English PB-SMT systems for our experiments: STR, CON, DEP, and PERC. We use two different datasets. We obtain results on a small parallel corpora of approximately 7,700 parallel sentences-the JOC English-French parallel corpus (Chiao et al., 2006) [7,723 train + 400 dev + 599 test sentences]-and a larger set of 100,000 parallel sentences extracted from the freely available Europarl corpus (Koehn, 2005) [100,000 train + 1,889 dev + 2,000 test sentences]. Experimenting on the JOC corpus allows us compare our results directly with those of Hearne et al. (2008) , while at the same time we successfully scale up their experiments by almost 13 times. We also used an open source tree aligner (Zhechev, 2009) to obtain subtree-alignments for the linguistic chunks CON, DEP, and PERC. The tree aligner works by performing a greedy search on all possible alignments between the tree pair nodes and scores using lexical probabilities to select the highest scoring alignment hypothesis. Constituency parse trees were obtained by using the Berkeley parser (Petrov et al., 2006) for both the French and English sides, and dependency parse trees were obtained from the English and French versions of the Syntex parser (Bourigault et al., 2005) . The dependency structures were converted into bracketed format to enable using the tree aligner. We used GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) for building a 5gram language model, Minimum Error Rate Training (Och, 2003) for tuning, and the Moses decoder (Koehn et al., 2007) in each of our systems. Thus the only difference between each of the four systems is in the phrase table used in the translation model. Phrase Extraction We explore four different types of phrase pairs in this paper. The first type we term 'non-linguistic' in that phrase pairs are extracted by carrying out stringbased union and intersection of source-to-target and target-to-source GIZA++ word alignments (Koehn et al., 2003) . The resulting phrases are mere sequences of aligned words occurring together and have no a priori syntactic motivation (cf. footnote 1). We label these as STR. The remaining three phrase-pair inductions are syntactically motivated in that they are produced by first monolingually parsing both the source and target sides. These parse trees are then node-aligned using the statistical tree aligner described above, which also uses information from the GIZA++ word alignment probabilities. The phrase pairs are then extracted by obtaining the surface-level chunks from the aligned subtrees. CON and DEP phrase pairs are induced from the parse trees obtained using offthe-shelf source and target language parsers. Finally, PERC phrase pairs are induced from another dependency-annotated structure which is obtained by applying head percolation features on the phrase structure parse trees used to produce CON phrases. Note that PERC annotations do not require the availability of a dependency-structure parser. Hence, the phrase extraction techniques used to obtain CON, DEP, and PERC chunks differ in only their source of parse trees, i.e. the type of parser and heuristics used to obtain the corresponding parse trees. All other steps in the process remain the same. After each of the four types of aligned phrases are extracted, they are scored (estimating translation probabilities) using the same algorithm (as defined in Moses system) to build four translation tables. Experimental Analysis For the purposes of our experiments, we create 15 possible combinations of translation tables from the four types of phrase extractions, namely STR, CON, DEP, and PERC. The combining of two or more systems is carried out by merging the individual phrase tables and re-estimating the phrase translation scores as defined in Moses. For example, the translation table of the system C+D+P is computed by concatenating the extracted phrase tables CON, DEP, and PERC and then re-estimating the probabilities. Each of the 15 configurations were run on both the JOC and Europarl datasets in the French-English translation direction. The results are jointly displayed in Table 1 . We evaluate the MT system performance using five evaluation metrics. These are BLEU (Papineni et al., 2002) , NIST (Doddington, 2002) , ME-TEOR (Banerjee and Lavie, 2005) , WER (Word Error Rate) and PER (Position-independent WER). System Evaluation What is quite clear from analysing the results on both the JOC and Europarl corpora is the very strong baseline performance of the STR system. For the pairwise comparison, any system combination omitting STR-induced phrase pairs underperforms. Note that in their experiments, both Groves and Way (2005) and Tinsley et al. (2007) acknowledge, as we do here, that n-gram-induced phrase pairs are required for both improved translation performance and coverage. Working on the JOC corpus allowed us to directly compare our novel phrase induction method against the work of Hearne et al. (2008) . While we could not improve upon their results (when substituting D with P in any system in Table 1 (a)) for the JOC corpus, running experiments on the 13 times larger Europarl data set showed clear performance gains (a relative increase of as high as 2.49% in BLEU when replacing D with P in any system in Table 1 (b)) over their method when the PERC phrases were utilised. Even if our method did not outperform theirs, our method would still be of use if no separate dependency parser was available for either the source or target language or both. While the best-performing system combination on both tasks was where STR and CON phrases were merged, for almost all metrics, the lowest WER rates were observed when PERC chunks were included. In addition, there are quite a few sentences (when computing sentence-level WER scores for each of the four base systems, PERC ranked 2nd best with nearly 25% sentences on both datasets) where PERC performs better than any other system, as in (1): (1) Source: La commission entend-elle garantir plus de transparence à cet égard? Ref: Does the commission intend to seek more transparency in this area? STR: Will the commission ensure that more than transparency in this respect? CON: The commission will the commission ensure greater transparency in this respect? DEP: The commission will the commission ensure greater transparency in this respect? PERC: Does the commission intend to ensure greater transparency in this regard? Note that the propensity of the baseline STR model to omit the verb can be seen to good effect here. Both CON and DEP phrases repeat the translation of the subject NP. In contrast, the translation using PERC phrases is both fluent and accurate, despite not mimicking exactly the reference translation. The lexical differences between the outputs and the reference translation leads us to speculate that the gains from PERC are not accurately reflected in the automatic evaluation scores. Therefore we also performed manual evaluation on a random selection of 100 sentences from the Europarl testset. A human annotator was shown pairs of sentences along with the source and reference translations and asked to grade whether one system was better than the other or if they were of equal calibre. While PERC and CON systems performed better than the other on the same number of sentences (27%), PERC performed 5% better than DEP. When comparing systems S+C and S+C+P (where the automatic evaluation score differences were not statistically significant), the former system was 11% better. However there were a number of sentences (around 30%) in which PERC was responsible for an output's superior quality. Although no pattern was immediately discernible, a more thorough analysis of these sentence types is left for future work. We conducted a range of other tests in order to evaluate the uniqueness (degree of difference from other phrase extractions) and usefulness (contribution to MT system performance) of PERC chunks, as described in the next two sections. The total number of entries in each of the four phrase tables (Europarl data) are STR:2,145,614 CON:663,136 DEP:583,532 and PERC:565,012. We can see that the CON t-table is just 31% of the size of the full STR t-table, with DEP just 27% and PERC even smaller at just 26% of the size. Uniqueness Table 2 provides clear evidence of the differences between the types of chunks produced by each of the four methods. It is interesting that despite the huge size of the STR phrase table, there is very little overlap with any of the other methods; the largest overlap with STR is using CON phrases, but this amounts to only 6% of the STR phrase table being also derived via CON, and only 22% of the CON phrase table being also derived via STR. The largest overlap in pure numerical terms is between CON and PERC; 74% of the CON phrase table are common with PERC, whereas 87% of the PERC phrase pairs are common with CON. Given that the PERC phrases are derived from the CON trees, one might have expected these two to have the biggest intersection. However, surprisingly, the output (translated sentences produced by CON and PERC systems) has a 30% overlap only. Therefore, it seems that despite a huge overlap in the phrase table configurations, the systems are different enough to produce different translations. We leave for future work an investigation into any bias here. By using two different constituency parsers to produce two sets of PERC chunks, we plan to study their correlation as a measure of bias. For each of the four phrase extraction methods, the average number of phrase pairs per sentence and the highest number of phrase pairs in a sentence were computed as follows: JOC corpus- (STR: 35.37 (134) 26 )). Similar performance is seen between the three non-STR methods on Europarl, whereas on JOC our PERC model produces a sizeable number of fewer alignments. The smaller number of phrase pair alignments might very well prove useful for systems with a smaller footprint requiring smaller ttables (Sanchez-Martinez and Way, 2009) . Having investigated the differences between the chunking methods, the next, more important step is to evaluate whether these unique chunks are of use in PB-SMT. Usefulness Test Moses (Koehn et al., 2007) can be run in 'trace' mode (-t switch) in order to investigate what particular phrases are being selected to derive the translation at any particular time. In Table 1 , we demonstrated that all four sets of phrase pairs could be combined in one phrase table in what we called the 'S+C+D+P' system. In order to translate the Europarl test set of 2,000 sentences, 11,748 phrases were found to be of use. These comprised 5204 STR phrases (of which 3423 were unique, i.e. not produced by any of the other three phrase tables), 2441 CON (419), 2319 DEP (402), and 2368 PERC (385) . When it came to a pairwise comparison, the biggest overlap was between CON and PERC. As with our finding regarding Table 2, we will investigate in further work whether there was any bias between these two phrase induction methods. In case of the JOC corpus, for a test set of 599 sentences, 6,121 phrases were found to be of use. These comprised 3820 STR (2090 unique), 2841 CON (95), 2775 DEP (111) , and 2541 PERC (236). Note, however, that for the JOC corpus, we found the biggest overlap to be between the CON and DEP phrase tables. As far as triples are concerned, by far the greatest overlap was between CON, DEP and PERC, with an intersection of 780 phrase pairs (the next nearest was just 196). Overall, 1261 phrase pairs were found by each of the four methods. The details for both corpora can be found in Table 3 . In future work, we plan to extract each of these resources as separate phrase tables in the log-linear framework, as it should be the case that where a set of phrase pairs has been verified by all four methods, these can be considered to be of high quality, and worthy of a large weight in the combination of translation resources. With respect to actual system performance, Figure 1 shows that adding PERC chunks to any system shows a general trend towards boosting scores for BLEU. While we do not include similar graphs for the other automatic evaluation metrics, this tendency Conclusion and Future Work While producing smaller translation models and believed to contain more useful (syntax-aware) phrases than the standard string-based extraction, the syntaxbased extractions may perform worse than the PB-SMT string-based baseline, especially as the amount of training data increases (cf. (Zollmann et al., 2008) ). 2 However, it has been observed by many researchers that rather than replacing one with the other, combining both types of induced phrases into one translation model significantly improves the translation accuracy. Thus we can supplement SMT phrases with syntax-aware phrases. Most system development today uses one particular approach to generate phrase pairs for us in translation, namely that of Koehn et al. (2003) (or perhaps more accurately, using the word-and phrasealignment scripts in Moses (Koehn et al., 2007) ). However, some researchers have pointed out that system performance can be increased when chunks induced by other methods (EBMT (Groves and Way, 2005) ; constituency parsers (Tinsley et al., 2007) ; dependency parsers (Hearne et al., 2008) ) are added to the SMT phrase table. 2 cf. also (Lopez, 2009) , who argues that due the lack of systematicity in MT system development, it is extremely difficult to compare systems purporting to be of different types, and nigh on impossible to pinpoint exactly to which component any gains in performance might accurately be attributed. The point is: adherence to one approach may lead to sub-optimal system performance; if any one phrase pair induced by some other method proves to be useful, then ignoring other approaches will cause translation performance to deteriorate, even when the data size is increased. Accordingly, in this paper we invesigated whether phrase pairs induced via head percolation (Magerman, 1995) might prove useful in PB-SMT. In a number of experiments, we showed that the number of chunks, and their content, was different for each of the four methods: STR, CON, DEP, and PERC. Furthermore, we showed that system perfomance improved significantly when PERC phrases were added to the phrase table of any other system. This was validated on two tasks for French-English: a small (JOC) and a larger (Europarl) dataset. Working on the JOC corpus allowed us to directly compare our novel phrase induction method against the work of ( Hearne et al., 2008) . While we could not improve upon their results for the JOC corpus, running experiments on the far larger Europarl data set showed clear performance gains over their method (dependencies using a parser) when the PERC phrases were utilised. In any case, our method would be useful in language pairs for which no separate dependency parser was available. It was also discovered through automatic evaluation measures that the 'S+C' system gave the best performance. However lack of statistical significance in the results and manual evaluation leads us to believe that PERC is useful enough to grant further investigation. Perhaps, better ways of combining the individual phrase tables need to be studied. As regards further work, we plan to conduct a more in-depth manual analysis to discover exactly what are the individual contributions of each of the phrase induction methods to translation quality. In addition to exploring some of the issues raised in section 4 we also intend to verify our results on larger data sizes, more domains and different language pairs. Acknowledgements Thanks to Sylwia Ozdowska for helping us with manual evaluation and John Tinsley for providing us with initial data. This work is supported by Science Foundation Ireland (grant number: 07/CE/I1142).",
         "16124289",
         "cc2691ec73683a647ca6b33fca320e3574aedd20",
         "7",
         "https://aclanthology.org/2009.mtsummit-posters.18",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Srivastava, Ankit  and\nWay, Andy",
         "Using Percolated Dependencies for Phrase Extraction in {SMT}",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "srivastava-way-2009-using",
         null,
         null
        ],
        [
         "44",
         "2020.iwltp-1.8",
         "Eco is Pangeanic's customer portal for generic or specialized translation services (machine translation and post-editing, generic API MT and custom API MT). Users can request the processing (translation) of files in different formats. Moreover, a client user can manage the engines and models allowing their cloning and retraining.",
         "Eco is Pangeanic's customer portal for generic or specialized translation services (machine translation and post-editing, generic API MT and custom API MT). Users can request the processing (translation) of files in different formats. Moreover, a client user can manage the engines and models allowing their cloning and retraining. Introduction Pangeanic is a language service provider (LSP) and language processing tool developer specialised in natural language processing and machine translation. It provides solutions to cognitive companies, institutions, translation professionals, and corporations. Pangeanic was the first company in the world to make use of the Moses statistical machine translation models in the translation industry (Yuste et al., 2010; Yuste et al., 2012) . To this purpose, a platform to build models by the user was developed (PangeaMT's first platform 1 ). Eco.pangeamt 2 is a platform managing translation engines and an NLP ecosystem. It allows the access of three types of user profiles: • Super Admin, is a reserved profile with which the translation infrastructure can be monitored and managed. • Client, is an admin profile that allows the management of users and their access rights and statistics. Clients can check metrics and usage of their users, manage the access of the users to the different engines and process files. Moreover, Clients can also manage their models, they can clone models and train them from a baseline with new bilingual material, thus automating the task of engine specialization. • User, this profile allows the processing of files and checking of information about usage and metrics of the API calls and processed files. After logging in, the home page of the website shows the Dashboard with the charts about statistics and usage (see Figure 1 ). The dashboard will be shown with information about the processes (translations) that have been carried out (processes per week, per month, total expenses, weekly, last processes, etc.). The options (appearing in the left-side menu) are: 1. New Process: in this page Clients can process files and check their processes. 2. Services/Processes: in this page, Clients can check the files that are being processed and the ones already finished. Here, they will find all the information about them. 3. Profile: here, Clients can change their name, email, password and billing information. 4. Stats: here Clients can check their API stats, File stats and in the Details tab they can check the number of characters, words, segments, files and pages processed by their Users. The Range Date can be set to check the statistics of a particular period of time. 5. Corporate: in this tab Clients can manage their models and engines. 6. Users: where the list of created users is displayed. For a user it is possible to check which engines can be accessed and data about the usage. New users can be created with credentials for their access and with an APIKey that can be used in API or other applications access. 7. Subscriptions: in this page, Clients can check the assigned subscription and manage it. User and Client profiles can directly translate text or send a file to be translated via the Eco platform. The system saves the files privately, only the file's owner has access to those files handling GDPR compliance. After processing, the translated file in its original format will be available to download. These features are described in services and processes (see Section 2.). One of the main features of Eco.pangeamt is the possibility of adapting a neural machine translation (NMT) model to the user's own data in a friendly user interface. This feature is presented in Section 3. Finally, conclusions are explained in Section 4. Services/Processes The services and processes option allows Clients to process new files or translate paragraphs or sentences directly. In order to start a process, Clients have to choose the Upload file or Translate text option. Processing a file For processing a file the Upload file option is selected, the user selects the source language of the document or documents and the target language (into which language it is translated). To upload the files, the user clicks on the gray box or drags the files to the box (see Figure 2 ). Once the files have been selected (the name of the selected files appears below the box) the user can click Start upload to upload them. A confirmation message will appear. A process must be carried out per language combination, i.e. if two files need to be translated from English to French, they can be uploaded together, if another file needs to be translated, for example from Japanese to Korean, another process must be carried out by pressing Send another. Clicking on List of processes will display the processes that are being carried out and those that have already been completed. In the Finished tab (see Figure 3 ), the details of the process are displayed: file name, process type, language combination and status. In the Actions column, the option to download the translated file appears. Once it has been downloaded, next to the download button, the Trash icon appears; pressing it deletes the selected file from the list of completed files. In the Dashboard page, Clients can check that the process has been added to the list of last processes. Translation of text If Translate text is selected, users can enter text to translate in the box, the source and the target language have to be selected and pressing Translate the translated text will appear (see Figure 4 ) as the output of the selected engine. 6 ). Clients can also manage their models and engines. By clicking models on the corporate menu, Clients find all their models and data. Here, they can check all the information about these models: which models they can clone and train, the language pairs, description, the model's father, updates, when it was last saved, etc. Adapting an NMT model via Eco In the Engines section, Clients can verify their engines and check which ones are granted to their users. If the Grant all option is activated all users will have access to the engine. Training models Eco makes training models easy thanks to its user friendly interface. Clients just have to click on the To Train icon and upload a bilingual file with language declaration or ID. The allowed file formats are preferably .tmx although .csv and .af (aligned format) are also accepted. Training files must contain perfectly aligned and recognisable source and target segments. Clients can decide the weight or aggressivity of the training. This affects how data will be incorporated into the model and its impact on the engine. A series of ML techniques weigh the data, its length, its vocabulary, etc. Effects on the model are to train it heavily on specific data to ultra-specialise it on the field of application or to just add domain data without changing severely whilst keeping its more generic features. Eco has 3 selectable levels of aggressivity from less to more weighing, shallower or deeper learning: Conservative, Normal or Aggressive (see Figure 7 ). The time needed for training depends on the size of the file and the level of aggressivity. Training is available with GPU making it much faster. After a training file has been sent, Clients can access the training page by clicking on the Trainings icon. This page shows the completed trainings, the requested ones and the failed trainings. If a training fails, the system notifies where the error is. The effectiveness of model retraining allowing its specialization in specific data is well known and it has been shown (Domingo et al., 2019a; Domingo et al., 2019b) . Pangeanic has run many trials with the training feature. For that, we used a generic English to Spanish transla-     The first test file (DGT test1) has been used to retrain the generic model. We used the 3 options of aggressivity and compared them with no training. We translated the 2 DGT test files (test1 and test2) and a generic test and we compared the results using the standard automatic translation metric BLEU score (Papineni et al., 2002) . The results are shown in Generic test results show a decrease in BLEU score when specializing in DGT domain, this is a normal behaviour because the model will translate better within the same domain. By contrast, the translation of DGT test1 file results show how BLEU score increases with the number of trainings as expected. Furthermore, when translating DGT test2 file, BLEU score improves using retraining. We expect this due to the fact that DGT test1 and test2 files are from the same domain. However, when translating DGT test2 file using aggressive training we obtain lower BLEU score than using conservative and normal training. This can be the case if the model has been adapted too much (overfitting) to data from DGT test1 file and translations to other files do not obtain the best results. Conclusion We have introduced Eco, Pangeanic's commercial translation platform describing its usage and different options. Eco incorporates a user friendly option for model adaptation. We have shown its effectiveness in a small set of experiments. This platform allows the translation of text translation-memory and documents as well as APIKey machine translation. The platform is hosted by Pangeanic but can be hosted by clients. Moreover, users are able to build their own models by cloning a generic model and can retrain those models with their own data and as many times as they wish to obtain specific results. Engines can be stored and recalled at a later date. These adapted models will adjust to their domain and generate translations with more quality for their purposes. As a result, machine translation output will be more accurate and productivity will increase due to a decrease in machine translation manual corrections. For future work, in addition to machine translation more tasks will be added to this platform such as anonymization, summarization or sentiment analysis. Acknowledgements The research leading to these results has received funding from the Spanish Centre for Technological and Industrial Development (Centro para el Desarrollo Tecnológico Industrial) (CDTI) and the European Union through Programa Operativo de Crecimiento Inteligente (Project IDI-20170964).",
         "218973998",
         "8d99dd7a62c65711aeccea6046040c3fe6048090",
         "0",
         "https://aclanthology.org/2020.iwltp-1.8",
         "European Language Resources Association",
         "Marseille, France",
         "2020",
         "May",
         "Proceedings of the 1st International Workshop on Language Technology Platforms",
         "Garc{\\'\\i}a-Mart{\\'\\i}nez, Mercedes  and\nHerranz, Manuel  and\nEstela, Amando  and\nFranco, {\\'A}ngela  and\nBi{\\'e}, Laurent",
         "{E}co.pangeamt: Industrializing Neural {MT}",
         "50--53",
         null,
         null,
         null,
         null,
         null,
         "979-10-95546-64-1",
         "inproceedings",
         "garcia-martinez-etal-2020-eco",
         "English",
         null
        ],
        [
         "45",
         "2012.iwslt-evaluation.13",
         "This paper presents the LIG participation to the E-F MT task of IWSLT 2012. The primary system proposed made a large improvement (more than 3 point of BLEU on tst2010 set) compared to our last year participation. Part of this improvment was due to the use of an extraction from the Gigaword corpus. We also propose a preliminary adaptation of the driven decoding concept for machine translation. This method allows an efficient combination of machine translation systems, by rescoring the log-linear model at the N-best list level according to auxiliary systems: the basis technique is essentially guiding the search using one or previous system outputs. The results show that the approach allows a significant improvement in BLEU score using Google translate to guide our own SMT system. We also try to use a confidence measure as an additional log-linear feature but we could not get any improvment with this technique.",
         "This paper presents the LIG participation to the E-F MT task of IWSLT 2012. The primary system proposed made a large improvement (more than 3 point of BLEU on tst2010 set) compared to our last year participation. Part of this improvment was due to the use of an extraction from the Gigaword corpus. We also propose a preliminary adaptation of the driven decoding concept for machine translation. This method allows an efficient combination of machine translation systems, by rescoring the log-linear model at the N-best list level according to auxiliary systems: the basis technique is essentially guiding the search using one or previous system outputs. The results show that the approach allows a significant improvement in BLEU score using Google translate to guide our own SMT system. We also try to use a confidence measure as an additional log-linear feature but we could not get any improvment with this technique. Introduction This paper describes LIG approach for the evaluation campaign of the 2012 International Workshop on Spoken Language Translation (IWSLT-2012), English-French MT task. This year the LIG participated only to the E-F MT task and focused on the use of driven decoding to improve statistical machine translation. In addition, we used much more parallel data than last year (trying to make use of the Giga-10 9 corpus). Some (un-successful) attempts to use confidence measures to re-rank our N-best hypotheses were also investigated. The remainder of the paper is structured as follows. Section 2 describes the data we used for training our translation and language models. Section 3 presents the concept of driven decoding that allowed us to get improvements using an auxiliary translation (of an online system) to guide the decoding process. Section 4 presents our attempt to use confidence measures and section 5 details the experiments as well as the LIG official results obtained this year. Resources used in 2012 The following sections describe the resources used to build the translation models as well as the language models. Translation models training data We built three translation models for our machine translation systems (see table 1 ). • An in-domain translation model trained on TED Talks collection (TED) corpus. • A (bigger) out-of-domain translation model trained on six different (freely available) corpora in which three of them are part of the WMT 2012 shared task training data: the latest version of the Europarl (version 7) corpus (EUROPARL 1 [1] ) the latest version of the News-Commentary (version 7) corpus (NEWS-C) the United Nations corpus (UN 2 [2] ) • We also used the Corpus of Parallel Patent Applications (PCT 3 ), the DGT Multilingual Translation Memory of the Acquis Communautaire (DGT-TM [3] ), and the EUconst corpus (EU-CONST [4] ). These three corpora are all freely available. • An additional out-of-domain translation model was trained on a subset of the French-English Gigaword corpus (GIGA-5M). After cleaning, the whole Gigaword corpus was sorted at sentence level according to the sum of perplexities of the source (English) and the target (French) based on two French and English pretrained language models. For this, LMs were trained separately on all the data listed in table 2 except the Gigaword corpus itself (the News Shuffle corpus was also available on the source English side). The separate LMs were then interpolated using weights estimated on dev2010 using EM algorithm (more details on this process are given in the next section). Finally, the GIGA-5M subset was obtained after filtering out the whole Gigaword corpus with a cut-off limit of 300 (ppl). This leads to a subset of 5M aligned sentences. These data were used to train three different translation tables in a multiple phrase table decoding framework (corresponding to the either option defined in the Moses advanced features). Language model training data For the language model training, in addition to the French side of all of the parallel corpora described above, we used the News Shuffle corpus provided by the WMT 2012 shared task. First a 5-gram back-off interpolated language model with the modified (improved) Kneser-Ney smoothing was trained on each resource using the SRI language modeling toolkit [5] . Then we created a merged LM optimized on a development corpus (dev2010) using EM algorithm. The details on these LM resources and their weights are given in table 2 . The table shows that the in-domain data obviously have a strong weight and that the LM trained on Gigaword subset is also well matched to the TED task. On the contrary, the 3 additional corpora PCT, DGT-TM and EU-CONST are the ones that lead to the highest perplexities and they seem quite far from the TED domain (PCT covers different topics like patents, EU-CONST is too small and DGT-TM covers a topic too far from TED). Development and test sets The TED dev2010 set (934 aligned sentences) was used for tuning and the TED tst2010 set (1 664 aligned sentences) was used for testing and making a choice on the best systems to be presented at the evaluation. These sets will be referred to as dev2010 and tst2010 in the rest of this paper. In addition, the TED tst2011 set (818 aligned sentences) and the TED tst2012 set (1 124 aligned sentences) were used for the official evaluation. Data pre-processing This year we used a fully in-house pre-processing. The goal was to use a more specific pre-processing and postprocessing steps for English as well as for French. In short, we applied the following steps: • filter out badly aligned sentences (using several heuristics) • filter out empty sentences and sentences having more than 50 words • filter out pairs of sentences where the ratio is more than 9 • punctuation normalization (extra punctuation mark deletion, transform several encodings of a same punctuation mark function to a canonical version, etc.) • tokenize (different to the default Moses tokenizer using French grammar rules) • truecase (remove case for the words at the beginning of the sentence while keeping information on the word position) • spell correction on both source and target sides • diacritics restoration (notably on uppercase letters at the beginning of sentences) • Unicode normalization (NFKC) • normalization of several words (e.g. coeur ) • disambiguate abbreviations and clitics • HTML entities conversion To clean the GigaWord corpus, we applied additional cleaning steps. Many heuristics (rules) were used in order to keep only good quality bi-texts. System configuration In the experiments reported here, 26 or 38 features (according to the total number of PT used) were used in our statistical machine translation system: 10 or 15 translation model scores, 14 or 21 distortion scores, 1 LM score, and 1 word penalty score. We used the Minimum Error Rate Training (MERT) method to tune the weights on dev2010 corpus. We are aware that in the future better optimization techniques like MIRA should be used for such a large number of parameters. Driven Decoding for SMT Recently, the concept of driven decoding (DD), introduced by [6] has been successfully applied to the automatic speech recognition (speech-to-text) task. This idea is to use an auxiliary transcription (coming from another system output or from another source of information) to guide the decoding process. There is a strong interest in applying this concept to statistical machine translation (SMT). The potential applications are: system combination, multi-source translation (from several languages, from several ASR outputs in the case of speech translation), use of an online system (like Google-translate) as auxiliary translation, on-line hypothesis re-calculation in a post-edition interface, etc. In short, our first attempt in driven decoding consists in adding several feature functions corresponding to the distance between the current hypothesis decoded (called H) and the auxiliary translation available (T) : d(T,H). Different estimation methods to calculate d(T,H) can be proposed : editdistance, metrics based-on information theory (entropy, perplexity), metrics based on n-gram coverage (BLEU), etc. As a first attempt, we started to experiment in a re-scoring framework for which N-Best hypotheses from the baseline MT system are re-ordered after adding the new feature functions proposed. Related Work This section presents a brief description of related works. They are found mainly in system combination for both speech recognition and machine translation. Unlike speech recognition, system combination in statistical machine translation involves systems based on potentially different standards such as phrasal, hierarchical and syntax based. This introduces new issues such as breaking up of phrases and alterations of word order. We first propose a description of the application of Driven Decoding (DD) algorithm in ASR systems. Then, various system combination attempts in Machine Translation are presented. Detailed presentation of these two concepts -DD and SMT systems combinationis needed to understand our approach. Imperfect transcript driven speech recognition In the paper introduced by [6] , the authors try to make use of auxiliary textual information associated with speech signals (such as subtitles associated to the audio channel of a video) to improve speech recognition performance. It is demonstrated that those imperfect transcripts which result in misalignments between the speech and text could actually be taken advantage of. In brief, two methods were proposed. The first method involved the combination of generic language model and a language model estimated on the imperfect transcript resulting in cutting down the linguistic space. The second method involved modifying the decoding algorithm by rescoring the estimate function. The probability of the current hypothesis which results from partial exploration of the search graph is dynamically rescored based on the alignment (with imperfect transcript) scores (done using Dynamic Time Warping). The experimental results which used both dynamic synchronization and linguistic rescoring displayed interesting gains. Another kind of imperfect transcript that can be used is the output hypothesis of another system, leading to an integrated approach for system combination. Thus, in the same paper is proposed a method in which the outputs of the contrastive system drives the decoder of the primary system. The results showed that the new system run by driven decoding algorithm outperformed both primary and contrastive systems. Various cross adaptation schemes were also examined. The principle proposed is that firstly, one-best hypothesis is generated from the auxiliary system and a confidence score is evaluated for each word. Then these informations are used to dynamically modify the linguistic score during decoding. The method was evaluated on a radio broadcast transcription task and it was found that WER reduced significantly (about 1.9%) . The WER gain was even better (2.9%) by combining DD and cross adaptation. System Combination for Machine Translation -Confusion Network (CN) Decoding There are important issues to address for machine translation system combination using confusion network decoding. An important one is the presence of errors in the alignment of hypotheses which lead to ungrammatical combination outputs. [7] proposed arbitrary features that can be added loglinearly into the objective function in this method. This addition of new features is the core idea we followed in our proposal. Confusion Network decoding for MT system combination has been proposed in [8] . The hypothesis have to be aligned using Levenshtein alignment to generate the confusion network. One hypothesis is chosen as skeletal hypothesis and others are aligned against it. In [7], 1-best output from each system is used as the skeleton to develop the confusion network and the average of the TER scores between the skeleton and other hypotheses were used to evaluate the prior probability. Finally a joint lattice is generated by aggregating all the confusion networks parallely. Through this work it is shown that arbitrary features could be added log-linearly by evaluating log-posterior probabilities for each confusing network arc. In confusion network decoding, the word order of the combination is affected by the skeletal hypothesis. Hence the quality of the output from the combination also depends on the skeletal hypothesis. The hypothesis with the minimum average TER-score on aligning with all other hypothesis is proposed as an improved skeletal hypothesis. E s = arg min E∈Ei Ns j=1 T ER(E j , E i ) (1) where N s is the number of systems and E s is the skeletal hypothesis. In [9] system specific confidence scores are also introduced. The better the confidence score the higher the impact of that system. In the experimental part of this same work, three phrase-based (A,C,E), two hierarchical (B,D) and one syntax based (F) systems are combined. All of them are trained on the same data. The decoder weights are tuned to optimize TER for systems A and B and BLEU for the remaining systems. Decoder weight tuning is done on the NIST MT02 task. The results of the combination system were better than single system on all the metrics but for only TER and BLEU tuning. In the case of METEOR tuning, the combination system produced high TER and low BLEU score. The experiments were performed on Arabic and Chinese NIST MT tasks. -N-Best Concatenation and Rescoring Another paper [10] presents a slightly different method where N-Best hypotheses are re-scored instead of building a synthesis (CN) of the MT outputs (as described in previous sub-section). The N-Best list from all input systems are combined and then the best hypothesis is selected according to feature scores. Three types of features are: language model features, lexical features, N-Best list based features. The feature weights are modified using Minimum Error Rate Training (MERT). Experiments are performed to find the optimal size for N-Best list combination. Four systems are used and analysed on combination of two best systems and all the systems. 50-best list was found to be optimal size for both cases. The authors showed that the impact of gradually introducing a new system for combination becomes lower as the number of systems increases. Anyway the best result is obtained when all of the systems are combined. -Co-decoding Recently, the concept of collaborative decoding (codecoding) was introduced by [11] to improve machine translation accuracy by leveraging translation consensus between multiple machine translation decoders. Different from what we described earlier (postprocess the n-best lists or word graphs), this method uses multiple machine translation decoders that collaborate by exchanging partial translation results. Using an iterative decoding approach, n-gram agreement statistics between translations of multiple decoders are employed to re-rank full and partial hypotheses explored in decoding. Overview of the Driven Decoding Concept Driven Decoding As said in the introduction part, driven decoding consists in adding several feature functions to the log-linear model before N-Best list re-ordering. Practically, after N-Best lists are generated by an individual system, additional scores are added to each line of the N-Best list file. Theses additional scores correspond to the distance between the current hypothesis decoded (called H) and the auxiliary translation available (T) : d(T,H). Let's say that 2 auxiliary translations are available (from system 1 and system 2) and that 4 distance metrics are available (BLEU, TER, TERp-A and PER); in that case, 8 scores are added to each line of the N-Best list. The distance metrics used in our experiments are described in the next section and then N-Best reordering process is detailed. Distance Metrics used The distance metrics used are Translation Error Rate (TER), Position independent Error Rate (PER), TERp-A and BLEU [12] . The TER score reflects the number of edit operations (insertions, deletions, words substitutions and blocks shifts) needed to transform a hypothesis translation into the reference translation, while the BLEU score is the geometric mean of n-gram precision. Lower TER and higher BLEU score suggest better translation quality. In addition, we use PER score (position independent error rate) which can be seen as a bag-of-words metric potentially interesting in the context of the driven decoding proposed. In addition we use TERp [13] which is an extension of TER eliminating its shortcomings by taking into account the linguistic edit operations, such as stem matches, synonyms matches and phrase substitutions besides the TER's conventional ones. These additions allow us to avoid categorizing the hypothesis word as Insertion or Substitution in case that it shares same stem, or belongs to the same synonym set represented by WordNet, or is the paraphrase of word in the reference. More precisely, we used TERp-A, another version of TERp, in which each above mentioned edit cost has been tuned to maximize the correlation with human judgment of Adequacy at the segment level (from the NIST Metrics MATR 2008 Challenge development data). However, it is worth mentionning that for this particular task, we use a degraded version of TERp-A which does not take into account synonymy, because the target language is French while the TERp-A metric only implements the use of (English) Wordnet. N-Best Reordering and Combination In this framework the system combination is based on the 1000-best outputs (we generally have less on IWSLT data) generated by the LIG primary system using the \"uniq\" option. Our primary system uses 3 different translation and re-ordering tables. So each N-best list is associated with a set of 38 scores: 1 LM score, 15 translation model scores, 1 distance-based reordering score, 21 lexicalized reordering scores. In addition we introduce 8 distance metrics scores for each sentence. -The training step The score combination weights are optimized in order to maximize the BLEU score at the sentence level. This step is performed by using the MERT tool. The weights of \"standard\" scores are initialized with the tuned weights computed during the usual tuning phase. In a second time, we fine tune weights of the introduced distance metrics (this can be seen as an additional iteration of MERT). -The decoding step The decoding step combines all the scores: a global score is computed for each sentence (i.e. the log-linear score ) and sentences are reordered according to the final combined score. Use of Confidence Measures for SMT Besides driven decoding (DD) scores, a sentence confidence score can be added as an additional feature in the N-best list to improve the re-ordering performance. To obtain such a confidence score, a classifier must be constructed. We concatenate two data sets dev2010 + tst2010 to form the training data. Features used to train our model come from the baseline features of the WMT2012 quality estimation shared task (features originally presented in [14] ), which can be summarized as follows: • Source and target sentence: number of tokens and their ratio, number of punctuation marks. • Source and target sentence's language model probabilities. • Percentage of unigrams / bigrams / trigrams in quartiles 1 (and 4) of frequency (lower and higher frequency ngrams) in a corpus of the source language. • Average number of translation per source word in the sentence, unweighted or weighted by the inverse frequency of each word in the source corpus. The core element needed for the classifier construction process is the training label for each sentence. The TERp-A metric [13] , which we select to perform this task, provides the linguistic and semantic matching between each sentence in training set and its reference (available for dev2010 and tst2010 corpora), then yields the minimum cost for matching normalized by its number of tokens as its score. We then categorize them in a binary set: sentences with score higher than 0.3 is assigned with \"Good\" (G) label, otherwise, \"Bad\" (B). A CRF-based toolkit, WAPITI [15] , is then called to build the classifier. The training phase is conducted using stochastic gradient descent (SGD-L1) algorithm, with values for maximum number of iterations done by the algorithm (-maxiter), stop window size (-stopwin) and stop epsilon (-stopeps) to 200, 6, and 0.00005 respectively. Applying this classifier in both test sets (test2011 + test2012, with WAPITI's default threshold = 0.5) gives us the result files detailing hypothesized label along with its probability at the sentence level. Then, the confidence score used is the probability of sentence to be regarded as a \"Good\" sentence. For instance, a sentence classified as \"G\" with related probability of 0.8 gets obviously the confidence score of 0.8; meanwhile the other one labeled as \"B\" with probability of 0.7 will have the score of 0.3. This score is used as an additional feature in the log-linear model just as it is done for driven decoding (see previous section). Performance of the re-ordering task with and without the use of confidence measure will be shown in Table 3 . Experimental Results of LIG Systems We recall that our systems were systematically tuned on dev2010 corpus. Our baseline system, trained as described in section 2, lead to a BLEU score of 30.28 on tst2010 using 2 translation and re-ordering models (no GIGAword) while it improves to 30.80 using 3 translation and reordering models (using GIGAword). This result has to be compared with 27.58 obtained on tst2010 with our system last year. As far as the driven decoding is concerned, the results show that using the Google 1best hypothesis to guide the rescoring of the LIG Nbest list leads to significant improvements on all data sets. On dev2010 data, the performance obtained is even better that both LIG and Google systems evaluated separately. On tst2010 and tst2011 the driven decoding is slightly below google. This can be explained by the fact that google has a very different behavior from one set to another (on the dev google is significantly worse than LIG system while he gets better results on tst2011). The LIG system driven by Google 1best was, however, not submitted as a primary run since we used an online system to improve our own module (contrastive system). On the contrary, adding confidence measures gives only slight improvement on the dev2010 set and does not generalize on tst2010 so it was finally not used in our final submission. According to our analysis, this unsuccessful experiment can be originated from the following reasons: (1) The feature set is simply and superficially constructed hence fails to cover all aspect of quality. This hypothesis can motivate us to explore more types of features (lexical, syntactic, semantic...) in the future work ; (2) the whole combination of features without any selection strategy might be an unskilful option weakening our classifier capability. For information, the oracle obtained, using the golden reference as an auxiliary system, is given in the last line of the table, as well as the performance of the online Google system. Conclusions This paper described the LIG participation to the E-F MT task of IWSLT 2012. The primary system proposed made a large improvement (more than 3 point of BLEU on tst2010 set) compared to our last year participation. Part of this improvement was due to the use of an extraction from the Gigaword corpus. We have proposed a preliminary adaptation of the driven decoding concept for machine translation. This method allows an efficient combination of machine translation systems, by rescoring the log-linear model at the N-best list level according to auxiliary systems: the basis technique is essentially guiding the search using one or previous system outputs. The results show that the approach allows a significant improvement in BLEU score using Google translate to guide our own SMT system (such system was submitted as contrastive since it uses an online translation). We also tried to use a confidence measure as an additional log-linear feature but we could not get any improvement with this technique.",
         "2443354",
         "f00bc91161a92667309feeda31545b82e9ec1ec8",
         "13",
         "https://aclanthology.org/2012.iwslt-evaluation.13",
         null,
         "Hong Kong, Table of contents",
         "2012",
         "December 6-7",
         "Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign",
         "Besacier, Laurent  and\nLecouteux, Benjamin  and\nAzouzi, Marwen  and\nLuong, Ngoc Quang",
         "The {LIG} {E}nglish to {F}rench machine translation system for {IWSLT} 2012",
         "102--108",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "besacier-etal-2012-lig",
         null,
         null
        ],
        [
         "46",
         "2009.mtsummit-posters.12",
         "In most statistical machine translation (SMT) systems, bilingual segments are extracted via word alignment. In this paper we compare alignments tuned directly according to alignment F-score and BLEU score in order to investigate the alignment characteristics that are helpful in translation. We report results for two different SMT systems (a phrase-based and an n-gram-based system) on Chinese to English IWSLT data, and Spanish to English European Parliament data. We give alignment hints to improve BLEU score, depending on the SMT system used and the type of corpus.",
         "In most statistical machine translation (SMT) systems, bilingual segments are extracted via word alignment. In this paper we compare alignments tuned directly according to alignment F-score and BLEU score in order to investigate the alignment characteristics that are helpful in translation. We report results for two different SMT systems (a phrase-based and an n-gram-based system) on Chinese to English IWSLT data, and Spanish to English European Parliament data. We give alignment hints to improve BLEU score, depending on the SMT system used and the type of corpus. Introduction Most statistical machine translation (SMT) systems (e.g. phrase-based, n-gram-based) extract their translation models from word alignment trained in a previous stage. Many papers have shown that alignment quality is poorly correlated with MT quality (for example Vilar et al. (2006) ). Then, we can tune the alignment directly according to MT metrics (Lambert et al., 2007) . In this paper we rather try to find out which alignment characteristics help or worsen translation. In the related papers (see next section) some alignment characteristics are usually considered, and the impact on MT of alignments with different values for these characteristics is evaluated. The contributions of this paper are twofold. Firstly, the problem is considered from the inverse point of view: we start from an initial alignment and tune it directly according to a translation quality metric (BLEU score (Papineni et al., 2002) ) and according to an alignment quality metric (F-score, see Section 4.3) . In this way, we can investigate for any alignment characteristic how it is affected by the change of tuning criterion. If there exist alignment characteristics which are helpful in translation, they should not depend on the aligner used. However, they could depend on the MT system, the language pair, or the corpus size or type. The second contribution of this paper is to study more systematically how the considered characteristics depend on these parameters. We report results for two different SMT systems: a phrase-based system (Moses (Koehn et al., 2007) ) and an n-gram-based system (Crego and Mariño, 2007) . We performed this comparison on two different tasks: translation from Chinese to English, trained with IWSLT data (BTEC corpus, a small corpus in the travelling domain), and translation from Spanish to English, trained on a 2.7 million word corpus of the European Parliament proceedings. First we discuss related work. In Section 3, we describe the alignment optimisation procedures according to F-score and BLEU, and give more details on the alignment system used. Then in Section 4, we provide a summary of the experiments performed on each task, together with a description of the data used. In Section 5, the results are discussed. Finally, some conclusions are provided together with avenues for further research. Related Work In this section, we review some alignment characteristics that have been observed to have some impact in phrase extraction and MT output, for the SMT approaches we consider in this paper. We will thus consider these characteristics (and more) to investigate what kind of alignment helps depending on the system and type or amount of training data. In several papers the impact of higher precision or higher recall alignments has been studied. Ayan and Dorr (2006) and Chen and Federico (2006) observed that higher precision alignments favoured a phrase-based SMT system. In the former case, it was observed with an English Chinese training corpus of 1.1 million running words (for the English side), and with an English Arabic corpus of 3.3 million words of News and treebank data. In the latter case, the BTEC corpus was used (Chinese, Japanese, Arabic and Italian to English, with 180k English words). Fraser and Marcu (2007) compared the performance of translation systems trained on several alignments of varying quality. Their results on large corpora do not confirm the hypothesis that higher precision alignments help phrase-based SMT systems more than higher recall alignments. For example, among their 3 systems trained on a 67 million word French English corpus, the highest precision alignment has the best BLEU score when alignment quality is low, and the highest recall alignment is the best when alignment quality is high. Their results suggest that when there is not enough data to produce good quality alignments, increasing the alignment precision improves phrase-based SMT systems. However, with larger corpora, higher recall alignments seem to be better. Our experiments give some more insight on this point. Mariño et al. (2006) observed that a higher recall alignment improved the performance of an n-grambased translation model on the Europarl corpus (35 million running words). Another important issue in the extraction of bilingual segments is the presence of long-distance links. Vilar et al. (2006) improved the translation quality of a German English phrase-based SMT system by deleting links between the English verb and the German particle part of the verb, which is situated far from the main part of the verb and produces a longdistance link. This issue is particularly relevant for the n-grambased system, where a unique segmentation of the sentence pair is performed. It is nevertheless partially addressed by the reordering strategy, which is based on a monotonisation of the alignment prior to the extraction of bilingual phrases. In this monotonisation process, the source words are reordered. Thus long-distance links are only problematic when a source word is linked to two or more non-adjacent target words. In this case, the possible bilingual units involving the words embedded between these target word positions cannot be extracted. Long-distance links can also restrict the number of bilingual phrases which are extracted in Moses. In this case, the bilingual phrases involved with embedded words are still extracted. However, those bilingual phrases involving the long-distance oneto-many link itself may be large and thus not easy to reuse. The same problem may happen with long crossing links. Thus we expect that alignments optimised according to BLEU will have shorter links, or shorter crossing links, or fewer embedded words than manual alignments. Alignment Optimisation Procedure Our aim was to obtain alignments optimised according to both an intrinsic and an extrinsic criterion. To achieve this, we used a discriminative alignment system (Moore, 2005) because of its flexibility. For both criteria, the optimisation consisted of maximising a function of the alignment system parameters: F-score (intrinsic criterion) and BLEU score (extrinsic criterion). First we describe the alignment system used, then the optimisation procedure. Discriminative Alignment System This aligner implements a log-linear combination of feature functions calculated at the sentence pair level. In a first pass, the training corpus was aligned selecting for each sentence pair (s, t) the alignment hypothesis â which maximises a combination of various models, as expressed in (1): â(1) = arg max a λ (1) a1 h a1 + λ (1) a2 h a2 + λ (1) lb h lb + λ (1) um h um + λ (1) cn h cn + λ (1) cl h cl + λ (1) hp h hp (1) where h stands for the feature functions h(s, t) used, and the λs are their corresponding weights. h a1 and h a2 are word association models based on source-target and target-source IBM model 1 probabilities (Brown et al., 1993) . h lb is proportional to the number of links in a. h um is an unlinked word model proportional to the IBM model 1 NULL link probability. h cn and h cl are distortion models, counting respectively the number and amplitude (the difference between target word positions) of crossing links. Finally, h hp is a \"hole penalty\" model, proportional to the number of embedded positions between two target words linked to the same source words (or vice-versa). We performed a second alignment pass in which the association score model with IBM1 probabilities and the unlinked model were substituted by two improved models benefiting from the first-pass links: an Association score model h ar with Relative link probabilities (Melamed, 2000) , and source and target fertility models (h f s and h f t ) giving the probability for a given word to have one, two, three or four or more links. Second pass models are listed in (2). â(2) = arg max a λ (2) ar h ar + λ (2) lb h lb + λ (2) f s h f s + λ (2) f t h f t + λ (2) cn h cn + λ (2) cl h cl + λ (2) hp h hp (2) To find the best hypothesis, we implemented a beam-search algorithm based on dynamic programming. In a given sentence pair, the best 3 links for each source and for each target word are considered in search. The parameters of the first and second alignment passes were optimised together, to give the following objective function: 1 F U N CT ION (λ (1) a2 , λ (1) lb , λ (1) um h, λ (1) cn , λ (1) cl , λ hp , λ (2) lb , λ (2) f s , λ (2) f t , λ (2) cn , λ (2) cl , λ (2) hp ), where F U N CT ION refers either to F or to BLEU . With this many parameters, an optimisation algorithm was necessary (see Section 3.3). Optimisation Set-up As mentioned above, the following objective functions were maximised: F ({aligner parameters}) (3) BLEU ({aligner parameters}) (4) In the case of Function (3), the whole training corpus was aligned for the first pass (Equation 1 ). For the second pass, only manually aligned development data were aligned to calculate the F-score (see Section 4). This constitutes the first iteration of the optimisation algorithm, realised with initial parameters. 1 The weights in each pass can be normalised such that one weight is set to 1. This is why λ (1) a1 and λ (2) ar were not free parameters. Then, alignment system parameters were simply adjusted by the optimisation algorithm so as to maximise the F-score. In the case of Function (4), the training corpus was aligned with initial parameters and these alignments were used to build either an n-gram-based or a phrase-based SMT system. The model weights were tuned via MERT (Och, 2003) , with the Moses MERT utility (which was adapted to the n-grambased system). Then a translation of a development corpus was obtained and evaluated using BLEU (see Section 4). Thus, at each iteration, the considered parallel corpus was aligned (with the two successive passes), an SMT system was built from the resulting alignments (including bilingual phrase extraction, model(s) estimation and MERT) and the development set was translated to obtain the BLEU score. At the end of this process we obtained the alignment parameters which maximise the BLEU score. Note that we used two developments sets: one for the alignment weight optimisation, one for MERT. Optimisation Algorithm The optimisation procedure was performed using the SPSA algorithm (Spall, 1992) . SPSA is a stochastic implementation of the conjugate gradient method which requires only two evaluations of the objective function, regardless of the dimension of the optimisation problem. The SPSA procedure is in the general recursive stochastic approximation form: λk+1 = λk − a k ĝk ( λk ) (5) where ĝk ( λk ) is the estimate of the gradient g(λ) ≡ ∂E/∂λ at the iterate λk based on the previous evaluations of the objective function. a k denotes a positive number that usually decreases as k increases. We performed about 80 evaluations of the objective function. Note that in general, SPSA converges to a local maximum. Experiments Chinese English BTEC Task The experiments were carried out using the Chinese-English datasets provided within the IWSLT 2007 evaluation campaign, extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002) . Training data consisted of the default training set, to which we added the sets devset1, devset2 and devset3. The resulting corpus contains 41.5k sentence pairs having respectively 9.4 and 8.7 words on average for English and Chinese. English and Chinese vocabulary sizes are respectively 9.8k and 11.4k. Manual annotation of word alignment was carried out on devset3, of which 251 sentence pairs were used as the development set and 251 for testing. For MT evaluation, we used IWSLT 2006 test set (500 sentences, 6.1k words, 7 references) as development set for the internal SMT MERT procedure, and devset4 (489 sentences, 5.7k words, 7 references) as development set to calculate the BLEU score at each optimisation iteration. Our test set was IWSLT 2007 test set (489 sentences, 3.2k words, 6 references). Spanish English Europarl Task Another set of experiments was conducted using a part of the TC-STAR OpenLab 2 Spanish English EPPS parallel corpus, which contains proceedings of the European Parliament. We randomly selected 100k sentence pairs, having respectively 27.2 and 28.4 words on average for English and Spanish. English and Spanish vocabulary sizes are respectively 38k and 55k words. To calculate F-score we used freely available 3 alignment test data (Lambert et al., 2005) .We randomly divided the alignment test data into a 246 sentence development set and a 245 sentence test set. For MT evaluation, we had a development set of 735 sentences for the internal SMT MERT procedure, a development set of 1008 sentences to calculate the BLEU score at each optimisation iteration, and a test set of 1094 sentences to realise an extrinsic evaluation of the optimal alignment system. All three sets had two references. Evaluation Intrinsic (i.e. alignment) evaluation was performed with precision (P ), recall (R) and F-score (F ). In both tasks, the manual alignment reference contained mainly unambiguous (or Sure) links and some possible links (respectively 33.3% and 12.9% for Spanish English and Chinese English references). The scores were calculated in the following way: P = |A ∩ G| |A| , R = |A ∩ G S | |G S | , F = 2P R P + R , where A, G S and G are respectively the computed link set, the reference sure link set, and the total reference link set. Extrinsic evaluation was performed with the BLEU score (Papineni et al., 2002) . Translations were computed either by Moses (Koehn et al., 2007) with all default parameters, or by a baseline n-gram-based system with constrained reordered search (Crego and Mariño, 2007) . Results We present intrinsic and extrinsic evaluation results as well as some statistics for 9 alignment sets. 3 sets are baseline sets, and correspond to combinations of the Giza++ (Och and Ney, 2003) source-target and target-source alignments computed by Moses scripts: intersection (I), union (U) and grow-diagfinal heuristic (GDF) (Koehn et al., 2003) . The other sets were aligned with the optimum weights of the discriminative aligner (Section 3.1) resulting from optimisations according to the F-score, to the phrase-based system BLEU score and to the ngram-based system BLEU score (referred to as F, PB and NB, respectively). Because the optimisation algorithm can get stuck in a poor local maximum, the optimisation with each criterion was performed with three different random seeds. To have an idea of the error introduced by the optimisation process, we kept the weights of the two optimisations which reached the highest values in the development set. They are denoted with index 1 or 2 (as in F1 and F2). Intrinsic and Extrinsic Evaluations Table 1 shows the evaluation results for the different computed alignments on the Chinese English and Spanish English tasks. First, note that the optimisation procedure was effective since for each score the best systems built from discriminative alignments were those optimised with this score as objective. Note also that although the discriminative aligner could not achieve better alignments in terms of F-score than Giza++,  it was able to produce alignments resulting in better MT systems than with Giza++ combinations, except for the Spanish English n-gram system. On Chinese English data, the impact on recall or precision of optimising alignment according to the phrase-based system BLEU score is not clear. For the n-gram based system, the effect is a decrease of alignment precision. For the Spanish English task, recall is slightly better for systems PB1 and PB2, and precision is lower for both NB and PB systems. Except for the Chinese English phrase-based system, the main effect of tuning alignment according to BLEU score seems therefore to be a decrease in precision. This suggest that in those cases, alignment precision is less relevant when the end-product is MT than when it is word alignment itself. The MT evaluation reveals that the phrase-based system is fairly robust across alignment variations on this type of corpora. The variation in BLEU is 8% relative on IWSLT data, and only 1.8% relative on the Europarl data. The n-gram-based system is more sensitive to word alignment differences on IWSLT data, but not on the Europarl data (17% and 1.6% relative variation respectively). Finally, we observed that the discriminative aligner has some difficulty to produce high recall alignments. In the next sections we analyse the impact of the word alignment differences in the phrase table of the phrase-based and n-gram-based systems. Figure 1 shows a clear relation, on both tasks, between the phrase table size, the ambiguity of the phrase table (number of bilingual phrases per source phrase) and the number of links. The more links, the less bilingual phrases, and the less ambiguous is the model. Thus higher precision alignments will increase the coverage of the phrase-based system and will be most helpful when little training data is available. Higher recall alignments will produce less ambiguous and thus more accurate models. Therefore, this type of alignment will be useful when enough data is available so that the coverage is not the main issue. This is in agreement with the hypothesis proposed in Section 2. Moses Phrase Table Analysis This hypothesis is further confirmed by the results depicted in Figure 2 , in which the BLEU score is plotted versus the number of untranslated words (words of the translation output which were not seen in the target training corpus but were seen in the source training corpus). For IWSLT data, the less untranslated words, the higher the BLEU score. Therefore, with this small corpus the coverage is the main way of improving translation quality, and increasing alignment precision will often result in higher BLEU scores. The story is actually not so simple since there are other parameters as precision involved. Giza++ intersection is for example the alignment with highest precision, but yielded more untranslated words (and a lower BLEU score) than the discriminative alignments. For the Spanish English task, there is no clear relation between BLEU score and the number of untranslated words. This suggests that with this amount of data the coverage is not the main issue any more. In this case one alignment characteristic which helps increasing the accuracy of the SMT model is the recall. The highest recall alignment for each aligner (U and PB2) yielded indeed the best BLEU scores (although not directly depending on the recall value). In Section 5.4 we investigate some other alignment characteristics which may be useful on this type of task. N-gram-based Phrase Table Analysis Although we cannot display all curves because of space limitation, the BLEU score versus number of untranslated words relation for the n-gram-based system is similar to the one of the phrase-based system. On the small Chinese English corpus, the less untranslated words, the higher BLEU score. On the larger Spanish English corpus, there is no apparent relation between the two quantities. Thus the coverage is still the main problem on IWSLT data for the n-gram-based approach, whereas model accuracy is probably more important when more data is avail- able. A difference with respect to the phrase-based approach is how more coverage is achieved. In the phrase-based approach, the bilingual phrase vocabulary increases as the number of links decreases (Figure 1 ). With the n-gram-based system, the relation is reversed, as depicted in Figure 3 for the Chinese English corpus (we observe a similar behaviour on Spanish English data). Except for Giza++ Union (rightmost point, at the bottom corner), the more links, the larger the bilingual units vocabulary. One possible explanation to this is that bilingual phrases with no target word (target nulled phrases) are allowed in the n-gram-based model. When the number of links increases, target nulled phrases are replaced by target phrases with words and the bilingual phrase vocabulary is enriched. As in the phrasebased approach, the model ambiguity is also reduced as the number of links increases. Thus we do not have in this case a trade-off between coverage and accuracy. As a result, higher recall alignments may be better than higher precision alignments, even for small corpora. This is the case for Giza++ union and GDF on the Chinese English task, which yielded a higher BLEU score than the intersection. However F1 has a higher recall than F2, but a much lower BLEU score. Table 2 shows the average link length, the number of crossing links, the distortion and the number of source and target embedded words (see Section 2) per sentence, on the Spanish English task. The length of a link is defined as the absolute value of the link target and source word position difference: length = position source − position target . To calculate the number of crossing links, we sort the links according to the source and target positions as first and second keys. When the target position of a link is less than that of the anterior link according to that order, and the source position is different, we count a crossing link. The crossing (i.e. distortion) amplitude is the difference between both links target positions. Link Links of the PB and NB alignments are shorter than in the test data. This might be due to the difficulties caused by long-distance links to the considered MT systems. However they are not shorter than links in F alignments. The alignment system might indeed discard long-distance links to gain precision. The number of crossing links is at least as high in PB1 and PB2 systems than in the test data. However, the length of crossing links is clearly lower. This suggests that the phrase-based system BLEU score can be improved by avoiding too many long-distance crossing links. The average distortion is actually even lower for NB1 and NB2 alignments. Therefore, removing well selected longdistance crossing links may be another alignment clue to improve SMT systems. The number of embedded words is much lower in the discriminative alignments, probably due to the difficulty of our aligner to produce high recall alignments. The n-gram-based system was expected to avoid target embedded words. This is observed if we compare NB and PB systems, but we don't understand why there are less source embedded words than target embedded words in the discriminative alignments. Conclusions and further work We tracked helpful alignment characteristics for MT by tuning a discriminative alignment system according to alignment F-score and translation BLEU score, and compared the resulting alignments and their impact on MT quality (evaluated with the BLEU score). We conducted this experiment for two SMT systems and on two distinct tasks, representing different corpus sizes and language pairs. Our conclusion is that the alignment characteristics which help in translation greatly depend on the MT system and on the corpus size. Some related work and our results suggest that with small corpora, the coverage is the main issue governing translation quality. In the phrase-based system, coverage may be increased by increasing the alignment precision. Thus higher precision alignments may yield better SMT systems. In the n-grambased system, higher recall alignments may still be better than higher precision ones, even on small corpora. Experiments on a fraction only of our Spanish English data would be an interesting future experiment to confirm these statements. When more data are available, higher recall alignments allow to build less ambiguous and more accurate models. Scaling these experiments to larger corpora, as suggested above, would also be interesting to confirm this. Our results on the Spanish English data set also suggest that the phrase-based system may be improved if the alignment is more monotonic, that is if links are shorter and there is less distortion. This might also depend on the language pair, so we could repeat the experiment on a Chinese English corpus of similar size as our Spanish English corpus. Acknowledgements This research is supported by the Science Foundation Ireland (www.sfi.ie) (Grants 05/IN/1732 and 07/CE/I1142) and was carried out using the Irish Center for High-End Computing resources.",
         "14728979",
         "c5ba0623a8efc91ce16fee91da7cfb47b263591d",
         "9",
         "https://aclanthology.org/2009.mtsummit-posters.12",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Lambert, Patrik  and\nMa, Yanjun  and\nOzdowska, Sylwia  and\nWay, Andy",
         "Tracking Relevant Alignment Characteristics for Machine Translation",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "lambert-etal-2009-tracking",
         null,
         null
        ],
        [
         "47",
         "W09-4637",
         "In cases when phrase-based statistical machine translation (SMT) is applied to languages with rather free word order and rich morphology, translated texts often are not fluent due to misused inflectional forms and wrong word order between phrases or even inside the phrase. One of possible solutions how to improve translation quality is to apply factored models. The paper presents work on English-Latvian phrase-based and factored SMT systems and, using evaluation results, demonstrates that although factored models seem more appropriate for highly inflected languages, they have rather small influence on translation results, while using phrase-model with more data better translation quality could be achieved.",
         "In cases when phrase-based statistical machine translation (SMT) is applied to languages with rather free word order and rich morphology, translated texts often are not fluent due to misused inflectional forms and wrong word order between phrases or even inside the phrase. One of possible solutions how to improve translation quality is to apply factored models. The paper presents work on English-Latvian phrase-based and factored SMT systems and, using evaluation results, demonstrates that although factored models seem more appropriate for highly inflected languages, they have rather small influence on translation results, while using phrase-model with more data better translation quality could be achieved. Introduction In the last decade statistical machine translation (SMT) has become one of the most popular approaches in the field of automated translation. SMT started with word-based models, but significant advances were made with the introduction of phrase-based models. Statistical Machine Translation tries to generate translations on the basis of statistical models, with parameters derived from the analysis of bilingual text corpora. SMT approach is language independent, but it requires large bilingual corpora for training. If such corpora are available, good results can be achieved in translating texts of a similar kind. The main advantage of SMT approach is a possibility to build up the system in a relatively small period of time. One of the prerequisites for classical SMT systems is availability of large parallel corpus which computer then uses in the training process. The lack of large parallel corpus is the main reason why experiments with SMT in Baltic countries have been started only recently, i.e., implementation of Estonian-English (Fishel et al., 2007) and English-Latvian (Skadiņa and Brālītis, 2007) SMT systems have been reported only in 2007. Phrase-based models (Koehn et al., 2003) typically deals with words or phrases thus often generating wrong form if the text is translated into morphologically rich language. In factored translation models (Koehn and Hoang, 2007) , the surface forms are augmented with factors, such as grammatical information and base form. Thus factored models usually improve machine translation performance for problems such as morphology, free word order, and sentence-level grammatical coherence. For instance, English-Czech factored SMT reached 27.04% BLEU for all morphological features and 27.45% BLEU for selected morphological features, in comparison to the baseline of 25.82% BLEU (Koehn and Hoang, 2007) . The paper presents application of factored approach to English-Latvian SMT and discusses evaluation results, demonstrating that simple factored models have no enough influence on translation quality, i.e., with phrase-based models and more data better results could be achieved as with factored models and less data. English-Latvian factored translation model Latvian language is typical representative of morphologically rich languages. Almost all open word classes, i.e., nouns, adjectives, numerals, pronouns, and verbs, are inflective. Latvian nouns and pronouns have 6 cases in both singular and plural. Adjectives, numerals and participles have 6 cases in singular and plural, 2 genders and definite and indefinite form. In Latvian conjugation system there are two numbers, three persons and three tenses (present, future and past tenses), both simple and compound and 5 moods. Moreover, inflected forms are highly ambiguous. Nouns in Latvian have 29 graphically different endings and only 13 of them are unambiguous, adjectives have 24 graphically different endings and half of them are ambiguous, verbs have 28 graphically different endings and only 17 of them are unambiguous. The most common ambiguity classes are feminine singular genitive vs. feminine plural nominative and masculine singular accusative vs. masculine plural genitive. Initially the phrase-based model was built for JRC Acquis 2.2. corpus (Steinberger et al., 2006) . Human analysis of translation results allowed us to conclude that one of the central problems, which make translation abstruse, is wrong inflectional form (Skadiņa and Brālītis, 2007) . Selection of wrong inflectional form not only influences fluency of translation, but in complex sentences (as most of legal texts) makes translation abstruse. Therefore, to improve translation quality, factored SMT system which uses Latvian morphological analyzer was built (Figure 1 ). For Latvian language three factor model was chosen: inflected form (0), base form (or lemma) (1) and morphological tag (2). The translation process has been decompiled into the following steps: 1. English sentence has been translated into sequence of Latvian factors 1 and 2, using translation table 0-1,2 2. Sequence of Latvian factors 1 and 2 were translated into factor 0, using generation table 1,2-0 In addition three Latvian language models were implemented for each factor. All language models have the same weight during translation process. The system was built using well known tools and techniques: after text normalization (texts were converted to lower-case, empty lines deleted, punctuation marks were separated from words) the GIZA++ tool (Och and Ney, 2003) was used for translation models. For Latvian language models SRI LM Toolkit (Stolcke, 2002) with recommended parameters (modified Kneser-Ney discounting and interpolation) were used. We used Latvian morphological analyzer by Paikens ( 2007 ) and Latvian tagger developed by Virza (unpublished work). For decoding Moses decoder (Koehn, 2004) was used. Evaluation For test purposes two test collections were created. For automatic evaluation sentences were selected randomly (1 from 1000) from JRC 3.0 corpus after omitting sentences from JRC2.2 corpus, and excluding sentences with possibly wrong alignment. As result text collection for automatic evaluation contains 843 sentences. For human evaluation 200 sentences were chosen from the test collection. Sentences which were included into test collections were deleted from JRC3.0 and JRC2.2 corpora before the training. The evaluation was performed for four systems: phrase-based model built from JRC2.2 corpus, factored model built from JRC2.2 corpus, phrasebased model built from JRC3.0 corpus and factored model built from JRC3.0 corpus. At first influence of different parameters, i.e., n-grams in language model, target language corpus, choice of decoder, on phrase-based models was evaluated (Table 1 ). As it is shown below the size of corpora has considerable influence on BLEU score (Papineni et al., 2002) , while choice of decoder and number of n-grams in language model has relatively small influence on translation quality. 1 . Evaluation results (Bleu scores) for phrase-based models While influence of size of training corpora on translation quality is obvious result, our main goal was to evaluate the influence of factored models on translation quality (Table 2 ). The first results show that it is possible to increase translation performance using factored models as it is in case of phrase-based model built form JRC Acquis 2.2 corpus and corresponding (same training data, language model order and other parameters) factored model. Factored model built from JRC3.0 Acquis corpus is slightly outperformed by corresponding phrase-based model. The human evaluation showed the similar tendencythe size of training corpus has great influence on translation performance. 58 translations (29%) generated by systems trained on JRC Acquis 3.0 corpus are evaluated as understandable, while for systems trained on JRC Acquis 2.2 only 30 translations (15%) are evaluated as understandable. In 71 cases (35.5%) human evaluator has all translations as equal in translation quality; however, most of them are not easily understandable. Phrase table data SMT Conclusions The paper presents first results of English-Latvian factored SMT systems showing that at current stage, better results could be achieved with more data as by intelligence, i.e., factored models. We plan to make deeper and more precise human evaluation of current systems for further elaborations. We plan to research reasons why factored models have not demonstrated sufficient improvements in translation quality, especially for system trained on large (JRC Acquis 3.0) corpus and research possibilities to elaborate factored models. Recent versions of SMT systems presented here are available at eksperimenti.ailab.lv/smt. Acknowledgments The work presented was supported by Latvian Council of Science through projects Evaluation of Statistical Machine Translation Methods for English-Latvian Translation system (2005)(2006)(2007)(2008) and Application of Factored Methods in English-Latvian Statistical Machine Translation System  (2009-2012). We would also like to thank reviewers for useful comments.",
         "11047550",
         "5a851f23114e1d9d8dc412d438fadcd0a6f08fb5",
         "6",
         "https://aclanthology.org/W09-4637",
         "Northern European Association for Language Technology (NEALT)",
         "Odense, Denmark",
         "2009",
         "May",
         "Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009)",
         "Skadi{\\c{n}}a, Inguna  and\nBr{\\=a}l{\\=\\i}tis, Edgars",
         "{E}nglish-{L}atvian {SMT}: knowledge or data?",
         "242--245",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "skadina-bralitis-2009-english",
         null,
         null
        ],
        [
         "48",
         "W12-0102",
         "In this paper we present and evaluate three approaches to measure comparability of documents in non-parallel corpora. We develop a task-oriented definition of comparability, based on the performance of automatic extraction of translation equivalents from the documents aligned by the proposed metrics, which formalises intuitive definitions of comparability for machine translation research. We demonstrate application of our metrics for the task of automatic extraction of parallel and semiparallel translation equivalents and discuss how these resources can be used in the frameworks of statistical and rule-based machine translation.",
         "In this paper we present and evaluate three approaches to measure comparability of documents in non-parallel corpora. We develop a task-oriented definition of comparability, based on the performance of automatic extraction of translation equivalents from the documents aligned by the proposed metrics, which formalises intuitive definitions of comparability for machine translation research. We demonstrate application of our metrics for the task of automatic extraction of parallel and semiparallel translation equivalents and discuss how these resources can be used in the frameworks of statistical and rule-based machine translation. Introduction Parallel corpora have been extensively exploited in different ways in machine translation (MT) -both in Statistical (SMT) and more recently, in Rule-Based (RBMT) architectures: in SMT aligned parallel resources are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008) . Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011) , parallel phrase extraction (Munteanu and Marcu, 2006) , and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010) . Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguistic concepts. Research on comparable corpora needs not only good measures for comparability, but also a clearer, technologicallygrounded and quantifiable definition of comparability in the first place. In this paper we relate comparability to usefulness of comparable texts for MT. In particular, we propose a performance-based definition of comparability, as the possibility to extract parallel or quasi-parallel translation equivalents -words, phrases and sentences which are translations of each other. This definition directly relates comparability to texts' potential to improve the quality of MT by adding extracted phrases to phrase tables, training corpus or dictionaries. It also can be quantified as the rate of successful extraction of translation equivalents by automated tools, such as proposed in Munteanu and Marcu (2006) . Still, successful detection of translation equivalents from comparable corpora very much de-pends on the quality of these corpora, specifically on the degree of their textual equivalence and successful alignment on various text units. Therefore, the goal of this work is to provide comparability metrics which can reliably identify crosslingual comparable documents from raw corpora crawled from the Web, and characterize the degree of their similarity, which enriches comparable corpora with the document alignment information, filters out documents that are not useful and eventually leads to extraction of good-quality translation equivalents from the corpora. To achieve this goal, we need to define a scale to assess comparability qualitatively, metrics to measure comparability quantitatively, and the sources to get comparable corpora from. In this work, we directly characterize comparability by how useful comparable corpora are for the task of detecting translation equivalents in them, and ultimately to machine translation. We focus on document-level comparability, and use three categories for qualitative definition of comparability levels, defined in terms of granularity for possible alignment: • Parallel: Traditional parallel texts that are translations of each other or approximate translations with minor variations, which can be aligned on the sentence level. • Strongly-comparable: Texts that talk about the same event or subject, but in different languages. For example, international news about oil spill in the Gulf of Mexico, or linked articles in Wikipedia about the same topic. These documents can be aligned on the document level on the basis of their origin. • Weakly-comparable: Texts in the same subject domain which describe different events. For example, customer reviews about hotel and restaurant in London. These documents do not have an independent alignment across languages, but sets of texts can be aligned on the basis of belonging to the same subject domain or sub-domain. In this paper, we present three different approaches to measure the comparability of crosslingual (especially under-resourced languages) comparable documents: a lexical mapping based approach, a keyword based approach, and a machine translation based approach. The experimental results show that all of them can effectively predict the comparability levels of the compared document pairs. We then further investigate the applicability of the proposed metrics by measuring their impact on the task of parallel phrase extraction from comparable corpora. It turns out that, higher comparability level predicted by the metrics consistently lead to more number of parallel phrase extracted from comparable documents. Thus, the metrics can help select more comparable document pairs to improve the performance of parallel phrase extraction. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 introduces our comparability metrics. Section 4 presents the experimental results and evaluation. Section 5 describes the application of the metrics. Section 6 discusses the pros and cons of the proposed metrics, followed by conclusions and future work in Section 7. Related Work The term \"comparability\", which is the key concept in this work, applies to the level of corpora, documents and sub-document units. However, so far there is no widely accepted definition of comparability. For example, there is no agreement on the degree of similarity that documents in comparable corpora should have or on the criteria for measuring comparability. Also, most of the work that performs translation equivalent extraction in comparable corpora usually assumes that the corpora they use are reliably comparable and focuses on the design of efficient extraction algorithms. Therefore, there has been very little literature discussing the characteristics of comparable corpora (Maia, 2003) . In this section, we introduce some representative work which tackles comparability metrics. Some studies (Sharoff, 2007; Maia, 2003; McEnery and Xiao, 2007) analyse comparability by assessing corpus composition, such as structural criteria (e.g., format and size), and linguistic criteria (e.g., topic, domain, and genre). Kilgarriff and Rose (1998) measure similarity and homogeneity between monolingual corpora. They generate word frequency list from each corpus and then apply χ 2 statistic on the most frequent n (e.g., 500) words of the compared corpora. The work which deals with comparability measures in cross-lingual comparable corpora is closer to our work. Saralegi et al. (2008) measure the degree of comparability of comparable corpora (English and Basque) according to the distribution of topics and publication dates of documents. They compute content similarity for all the document pairs between two corpora. These similarity scores are then input as parameters for the EMD (Earth Mover's Distance) distance measure, which is employed to calculate the global compatibility of the corpora. Munteanu and Marcu (2005; 2006) select more comparable document pairs in a cross-lingual information retrieval based manner by using a toolkit called Lemur 1 . The retrieved document pairs then serve as input for the tasks of parallel sentence and sub-sentence extraction. Smith et al. (2010) treat Wikipedia as a comparable corpus and use \"interwiki\" links to identify aligned comparable document pairs for the task of parallel sentence extraction. Li and Gaussier (2010) propose a comparability metric which can be applied at both document level and corpus level and use it as a measure to select more comparable texts from other external sources into the original corpora for bilingual lexicon extraction. The metric measures the proportion of words in the source language corpus translated in the target language corpus by looking up a bilingual dictionary. They evaluate the metric on the rich-resourced English-French language pair, thus good dictionary resources are available. However, this is not the case for under-resourced languages in which reliable language resources such as machine-readable bilingual dictionaries with broad word coverage or word lemmatizers might be not publicly available. Comparability Metrics To measure the comparability degree of document pairs in different languages, we need to translate the texts or map lexical items from the source language into the target languages so that we can compare them within the same language. Usually this can be done by using bilingual dictionaries (Rapp, 1999; Li and Gaussier, 2010; Prachasson and Fung, 2011) or existing machine translation tools. Based on this process, in this section we present three different approaches to measure the 1 Available at http://www.lemurproject.org/ comparability of comparable documents. Lexical mapping based metric It is straightforward that we expect a bilingual dictionary can be used for lexical mapping between a language pair. However, unlike the language pairs in which both languages are rich-resourced (e.g., English-French, or English-Spanish) and dictionary resources are relatively easy to obtain, it is likely that bilingual dictionaries with good word coverage are not publicly available for underresourced languages (e.g., English-Slovenian, or English-Lithuanian). In order to address this problem, we automatically construct dictionaries by using word alignment on large-scale parallel corpora (e.g., Europarl and JRC-Acquis 2 ). Specifically, GIZA++ toolkit (Och and Ney, 2000) with default setting is used for word alignment on the JRC-Acquis parallel corpora (Steinberger et al., 2006) . The aligned word pairs together with the alignment probabilities are then converted into dictionary entries. For example, in Estonian-English language pair, the alignment example \"kompanii company 0.625\" in the word alignment table means the Estonian word \"kompanii\" can be translated as (or aligned with) the English candidate word \"company\" with a probability of 0.625. In the dictionary, the translation candidates are ranked by translation probability in descending order. Note that the dictionary collects inflectional form of words, but not only base form of words. This is because the dictionary is directly generated from the word alignment results and no further word lemmatization is applied. Using the resulting dictionary, we then perform lexical mapping in a word-for-word mapping strategy. We scan each word in the source language texts to check if it occurs in the dictionary entries. If so, the first translation candidate are recorded as the corresponding mapping word. If there are more than one translation candidate, the second candidate will also be kept as the mapping result if its translation probability is higher than 0.3 3 . For non-English and English language pair, the non-English texts are mapped into English. If both languages are non-English (e.g., Greek-Romanian), we use English as a pivot langauge and map both the source and target language texts into English 4 . Due to the lack of reliable linguistic resources in non-English languages, mapping texts from non-English language into English can avoid language processing in non-English texts and allows us to make use of the rich resources in English for further text processing, such as stop-word filtering and word lemmatization 5 . Finally, cosine similarity measure is applied to compute the comparability strength of the compared document pairs. Keyword based metric The lexical mapping based metric takes all the words in the text into account for comparability measure, but if we only retain a small number of representative words (keywords) and discard all the other less informative words in each document, can we judge the comparability of a document pair by comparing these words? Our intuition is that, if two document share more keywords, they should be more comparable. To validate this, we then perform keyword extraction by using a simple TFIDF based approach, which has been shown effective for keyword or keyphrase extraction from the texts (Frank et al., 1999; Hulth, 2003; Liu et al., 2009) . More specifically, the keyword based metric can be described as below. First, similar to the lexical mapping based metric, bilingual dictionaries are used to map non-English texts into English. Thus, only the English resources are applied for stop-word filtering and word lemmatization, which are useful text preprocessing steps for keyword extraction. We then use TFIDF to measure the weight of words in the document and rank the words by their TFIDF weights in descending order. The top n (e.g., 30) words are extracted as keywords to represent the document. Finally, the comparability of each document pair is determined by applying cosine similarity to their key-word lists. Machine translation based metrics Bilingual dictionary is used for word-for-word translation in the lexical mapping based metric and words which do not occur in the dictionary will be omitted. Thus, the mapping result is like a list of isolated words and information such as word order, syntactic structure and named entities can not be preserved. Therefore, in order to improve the text translation quality, we turn to the state-of-the-art SMT systems. In practice, we use Microsoft translation API 6 to translate texts in under-resourced languages (e.g, Lithuanian and Slovenian) into English and then explore several features for comparability metric design, which are listed as below. • Lexical feature: Lemmatized bag-of-word representation of each document after stopword filtering. Lexical similarity (denoted by W L ) of each document pair is then obtained by applying cosine measure to the lexical feature. • Structure feature: We approximate it by the number of content words (adjectives, adverbs, nouns, verbs and proper nouns) and the number of sentences in each document, denoted by C D and S D respectively. The intuition is that, if two documents are highly comparable, their number of content words and their document length should be similar. The structure similarity (denoted by W S ) of two documents D1 and D2 is defined as bellow. W S = 0.5 * (C D1 /C D2 ) + 0.5 * (S D1 /S D2 ) suppose that C D1 <=C D2 , and S D1 <=S D2 . • Keyword feature: Top-20 words (ranked by TFIDF weight) of each document. keyword similarity (denoted by W K ) of two documents is also measured by cosine. • Named entity feature: Named entities of each document. If more named entities cooccur in two documents, they are very likely to talk about the same event or subject and thus should be more comparable. We use Stanford named entity recognizer 7 to extract named entities from the texts (Finkel et al., 2005) . Again, cosine is then applied to measure the similarity of named entities (denoted by W N ) between a document pair. We then combine these four different types of score in an ensemble manner. Specifically, a weighted average strategy is applied: each individual score is associated with a constant weight, indicating the relative confidence (importance) of the corresponding type of score. The overall comparability score (denoted by SC) of a document pair is thus computed as below: SC = α * W L + β * W S + γ * W K + δ * W N where α, β, γ, and δ ∈ [0, 1], and α + β + γ + δ = 1. SC should be a value between 0 and 1, and larger SC value indicates higher comparability level. Experiment and Evaluation Data source To investigate the reliability of the proposed comparability metrics, we perform experiments for 6 language pairs which contain underresoured languages: German-English (DE-EN), Estonian-English (ET-EN), Lithuanian-English (LT-EN), Latvian-English (LV-EN), Slovenian-English (SL-EN) and Greek-Romanian (EL-RO). A comparable corpus is collected for each language pair. Based on the definition of comparability levels (see Section 1), human annotators fluent in both languages then manually annotated the comparability degree (parallel, stronglycomparable, and weakly-comparable) at the document level. Hence, these bilingual comparable corpora are used as gold standard for experiments. The data distribution for each language pair, i.e., number of document pairs in each comparability level, is given in Table 1 . Experimental results We adopt a simple method for evaluation. For each language pair, we compute the average scores for all the document pairs in the same comparability level, and compare them to the gold standard comparability labels. In addition, in order to better reveal the relation between the scores obtained from the proposed metrics and comparability levels, we also measure the Pearson correlation between them 8 . For the keyword based metric, top 30 keywords are extracted from each text for experiment. For the machine translation based metric, we empirically set α = 0.5, β = γ = 0.2, and δ = 0.1. This is based on the assumption that, lexical feature can best characterize the comparability given the good translation quality provided by the powerful MT system, while keyword and named entity features are also better indicators of comparability than the simple document length information. The results for the lexical mapping based metric, the keyword based metric and the machine translation based metric are listed in ably reflect the comparability levels across different language pairs, as the average scores for higher comparable levels are always significantly larger than those of lower comparable levels, namely SC(parallel)>SC(stronglycomparable)>SC(weakly-comparable). In addition, in all the three metrics, the Pearson correlation scores are very high (over 0.93) across different language pairs, which indicate that there is strong correlation between the comparability scores obtained from the metrics and the corresponding comparability level. Moreover, from the comparison of Table 2 , 3, and 4, we also have several other findings. Firstly, the performance of keyword based metric (see Table 3 ) is comparable to the lexical mapping based metric (see Table 2 ) as their comparability scores for the corresponding comparability levels are similar. This means it is reasonable to determine the comparability level by only comparing a small number of keywords of the texts. Secondly, the scores obtained from the machine translation based metric (see Table 4 ) are significantly higher than those in both the lexical mapping based metric and the keyword based metric. Clearly, this is due to the advantages of using the state-of-theart MT system. In comparison to the approach of using dictionary for word-for-word mapping, it can provide much better text translation which allows detecting more proportion of lexical over-lapping and mining more useful features in the translated texts. Thirdly, in the lexical mapping based metric and keyword based metric, we can also see that, although the average scores for EL-RO (both under-resourced languages) conform to the comparability levels, they are much lower than those of the other 5 language pairs. The reason is that, the size of the parallel corpora in JRC-Acquis for these 5 language pairs are significantly larger (over 1 million parallel sentences) than that of EL-EN, RO-EN 9 , and EL-RO, thus the resulting dictionaries of these 5 language pairs also contain many more dictionary entries. Application The experiments in Section 4 confirm the reliability of the proposed metrics. The comparability metrics are thus useful for collecting highquality comparable corpora, as they can help filter out weakly comparable or non-comparable document pairs from the raw crawled corpora. But are they also useful for other NLP tasks, such as translation equivalent detection from comparable corpora? In this section, we further measure the impact of the metrics on parallel phrase extraction (PPE) from comparable corpora. Our intuition is that, if document pairs are assigned higher comparability scores by the metrics, they should be more comparable and thus more parallel phrases can be extracted from them. The algorithm of parallel phrase extraction, which develops the approached presented in Munteanu and Marcu (2006) , uses lexical overlap and structural matching measures (Ion, 2012) . Taking a list of bilingual comparable document pairs as input, the extraction algorithm involves the following steps. 1. Split the source and target language documents into phrases. 2. Compute the degree of parallelism for each candidate pair of phrases by using the bilingual dictionary generated from GIZA++ (base dictionary), and retain all the phrase pairs with a score larger than a predefined parallelism threshold. 3. Apply GIZA++ to the retained phrase pairs to detect new dictionary entries and add them to the base dictionary. Repeat Step 2 and 3 for several times (empirically set at 5) by using the augmented dictionary, and output the detected phrase pairs. Phrases which are extracted by this algorithm are frequently not exact translation equivalents. Below we give some English-German examples of extracted equivalents with their corresponding alignment scores: 1. But a successful mission -seiner überaus erfolgreichen Mission abgebremst -0.815501989333333 2. Former President Jimmy Carter -Der ehemalige US-Präsident Jimmy Carter -0.69708324976825 3. on the Korean Peninsula -auf der koreanischen Halbinsel -0.8677432145 4. across the Muslim world -mit der muslimischen Welt ermöglichen -0.893330864 5. to join the United Nations -der Weg in die Vereinten Nationen offensteht -0.397418711927629 Even though some of the extracted phrases are not exact translation equivalents, they may still be useful resources both for SMT and RBMT if these phrases are passed through an extra preprocessing stage, of if the engines are modified specifically to work with semi-parallel translation equivalents extracted from comparable texts. We address this issue in the discussion section (see Section 6). For evaluation, we measure how the metrics affect the performance of parallel phrase extraction algorithm on 5 language pairs (DE-EN, ET-EN, LT-EN, LV-EN, and SL-EN) . A large raw comparable corpus for each language pair was crawled from the Web, and the metrics were then applied to assign comparability scores to all the document pairs in each corpus. For each language pair, we set three different intervals based on the comparability score (SC) and randomly select 500 document pairs in each interval for evaluation. For the MT based metric, the three intervals are (1) 0.1<=SC<0.3, (2) 0.3<=SC<0.5, and (3) SC>=0.5. For the lexical mapping based metric and keyword based metric, since their scores are lower than those of the MT based metric for each comparability level, we set three lower intervals at (1) 0.1<=SC<0.2, (2) 0.2<=SC<0.4, and (3) SC>=0.4. The experiment focuses on counting the number of extracted parallel phrases with parallelism score>=0.4 10 , and computes the average number of extracted phrases per 100000 words (the sum of words in the source and target language documents) for each interval. In addition, the Pearson correlation measure is also applied to measure the correlation between the interval 11 of comparability scores and the number of extracted parallel phrases. The results which summarize the impact of the three metrics to the performance of parallel phrase extraction are listed in  In all the three metrics, the Pearson correlation scores are very close to 1 for all the language pairs, which indicate that the intervals of comparability scores obtained from the metrics are in line with the performance of equivalent extraction algorithm. Therefore, in order to extract more parallel phrases (or other translation equivalents) from comparable corpora, we can try to improve the corpus comparability by applying the comparability metrics beforehand to add highly comparable document pairs in the corpora. Discussion We have presented three different approaches to measure comparability at the document level. In this section, we will analyze the advantages and limitations of the proposed metrics, and the feasibility of using semi-parallel equivalents in MT. Pros and cons of the metrics Using bilingual dictionary for lexical mapping is simple and fast. However, as it adopts the wordfor-word mapping strategy and out-of-vocabulary (OOV) words are omitted, the linguistic structure of the original texts is badly hurt after mapping. Thus, apart from lexical information, it is difficult to explore more useful features for the comparability metrics. The TFIDF based keyword extraction approach allows us to select more representative words and prune a large amount of less informative words from the texts. The keywords are usually relevant to subject and domain terms, which is quite useful in judging the comparability of two documents. Both the lexical mapping based approach and the keyword based approach use dictionary for lexical translation, thus rely on the availability and completeness of the dictionary resources or large scale parallel corpora. For the machine translation based metric, it provides much better text translation than the dictionary-based approach so that the comparability of two document can be better revealed from the richer lexical information and other useful features, such as named entities. However, the text translation process is expensive, as it depends on the availability of the powerful MT systems 12 and takes much longer than the simple dictionary based translation. In addition, we use a translation strategy of translating texts from under-resourced (or lessresourced) languages into rich-resourced language. In case that both languages are underresourced languages, English is used as the pivot langauge for translation. This can compensate the shortage of the linguistic resources in the underresourced languages and take advantages of various resources in the rich-resourced languages. Using semi-parallel equivalents in MT systems We note that modern SMT and RBMT systems take maximal advantage of strictly parallel phrases, but they still do not use full potential of the semi-parallel translation equivalents, of the type that is illustrated in the application section (see Section 5). Such resources, even though they are not exact equivalents contain useful information which is not used by the systems. In particular, the modern decoders do not work with under-specified phrases in phrase tables, and do not work with factored semantic features. For example, the phrase: But a successful mission -seiner überaus erfolgreichen Mission abgebremst The English side contains the word but, which pre-supposes contrast, and on the Greman side words überaus erfolgreichen (\"generally successful\") and abgebremst (\"slowed down\") -which taken together exemplify a contrast, since they have different semantic prosodies. In this example the semantic feature of contrast can be extracted and reused in other contexts. However, this would require the development of a new generation of decoders or rule-based systems which can successfully identify and reuse such subtle semantic features. Conclusion and Future work The success of extracting good-quality translation equivalents from comparable corpora to improve machine translation performance highly depends on \"how comparable\" the used corpora are. In this paper, we propose three different comparability measures at the document level. The experiments show that all the three approaches can effectively determine the comparability levels of comparable document pairs. We also further investigate the impact of the metrics on the task of parallel phrase extraction from comparable corpora. It turns out that higher comparability scores always lead to significantly more parallel phrases extracted from comparable documents. Since better quality of comparable corpora should have better applicability, our metrics can be applied to select highly comparable document pairs for the tasks of translation equivalent extraction. In the future work, we will conduct more comprehensive evaluation of the metrics by capturing its impact to the performance of machine translation systems with extended phrase tables derived from comparable corpora. Acknowledgments We thank Radu Ion at RACAI for providing us the toolkit of parallel phrase extraction, and the three anonymous reviewers for valuable comments. This work is supported by the EU funded ACCURAT project (FP7- ICT-2009-4-248347)",
         "15346148",
         "5c983fb27dee4bd4cd42fcde08dbc02f4431e2bc",
         "26",
         "https://aclanthology.org/W12-0102",
         "Association for Computational Linguistics",
         "Avignon, France",
         "2012",
         "April",
         "Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and Machine Translation ({ESIRMT}) and Hybrid Approaches to Machine Translation ({H}y{T}ra)",
         "Su, Fangzhong  and\nBabych, Bogdan",
         "Measuring Comparability of Documents in Non-Parallel Corpora for Efficient Extraction of (Semi-)Parallel Translation Equivalents",
         "10--19",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "su-babych-2012-measuring",
         null,
         null
        ],
        [
         "49",
         "W12-0110",
         "The main purpose of the project ATLAS (Applied Technology for Language-Aided CMS) is to facilitate multilingual web content development and management. Its main innovation is the integration of language technologies within a web content m a n a g e m e n t s y s t e m . T h e l a n g u a g e processing framework, integrated with web content management, provides automatic annotation of important words, phrases and named entities, suggestions for categorisation o f d o c u m e n t s , a u t o m a t i c s u m m a r y generation, and machine translation of summaries of documents. A machine translation approach, as well as methods for obtaining and constructing training data for machine translation are under development.",
         "The main purpose of the project ATLAS (Applied Technology for Language-Aided CMS) is to facilitate multilingual web content development and management. Its main innovation is the integration of language technologies within a web content m a n a g e m e n t s y s t e m . T h e l a n g u a g e processing framework, integrated with web content management, provides automatic annotation of important words, phrases and named entities, suggestions for categorisation o f d o c u m e n t s , a u t o m a t i c s u m m a r y generation, and machine translation of summaries of documents. A machine translation approach, as well as methods for obtaining and constructing training data for machine translation are under development. Introduction The main purpose of the European project ATLAS (Applied Technology for Language-Aided CMS) 1 is to facilitate multilingual web content development and management. Its main innovation is the integration of language technologies within a web content management system. ATLAS combines a language processing framework with a content management component (i-Publisher) 2 used for creating, running and managing dynamic content-driven websites. Examples of such sites are i-Librarian, 3 a free online library of digital documents that may be personalised according to the user's needs and requirements; and EUDocLib, 4 a free online library of European legal documents. The language processing framework of these websites provides automatic annotation of important words, phrases and named entities, suggestions for categorisation of documents, automatic summary generation, and machine translation of a summary of a document (Karagyozov et al. 2012) . Six European Union languages -Bulgarian, German, Greek, English, Polish, and Romanian are supported. 2. Brief overview of existing content management systems The most frequently used open-source multilingual web content management systems (WordPress, Joomla, Joom!Fish, TYPO3, Drupal) 5 offer a relatively low level of multilingual content management. None of the platforms supports multiple languages in their native states. Instead, they rely on plugins to handle this: WordPress uses the WordPress Multilingual Plugin, Drupal needs a module called Locale, and Joomla needs a module called Joomfish. There are modules, like those provided by ICanLocalize 6 , than can facilitate selection within Drupal and WordPress of the material to be translated, but the actual translation is done by human translators. To the best of our knowledge, none of the existing content management systems exploits language technologies to provide more sophisticated text content management. This is proved by the data published at the CMS Critic 7 -an online media providing news, reviews, articles and interviews for about 60 content management systems. Taking into account that the online data are in many cases multilingual and documents stored in a content management system are usually related by means of sharing similar topics or domains it can be claimed that the web content management systems need the power of modern language technologies. In comparison ATLAS offers the advantage of integration of natural language processing in the multilingual content management. 3 Selection of \"core\" words ATLAS suggests \"core\" words (plus phrases and named entities), i.e., the most essential words that capture the main topic of a given document. Currently the selection of core words is carried out in a two-stage process: identification of candidates and ranking. For the identification stage a language processing chain is applied that consists of the following tools: sentence splitter, tokenizer, PoS tagger, lemmatizer, word sense disambiguator (assigns a unique sense to a word), NP extractor (marks up noun phrases in the text) and NE extractor (marks up named entities in the text). After this stage, the target core words are ranked according to their importance scores, which are estimated by features such as frequency, linguistic correlation, phrase length, etc., combined by heuristics to obtain the final ranking strategy. The core words are displayed in several groups: named entities (locations, names, etc.) -both single words and phrases, and noun phrases -terms, multiword expressions or noun phrases with a hight frequency. For example among the \"core\" noun phrases extracted from Cocoa Fundamentals Guide 8 are the following phrases: Object-Oriented Programming, Objective-C language, Cocoa application, Cocoa program, etc. Even though the language processing chains that are applied differ from language to language, this approach offers a common ground for language processing and its results can be comfortably used by advanced language components such as d o c u m e n t c l a s s i f i c a t i o n , c l a u s e -b a s e d summarisation, and statistical machine translation. Content navigation (such as lists of similar documents) based on interlinked text annotations is also provided. Automatic categorisation Automatic 6 Machine translation For i-Publisher, machine translation serves as a translation aid for publishing multilingual content. The ability to display content in multiple languages is combined with a computer-aided localization of the templates. Text for a localization is submitted to the translation engine and the output is subject to human postprocessing. For i-Librarian and EuDocLib, and for any website developed with i-Publisher, the machine translation engine provides a translation of the document summary provided earlier in the chain. This will give the user rough clues about documents in different languages, and a basis to decide whether they are to be stored. Obtaining training corpora The development of a translation engine is particularly challenging, as the translation should be able to be used in different domains and within different text genres. In addition, most of the language pairs in question belong to the less resourced group for which bilingual training and test material is available in limited amounts (Gavrila and Vertan 2011) (Tiedemann 2009) . Automatic collection of corpora is preferred to manual, and for that purpose a set of simple crawlers was designed. They are modified for each source to ensure efficiency. Figure 1 presents some statistical data for the Bulgarian-English parallel corpus, the largest in the collection (the vertical axis shows the number of words, while the horizontal -the domain distribution). Figure 1 Bulgarian-English parallel corpus Two basic methods are used to enlarge the existing parallel corpora. In the first, the available training data for statistical machine translation are extended by means of generating paraphrases (e.g. compound nouns are paraphrased into (semi-) equivalent phrases with a preposition, and vice versa). The paraphrases can be classified as morphological (where the difference is between the forms of the phrase constituents), lexical (based on semantic similarity between constituents) and phrasal (based on syntactic transformations). Paraphrase generation methods that operate both on a single monolingual corpus or on parallel corpus are discussed by Madnani and Dorr 2010. For instance, one of the methods for paraphrase generation from a monolingual corpus considers as paraphrases all words and phrases that are distributionally similar, that is, occurring with the Bulgarian English same sets of anchors (Paşca and Dienes 2005 ). An approach using phrase-based alignment techniques shows how paraphrases in one language can be identified using a phrase in a second language as a pivot (Bannard and Callison-Burch 2005) . The second method performs automatic generation of parallel corpora (Xu and Sun 2011) by means of automatic translation. This method can be applied for language pairs for which parallel corpora are still limited in quantity. If, say, a Bulgarian-English parallel corpus exists, a Bulgarian Polish parallel corpus can be constructed by means of automatic translation from English to Polish. To control the quality of the automatically generated data, multiple translation systems can be used, and the compatibility of the translated outputs can be calculated. Thus, both methods can fill gaps in the available data, the first method by extending existing parallel corpora and the second by automatic construction of parallel corpora. Accepted approach Given that the ATLAS platform deals with languages from different language families and that the engine should support several domains, an interlingua approach is not suitable. Building transfer systems for all language pairs is also time-consuming and does not make the platform easily portable to other languages. When all requirements and limitations are taken into account, corpus-based machine translation paradigms are the best option that can be considered (Karagyozov et al. 2012) . For the ATLAS translation engine it was decided to use a hybrid architecture combining example-based and statistical machine translation at the wordbased level (i.e., no syntactic trees will be used). The ATLAS translation engine interacts with other modules of the system. For example, the document categorisation module assigns one or more domains to each document, and if no specific trained translation model for the respective domain exists, the user gets a warning that the translation may be inadequate with respect to lexical coverage. Each input item to the translation engine is then processed by the example-based machine translation component. If the input as a whole or important chunks of it are found in the translation database, the translation equivalents are used and, if necessary, combined (Gavrila 2011) . In all other cases the input is sent further to the Moses-based machine translation component which uses a part-ofspeech and domain-factored model (Niehues and Waibel 2010). Like the architecture of the categorization engine, the translation system in ATLAS is able to accommodate and use different third-party translations engines, such as those of Google, Bing, and Yahoo. The ATLAS machine translation module is still under development. Some experiments in translation between English, German, and Romanian have been performed in order to define: what parameter settings are suitable for language pairs with a rich morphology, what tuning steps lead to significant improvements, wheather the PoS-factored models improve significantly the quality of results (Karagyozov et al. 2012) . 7! Conclusion To conclude, ATLAS enables users to create, organise and publish various types of multilingual documents. ATLAS reduces the manual work by using automatic classification of documents and helps users to decide about a document by providing summaries of documents and their translations. Moreover, the user can easily find the most relevant texts within large document collections and get a brief overview of their content. A modern web content management systems should help users come to grips with the growing complexity of today's multilingual websites. ATLAS answers to this task. Acknowledgments ATLAS (Applied Technology for Language-Aided CMS) is a European project funded under the CIP ICT Policy Support Programme, Grant Agreement 250467.",
         "1187445",
         "a5e5a8e581c397433c1fb43b06c6e7630ad71095",
         "0",
         "https://aclanthology.org/W12-0110",
         "Association for Computational Linguistics",
         "Avignon, France",
         "2012",
         "April",
         "Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and Machine Translation ({ESIRMT}) and Hybrid Approaches to Machine Translation ({H}y{T}ra)",
         "Koeva, Svetla",
         "{ATLAS} - Human Language Technologies integrated within a Multilingual Web Content Management System",
         "72--76",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "koeva-2012-atlas",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 7680
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "      <th>corpus_paper_id</th>\n",
       "      <th>pdf_hash</th>\n",
       "      <th>numcitedby</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>address</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>doi</th>\n",
       "      <th>number</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>editor</th>\n",
       "      <th>isbn</th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>ID</th>\n",
       "      <th>language</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W05-0819</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>1215281</td>\n",
       "      <td>b20450f67116e59d1348fc472cfc09f96e348f55</td>\n",
       "      <td>15</td>\n",
       "      <td>https://aclanthology.org/W05-0819</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>aswani-gaizauskas-2005-aligning</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W05-0821</td>\n",
       "      <td>Statistical machine translation systems use a ...</td>\n",
       "      <td>Statistical machine translation systems use a ...</td>\n",
       "      <td>1966857</td>\n",
       "      <td>2a05c9c5373a3e1e01b8161e6687b960ab3d2ff5</td>\n",
       "      <td>55</td>\n",
       "      <td>https://aclanthology.org/W05-0821</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>kirchhoff-yang-2005-improved</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W05-0820</td>\n",
       "      <td>Eleven groups participated in the event. This ...</td>\n",
       "      <td>Eleven groups participated in the event. This ...</td>\n",
       "      <td>9271055</td>\n",
       "      <td>5025a4b2b724867f7b6b24f3e9253a61b76a3406</td>\n",
       "      <td>83</td>\n",
       "      <td>https://aclanthology.org/W05-0820</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>koehn-monz-2005-shared</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W05-0823</td>\n",
       "      <td>This work discusses translation results for th...</td>\n",
       "      <td>This work discusses translation results for th...</td>\n",
       "      <td>8303276</td>\n",
       "      <td>bd500b912faec09be996f76ab49e33eb5a8b5f8f</td>\n",
       "      <td>22</td>\n",
       "      <td>https://aclanthology.org/W05-0823</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>banchs-etal-2005-statistical</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009.mtsummit-posters.23</td>\n",
       "      <td>Rule based machine translation methods require...</td>\n",
       "      <td>Rule based machine translation methods require...</td>\n",
       "      <td>34218208</td>\n",
       "      <td>2e4ffd53b290960a18679c8d6f3db4e03bbacdc8</td>\n",
       "      <td>3</td>\n",
       "      <td>https://aclanthology.org/2009.mtsummit-posters.23</td>\n",
       "      <td>None</td>\n",
       "      <td>Ottawa, Canada</td>\n",
       "      <td>2009</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>varga-yokoyama-2009-transfer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7675</th>\n",
       "      <td>2021.dravidianlangtech-1.50</td>\n",
       "      <td>The Dravidian language family is one of the la...</td>\n",
       "      <td>The Dravidian language family is one of the la...</td>\n",
       "      <td>233365300</td>\n",
       "      <td>6d187ca54ef0a1d081de705a380da36fb4b69303</td>\n",
       "      <td>1</td>\n",
       "      <td>https://aclanthology.org/2021.dravidianlangtec...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Kyiv</td>\n",
       "      <td>2021</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>hegde-etal-2021-mucs</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7676</th>\n",
       "      <td>2021.dravidianlangtech-1.18</td>\n",
       "      <td>In this paper, we describe the GX system in th...</td>\n",
       "      <td>In this paper, we describe the GX system in th...</td>\n",
       "      <td>233365272</td>\n",
       "      <td>d3bc0c6ea3614bc02d67f94fb7dcfcf345a110f4</td>\n",
       "      <td>2</td>\n",
       "      <td>https://aclanthology.org/2021.dravidianlangtec...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Kyiv</td>\n",
       "      <td>2021</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>xie-2021-gx</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7677</th>\n",
       "      <td>2021.blackboxnlp-1.9</td>\n",
       "      <td>Despite their failure to solve the composition...</td>\n",
       "      <td>Despite their failure to solve the composition...</td>\n",
       "      <td>235732335</td>\n",
       "      <td>f9a1a3eb45f5f86d848f83f3eabac60165aca3b6</td>\n",
       "      <td>8</td>\n",
       "      <td>https://aclanthology.org/2021.blackboxnlp-1.9</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Punta Cana, Dominican Republic</td>\n",
       "      <td>2021</td>\n",
       "      <td>...</td>\n",
       "      <td>10.18653/v1/2021.blackboxnlp-1.9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>chaabouni-etal-2021-transformers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>2021.americasnlp-1.25</td>\n",
       "      <td>We describe the NRC-CNRC systems submitted to ...</td>\n",
       "      <td>We describe the NRC-CNRC systems submitted to ...</td>\n",
       "      <td>235097526</td>\n",
       "      <td>2179c387d5cfef7388770acb83ca2a625d0ce7c0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://aclanthology.org/2021.americasnlp-1.25</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Online</td>\n",
       "      <td>2021</td>\n",
       "      <td>...</td>\n",
       "      <td>10.18653/v1/2021.americasnlp-1.25</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>knowles-etal-2021-nrc</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7679</th>\n",
       "      <td>P99-1027</td>\n",
       "      <td>Previous comparisons of document and query tra...</td>\n",
       "      <td>Previous comparisons of document and query tra...</td>\n",
       "      <td>771053</td>\n",
       "      <td>0b41459e7d8dd2183c359aa2f296b0477261a6ea</td>\n",
       "      <td>144</td>\n",
       "      <td>https://aclanthology.org/P99-1027</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>College Park, Maryland, USA</td>\n",
       "      <td>1999</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3115/1034678.1034716</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>mccarley-1999-translate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7680 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           acl_id  \\\n",
       "0                        W05-0819   \n",
       "1                        W05-0821   \n",
       "2                        W05-0820   \n",
       "3                        W05-0823   \n",
       "4        2009.mtsummit-posters.23   \n",
       "...                           ...   \n",
       "7675  2021.dravidianlangtech-1.50   \n",
       "7676  2021.dravidianlangtech-1.18   \n",
       "7677         2021.blackboxnlp-1.9   \n",
       "7678        2021.americasnlp-1.25   \n",
       "7679                     P99-1027   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     In this paper, we describe a word alignment al...   \n",
       "1     Statistical machine translation systems use a ...   \n",
       "2     Eleven groups participated in the event. This ...   \n",
       "3     This work discusses translation results for th...   \n",
       "4     Rule based machine translation methods require...   \n",
       "...                                                 ...   \n",
       "7675  The Dravidian language family is one of the la...   \n",
       "7676  In this paper, we describe the GX system in th...   \n",
       "7677  Despite their failure to solve the composition...   \n",
       "7678  We describe the NRC-CNRC systems submitted to ...   \n",
       "7679  Previous comparisons of document and query tra...   \n",
       "\n",
       "                                              full_text  corpus_paper_id  \\\n",
       "0     In this paper, we describe a word alignment al...          1215281   \n",
       "1     Statistical machine translation systems use a ...          1966857   \n",
       "2     Eleven groups participated in the event. This ...          9271055   \n",
       "3     This work discusses translation results for th...          8303276   \n",
       "4     Rule based machine translation methods require...         34218208   \n",
       "...                                                 ...              ...   \n",
       "7675  The Dravidian language family is one of the la...        233365300   \n",
       "7676  In this paper, we describe the GX system in th...        233365272   \n",
       "7677  Despite their failure to solve the composition...        235732335   \n",
       "7678  We describe the NRC-CNRC systems submitted to ...        235097526   \n",
       "7679  Previous comparisons of document and query tra...           771053   \n",
       "\n",
       "                                      pdf_hash  numcitedby  \\\n",
       "0     b20450f67116e59d1348fc472cfc09f96e348f55          15   \n",
       "1     2a05c9c5373a3e1e01b8161e6687b960ab3d2ff5          55   \n",
       "2     5025a4b2b724867f7b6b24f3e9253a61b76a3406          83   \n",
       "3     bd500b912faec09be996f76ab49e33eb5a8b5f8f          22   \n",
       "4     2e4ffd53b290960a18679c8d6f3db4e03bbacdc8           3   \n",
       "...                                        ...         ...   \n",
       "7675  6d187ca54ef0a1d081de705a380da36fb4b69303           1   \n",
       "7676  d3bc0c6ea3614bc02d67f94fb7dcfcf345a110f4           2   \n",
       "7677  f9a1a3eb45f5f86d848f83f3eabac60165aca3b6           8   \n",
       "7678  2179c387d5cfef7388770acb83ca2a625d0ce7c0           2   \n",
       "7679  0b41459e7d8dd2183c359aa2f296b0477261a6ea         144   \n",
       "\n",
       "                                                    url  \\\n",
       "0                     https://aclanthology.org/W05-0819   \n",
       "1                     https://aclanthology.org/W05-0821   \n",
       "2                     https://aclanthology.org/W05-0820   \n",
       "3                     https://aclanthology.org/W05-0823   \n",
       "4     https://aclanthology.org/2009.mtsummit-posters.23   \n",
       "...                                                 ...   \n",
       "7675  https://aclanthology.org/2021.dravidianlangtec...   \n",
       "7676  https://aclanthology.org/2021.dravidianlangtec...   \n",
       "7677      https://aclanthology.org/2021.blackboxnlp-1.9   \n",
       "7678     https://aclanthology.org/2021.americasnlp-1.25   \n",
       "7679                  https://aclanthology.org/P99-1027   \n",
       "\n",
       "                                      publisher  \\\n",
       "0     Association for Computational Linguistics   \n",
       "1     Association for Computational Linguistics   \n",
       "2     Association for Computational Linguistics   \n",
       "3     Association for Computational Linguistics   \n",
       "4                                          None   \n",
       "...                                         ...   \n",
       "7675  Association for Computational Linguistics   \n",
       "7676  Association for Computational Linguistics   \n",
       "7677  Association for Computational Linguistics   \n",
       "7678  Association for Computational Linguistics   \n",
       "7679  Association for Computational Linguistics   \n",
       "\n",
       "                             address  year  ...  \\\n",
       "0                Ann Arbor, Michigan  2005  ...   \n",
       "1                Ann Arbor, Michigan  2005  ...   \n",
       "2                Ann Arbor, Michigan  2005  ...   \n",
       "3                Ann Arbor, Michigan  2005  ...   \n",
       "4                     Ottawa, Canada  2009  ...   \n",
       "...                              ...   ...  ...   \n",
       "7675                            Kyiv  2021  ...   \n",
       "7676                            Kyiv  2021  ...   \n",
       "7677  Punta Cana, Dominican Republic  2021  ...   \n",
       "7678                          Online  2021  ...   \n",
       "7679     College Park, Maryland, USA  1999  ...   \n",
       "\n",
       "                                    doi number volume journal editor  isbn  \\\n",
       "0                                  None   None   None    None   None  None   \n",
       "1                                  None   None   None    None   None  None   \n",
       "2                                  None   None   None    None   None  None   \n",
       "3                                  None   None   None    None   None  None   \n",
       "4                                  None   None   None    None   None  None   \n",
       "...                                 ...    ...    ...     ...    ...   ...   \n",
       "7675                               None   None   None    None   None  None   \n",
       "7676                               None   None   None    None   None  None   \n",
       "7677   10.18653/v1/2021.blackboxnlp-1.9   None   None    None   None  None   \n",
       "7678  10.18653/v1/2021.americasnlp-1.25   None   None    None   None  None   \n",
       "7679            10.3115/1034678.1034716   None   None    None   None  None   \n",
       "\n",
       "          ENTRYTYPE                                ID language  note  \n",
       "0     inproceedings   aswani-gaizauskas-2005-aligning     None  None  \n",
       "1     inproceedings      kirchhoff-yang-2005-improved     None  None  \n",
       "2     inproceedings            koehn-monz-2005-shared     None  None  \n",
       "3     inproceedings      banchs-etal-2005-statistical     None  None  \n",
       "4     inproceedings      varga-yokoyama-2009-transfer     None  None  \n",
       "...             ...                               ...      ...   ...  \n",
       "7675  inproceedings              hegde-etal-2021-mucs     None  None  \n",
       "7676  inproceedings                       xie-2021-gx     None  None  \n",
       "7677  inproceedings  chaabouni-etal-2021-transformers     None  None  \n",
       "7678  inproceedings             knowles-etal-2021-nrc     None  None  \n",
       "7679  inproceedings           mccarley-1999-translate     None  None  \n",
       "\n",
       "[7680 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83faa406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['acl_id', 'abstract', 'full_text', 'corpus_paper_id', 'pdf_hash',\n",
       "       'numcitedby', 'url', 'publisher', 'address', 'year', 'month',\n",
       "       'booktitle', 'author', 'title', 'pages', 'doi', 'number', 'volume',\n",
       "       'journal', 'editor', 'isbn', 'ENTRYTYPE', 'ID', 'language', 'note'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec67cbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1957', '2022')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'].min(), df['year'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14cbe270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "booktitle\n",
       "Proceedings of the Fifth Conference on Machine Translation                                                                                                                      94\n",
       "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics                                                                                         81\n",
       "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing                                                                                          75\n",
       "Proceedings of the Sixth Conference on Machine Translation                                                                                                                      73\n",
       "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics                                                                                         67\n",
       "                                                                                                                                                                                ..\n",
       "Proceedings of the {IJCNLP}-08 Workshop on Named Entity Recognition for South and South East {A}sian Languages                                                                   1\n",
       "Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing                                                                                           1\n",
       "Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature                                  1\n",
       "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)     1\n",
       "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP                                                                              1\n",
       "Name: count, Length: 1148, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['booktitle'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e74ca91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.53513425645421"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstract'].str.split().str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a863b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "numcitedby",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8a954f18-d0a0-426b-b7ae-fec21695eb2e",
       "rows": [
        [
         "41337",
         "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
         "37353"
        ],
        [
         "53174",
         "{G}lo{V}e: Global Vectors for Word Representation",
         "23467"
        ],
        [
         "88",
         "{B}leu: a Method for Automatic Evaluation of Machine Translation",
         "17139"
        ],
        [
         "3927",
         "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
         "15755"
        ],
        [
         "61918",
         "{W}ord{N}et: A Lexical Database for {E}nglish",
         "14182"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>numcitedby</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41337</th>\n",
       "      <td>{BERT}: Pre-training of Deep Bidirectional Tra...</td>\n",
       "      <td>37353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53174</th>\n",
       "      <td>{G}lo{V}e: Global Vectors for Word Representation</td>\n",
       "      <td>23467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>{B}leu: a Method for Automatic Evaluation of M...</td>\n",
       "      <td>17139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3927</th>\n",
       "      <td>Learning Phrase Representations using {RNN} En...</td>\n",
       "      <td>15755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61918</th>\n",
       "      <td>{W}ord{N}et: A Lexical Database for {E}nglish</td>\n",
       "      <td>14182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  numcitedby\n",
       "41337  {BERT}: Pre-training of Deep Bidirectional Tra...       37353\n",
       "53174  {G}lo{V}e: Global Vectors for Word Representation       23467\n",
       "88     {B}leu: a Method for Automatic Evaluation of M...       17139\n",
       "3927   Learning Phrase Representations using {RNN} En...       15755\n",
       "61918      {W}ord{N}et: A Lexical Database for {E}nglish       14182"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nlargest(5, 'numcitedby')[['title', 'numcitedby']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25255f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author\n",
      "Tiedemann, J{\\\"o}rg    28\n",
      "S{\\o}gaard, Anders     27\n",
      "Ferret, Olivier        25\n",
      "Bick, Eckhard          25\n",
      "Wilks, Yorick          24\n",
      "Kay, Martin            23\n",
      "Johnson, Mark          23\n",
      "Kwong, Oi Yee          23\n",
      "Grishman, Ralph        23\n",
      "Somers, Harold         20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top_authors = (\n",
    "    df['author']\n",
    "    .dropna()  # Drop missing entries first\n",
    "    .str.split(' and ')\n",
    "    .explode()\n",
    "    .str.strip()\n",
    "    .value_counts()\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(top_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67f2ba23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acl_id                 0\n",
       "abstract            5616\n",
       "full_text           5830\n",
       "corpus_paper_id        0\n",
       "pdf_hash            1209\n",
       "numcitedby             0\n",
       "url                    0\n",
       "publisher          10119\n",
       "address             7192\n",
       "year                   0\n",
       "month               7323\n",
       "booktitle           2041\n",
       "author               667\n",
       "title                  0\n",
       "pages              13807\n",
       "doi                43607\n",
       "number             71811\n",
       "volume             71445\n",
       "journal            71248\n",
       "editor             73272\n",
       "isbn               71915\n",
       "ENTRYTYPE              0\n",
       "ID                     0\n",
       "language           70265\n",
       "note               73088\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ba358c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "acl_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "full_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "corpus_paper_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "pdf_hash",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "numcitedby",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "publisher",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "address",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "year",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "month",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "booktitle",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pages",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "number",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "volume",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "journal",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "editor",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "isbn",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "ENTRYTYPE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "note",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "13c205cc-26eb-4473-8d13-e9c3038b1d25",
       "rows": [
        [
         "0",
         "O02-2002",
         "There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.",
         "There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together. Introduction It is well known that word-sense is defined by a word's co-occurrence context. The context vectors of a word are defined as the probabilistic distributions of its left and right co-occurrence contexts. Conventionally , the similarity between two context vectors is measured based on their cosine distance [Alshawi and Cater, 1994; Grishman and Sterling, 1994; Pereira et al., 1993; Ruge, 1992; Salton, 1989] . However, the conventional measurement * Institute of Information Science, Academia Sinica E-mail: kchen@iis.sinica.edu.tw; swimming@hp.iis.sinica.edu.tw suffers from the following drawbacks. First of all, the information in the context vectors is vague. All co-occurrence words are collected without distinguishing whether they are syntactically or semantically related. Second, the coordinates are not pair-wise independent (i.e., the axes are not orthogonal), and it is hard to apply singular value decomposition to find the orthogonal vectors [Schutze, 1992] . In this paper, we propose to use only syntactic related co-occurrences as context vectors [Dekang Lin, 1998 ] and adopt information theoretic models to solve the above problems. In our study, the context vectors of a word are defined as the probabilistic distributions of its thematic roles and left/right co-occurrence semantic classes. The context features are derived from a treebank. All context features are weighted according to their TF × IDF values (the product of the term frequency and inverse document frequency) [Salton, 1989] . For the context features, the Cilin semantic classes (a Chinese thesaurus) are adopted. The Cilin semantic classes are divided into 4 different levels of granularity. In order to cope with the data sparseness problem, the weighted average of the similarity values at four different levels will be the similarity measure of two words. The weight for each level is equal to the information-content of that level [Shannon, 1948; Manning and Schutze 1999] . A agglomerative clustering algorithm is applied to group similar words according to the above defined similarity measure. Obviously, words with similar behavior in the corpus will be grouped together. We have compared the clustering results to the Cilin classifications. It turns out that words in the same synonym class and with the same syntactic categories have higher similarity values than the words with different syntactic categories. Data Resources Ideally, to derive context vectors, a large corpus with semantic tags is required. Furthermore, to extract co-occurrence words along with their exact syntactic and semantic relations, the corpus structure has to be annotated. However, such an ideal corpus does not exist. Therefore, in this paper we will adopt the resources that are available and try to derive a useful but imperfect Chinese tree bank. Since the similarity measure based on the vector space model is a rough estimation, minor errors made at the stage of context vector extraction are acceptable. Sinica Corpus The Sinica corpus contains 12,532 documents and nearly 5 million words. Each sentence in the corpus was parsed by a rule parser [Chen, 1996] . The parsed trees were tagged with the structure brackets, syntactic categories and thematic roles of each constituent [Huang et al., 2000] as exemplified below, (Sinica corpus: http://www.sinica.edu.tw/ftms -bin/kiwi.sh): Original sentence: 小 'small' 狗 'dog' 跳舞 'dance' Parsed tree : S(Agent:NP:(Property:Adj:'小 small'| Head: N: ' 狗 dog' )|Head:V: ' 跳舞 dance') Although these labels may not be exactly correct, we believe that, even with these minor errors, the majority of word -to-word relations extracted from the trees are correct. However, the semantic label is not provided for each word in the parsed trees. In this paper, we will use Cilin classifications for semantic labeling. Cilin-a Chinese Thesaurus Cilin provides the Mandarin synonym sets in a hierarchical structure [Mei et. al., 1984] . It contains 51,708 Chinese words, and 3918 classes. There are five levels in the Cilin semantic hierarchy, denoted in the format L 1 -L 2 -L 3 -L 4 -L 5 . For example, the Cilin class of the word 我們 'we' is \"A -a-02-2-01\". In level 1, \"A\", denotes the semantic class of human; in level 2, \"a\", indicates a group of general terms; level 3, \"02\", means pronouns in the first person, and in level 4, \"2\" represents the plural property. In level 5, \"01\" represents the order rank in the level 4 group. This means that \"01\" in level 5 is the first prototypical concept representation of \"A-a-02-2\". In the rest of this paper, only the first four levels will be used. The fifth level is for sense disambiguation only (section 2.3). Sense Disambiguation A polysemous word has more than one Cilin semantic class. In order to tag appropriate Cilin classes, we have designed a simple sense tagging method as follows [Wilks, 1999] . The sense tagging algorithm is based on the facts that the syntactic categories of each word in the tree bank are assigned uniquely, and that each Cilin class has its own major syntactic category. If a word has multiple Cilin classes, we select the sense class whose major syntactic category is the same as the tagged category of this word. For example , 計畫 \" Jie -Hwa\" has two meanings. One is for \"project\" as a noun and the other is \"attempt\", therefore, if 計畫 \"Jie -Hwa\" was tagged with a noun category, we will assign the Cilin class whose major category is \"project\". Sense ambiguity can be distinguished by measure of syntactic properties for most words. However, there are still cases in which the syntactic category constraints cannot resolve the sense ambiguities. Then, we simply choose the prototypical sense class, i.e, the word that has the highest rank in this sense class with respect to all its sense classes in Cilin. The Extraction of Co-occurrence Data The extracted syntactically related pairs have either a head-modifier relation or head-argument relation. For instance, two syntactically related pairs extracted from the example in section 2.1 are: (<Thematic role > <Cilin> <word1>), (<Thematic role> <Cilin> <word2>) (agent B -i-07-2 狗'dog'), (Head(S) H-h-04-2 跳舞'dance' ) (property E -a-03-3 小'small'), (Head(NP) B -i-07-2 狗'dog') The context data of the word 1 狗 \"dog\" consists of its thematic role \"agent\" and the Cilin class \"B-i-07-2\" ; the word 2 跳舞 'dance' consists the thematic role \"Head(S)\" and the Cilin class \"H-h-04-2\" and so on. The word 小 \"small\" and 跳舞 \"dance\" are not syntactically related even though they co-occur. Therefore , they will not be extracted. Context Vector Model There are three context vectors of a word: role vector, left context vector and right context vector. The role vector is a fixed 48-dimension vector, and each dimension value is equal to the probabilistic distribution of its thematic roles. The left/right context vectors are closer to the probabilistic distributions of its left/right co-occurrence words and their semantic classes. The role vector characterizes a word based more on syntax and less on semantics, but the left/right context vectors are just the opposite. The cosine distance between their context vectors is a measure of the similarity of the two words. We will illustrate the derivations of context vectors and their similarity rating with a simplified example using (貓 \"cat\", 狗 \"dog\"). The role vector of \"dog\" is {127, 207, 169… , 0} 48 , which represents the values of \"agent\", \"goal\", \"theme\"… and \"topic\" respectively, generated by Equation (1). The role vector of \"cat\" is {28, 73, 56… , 0} 48 , which is also acquired by Equation (1). Role vector of word W = { V 1 , V 2 ,… ,V 48 } 48 Vi = Frequency (R i ) × log (1/P i ) (1) R i : We label thematic roles \"agent\", \"goal\"… and \"topic\" from R 1 to R 48 listed in Table2 in the Appendix. Frequency (R i ): The frequency of R i played by word W in the corpus. P i : = Total frequency of R i in the corpus / Total frequency of all roles in the corpus. log(1/P i ): The information-context of R i [Shannon, 1948; Manning and Schutze, 1999] The derivation of left/right context vectors is a bit more complicated. The syntactically related co-occurrence word pairs are derived first as illustrated in section 2.4. We will illustrate the derivation of the left context vector only. The right context vector can be derived simila rly. The left co-occurrence word vector of word W is generated from frequency(word i ), where word i precedes and is syntactically related to W in the corpus. Due to the data sparseness problem, the feature dimensions of context vectors are generalized into C ilin classes instead of co-occurrence words. The generalization process reduces the effect of data sparseness. On the other hand, it also reduces the precision of characterization since each word has different information content and two words that have the same co-occurrence semantic classes may not share the same co-occurrence words. In order to resolve the above dilemma, when we compare the similarity between word X and word Y , 4 levels of left context vectors and right context vectors for word X and word Y are created 1 . The weighting of each feature dimension is adjusted using the TF*IDF value if word X and word Y have shared context words. Equation ( 2 ) illustrates the creation of the 4 th level left context vector of word X . The other context vectors for word X and word Y are created by a similar way. Left context vector of word X = {f1, f2,… ,f 3918 } L4 Where fi = Sum of { TF(word j ) × IDF(word j ) if word X has the same neighbor word j with word Y TF(word j ) if word X does not have the same neighbor word j with word Y } ; for every left co-occurrence word j with the ith Cilin semantic class. (2) TF(word j ): the frequency of pair ( word j , Cilin(word j ) ) in wordx' s co-occurrence context. IDF(word j ): -log(the number of the documents that contains the word j /total document number of the corpus) In Equation ( 2 ), we adjust the term weight using TF × IDF, which is commonly used in the field of information retrieval [Salton, 1989] to adjust the discrimination power of each feature dimension. We will examine the difference in the adjustment of weights using TF × IDF and TF in section 4.2. We will next give a simplified example. Assume that the word 狗 \"dog\" has only three left syntactically related words: ( 小 \"small\" Ea033 ) with frequency 30, ( 可愛 \"cute\" Ed401 ) with frequency 5 and ( 養 \"raise\" Ib011 ) with frequency 10; and assume that the word 貓 \"cat\" has only two left syntactically related words: ( 黑 \"black\" Ec043 ) and ( 養 \"raise\" Ib011 ). Assume that we are measuring the similarity between 狗 \"dog\" and 貓 \"cat\". Then, we can compute the left context data of 狗 \"dog\" as {TF(Aa011),… ,TF(Ea033),… ,TF(Ed041),… ,TF(Ib011) × IDF(養'raise') 2 , … , TF(La064) } L4 3 1 IDF(養'raise') = 4.188, the IDF values of all words range from 0.19 to 9.12. 2 The granularities of the 4 levels of semantic classes are partially shown in Figure2. The four left context vectors and their dimensions are shown below and the right context vectors are similarly derived. <LeftCilin1> L1 A vector of 12 dimensions from \"A\" to \"L\". <LeftCilin2> L2 A vector of 94 dimensions from \"Aa\" to \"La\". <LeftCilin3> L3 A vector of 1428 dimensions from \"Aa01\" to \"La06\". <LeftCilin4> L4 A vector of 3918 dimensions from \"Aa011\" to \"La064\". Similarities between Two Context Vectors Once we know the feature vectors of these two words, we can calculate the cosine distance of two vectors as shown in Equation (3). vector A= <a1,a2… ,an>,vector B= <b1,b2… ,bn> ... ) , cos( 1 2 1 2 1 ∑ ∑ ∑ = = = × × = n i n i n i bi ai bi ai B A (3) Therefore, the similarity of the two words x and y can be calculated as the linear combination of the cosine distances of all the feature vectors as shown in the Equation 4 . The weight of each feature vector can be adjusted according to different requirements. For instance, if the syntactic similarity is more important, we can increase the weight w. On the other hand, if the semantic similarity is more important, the weights w1 to w4 can be increased. If more training data is available, the level 4 vector will be more reliable. Hence, the weight w4 should increase. (4) y)} > 4 RightCilin < x, > 4 RightCilin cos(< w42 + y) > LeftCilin4 < x, > LeftCilin4 (< cos w41 { w4 y)} > 3 RightCilin < x, > 3 RightCilin (< cos w32 + y) > LeftCilin3 < x, > LeftCilin3 (< cos w31 { w3 y)} > 2 RightCilin < x, > 2 RightCilin (< cos w22 + y) > LeftCilin2 < x, > LeftCilin2 (< cos w21 { 2 y)} > 1 RightCilin < x, > 1 RightCilin (< cos w12 + y) > LeftCilin1 < , x > LeftCilin1 (< cos w11 { 1 y) > vector role < x, > vector role (< cos w = y) (x, similarity × + × + × + × + × w w In the experiments, w = 0.3, w1 = 0.1 × 0.7, w2 = 0.1 × 0.7, w3 = 0.4 × 0.7, and w4 = 0.4 × 0.7. Similarity Clustering Because of the lack of objective standards for evaluating of similarity measures, a agglomerative clustering algorithm is applied to group similar words according to a similarity value. It turns out that words with similar syntactic usage and similar semantic classes are grouped together. We will evaluate our algorithm by comparing the automatic clustering results with manual classifications of Cilin. Clustering Algorithm To evaluate the proposed similarity measure, we tried to group words according to various parameters. We adopted bottom-up agglomerative clustering algorithm to group words. In order to compare the clustered results with Cilin classifications and reduce the data sparseness, we picked the 1000 highest frequency words in Cilin for testing. First of all, we produced a 1000 × 1000 symmetric similarity matrix called SMatrix, where SMatrix (x, y) = similarity 4 3 2 1 4 , 3 , 2 , 1 K RightCilin K n RighttCili K LeftCilin x K LeftCilin K RightCilin x K RightCilin K RightCilin K n RighttCili K LeftCilin x K LeftCilin K LeftCilin x K LeftCilin 2 1 > < = + + + + = > < + > < + > < + > < > < + > < = > < + > < + > < + > < > < + > < = [i] SMatrix[x] , q , p 1 q p 1 q ≤ ≤ ≤ ≤ < < × = ∑∑ = = p n m m p n n q 1 word contains ) wordj ( Group m 1 word )contains wordx ( Group x j 0 ) word , (word similarity [x] SMatrix[j] , q , p 1 q p 1 q ≤ ≤ ≤ ≤ < < × = ∑∑ = = p n m m p n Clustering Results vs Cilin Classification We will make a comparison between the clustering results and Cilin classifications. There are two simple examples shown in Figure 1 Among our 1000 testing words, the number of words that were clustered in the 4 th level of Cilin was 658; i.e., they were labeled with 459 different level-4 Cilin classes and among them, 342 classes contained only one testing word, and the classes with multiple testing words contained a total of 658 testing words. With the threshold=0.7, our method clustered 830 words, and only 167 words of them were clustered in the correct Cilin class. Therefore, by Equation ( 5 ), recall 4 = 167/658 = 0.25 and with Equation ( 6 ) precision 4 = 167/830 = 0.20. We adopted two methods for measuring similarity; one used Equation TF × IDF, and the other used Equation TF. The results are shown in Figure 3 to Figure 6 in the Appendix. We measured the performance by computing the F -score, which is (recall+precision)/2. We discovered that the best F-score of level1 was that 0.7648 located at a threshold equal to 0.65, the best F-score of level2 was that 0.5178 located at a threshold equal to 0.7, the best F-score of level3 was that 0.3165 located at a threshold equal to 0.8, and that the best F-score of level4 was that 0.2476 located at a the threshold equal to 0.8. All were obtained using TF × IDF strategy. Hence, we can see that the TF × IDF equation achieves better performance than the TF equation does. We list the detailed F-socre data for various parameters in appendix. Although the clustering results didn't fit Cilin completely, they are still alike to some degree. From the results, we find that they are similar to syntax taxonomy under a lower threshold and close to semantic taxonomy under higher thresholds. Cilin classifications re -examined To examine the practicability of our proposed method, we also inspected the similarity values of these 658 testing words which were clustered into 117 4 th level Cilin classes. For each semantic class, the average similarity between words in the class and their standard deviation was comp uted. The results are listed in Table1 in the Appendix. We expected that synonyms would have high similarity values, but this was not always the case. According to the assumption noted above, synonyms might have similar syntactic and semantic contexts in language use. Therefore, the average similarity should be pretty high, and the standard deviation should be quite low. However, some of the results didn't follow the assumption. We analyzed the data offer explanations in the following. The context s of the noun (思想, \"thinking\") and the verbs (考慮,考量,思考, \"think\", \"consider\", \"deliberate\") were quite different. As a result, the average similarity value was quite low, and the standard deviation was very high. A fter we remo ved the noun from the word-set, we recomputed the values and obtained the The results conform to our assumption. They also reveal that the context of synonyms may vary from POS to POS. b) Error in Cilin Classification: The classifications in Cilin could be arbitrary. For example, the three words, 數量 \"quantity\", 多少 \"how many\" and 人數 \"the number of people\", were classified in a Cilin group. They might be slightly related, but grouping them together seems inappropriate according to the following table : Word set Average similarity Stand deviation 數量,多少 人數 0.379825 0.253895 c) Different uses: Differences in their usage cause synonyms to behave differently . For example, when we measured the similarity of 美國 \"America\" to 日本 \"Japan\" and to 中國 \"China\", the results we obtained were 0.86 and 0.62,respectively, for each pair. Accroding to human intuition, they simply refer to names of countries and should not have such different similarity values. The reason for these result is that the corpus we adopted is an original Taiwan corpus. As a result, the usage of 中國 \"China\" is different from that of 美國 \"America\" and 日本 \"Japan\". d) Polysemy: The word senses that Cilin adopted were not those frequently used in the corpus. See the following table : . Word set Average similarity Stand deviation 十分,非常, 特別 0.45054 0.305209 Although the three words, 十分 \"very/ten points\", 非常 \"very\" and 特 別\"special, extraordinary\" might seem to be very close in meaning to \"very\", the polysemous word 特別 \"special, extraordinary\" is different in its major sense. This influenced the result. e) Words with similar contexts might not be synonyms: A disadvantage does exist when the context vector model is used. Words that are similar in terms of their context s might not be similar in meaning. For example, the similarity value of 結婚 \"marry\" and 長大 \"grow\" is 0.8139. Although the two words have similar contexts, they are not alike in meaning. Therefore, the vector space model should incorporate the taxonomy approach to solve this phenomenon. Conclusions In this paper, we have adopted the context vector model to measure word similarity. The following new features have been proposed to enhance the context vector models : a) The weights of features are adjusted by applying TF × IDF. b) Feature vectors are smoothed by using Cilin categories to reduce data sparseness. c) Syntactic and semantic similarity is balanced by using related syntactic contexts only. The performance of our method might have been influenced by the small scale of the Chinese corpus and accuracy of the extracted relations. Further more, Cilin was published a long time ago and has not been update recently, which may have influenced our results. However, our experimental results are encouraging. They supports the theory that using context vectors to measure similarity is feasible and worthy of further research. Appendix",
         "18022704",
         "0b09178ac8d17a92f16140365363d8df88c757d0",
         "14",
         "https://aclanthology.org/O02-2002",
         null,
         null,
         "2002",
         "August",
         "International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 7, Number 2, August 2002: Special Issue on Computational {C}hinese Lexical Semantics",
         "Chen, Keh-Jiann  and\nYou, Jia-Ming",
         "A Study on Word Similarity using Context Vector Models",
         "37--58",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "chen-you-2002-study",
         null,
         null
        ],
        [
         "1",
         "R13-1042",
         "Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus.",
         "Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus. Introduction Law enforcement agencies frequently obtain large amounts of electronic messages, such as emails, which they must search for evidence. However, individual messages may be useless without the conversational context they occur in. Most modern emails contain useful metadata such as the MIME header In-Reply-To, which marks relations between emails in a thread and can be used to disentangle threads. However, there are easy methods of obfuscating email threads: opening an email account for a single purpose; using multiple email accounts for one person; sharing one email account among multiple persons; changing the Subject header; and removing quoted material from earlier in the thread. How can emails be organized by thread without metadata such as their MIME headers? We propose to use text similarity metrics to identify emails belonging to the same thread. In this paper, as a first step for temporal thread disentanglement, we perform pairwise classification experiments on texts in emails using no MIME headers or quoted previous emails. We have found that content-based text similarity metrics outperform a Dice baseline, and that structural and style text similarity features do not; adding these latter feature groups does not significantly improve total performance. We also found that contentbased features continue to outperform the others in both a class-balanced and class-imbalanced setting, as well as with semantically controlled or non-controlled negative instances. In NLP, Elsner and Charniak (2010) described the task of thread disentanglement as \"the clustering task of dividing a transcript into a set of distinct conversations,\" in which extrinsic thread delimitation is unavailable and the threads must be disentangled using only intrinsic information. In addition to emails with missing or incorrect MIME headers, entangled electronic conversations occur in environments such as interspersed Internet Relay Chat conversations, web 2.0 article response conversations that do not have a hierarchical display order, and misplaced comments in Wiki Talk discussions. Research on disentanglement of conversation threads has been done on internet relay chats (Elsner and Charniak, 2010) , audio chats (Aoki et al., 2003) , and emails with headers and quoted material (Yeh, 2006; Erera and Carmel, 2008) . However, to the best of our knowledge, no work has investigated reassembling email threads without the help of MIME headers or quoted previous emails. Previous researchers have used a number of email corpora with high-precision (non-Subject-clustered) thread marking. Joti et al. (2010) used the BC3 corpus of 40 email threads and 3222 emails for topic segmentation. Carenini et al. (2008) annotated 39 email \"conversations\" from the Enron Email Corpus for email summariation. Wan and McKeown (2004) used a privatelyavailable corpus of 300 threads for summary generation. Rambow et al. (2004) used a privatelyavailable corpus of 96 email threads for thread summarization. Data The Enron Email Corpus (EEC) 1 consists of the 517,424 emails (159 users' accounts and 19,675 total senders) that existed on the Enron Corporation's email server (i.e., other emails had been previously deleted, etc) when it was made public . Gold Standard Thread Extraction from the Enron Email Corpus We define an email thread as a directed graph of emails connected by Reply and Forward relations. In this way, we attempt to identify email discussions between users. However, the precise definition of an email thread actually depends on the implementation that we, or any other researchers, used to identify the thread. Previous researchers have derived email thread structure from a variety of sources. Wu and Oard (2005) , and Zhu et al. (2005) auto-threaded all messages with identical, non-trivial, Fwd: and Re:-stripped Subject headers. Klimt and Yang (2004) auto-threaded messages that had stripped Subject headers and were among the same users (addresses). Lewis and Knowles (1997) assigned emails to threads by matching quotation structures between emails. Wan and McKeown (2004) reconstructed threads by header Message-ID information. Rambow et al. (2004) used a privately-available corpus of 96 email threads, but did not specify how they determined the threads. As the emails in the EEC do not contain any inherent thread structure, it was necessary for us to create email threads. First, we implemented Klimt and Yang (2004) 's technique of clustering the emails into threads that have the same Subject header (after it has been stripped of pre-fixes such as Re: and Fwd:) and shared participants. To determine whether emails were among the same users, we split a Subject-created email proto-thread apart into any necessary threads, such that the split threads had no senders or recipients (including To, CC, and BCC) in common. The resulting email clusters had a number of problems. Clusters tended to over-group, because a single user included as a recipient for two different threads with the Subject \"Monday Meeting\" would cause the threads to be merged into a single cluster. In addition, many clusters consisted of all of the issues of a monthly subscription newsletter, or nearly identical petitions (see Klimt and Yang (2004) 's description of the \"Demand Ken Lay Donate Proceeds from Enron Stock Sales\" thread), or an auto-generated log of Enron computer network problems auto-emailed to the Enron employees in charge of the network. Such clusters of \"broadcast\" emails do not satisfy our goal of identifying email discussions between users. Many email discussions between users exist in previously quoted emails auto-copied at the bottom of latter emails of the thread. A singleannotator hand-investigation of 465 previously quoted emails from 20 threads showed that none of them had interspersed comments or had otherwise been altered by more recent thread contributors. Threads in the EEC are quoted multiple times at various points in the conversation in multiple surviving emails. In order to avoid creating redundant threads, which would be an information leak risk during evaluation, we selected as the thread source the email from each Klimt and Yang (2004) cluster with the most quoted emails, and discarded all other emails in the cluster. We used the quoteidentifying regular expressions from Yeh (2006) (see Table 1 ) to identify quoted previous emails. 2  There are two important benefits to the creation methodology of the Enron Threads Corpus 3 . First, since the emails were extracted from the same document, and the emails would only have been included in the same document by the email client if one was a Reply or Forward of the other, precision is very high (approaching 100%). 4 This is  better precision than threads clustered from separate email documents, which may have the same Subject, etc. generating false positives. Some emails will inevitably be left out of the thread, reducing recall, because they were not part of the thread branch that was eventually used to represent the thread, or simply because they were not quoted. Our pairwise classification experiments, described in Section 4, are unaffected by this reduced recall, because each experimental instance includes only a pair of emails, and not the entire thread. Second, because the thread source did not require human annotation, using quoted emails gives us an unprecedented number of threads as data: 209,063 emails in 70,178 threads of two emails or larger. The sizes of email threads in the Enron Threads Corpus is shown in Table 2 . Emails have an average of 80.0±201.2 tokens, and an average count of 4.4±9.3 sentences. Many of the emails are quite short: 18% are under 10 tokens, 19% are 10-20 tokens, and 13% are 20-30 tokens. Text Similarity Features We cast email thread disentanglement as a text similarity problem. Ideally, there exists a text similarity measure that marks pairs of emails from the system misidentified about 1% of emails from regular expression error. same thread as more similar than pairs of emails from different threads. We evaluate a number of text similarity measures, divided according to Bär et al. (2011) 's three groups: Content Similarity, Structural Similarity, Style Similarity. Each set of features investigates a different manner in which email pairs from the same thread may be identified. In our experiments, all features are derived from the body of the email, while all headers such as Recipients, Subject, and Timestamp are ignored. Content features. Content similarity metrics capture the string overlap between emails with similar content. A pair of emails with a high content overlap is shown below. The Longest Common Substring measure (Gusfield, 1997) identifies uninterrupted common strings, while the Longest Common Subsequence measure (Allison and Dix, 1986 ) and the singletext-length-normalized Longest Common Subsequence Norm measure identify common strings containing interruptions and text replacements and Greedy String Tiling measure (Wise, 1996) allows reordering of the subsequences. Other measures which treat texts as sequences of characters and compute similarities with various metrics include Levenshtein (1966) , Monge Elkan Second String measure (Monge and Elkan, 1997) , Jaro Second String measure (Jaro, 1989) , and Jaro Winkler Second String measure (Winkler, 1990) . A Cosine Similarity-type measure was used, based on term frequency within the document. Sets of ngrams from the two emails are compared using the Jaccard coefficient (from Lyon et al. (2004) ) and Broder's (1997) Containment measure. Structural features. Structural features attempt to identify similar syntactic patterns between the two texts, while overlooking topicspecific vocabulary. We propose that sturctural features, as well as style features below, may help in classification by means of communication accommodation theory (Giles and Ogay, 2007 ). Stamatatos's Stopword n-grams (2011) capture syntactic similarities, by identifying text reuse where just the content words have been replaced and the stopwords remain the same. We measured the stopword n-gram overlap with Broder's (1997) Containment measure and four different stopword lists. We also tried the Containment measure and an NGram Jaccard measure with part-of-speech tags. Token Pair Order (Hatzivassiloglou et al. 1999 ) uses pairs of words occurring in the same order for the two emails; Token Pair Distance (Hatzivassiloglou et al., 1999) measures the distance between pairs of words. Both measures use computed feature vectors for both emails along all shared word pairs, and the vectors are compared with Pearson correlation. Style features. Style similarity reflects authorship attribution and surface-level statistical properties of texts. Type Token Ratio (TTR) measure calculates text-length-sensitive and text-homogeneitysensitive vocabulary richness (Templin, 1957) . However, as this measure is sensitive to differences in document length between the pair of documents (documents become less lexically diverse as length and token count increases but type count levels off), and fluctuating lexical diversity as rhetorical strategies shift within a single document, we also used Sequential TTR (McCarthy and Jarvis, 2010), which corrects for these problems. Sentence Length and Token Length (inspired by (Yule, 1939) ) measure the average number of tokens per sentence and characters per token, respectively. Sentence Ratio and Token Ratio compare Sentence Length and Token Length between the two emails (Bär et al., 2011) . Function Word Frequencies is a Pearson's correlation between feature vectors of the frequencies of 70 pre-identified function words from Mosteller and Wallace (1964) across the two emails. We also compute Case Combined Ratio, showing the percentage of UPPERCASE characters in both emails combined ( U P P ERCASE e1 +U P P ERCASE e2 ALLCHARS e1 +ALLCHARS e2 ), and Case Document similarity, showing the similarity between the percentage of UPPERCASE characters in one email versus the other email. Evaluation In this series of experiments, we evaluate the effectiveness of different feature groups to classify pairs of emails as being from the same thread (positive) or not (negative). Each instance to be classified is represented by the features from a pair of emails and the instance classification, positive or negative. We used a variation of K-fold cross-validation for evaluation. The 10 folds contained carefully distributed email pairs such that email pairs with emails from the same thread were never used in pairs of training, development, and testing sets, to avoid information leakage. All instances were at one point in a test set. Instance division was roughly 80% training, 10% development, and 10% test data. Reported results are the weighted averages across all folds. The evaluation used logistic regression, as implemented in Weka (Hall et al., 2009) . Default parameters were used. We use a baseline algorithm of Dice Similarity between the texts of the two emails as a simple measure of set similarity. We created an upper bound by annotating 100 positive and 100 negative instances. A single native English speaker annotator answered the question, \"Are these emails from the same thread?\" Data Sampling Although we had 413,814 positive instances available in the Enron Threads Corpus, we found that classifier performance was unaffected by the amount of training data, down to very low levels (see Figure 1 ). However, because the standard deviation in the data did not level out until around 1,200 class-balanced training instances 5 , we used this number of positive instances (600) in each of our experiments. In order to estimate effectiveness of features for different data distributions, we used three different subsampled datasets. Random Balanced (RB) Dataset. The first dataset is class-balanced and uses 1200 training instances. Minimum email length is one word. For every positive instance we used, we created a negative email pair by taking the first email from the positive pair and pseudo-randomly pairing it with another email from a different thread that was assigned to the same training, development, or test set. However, the probability of semantic similarity between two emails in a positive instance is much greater than the probability of semantic similarity between two emails in a randomly-created negative instance. The results of experiments on our first dataset reflect both the success of our text similarity metrics and the semantic similarity (i.e., topical distribution) within our dataset. The topical distribution will vary immensely between different email corpora. To investigate the performance of our features in a more generaliable environment, we created a subsample dataset that con- trolls for semantic similarity within and outside of the email thread. Semantically Balanced (SB) Dataset. The second dataset combines the same positive instances as the first set with an equal number of semantically-matched negative instances for a training size of 1200 instances, and a minimum email length of one word. For each positive instance, we measured the semantic similarity within the email pair using Cosine Similarity and then created a negative instance with the same (±.005) similarity. Emails had an average of 96±287 tokens and 5±11sentences, and a similar token size distribution as SB. Random Imbalanced (RI) Dataset. However, both the RB and SB datasets use a class-balanced distribution. To see if our features are still effective in a class-imbalanced environment, we created a third dataset with a 90% negative, 10% positive distribution for both the training and test sets 6 . Specifically, we used the first dataset and then added an extra 8 negative instances for each positive instance. Experiments with this dataset use 10-fold cross validation, where each fold has 6000 training and 750 test instances. No minimum email length was used, similar to a more natural distribution. Results Our results are shown in Table 3 . Since we aim to detect pairs of emails belonging to the same thread rather than unrelated emails, we measure the system performance on the positive class. We use the standard F-measure of F 1 = 2×P (pos)×R(pos) P (pos)+R(pos) . As a measure to show performance on both positive and negative classes, we provide a standard accuracy measure of Acc= T P +T N T P +F N +T N +F P . Feature groups are shown in isolation as well as the complete set of features minus one group. 7  With the RB corpus, the best performing single feature configuration, content features group (P=.83 ±.04), matches the human upper bound precision(P=.84). The benefit of content features is confirmed by the reductions in complete feature set performance when they are left out. The content features group was the only group to perform significantly above the Dice baseline. Adding the other feature groups does not significantly improve the overall results. Further leave-one-out experiments revealed no single high performing feature within the content features group. Structural features produced low performance, failing to beat the Chance baseline. Structural similarity from rhetorical strategy is rare in an email conversational setting. Any structural benefits are likely to come from sources unavailable in a disguised email situation, such as auto-signatures identifying senders as the same person. The low results on structural features show that we are not relying on such artifacts for classification. Style features were also unhelpful, failing to significantly beat the Dice baseline. The features failed to identify communication accomodation within the thread. Results on the SB dataset show that there is a noticeable drop in classification for all feature groups when negative instances have a similar semantic similarity as positive instances. The configuration with all features showed a 15 percentage point drop in precision, and a 12 percentage point drop in accuracy. However, content features continues to be the best performing feature group with semantically similar negative instances, as with random negative instances, and outperformed the Dice baseline. Adding the additional feature groups does not significantly improve overall performance. The results on the RI corpus mirror results from the balanced (RB) corpus. 2011 ) use coherence models to disentangle chat, using some features (entity grid, topical entity grid) which correspond to the information in our content features group. They also found these content-based features to be helpful. Inherent limitations Certain limitations are inherent in email thread disentanglement. Some email thread relations cannot be detected with text similarity metrics, and require extensive discourse knowledge, such as the emails below. Email1: Can you attend the Directors Fund Equity Board Meeting next Wednesday, Nov 5, at 3pm? Email2: Yes, I will be there. Several other problems in email thread disentanglement cannot be solved with any discourse knowledge. One problem is that some emails are identical or near-identical; there is no way to choose between textually identical emails. Table 4 shows some of the most common email texts in our corpus, based on a <.05 similarity value from Jaro Second String similarity, as described in Section 3. However, near identical texts make up only a small portion of the emails in our corpus. In a sample of 5,296 emails, only 3.6% of email texts were within a .05 Jaro Second String similarity value of another text. Another problem is that some emails are impossible to distinguish without world and domain knowledge. Consider a building with two meeting rooms: A101 and A201. Sometimes A101 is used, and sometimes A201 is used. In response to the question, Which room is Monday's meeting in?, there may be no way to choose between A101 and A201 without further world knowledge. Another problem is topic overlap. For example, in a business email corpus such as the EEC, there are numerous threads discussing Monday morning 9am meetings. The more similar the language used between threads, the more difficult the disentanglement becomes, using text similarity. This issue is addressed with the SB dataset. Finally, our classifier cannot out-perform humans on the same task, so it is important to note human limitations in email disentanglement. Our human upper bound is shown in Table 3 . We will further address this issue in Sections 4.4. Error Analysis We inspected 50 email pairs each of true positives, false positives, false negatives, and true negatives from our RB experiments 8 . We inspected for both technical details likely to affect classification, and for linguistic features to guide future research. Technical details included small and large text errors (such as unidentified email headers or incorrect email segmentation), custom and noncustom email signatures, and the presense of large signatures likely to affect classification. Linguistic features included an appearance of consecutivity (emails appear in a Q/A relation, or one is informative and one is 'please print', etc.), similarity of social style (\"Language vocab level, professionalism, and social address are a reasonable match\"), and the annotator's perception that the emails could be from the same thread. An example of a text error is shown below. Names and dates occur frequently in legitimate email text, such as meeting attendance lists, etc., which makes them difficult to screen out. Emails from false positives were less likely to contain these small errors (3% versus 14%), which implies that the noise introduced from the extra text has more impact than the false similarity potentially generated by similar text errors. Large text errors (such as 2 emails labelled as one) occurred in only 1% of emails and were too rare to correlate with results. Autosignatures, such as the examples below, mildly impacted classification. Instances classified as negative (both FN and TN) were marginally more likely to have had one email with a non-customized autosignature (3% versus 1.5%) or a customized auto-signature (6.5% versus 3.5%). Autosignatures were also judged likely to affect similarity values more often on instances classified as negative (20% of instances). The presence of the autosignature may have introduced enough noise for the classifier to decide the emails were not similar enough to be from the same thread. We define a non-custom auto-signature as any automatically-added text at the bottom of the email. We did not see enough instances where both emails had an autosignature to evaluate whether similarities in autosignatures (such as a common area code) impacted results. Some email pair similarities, observable by humans, are not being captured by our text similarity features. Nearly all (98%) positive instances were recognized by the annotator as potential consecutive emails within a thread, or non-consecutive emails but still from the same thread, whereas only 46% of negative instances were similarly (falsely) noted. Only 2% of negative instances were judged to look like they were consecutive emails within the same thread. The following TP instance shows emails that look like they could be from the same thread but do not look consecutive. Below is a TN instance with emails that look like they could be from the same thread but do not look consecutive. Email1: i do but i havent heard from you either, how are things with wade Email2: rumor has it that a press conference will take place at 4:00 -more money in, lower conversion rate. The level of professionalism (\"Language vocab level, professionalism, and social address are a reasonable match\") was also notable between class categories. All TP instances were judged to have a professionalism match, as well as 94% of FN's. However, only 64% of FP's and 56% of TN's were judged to have a professionalism match. Based on a review of our misclassified instances, we are surprised that our classifier did not learn a better model based on style features (F 1 =.60). Participants in an email thread appear to echo the style of emails they reply to. For instance, short, casual, all-lowercase emails are frequently responded to in a similar manner. Conclusion In this paper, we have described the creation of the Enron Threads Corpus, which we made available online. We have investigated the use of text similarity features for the pairwise classification of emails for thread disentanglement. We have found that content similarity features are more effective than style or structural features across class-balanced and class-imbalanced environments. There appear to be more stylistic features uncaptured by our similarity metrics, which humans access for performing the same task. We have shown that semantic differences between corpora will impact the general effectiveness of text similarity features, but that content features remain effective. In future work, we will investigate discourse knowledge, highly-tuned stylistic features, and other email-specific features to improve headerless, quoteless email thread disentanglement. Acknowledgments This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No. I/82806, and by the Center for Advanced Security Research (www.cased.de).",
         "16703040",
         "3eb736b17a5acb583b9a9bd99837427753632cdb",
         "10",
         "https://aclanthology.org/R13-1042",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Jamison, Emily  and\nGurevych, Iryna",
         "Headerless, Quoteless, but not Hopeless? Using Pairwise Email Classification to Disentangle Email Threads",
         "327--335",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "jamison-gurevych-2013-headerless",
         null,
         null
        ],
        [
         "2",
         "W05-0819",
         "In this paper, we describe a word alignment algorithm for English-Hindi parallel data. The system was developed to participate in the shared task on word alignment for languages with scarce resources at the ACL 2005 workshop, on \"Building and using parallel texts: data driven machine translation and beyond\". Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours. We trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in Hindi sentences. The system scored 77.03% precision and 60.68% recall on the shared task unseen test data.",
         "In this paper, we describe a word alignment algorithm for English-Hindi parallel data. The system was developed to participate in the shared task on word alignment for languages with scarce resources at the ACL 2005 workshop, on \"Building and using parallel texts: data driven machine translation and beyond\". Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours. We trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in Hindi sentences. The system scored 77.03% precision and 60.68% recall on the shared task unseen test data. Introduction This paper describes a word alignment system developed as a part of shared task on word alignment for languages with scarce resources at the ACL 2005 workshop on \"building and using parallel texts: data driven machine translation and beyond\". Participants in the shared task were provided with common sets of training data, consisting of English-Inuktitut, Romanian-English, and English-Hindi parallel texts and the participating teams could choose to evaluate their system on one, two, or all three language pairs. Our system is for aligning English-Hindi parallel data at the word level. The word-alignment algorithm described here is based on a hybridmulti-feature approach, which groups Hindi words locally within a Hindi sentence and uses dictionary lookup (DL) as the main method of aligning words along with other methods such as Transliteration Similarity (TS), Expected English Words (EEW) and Nearest Aligned Neighbors (NAN). We used the training data supplied to derive rules for local word grouping in Hindi sentences and to find Named Entities (NE) and cognates using our TS approach. In the following sections we briefly describe our approach. Training Data The training data set was composed of approximately 3441 English-Hindi parallel sentence pairs drawn from the EMILLE (Enabling Minority Language Engineering) corpus (Baker et al., 2004) . The data was pre-tokenized. For the English data, a token was a sequence of characters that matches any of the \"Dr.\", \"Mr.\", \"Hon.\", \"Mrs.\", \"Ms.\", \"etc.\", \"i.e.\", \"e.g.\", \"[a-zA-Z0-9]+\", words ending with apostrophe and all special characters except the currency symbols £ and $. Similarly for the Hindi, a token consisted of a sequence of characters with spaces on both ends and all special characters except the currency symbols £ and $. Word Alignment Given a pair of parallel sentences, the task of word alignment can be described as finding one-to-one, one-to-many, and many-to-many correspondences between the words of source and target sentences. It becomes more complicated when aligning phrases of one language with the corresponding words or phrases in the target language. For some words, it is also possible not to find any translation in the target language. Such words are aligned to null. The algorithm presented in this paper, is a blend of various methods. We categorize words of a Hindi sentence into one of four different categories and use different techniques to deal with each of them. These categories include: 1) NEs and cognates 2) Hindi words for which it is possible to predict their corresponding English words 3) Hindi words that match certain pre-specified regular expression patterns specified in a rule file (explained in section 3.3.) and finally 4) words which do not fit in any of the above categories. In the following sections we explain different methods to deal with words from each of these categories. Named Entities and Cognates According to WWW1, the Named Entity Task is the process of annotating expressions in the text that are \"unique identifiers\" of entities (e.g. Organization, Person, Location etc.). For example: \"Mr. Niraj Aswani\", \"United Kingdom\", and \"Microsoft\" are examples of NEs. In most text processing systems, this task is achieved by using local pattern-matching techniques e.g. a word that is in upper initial orthography or a Title followed by the two adjacent words that are in upper initial or in all upper case. We use a Hindi gazetteer list that contains a large set of NEs. This gazetteer list is distributed as a part of Hindi Gazetteer processing resource in GATE (Maynard et al., 2003) . The Gazetteer list contains various NEs including person names, locations, organizations etc. It also contains other entities such as time units -months, dates, and number expressions. Cognates can be defined as two words having a common etymology and thus are similar or identical. In most cases they are pronounced in a similar way or with a minor change. For example \"Bungalow\" in English is derived from the word \"बं गला\" in Hindi, which means a house in the Bengali style (WWW2). We use our TS method to locate such words. Section 3.2 describes the TS approach. Transliteration Similarity For the English-Hindi alphabets, it is possible to come up with a table consisting of correspondences between the letters of the two alphabets. This table is generated based on the various sounds that each letter can produce. For example a letter \"c\" can be mapped to two letters in Hindi, \"क\" and \"स\". This mapping is not restricted to one-to-one but also includes many-tomany correspondences. It is also possible to map a sequence of two or more characters to a single character or to a sequence two or more characters. For example \"tio\" and \"sh\" in English correspond to the character \"श\" in Hindi. Prior to executing our word alignment algorithm, we use the TS approach to build a table of NEs and cognates. We consider one pair of parallel sentences at a time and for each word in a Hindi sentence, we generate different English words using our TS table. We found that before comparing words of two languages, it is more accurate to eliminate vowels from the words except those that appear at the start of words. We use a dynamic programming algorithm called \"edit-distance\" to measure the similarity between these words (WWW3). We calculate the similarity measure for each word in a Hindi sentence by comparing it with each and every word of an English sentence. We come up with an m x n matrix, where m and n refer to the number of words in Hindi and English respectively. This matrix contains a similarity measure for each word in a Hindi sentence corresponding to each word in a parallel English sentence. From our experiments of comparing more than 100 NE and cognate pairs, we found that the word pairs should be considered valid matches only if the similarity is greater than 75%. Therefore, we consider only those pairs which have the highest similarity among the other pairs with similarity greater than 75%. The following example shows how TS is used to compare a pair of English-Hindi words. For example consider a pair \"aswani अ।सवानी\" and the TS table entries as shown below: A अ, S स, SS स, V व, W व and N न We remove vowels from both words: \"aswn असवन\", and then convert the Hindi word into possible English words. This gives four different combinations: \"asvn\", \"assvn\", \"aswn\" and \"asswn\". These words are then compared with the actual English word \"aswn\". Since we are able to locate at least one word with similarity greater than 75%, we consider \"aswani अ।सवानी\" as a NE. Once a list of NEs and cognates is ready, we switch to our next step: local word grouping, where all words in Hindi sentences, either those available in the gazetteer list or in the list derived using TS approach, are aligned using TS approach. Local Word Grouping Hindi is a partially free order language (i.e. the order of the words in a Hindi sentence is not fixed but the order of words in a group/phrase is fixed). Unlike English where the verbs are used in different inflected forms to indicate different tenses, Hindi uses one or two extra words after the verb to indicate the tense. Therefore, if the English verb is not in its base form, it needs to be aligned with one or more words in a parallel Hindi sentence. Sometimes a phrase is aligned with another phrase. For example \"customer benefits\" aligns with \" ाहक के फायदे \". In this example the first word \"customer\" aligns with the first word \" ाहक\" and the second word \"benefits\" aligns with the third word \"फायदे \". Considering \"customer satisfaction\" and \" ाहक के फायदे \" as phrases to be aligned with each other, \"के \" is the word that indicates the relation between the two words \" ाहक\" and \"फायदे \", which means the \"benefits of customer\" in English. These words in a phrase need to be grouped together in order to align them correctly. In the case of certain prepositions, pronouns and auxiliaries, it is possible to predict the respective Hindi postpositions, pronouns and other words. We derived a set of more than 250 rules to group such patterns by consulting the provided training data and other grammar resources such as Bal Anand (2001) . The rule file contains the following information for each rule: 1) Hindi Regular Expression for a word or phrase. This must match one or more words in the Hindi sentence. 2) Group name or a part-of-speech category. 3) Expected English word(s) that this Hindi word group may align to. 4) In case a group of one or more English words aligns with a group of one or more Hindi words, information about the key words in both groups. Key words must match each other in order to align English-Hindi groups. 5) A rule to convert Hindi word into its base form. We list some of the derived rules below: 1) Group a sequence of [X + Postposition], where X can be any category in the above list except postposition or verb. For example: \"For X\" = \"X के िलये \", where \"For\" = \"के िलये \". 2) Root Verb + (रहा, रही or रहे ) + (PH). Present continuous tense. We use \"PH\" as an abbreviation to refer to the present/past tense conjunction of the verb \"होना\" -ं , ह , है , हो, etc. 3) Group two words that are identical to each other. For example: \"अलग अलग\", which means \"different\" in English. Such bi-grams are common in Hindi and are used to stress the importance of a word/activity in a sentence. Once the words are grouped in a Hindi sentence, we identify those word groups which do not fit in any of the TS and EEW categories. Such words are then aligned using the DL approach. Dictionary lookup Since the most dictionaries contain verbs in their base forms, we use a morphological analyzer to convert verbs in their base forms. The English-Hindi dictionary is obtained from (WWW4). The dictionary returns, on average, two to four Hindi words referring to a particular English word. The formula for finding the lemma of any Hindi verb is: infinitive = root verb + \"ना\". Since in most cases, our dictionary contains Hindi verbs in their infinitive forms, prior to comparing the word with the unaligned words, we remove the word \"ना\" from the end of it. Due to minor spelling mistakes it is also possible that the word returned from dictionary does not match with any of the words in a Hindi sentence. In this case, we use edit-distance algorithm to obtain similarity between the two words. If the similarity is greater than 75%, we consider them similar. We use EEW approach for the words which remain unaligned after the DL approach. Expected English words Candidates for the EEW approach are the Hindi word groups (HWG) that are created by our Hindi local word grouping algorithm (explained in section 3.3). The HWGs such as postpositions, number expressions, month-units, day-units etc. are aligned using the EEW approach. For example, for the Hind word \"बावन\" in a Hindi sentence, which means \"fifty two\" in English, the algorithm tries to locate \"fifty two\" in its parallel English sentence and aligns them if found. For the remaining unaligned Hindi words we use the NAN approach. Nearest Aligned Neighbors In certain cases, words in English-Hindi phrases follow a similar order. The NAN approach works on this principle and aligns one or more words with one of the English words. Considering one HWG at a time, we find the nearest Hindi word that is already aligned with one or more English word(s). Aligning a phrase \"customer benefits\" with \" ाहक के फायदे \" (example explained in section 3.3) is an example of NAN approach. Similarly consider a phrase \"tougher controls\", where for its equivalent Hindi phrase \"अिधक िनयं ण\", the dictionary returns a correct pair \"controls िनयं ण\", but fails to locate \"tougher अिधक\". For aligning the word \"tougher\", NAN searches for the nearest aligned word, which, in this case, is \"controls\". Since the word \"controls\" is already aligned with the word \"िनयं ण\", the NAN method aligns the word \"tougher\" with the nearest unaligned word \"अिधक\". Test Data results We executed our algorithm on the test data consisting of 90 English-Hindi sentence pairs. We obtained the following results for non-null alignment pairs.",
         "1215281",
         "b20450f67116e59d1348fc472cfc09f96e348f55",
         "15",
         "https://aclanthology.org/W05-0819",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Aswani, Niraj  and\nGaizauskas, Robert",
         "Aligning Words in {E}nglish-{H}indi Parallel Corpora",
         "115--118",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "aswani-gaizauskas-2005-aligning",
         null,
         null
        ],
        [
         "3",
         "R13-1044",
         "The paper 1 presents a rule-based approach to semantic relation recognition within the Polish noun phrase. A set of semantic relations, including some thematic relations, has been determined for the need of experiments. The method consists in two steps: first the system recognizes word pairs and triples, and then it classifies the relations. Evaluation was performed on random samples from two balanced Polish corpora.",
         "The paper 1 presents a rule-based approach to semantic relation recognition within the Polish noun phrase. A set of semantic relations, including some thematic relations, has been determined for the need of experiments. The method consists in two steps: first the system recognizes word pairs and triples, and then it classifies the relations. Evaluation was performed on random samples from two balanced Polish corpora. Introduction Semantic relation recognition is a well-known task in natural language processing. Although the relation recognition within noun phrase and between nominals was studied intensely, the task is still challenge for semantic analysis of Polish. We are aware of few papers and projects dealing with Semantic Role Labelling between predicates and their arguments, cf. (Gołuchowski and Przepiórkowski, 2012) or (Lun, 2009) , but of none concerning semantic relation recognition inside Polish noun phrase. Related work In (Nastase et al., 2006) authors classify semantic relations between a head and a modifier of a noun phrase. Number of all relation types was equal to 30. These relations were grouped into 5 more general groups. The authors experimented with decision trees, instance-based learning and Support Vector Machines. For each relation they learnt the binary classifier; as the baseline for F-measure they used the model with all of examples classified as positive and recall being equal to 100%. With regard to the semantic relation the baseline ranged between 17.78% and 60.35%. Identifying the semantic relations inside compound nouns was presented in (Uchiyama et al., 2008) . The authors used SVM classifier and in the best configuration of features, they achieved accuracy of about 84%. In (Rosario and Hearst, 2001) authors used neural networks to determine 20 semantic relationssimilarily to (Nastase et al., 2006) -between a head and a modifier of noun phrase. They used a domain-specific lexical hierarchy of medicine. The authors achieved accuracy of about 60%. The workshop SemEval-2010 (task 8) concerned the recognition of semantic relations between nominals. In (Tratz and Hovy, 2010) the authors developed a system based on the Maximum Entropy classifier, able to detect 10 bidirectional semantic relations Achieved F-measures depended on the system configuration and lay between 66, 68% and 77, 75%. The same set of semantic relations was used in (Rink and Harabagiu, 2010) . The authors used Support Vector Machines classifier and a very rich set of features (i.e., part of speech for all constituents of a semantic relation pair, number of words between the nominals, features based on paths in the dependency tree from Stanford dependency parser). F-measure of this approach was 82.19%. Authors in (Tymoshenko and Giuliano, 2010 ) used shallow syntactic parsing and semantic information from ResearchCyc (Lenat, 1995) in the same task of recognizing semantic relations. They used liner combination of kernels (semantic and syntactic) using Support Vector Machines classifier. For the best combination of kernels, they obtained F-measure equal to 77.62%. There are some works, where rule-based approaches were used. In (Huang, 2009) there has been proposed an approach for automatic construction of rules identifying ten types of seman-tic relations, using five types of input informations. The relation instances were extracted from Modern Chinese Standard Dictionary. The authors achieved very high precision (range from 0, 81 to 0.99), but recall was low -about 0, 2. In (Hearst, 1992) authors used set of manually written rules for identification of hyperonymy relations. (Ben Abacha and Zweigenbaum, 2011) used linguistic patterns (built semi-automatically from corpora) to identify semantic relatios in medical texts. In this domain-specific task they achieved 75.72% precision and 60, 46% recall. Recognized semantic relation types We seek for semantic relations within nominal phrases. The relation set consists of 12 semantic relations, of which 5 are thematic (semantic) roles 2 . Definitions of our semantic relations are based on works of (Kearns, 2011), (Palmer et al., 2010) , (Van Valin, 2004) , (Larson, 1996) , (Dowty, 1991) , (Jędrzejko, 1993) , (Laskowski and Wróbel, 1997) . We tried to select relations that are very frequent or frequent in Polish texts. 3 The relation set is following (thematic roles are marked with theta, other relations -with rho): Proto-Agent θ -it is an instigator of an action or an entity that is in a particular state, it may undergoe change of state not caused by another participant; for predicates denoting relations -it is the first element of the relation: (człowiek) wykształcony przez Jana θ '(man) educated by John θ ', wyj ący wilk θ 'howling wolf θ '. The Proto-Agent macrorole covers subroles of Agent, Causer and nonagentive non-causative Actor (cf. Actor macrorole in (Kearns, 2011) ). Proto-Patient θ is the second macrorole -it is an entity undergoing action, event or change of state caused by another participant; for predicates denoting relations -it is the second element of a given relation: wykształcenie kogoś θ 'educating someone θ ', (Jan) posiadaj ący maj ątek θ '(John) possessing an estate θ '. According to (Dowty, 1991) 2 In Polish, as in other Indo-European languages, verbs could be nominalized during a process of syntactic transformation (Jędrzejko, 1993) , (Kolln, 1990) . Such nominalized predicates could be linked with nouns by thematic relations. 3 Rationale for selection of the presented semantic relation types was their frequencies in a four-text sample taken from a Polish corpus KPWr. Together chosen relations account for ca 80% of all semantic relation occurrences in these texts. Most of our relation types could be found on the list of the most frequent relation types in the English noun phrase (Moldovan et al., 2004, Tab. 1) . many thematic roles come down to the macroroles of Proto-Patient and Proto-Agent. Instrument θ is a tool, a device or means used by someone in order to cause something, it is sometimes regarded as a secondary cause of situation or change of state: przeszyty włóczni ą 'speared with a spear', lina θ cumownicza adjective 'a hawser, lit. mooring rope θ '. Material θ is an entity that is used by someone to produce something from it, material undergoes change of state resulting in its disappearance and emerging of a result: zrobiony z mosi ądzu θ 'made out of brass θ ', mosiężna θ figurka 'brass θ statuette'. Purpose θ -an entity or a situation toward which the event is directed or an individual which benefits from the event (purpose combines goal, beneficiary and recipient roles): wręczenie (medali) olimpijczykom 'giving (medals) to Olympians θ , sala koncertowa θ 'a concert θ hall'. Location is a physical place at which a given event is localised, a place being destination of an event, a path or a source of motion, or simply a place at which a particular individual is situated: wręczenie (medali) w auli 'giving (medals) at the lecture theatre ', przedzieranie się przez moczary 'struggling through the swamp '. Time is a particular moment or a duration of an event -it localises a situation within the flow of events or gives its duration: przedzieranie się przez godzinę /w środę 'struggling for an hour /on Wednesday '. Temporal/spatial meronymy -these relations point onto a spatial or temporal part of a place/location/time/period): poniedziałkowy poranek 'Monday morning ', środek zimy 'middle of the winter', koniec drogi 'end of the road', stolica kraju 'capital of the country'. Attribute is a property of an individual or an event, such as colour, size, weigth, intensity, duration etc., which might be expressed with a qualitative adjective: czerwony samochód 'red car', głośna muzyka 'loud music'. Family (member) is a relative or an in-law to someone, the relation is bidirectional and reflexive: syn króla 'king's son ', moja żona 'my wife ' (I am a relative to my wife). Order gives a position of an entity or an event in an ordered sequence/chain: druga odpowiedź '2nd answer', lata 80 . 'eighties, lit. eightieth years'. Quantity is an amount of something or a cardinality of a given set: pięciu panów 'five men', kieliszek wina 'glass of wine'. Semantic relation recognition rule-based algorithm Our rule-based system proceeds in two steps 4 : first it recognizes word pairs and triples, then operators classifying relations enter. Recognizing word pairs and triples Since we consider relations within noun phrases, we must identify them correctly. We made use of a CRF shallow parser (Radziszewski and Pawlaczek, 2012) trained on an annotated corpus of Polish (KPWr) (Broda et al., 2012) which comprises shallow syntactic annotation level (Radziszewski et al., 2012) . KPWr contains 326 annotated text samples representing different genres and styles: blogs, press articles, official and legal texts and Polish Wikipedia articles, it comprises 106358 annotations (phrases and phrase heads, and predicateargument relations). Noun and preposition phrases (NPs/PPs) from the corpus correspond to arguments of predicateargument structure. Each such NP/PP constists of one or several smaller phrases based on agreement (AgPs, for details, please look at cited works). Here is an example NP from the corpus (a head of the phrase is boldfaced, AgP heads are underlined): [[samolot wyprodukowany] AgP [przez PZL] AgP [w roku 1938] AgP [w Łodzi] AgP ] N P 'aircraft made by PZL in (year) 1938 in Łódź (city)' There is no reliable deep parser for Polish (Gołuchowski and Przepiórkowski, 2012) , thus we decided to construct a simple rule-based algorithm for deepened shallow parsing of Polish NPs/PPs. The algorithm works on tagged texts -we used (Radziszewski, 2013) tagger. Parsing rules make use of an output from the CRF shallow parser (Radziszewski and Pawlaczek, 2012) , in particular: borders of whole NPs/PPs, and of their constituents (i.e., phrases based on agreement, AgPs). Found pairs and triples are directly connected within a syntactic structure. Hand-written rules act like a partial dependency parser. The pairs consist of one subordinate and 4 Similarly to system presented in (Gamallo et al., 2002) . one superordinate token, the triples comprise one superordinate token and a subordinate preposition phrase (preposition + governed nominal head of a subordinate noun phrase). The whole algorithm runs in a main loop which iterates AgP i heads. We start from the first AgP 0 head to the left, then we proceed to the right, jumping from AgP i head to the closest AgP i+1 head to the right. For every AgP i head we run a cascadelike chain of rules (numbered from 1 to 7) for genetives, nominatives, small preposition phrases (being a part of larger NPs or PPs), coordination, other known to the tagger tokens, other unknown to the tagger tokens and for modifiers. The algorithm in pseudocode was shown in Algorithm 1 The algorithm gives following description for just analysed phrase, \"R + number\" denotes the number of a rule in the Algorithm 1 activated on the word pair or triple (for instance, R3 means that the rule number 3 was activated): R7: samolot ← wyprodukowany 'plane made', R3: wyprodukowany ← przez PZL 'by PZL',R3: wyprodukowany ← w roku 'in year', R3: wyprodukowany ← w Łodzi 'in Łódź' . Such simple shallow parsing algorithm operates quite well on an annotated part of KPWr with Fmeasure equal to 84%, P = 88%, R = 80%. 5 Applying WCCL operators Having identified pairs and triples we run on them operators written in a constraint language WCCL (Radziszewski et al., 2011) . The operators are language-specific and utilize morphosyntactic features (POS, case, number and gender), domains of Polish WordNet lexical units (word-sense pairs (Maziarz et al., 2012) ), thousands of derivational relation instances between nouns, adjectives and verbs from the wordnet 6 and information about syntactic frames of nominalized predicates, taken from Polish valence dictionary (Dębowski and Woliński, 2007) . Each of written operators refers to one semantic relation. In other words, each semantic relation is described by one or by many WCCL operators. If an operator is successfully applied to a pair (or a For example, our Proto-Patient relation was described by the 6 WCCL operators. One of them is presented in Listing 1. This operator uses two dictionaries with valence frames (acc -a list of verbs possessing any accusative frame, frames -a list of verbs described in the Polish valence dictionary (Dębowski, 2013) ) and morphosyntactic information about part of speech (class) and case. This operator PROTO-PATIENT-acc captures pairs like dręcz ący pact Janka noun.acc−θ 'tormenting John θ ' with a noun playing a Proto-Patient role of the predicate dręcz ący. The operator first checks whether a predicate (active participle) has an accusative frame or is outside the dictionary of Dębowski (\"frames\"). Since dręczyć 'to torment' is in acc dictionary and since Janek 'John' has subst class and acc case -the boolean operator returns 'true'. Let us present another example: the Proto-Agent macrorole is recognized by 5 operators, in Listing 2 was shown one of them. The PROTO-AGENT-ger-przez-acc operator is written for triples, i.e., for a triple wydanie pact przez pron wydawcę noun.acc−θ 'publishing by the publisher θ '. The first element in the triple is a gerund form of verb wydać 'to publish'. The operator checks whether the verb wydać has in its frame accusative/genetive or whether it cannot be found in Dębowski's dictionary (position 0 in the triple, frames). Listing 1: One of the WCCL operators describing Proto-Patient relation. Language details has been described in (Radziszewski et al., 2011) , abbreviations for grammatical categories has been explained in (Przepiórkowski et al., Next the operator seeks for the preposition przez 'by' at position 1. Then it tests if the first meaning of the lemma wydawca 'publisher' does not belong to the domain 'time' (= Polish czas) in Polish WordNet (position 2). Indeed, the first meaning of wydawca is in the domain 'person' (that iformation is avaiable in the dictionary noun_domain). At the end, we check whether the last token of our triple is in accusative. Because all of these conditions are fulfilled, the operator returns 'true', and we may assume that the last token takes the role of Proto-Agent. In Listing 3 one operator for family ralation was shown. FAMILY-agpp used to recognize this relation for word pairs. The operator, inter alia, uses semantic dictionary of kinship names built on the basis of Polish WordNet (the dictionary kinship), lammas of possessive pronouns (e.g., mój 'my', twój 'yours'). Listing 3: Two WCCL operators describing Family relation @b:\"FAMILY-agpp\" ( and( // agreement agrpp(0,1, {nmb, gen, cas}), // position 0 in(base[0], [\"moj\", \"twoj\", \"swoj\", \"nasz\", \"wasz\"]) // position 1 equal(lex(base[1], \"kinship\"), [\"1\"]), equal(lex( base[1], \"noun_domain\"), [\"os\"]), in(class[1], {ger, subst, depr}), ) ) Results and conclusions Evaluation of the presented semantic relation recognition algorithm was performed in three steps. First experiment (labelled kpwr) was performed on a random sample of the KPWr corpus (26 out of 326 texts, aproximately one thirteenth of the corpus). In this experiment we made use of syntactic annotations from KPWr (cf. Tab. 1). Second experiment was performed on a random sample of 100 texts taken from yet another Polish corpus, called NKJP (Przepiórkowski et al., 2012, nkjp, approximately one tenth of the corpus) 7 . Since NKJP lacked syntactic annotations of KPWr style, we were forced to run on it the CRF shallow parser (described in Sec. 4.1) . This experiment gave us information about performance of our algorithm on a 'bare' text (see Tab. 2). Evaluation in the experiments was done by a professional linguist. At last, four baseline models were constructed and evaluated on the two corpora (Tab. 3). We created baselines similar to that presented in (Uchiyama et al., 2008) , which was majority model. We chose the most frequent relation, which in the sample from KPWr was Proto-Patient (with the number of 113 instances out of 268 relation instances), this relation type was also the most frequent in the sample of NKJP (411 out of 1950 relation instances). For each corpora two baselines were calculated: in Baseline #1 we assumed that we had perfectly recognized all occurences of semantic relations (of any type), in Baseline #2 we simply signed with 'Proto-Patient' label every recognized by our system semantic relation instance. Baseline #2 is realistic, while #1 is idealistic, since to obtain #1 we should be able to recognize every single relation instance within a corpus. Baselines #1 are upper limits for all majority models (including #2). Our two idealistic baselines are higher than the realistic baselines (see Tab. 3). Percentile bootstrap methods (DiCiccio and Efron, 1996), (DiCiccio and Romano, 1988) were applied to statistical significance and confidence interval (CI) analysis of the data. 8 We took 10000 bootstrap resamplings for each measure (P, R, F1), α was equal to 0.05 for each one-tailed test and CI (a percentile CI need not be symmetrical). In nkjp we have beaten both idealistic and realistic baselines. Precision, recall and F1 for kpwr are higher than Baseline #2. Only idealistic Baseline #1 for the KPWr corpus has overtaken our rule-based algorithm with regard to recall (42.2% vs. 32.8%), while its precision is lower and F1's are statistically indistinguishable. Results are promising, precisions go above 50% (the lower endpoint for the kpwr confidence intervel), for nkjp we may assess it even more precisely as 60%-65%. Some semantic relations are recognized with higher precision: Proto-Agent (nkjp: 89-100%, kpwr: 90-100%, α = 0.05), Proto-Patient (nkjp: 88-95%, kpwr: 83%-98%), family (nkjp: 90-100%) and order (nkjp: 91-100%). Our system is thus comparable in this aspect to the systems described in Sec. 2. 9  Overall recall is low, but higher than realistic baselines. In kpwr we obtained R = 27-38%, while for nkjp we got statistically higher interval of 37-41%. It seems that recall was not affected by lack of marked NP/PP borders in the corpus (these should have been brought out by the CRF shallow parser). F-measures calculated on our both corpora are also much higher than realistic baselines #2. We can already conclude that our preliminary experiments turned successful. Now we are aiming at improving our operators to raise their recall and at expanding the semantic role set (e.g., for Agent, Causer, Experiencer, Possessor or Result). Parallel, we start work on construction of automatic algorithms for relation recognition. not avaiable. 9 Not directly, of course.",
         "2491460",
         "c0f1047fe0f95c367184d494e78bb07b11ee3608",
         "2",
         "https://aclanthology.org/R13-1044",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "K{\\k{e}}dzia, Pawe{\\l}  and\nMaziarz, Marek",
         "Recognizing semantic relations within {P}olish noun phrase: A rule-based approach",
         "342--349",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "kedzia-maziarz-2013-recognizing",
         null,
         null
        ],
        [
         "4",
         "W05-0818",
         "In this paper we describe LIHLA, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (NATools) and languageindependent heuristics to find links between single words and multiword units in sentence-aligned parallel texts. The method has achieved an alignment error rate of 22.72% and 44.49% on English-Inuktitut and Romanian-English parallel sentences, respectively.",
         "In this paper we describe LIHLA, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (NATools) and languageindependent heuristics to find links between single words and multiword units in sentence-aligned parallel texts. The method has achieved an alignment error rate of 22.72% and 44.49% on English-Inuktitut and Romanian-English parallel sentences, respectively. Introduction Alignment of words and multiword units plays an important role in many natural language processing (NLP) applications, such as example-based machine translation (EBMT) (Somers, 1999) and statistical machine translation (SMT) (Ayan et al., 2004; Och and Ney, 2000) , transfer rule learning (Carl, 2001; Menezes and Richardson, 2001) , bilingual lexicography (Gómez Guinovart and Sacau Fontenla, 2004) , and word sense disambiguation (Gale et al., 1992) , among others. Aligning two (or more) texts means finding correspondences (translation equivalences) between segments (paragraphs, sentences, words, etc.) of the source text and segments of its translation (the target text). Following the same idea of many recently proposed approaches on lexical alignment (e.g., Wu and Wang (2004) and Ayan et al. (2004) ), the method described in this paper, LIHLA (Language-Independent Heuristics Lexical Aligner) starts from statistical alignments between single words (defined in bilingual lexicons) and applies languageindependent heuristics to them, aiming at finding the best alignments between words or multiword units. Although the most frequent alignment category is 1 : 1 (in which one source word is translated exactly as one target word), other categories such as omissions (1 : 0 or 0 : 1) or those involving multiword units (n : m, with n and/or m ≥ 1) are also possible. This paper is organized as follows: section 2 explains how LIHLA works; section 3 describes some experiments carried out with LIHLA together with their results and, in section 4, some concluding remarks are presented. How LIHLA works As the first step, LIHLA uses alignments between single words defined in two bilingual lexicons (source-target and target-source) generated from sentence-aligned parallel texts using NATools. 1  Given two sentence-aligned corpus files, the NA-Tools word aligner -based on the Twenty-One system (Hiemstra, 1998) -counts the co-occurrences of words in all aligned sentence pairs and builds a sparse matrix of word-to-word probabilities (Model A) using an iterative expectation-maximization algorithm (5 iterations by default). Finally, the elements with higher values in the matrix are chosen to compose two probabilistic bilingual lexicons (source-target and target-source) (Simões and Almeida, 2003) . For each word in the corpus, each bilingual lexicon gives: the number of occurrences of that word in the corpus (its absolute frequency) and its most likely translations together with their probabilities. The construction of the bilingual lexicons is an independent prior step for the alignment performed by LIHLA and the same bilingual lexicons can be used several times to align parallel sentences. So, using the two bilingual lexicons generated by NATools and some language-independent heuristics, LIHLA tries to find the best alignment between source and target tokens (words, numbers, special characters, etc.) in a pair of parallel sentences. For each source token s j in source sentence S, LIHLA will look for the best token t i in the target parallel sentence T applying these heuristics in sequence: 1. Exact match LIHLA creates a 1 : 1 alignment between s j and t i if they are identical. This heuristic stays for exact matches, for instance, between proper names and numbers. Best candidate according to the bilingual lexicon LIHLA looks for possible translations of s j in the source-target bilingual lexicon (B S ) and makes an intersection between them and the words in T . In this intersection, if no candidate word identical to those in B S is found, then LIHLA tries to look for cognates for those words using the longest common subsequence ratio (LCSR). 2 By doing this, LIHLA can deal with small changes in possible translations such as different forms of the same verb, changes in gender and/or number of nouns, adjectives, and so on. Then, LIHLA selects the best target candidate word t i for s j -the best candidate word according to B S among those in a position which is favorably situated in relation to s j -and looks for multiword units involving s j and t i -those words that occur immediately before and/or after s j (for source multiword units) or 2 The LCSR of two words is computed by dividing the length of their longest common subsequence by the length of the longer word. For example, the LCSR of Portuguese word alinhamento and Spanish word alineamiento is 10 12 0.83 as their longest common subsequence is a-l-i-n-a-m-e-n-t-o. t i (for target multiword units) and are not possible translations for other words in T and S, respectively. According to the multiword units that have (or not) been found, a 1 : 1, 1 : n, m : 1 or m : n alignment is established. An omission alignment for s j (1 : 0) can also be established if no target candidate word t i that satisfies this heuristic is available. Cognates If no possible translation for s j is found in the bilingual lexicon and the target sentence (T ) at the same time, LIHLA uses the LCSR to look for cognates for s j in T and sets a 1 : 1 alignment between s j and its best cognate or a 1 : 0 alignment if there is no cognate available. These heuristics are applied while alignments can still be produced and a maximum number of iterations is not reached (see section 3 for the number of iterations performed in the experiments described in this paper). Furthermore, at the first iteration, all words with a frequency higher than a set threshold are ignored to avoid erroneous alignments since all subsequent alignments are based on the previous ones. In its last step (which is optional and has not been performed in the experiments described in this paper), LIHLA aligns the remaining unaligned source and target tokens between two pairs of already aligned tokens establishing several 1 : 1 alignments when there are the same number of source and target tokens, or just one alignment involving all source and target tokens if they exist in different quantities. The decision of creating n 1 : 1 alignments in spite of just one n : n alignment when there is the same number of source and target tokens is due to the fact that a 1 : 1 alignment is more likely to be found than a n : n one. Experiments In this section we present the experiments carried out with LIHLA for the \"Shared task on word alignment\" in the Workshop on Building and Using Parallel Texts during ACL2005. Systems participating in this shared task were provided with training data (consisting of sentence-aligned parallel texts) for three pairs of languages: English-Inuktitut, Romanian-English and English-Hindi. Furthermore, the systems would choose to participate in one or both subtasks of \"limited resources\" (where systems were allowed to use only the resources provided) and \"unlimited resources\" (where systems were allowed to use any resources in addition to those provided). The system described in this paper, LIHLA, participated in the subtask of limited resources aligning English-Inuktitut and Romanian-English test sets. The training sets -composed of 338,343 English-Inuktitut aligned sentences (omission cases were excluded from the whole set of 340,526 pairs) and 48,478 Romanian-English aligned ones-were used to build the bilingual lexicons. Then, without changing any default parameter (threshold for LCSR, maximum number of iterations, etc.), LIHLA aligned the 75 English-Inuktitut and the 203 Romanian-English parallel sentences on test sets. The whole alignment process (bilingual lexicon generation and alignment itself) did not take more than 17 minutes for English-Inuktitut (3 iterations per sentence, on average) and 7 minutes for Romanian-English (4 iterations per sentence, on average). The evaluation was run with respect to precision, recall, F -measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003) . Tables 1 and 2 present metric values for English-Inuktitut and Romanian-English alignments, respectively, as provided by the organization of the shared task. The results obtained in these experiments were not so good as those achieved by LIHLA on the language pairs for which it was developed, that is, 92.48% of precision and 88.32% of recall on Portuguese-Spanish parallel texts and 84.35% of precision and 76.39% of recall on Portuguese-English ones. 3  The poor performance in the English-Inuktikut task may be partly due to the fact that Inuktikut is a polysynthetic language, that is, one in which, unlike in English, words are formed by long strings of concatenated morphemes. This makes it difficult for NATools to build reasonable dictionaries and lead to a predominance of n : 1 alignments, which are harder to determine -this fact can be confirmed by the better precision of LIHLA when probable alignments were considered (see table 1 ). The performance in the English-Romanian task, not very far from the English-Portuguese task used to tune up the parameters of the algorithm, is harder to explain without further analysis. Metric The difference in precision and recall between the two language pairs is due to the fact that on the English-Inuktitut reference corpus in addition to sure alignments the probable ones were also annotated while in Romanian-English only sure alignments are found. This indicates that evaluating alignment systems is not a simple task since their performance depends not only on the language pairs and the quality of parallel corpora (constant criteria in this shared task) but also the way the reference corpus is built. So, at this moment, it would be unfair to blame the worse performance of LIHLA on its alignment methodology since it has been applied to the new language pairs without changing any of its default parameters. Maybe a simple optimization of parameters for each pair of languages could bring better results and also the impact of size and quality of training and reference corpora used in these experiments should be investigated. Then, the only conclusion that can be taken at this moment is that LIHLA, with its heuristics and/or default parameters, can not be indistinctly applied to any pair of languages. Despite of its performance, LIHLA has some advantages when compared to other lexical alignment methods found in the literature, such as: it does not need to be trained for a new pair of languages (as in Och and Ney (2000) ) and neither does it require pre-processing steps to handle texts (as in Gómez Guinovart and Sacau Fontenla ( 2004 )). Furthermore, the whole alignment process (bilingual lexical generation and alignment itself) has proved to be very fast as mentioned previously. Concluding remarks This paper has presented a lexical alignment method, LIHLA, which aligns words and multiword units based on initial statistical word-to-word correspondences and language-independent heuristics. In the experiments carried out at the \"Shared task on word alignment\" which took place at the Workshop on Building and Using Parallel Texts during ACL2005, LIHLA has been evaluated on English-Inuktitut and Romanian-English parallel texts achieving an AER of 22.72% and 44.49%, respectively. As future work, we aim at investigating the impact of using additional linguistic information (such as part-of-speech tags) on LIHLA's performance. Also, as a long-term goal, LIHLA will be part of a system implemented to learn transfer rules from sequences of aligned words. Acknowledgments We thank FAPESP, CAPES, CNPq and the Spanish Ministry of Science & Technology (Project TIC2003-08681-C02-01) for financial support.",
         "15322146",
         "ff3f05120d24e5dac2879f25402993bc6355f780",
         "5",
         "https://aclanthology.org/W05-0818",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Caseli, Helena M.  and\nNunes, Maria G. V.  and\nForcada, Mikel L.",
         "{LIHLA}: Shared Task System Description",
         "111--114",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "caseli-etal-2005-lihla",
         null,
         null
        ],
        [
         "5",
         "R13-1045",
         "We describe an approach to building a morphological analyser of Arabic by inducing a lexicon of root and pattern templates from an unannotated corpus. Using maximum entropy modelling, we capture orthographic features from surface words, and cluster the words based on the similarity of their possible roots or patterns. From these clusters, we extract root and pattern lexicons, which allows us to morphologically analyse words. Further enhancements are applied, adjusting for morpheme length and structure. Final root extraction accuracy of 87.2% is achieved. In contrast to previous work on unsupervised learning of Arabic morphology, our approach is applicable to naturally-written, unvowelled Arabic text.",
         "We describe an approach to building a morphological analyser of Arabic by inducing a lexicon of root and pattern templates from an unannotated corpus. Using maximum entropy modelling, we capture orthographic features from surface words, and cluster the words based on the similarity of their possible roots or patterns. From these clusters, we extract root and pattern lexicons, which allows us to morphologically analyse words. Further enhancements are applied, adjusting for morpheme length and structure. Final root extraction accuracy of 87.2% is achieved. In contrast to previous work on unsupervised learning of Arabic morphology, our approach is applicable to naturally-written, unvowelled Arabic text. Introduction The number and diversity of human languages makes it impractical to manually craft lexicons and morphological processors for more than a very small proportion of them. Further challenges are posed by the need to deal with dialects and colloquial forms of languages. This has motivated recent increased interest in approaches to morphological analysis based on unsupervised learning. Inspired by competitions such as the Morpho Challenge, many techniques have been proposed for unsupervised morphology learning. Although these techniques are often intended to be language independent, they are often directed to a specific group of languages. Most work has aimed at sequential separation or segmentation of morphemes concatenated together in a surface word form. This type of analysis, outputting stems and appended morphemes aims to identify some kind of border between the different morphemes. However, another type of word formation consists of the interdigitation of a root morpheme with an affix or pattern template; in this case there is no boundary between morphemes, since they are rather intercalated with each other. This type of non-concatenative morphology, which is characteristic of the Semitic group of languages, has attracted far less interest for unsupervised learning. In this paper we present an approach to unsupervised learning of non-concatenative morphology, applying it to Arabic. We describe an approach to learning tri-literal roots and affix template of Arabic by first inducing root and affix lexicons. Our approach uses Maximum Entropy modelling to obtain clusters 1 of words based on concatenative and non-concatenative orthographic features, and induces the lexicons from these clusters. Our data is an undiacritized version of the Quranic Arabic Corpus since we assume a realistic setting of unvowelled text, as most Arabic text is written without vowels; we chose this corpus since correct roots of each word are available, facilitating the evaluation process. The fact that the corpus contains a relatively small vocabulary of around 7000 words also simulates the scenario for most of the world's languages of scarcity of linguistic resources and data. This paper is structured as follows: Section 2 surveys previous related work. Section 3 provides an introduction to Arabic root and pattern morphology. Our approach to unsupervised lexicon induction based on Maximum Entropy (ME) modelling is explained in section 4. Section 5 describes the procedure for performing morphological analysis of words, followed by evaluation in section 6 and conclusions in section 7. 2 Related Work An active current area of natural language processing research is applying statistical and information-theoretic approaches to unsupervised learning of morphology and grammar. A common starting point is raw (unannotated) text corpora, inducing the target knowledge from word forms and their patterns of usage. Information theoretic approaches, particularly Minimum Description Length (MDL) as investigated by Goldsmith (2000 Goldsmith ( , 2006) ) and others (Cruetz and Lagus, 2005, 2007) , have brought a theoretical perspective considering input data to be 'compressed' into a morphologically analysed representation. This optimization scheme has achieved good results, and is amongst the most effective approaches for unsupervised morphological analysis. Most work on unsupervised learning of morphology has focused on concatenative morphology (De Pauw and Wagacha 2007; Hammarström and Borin 2011) . Another perspective adopted by Schone and Jurafsky (2001) incorporates orthographic and phonological features, and induces semantic relatedness between word pairs using Latent Semantic Indexing. Their work shows comparable performance to Goldsmith's (2000) Linguistica system. Yarowsky and Wicentowski (2000) experiment with learning irregular mnaturaorphology using a lightly supervised technique to align irregular words to their lemmas by estimating the distribution of ratios over part-of-speech classes of inflected words to lemmas. More recently, researchers have addressed nonconcatenative morphology, such as for Semitic languages, using a variety of empirical approaches. Daya et al. (2008) learn Semitic roots using supervised learning, building a multi-class classifier for individual root radicals. Clark (2007) uses Arabic as a test-bed to study semi-supervised learning of complex broken plural structure modelled using memory-based algorithms, with the aim of gaining insights into human language acquisition. Most work on unsupervised learning of morphology has focused on concatenative morphology (Hammarström and Borin 2011) . The few studies that have focussed on nonconcatenative morphology, such as for Semitic languages, have not used naturally written text. For example, Rodriguez and Ćavar (2005) learn roots using a number of orthographic heuristics and then apply constraint-based learning to improve the quality of roots. Xanthos (2008) works on phonetic transcriptions of Arabic text to decipher roots and patterns. The approach is to initially create crude Root and Pattern (RP) transcriptions from words based on vowel-consonant distinctions, and then to apply an MDL approach similar to Goldsmith's (2006) in order to refine the RP structures. In contrast to previous work, we learn intercalated morphology, identifying the root and transfixes/ incomplete pattern for words from 'natural' text without short vowels or diacritical markers. Root and Pattern Morphology Words in Arabic are formed through three morphological processes. The first (i) is the fusion of a root form and pattern template to derive a base word, which can be a noun, verb or adjective, all of which are semantically related to the root. The second (ii) is affixation, by means of prefixes, suffixes or infixes, including inflectional morphemes marking gender, plurality and/or tense, resulting in a stem. Thirdly (iii) a final layer of clitics may be attached to a word, including a subset of prepositions, conjunctions, determiners and pronouns; these appear at the beginning (proclitics) or end (enclitics) of a word but never in the middle. Since techniques for concatenative morphology learning are fairly advanced we have focused on using stemmed words, computable through such approaches. We used the QAC stem vocabulary where appended morphemes of type (iii) are mostly absent 2 and hence ignored from analysis. Most of type (ii) are present as part of the stem. In the case of (i), most derived forms consist of short vowels and occasional long vowels or a consonant interdigitated with the root. In unvowelled text the short vowels are ignored, so derived words have at most single letter affixation. Table 1 shows two example words with their roots and affix pattern templates. The 'y' and 't' in the respective words are clitic/inflectional markers, which are part of the affix template. 'A' is the derivational infix marker for nouns. Word Root Pattern ktAby Ktb --A-y tEArf Erf t-A-- For analysis, each word, ‫ݓ‬ , is decomposed, using a decomposition function, into a set of tuples encoding all ݊ possible combinations of a root (of at least 3 letters) and associated pattern: ‫ݓ(݀‬ ) → ‫ݎ〈{‬ ௫ , ‫‬ ௫ 〉} (Eq. 1) where ‫ݔ‬ ranges from 1 to ݊. For example, the decomposition of the word 'yErf', is shown in Figure 1 . ‫݂ݎܧݕ‬ → ⎩ ⎪ ⎨ ⎪ ⎧ ‫ݕ〈‬ ‫ܧ‬ ‫,ݎ‬ − − −݂ 〉, ‫ݕ〈‬ ‫ܧ‬ ݂, − − ‫,〉−ݎ‬ ‫ݕ〈‬ ‫ݎ‬ ݂, ‫ܧ−‬ − −〉, ‫ܧ〈‬ ‫ݎ‬ ݂, ‫ݕ‬ − − −〉, ‫ݕ〈‬ ‫ܧ‬ ‫ݎ‬ ݂, − − − −〉 ⎭ ⎪ ⎬ ⎪ ⎫ Using Maximum Entropy Modelling for Unsupervised Learning In this study we apply an supervised machine learning technique, Maximum Entropy (ME) modelling, in a completely unsupervised way, taking our inspiration from the work of De Pauw and Wagacha ( 2007 ), who applied the approach for extracting prefixes in an African language. Unlike for supervised learning, no annotated text is used. Instead we simply derive features automatically from the vocabulary words of the dataset. Each word is represented as an output class mapped to by the corresponding features of the words. These word-features are used to train a classifier. Rather than applying the classifier to classify unseen data, we apply the model back to the 'training data' to obtain, not the classification but the proximities of each word/class with every other word/class. These proximities are then utilized to root and pattern lexicons. The advantage of this approach to gauge relatedness of words over other approaches, such as minimum edit distance, is the ability to better capture morpheme dependencies between words with common roots which may be orthographically quite different due to substantial affixing. Building the Lexicons We derive two lexicons: a root lexicon and an affix or pattern lexicon. We do this by training ME classifiers on orthographic features computed from each word in the corpus dataset. The classifiers are then applied to the same data to obtain word clusters relating each word to every other word with respect to either common roots or common patterns. Thus, for the root lexicon we obtain neighbours of words that have the same or similar patterns. Conversely, for the pattern lexicon we obtain neighbours of words that have common root radicals. Modelling Orthographic Features We first extract orthographic features for obtaining word clusters with similar roots (i.e. for pattern lexicon acquisition). We then construct the inverse of these features for obtaining word clusters with similar patterns (i.e. for root lexicon acquisition). In the former case, feature extraction proceeds as follows: we first enclose each word with beginning and end boundary markers, '@' and '#' respectively. (This is in order to provide context information for the first and last characters of a word). We next compute the power-set of all the character combinations in a word, and then exclude features where the first and last letter of the word appear without the boundary markers (to give emphasis to word boundary features). The final set of these features for the word 'yErf' is shown in the first column of Table 2 . In the latter case, pattern features are obtained such that corresponding to each root feature, we replace root radicals with a placeholder; characters between root radicals that are omitted from the root features appear as potential affix characters in the pattern template. These inverse features are shown in the second column of Table 2 . Root Features (for Pattern Lexicon) Pattern features (for Root Lexicon) @y, @yE, @yEr, @yErf#, @yEr#, @yEf#, @yE#, @yr, @yrf#, @yr#, @yf#, @y#, @E, @Er, @Erf#, Er#, @Ef#, @E#, @r, @rf#, @r#, @f#, E, Er, Erf#, Er#, Ef#, E#, r, rf#, r#, f# @-, @--, @---, @----#, @---f#, @--r-#, @--rf#, @-E-, @-E--#, @-E-f#, @-Er-#, @-Erf#, @y-, @y--, @y---#, @y--f#, @y-r-#, @y-rf#, @yE-, @yE--#, @yE-f#, @yEr-#, -, --, ---#, --f#, -r-#, -rf#, -, --#, -f#, -# Word Nearest Neighbors The classifier is trained using Limited Variable LBFGS optimization method. The number of iterations for training is stopped automatically when 100% accuracy on the training data is achieved. Each trained classifier is reapplied to its respective training data features to get proximity values between each word and every other word. Sorting the list gives us the most related word in terms of root based or pattern based proximity values, with the highest value (≈ 1) for the headword, ℎ, i.e. the word's own features. Table 3 shows an example of the closest neighbours in a cluster, along with their headword. Using these words and proximity measures we next apply a strategy to induce the morpheme. Not all words in the list of N elements for each word are relevant to us since the proximity value starts to drop rapidly towards zero as we go down the ranked list. With each headword we choose a 500 nearest neighbours cluster for each type of morpheme as a sufficient number beyond which we expect no gain in efficiency is expected. Head-Word, h Proximity Dictionary Induction Using the respective word clusters we create dictionaries for two types of morphemes, roots and patterns, such that we score the morphemes thus: Higher scoring morphemes are more plausible and ranked higher in the lexical list than lower ones. The procedure for scoring is adapted and amended from the work of De Pauw and Wagacha (2007). For the pattern lexicon, we score each pattern in the following manner: for each headword, h i (having probability value ≈ 1) in cluster c i (with each of the i = 1,2,…N words in the vocabulary), we obtain all possible decompositins(equation 1) into template patterns ‫‬ ௫ (shown in column 1 of Table 4 ) and roots, ‫ݎ‬ ௫ (column 2 of Table 4 ) with respect to the headword, ℎ . Each pattern is scored with a function ܵ( ‫‬ ௫ ) (equation 2) which aggregates the Logarithmically Scaled ( ‫ܵܮ‬ ) probability value, ܲ of words k j (j = 1,2,…500 words in each cluster), such that ‫ݎ‬ ௫ matches any of the roots in word k, ‫ݎ‬ ௬ (y=1,2,…m root combinations in k). This aggregation is not only local to each cluster but covers all occurrences of the pattern in each of the N clusters. ܵ( ‫‬ ௫ ) = ቀ‫ܵܮ‬൫ܲ ൯× ‫|(ܣܮ‬ ௫ |)ቚ ‫ݎ‬ ௫ = ‫ݎ‬ ௬ ቁ ହ ୀଵ ே ୀଵ (Eq. 2) Logarithmic scaling is necessary since the probability drops too rapidly and too low in order to provide a feasible ratio between words. After taking the log of the probability the resulting ratios are negative which are then adjusted by subtracting the log of a base probability value, ܲ , thus linearly inverting the ratios (equation 3). ܲ is hence chosen to be small enough to ensure the resulting logarithmic score is positive. We chose the smallest occurring probability value in our clusters as the value for ܲ . ‫ܵܮ‬൫ܲ ൯= log ܲ(݇ ) − log ܲ (Eq. 3) The score is also exponentially Length Adjusted ‫)ܣܮ(‬ for each pattern, ‫,‬ according to the length of the pattern, ‫,||‬ in terms of the number of affix charaters in ‫.‬ This boosts the score for lengthier morphemes which are relatively infrequent. The intuition for adjustment formula comes from the work of (Chung and Gildea, 2009) and (Liang and Klein, 2009) , who use a exponential Length Penalty measure to adjust their model for morpheme length. ‫)||(ܣܮ‬ = ݁ || (Eq. 4) Thus the pattern is scored according to the score of words containing plausible roots. Commonly occurring patterns such as 'y---' gather weight and ascend the list of the most frequent (and hence potentially sound) affix templates. Table 4 shows how each pattern for the headword 'yErf' is scored, aggregating the logarithmic score over words (in column 4 of Similarly, we score the root, ܵ( ‫ݎ‬ ௫ ), with respect to the pattern occurrence in each word k of cluster c i : ܵ( ‫ݎ‬ ௫ ) = ቀ‫ܵܮ‬൫ܲ ൯ቚ ‫‬ ௫ = ‫‬ ௬ ቁ ହ ୀଵ ே ୀଵ (Eq. 5) The scoring aggregates over the log scaled probability of words in the affix-based clusters having pattern occurrences in a word in each cluster. There is no need for length adjustment to these ratios since we are considering only three letter roots. Table 5 exemplifies this for scoring roots with words (in column 3 of Table 5 ) that have corresponding patterns (in column 2 of Table 5 ). Root Pattern Word Table 6 shows the top lexicon entries for roots and patterns along with their respective scores. The top entries in the lexicon would plausibly be correct morphemes while lower entries would be not so plausible. Root Lexicon Morphological Analysis A word is analysed into its root and pattern template by considering every possible combination of trilateral root and corresponding pattern pairs, ‫ݎ〈‬ ௫ , ‫‬ ௫ 〉 , as defined in equation 1 for the word, w i , in the vocabulary, scoring each analysis with the sum of the scores for the root, ‫ݎ‬ ௫ , and pattern, ‫‬ ௫ , in the root lexicon and pattern lexicon, respectively. Due to the different ranges of scores for root and pattern, the score for the former is scaled with respect to the latter, as in equation 6, in order to guarantee equal contributions. ‫)ݎ(ܵܵ‬ = ‫)ݎ(ܵ‬ × max(ܵ(‫))‬ max(ܵ(‫))ݎ‬ (Eq. 6) The analysis, x, with the highest score is selected as the output, as illustrated in equation 7. max ௫ୀଵ.. ( ܵ( ‫ݎ‬ ௪ ௫ ) + ‫(ܵܵ‬ ௪ ௫ ) ) (Eq. 7) Since we are considering text without diacritics, due to absence of short vowels, we only expect words to contain single letter infixes. Hence we experiment with an alternative configuration of the word decomposition, ‫ݎ〈‬ ௭ , ‫‬ ௭ 〉: non-contiguous root radicals formed with more than one intervening character are dropped; correspondingly patterns with more than one consecutive character between radical place holder markers are dropped. Evaluation We carry out our evaluation using the Quranic Arabic Corpus (QAC) 3 , since it identifies the root of each word, facilitating the evaluation. In this section, we first detail some information about our dataset before going onto evaluation of the analyses for correct root extraction. Data The QAC consists of approximately 77,900 word tokens, with a total of around 19,000 unique tokens. Since we are interested in investigating learning from undiacritized text, we removed all short vowels and diacritical markers. The size of the resulting vocabulary, after removal of vowels, is approximately 14,850. We take as input lightly stemmed text, with clitics removed, but with most inflectional markers attached. We assume that stemmed words are obtainable using existing tools for unsupervised concatenative morphology learning. For example, the technique of Poon et al (2009) could be used to obtain accurate stems for each word. The stemmed unvowelled vocabulary size is around 7370. The original corpus is annotated with roots for all derived and inflected words. More than 95% of words are tagged with their root forms since the Quran consists mostly of words of derivable forms, with very few proper nouns. There are 7192 stemmed words with available roots. In Arabic, sometimes alterations in root radicals take place; for example, in hollow roots, when moving from a root containing a long vowel to the surface word, the long vowel might change its form to another type or get dropped. Such words with hollow roots or reduplicated radicals, whose characters do not match every radical of the root, were removed from the evaluation as they are beyond the scope of the learning algorithm to identify. Leaving aside these word and root evaluation pairs we evaluated with 5468 stemmed types. Baseline As a baseline for evaluation, we derived lexicons in a similar manner to procedure for derivation from clusters (section 5.3). Instead of using clusters we simply scored patterns that matched the largest number of vocabulary words having corresponding roots. Likewise, the root score was obtained by counting the number of words with corresponding patterns. Comparing our system to the baseline is meant to elucidate the advantage of using the machine learning technique to enhance our lexicons. In the baseline we do not have the ME based word clusters with proximities to the target word; only one cluster exist: the vocabulary set with unit promitiy of 1. Evaluation of Lexicons In this section we compare our lexicons, built using maximum entropy modeling approach, (ME), to the baseline(BL). We evaluated the effect of logarithmic scaling (ME_LS) comparing it to using raw probability values(ME_RW). Also we gauged the performance improvement with Length Adjustment (ME_LS_LA) for morphemes. Finally, we evaluated morphological analysis restricted to patterns with single affixes which correspond to roots with single non-contiguous characters from words (ME_NC1). We evaluate morphological analysis through correct identification of the root. The accuracy is measured in terms of percentage of the roots that are correctly identified. As stated above, we evaluate on a total of 5468 words. The results for the different configuration evaluations is given in table 7 . The accuracy of 74% shows a sound and competitive baseline. The low results for ME_RW highlights the weakness of considering raw probability values which are too low to provide adequate weightage to morphemes. Hence the dismal performace. The true value for the ME based processing is realized in ME_LS, where the probabilities have been logarithmically scaled be summing. We see an accuracy gain of 6% over the baseline which is quite significant and encouraging. Further improvements can be seen when the score has been adjusted for morpheme length, ME_LS_LA, with performance increase by further 5%. Still more improvement is seen using knowledge of word structure of undiacritized text, ME_LS_LS_NC1, with further accuracy gain of 2.25 %. The final result for ME based analysis with further enhancements gives an promising accuracy result of 87.20%. Configuration Conclusion and Future directions In this paper we have presented an approach to solve the problem of learning intercalated morphology in an unsupervised manner with no parameter settings and minimal linguistic knowledge. We applied the machine learning based techniques to learn clusters of words related on basis of either root or pattern morpheme. Thereafter, plausible morphemes are extracted using a scoring method which takes advantage of knowledge of word proximities from clusters built using a maximum entropy classifier. We further apply enhancements to the procedure by accommodating for length and structure of morphemes. The finalized procedure offers significant boost in performance. The dynamicity of the technique allows its applicability to other types of morphological structures. Also, the system can easily be extended to cater to roots beyond tri-literals by adapting the soring function to accommodate for morpheme length.",
         "690455",
         "0b125557ba23075532380e88fb990933838975b7",
         "2",
         "https://aclanthology.org/R13-1045",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Khaliq, Bilal  and\nCarroll, John",
         "Unsupervised Induction of {A}rabic Root and Pattern Lexicons using Machine Learning",
         "350--356",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "khaliq-carroll-2013-unsupervised",
         null,
         null
        ],
        [
         "6",
         "W05-0821",
         "Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model.",
         "Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model. Introduction Statistical machine translation (SMT) makes use of a noisy channel model where a sentence ē in the desired language can be conceived of as originating as a sentence f in a source language. The goal is to find, for every input utterance f, the best hypothesis ē * such that ē * = argmax ēP (ē| f ) = argmax ēP ( f |ē)P (ē) (1) P ( f |ē) is the translation model expressing probabilistic constraints on the association of source and target strings. P (ē) is a language model specifying the probability of target language strings. Usually, a standard word trigram model of the form P (e 1 , ..., e l ) ≈ l i=3 P (e i |e i−1 , e i−2 ) (2) is used, where ē = e 1 , ..., e l . Each word is predicted based on a history of two preceding words. Most work in SMT has concentrated on developing better translation models, decoding algorithms, or minimum error rate training for SMT. Comparatively little effort has been spent on language modeling for machine translation. In other fields, particularly in automatic speech recognition (ASR), there exists a large body of work on statistical language modeling, addressing e.g. the use of word classes, language model adaptation, or alternative probability estimation techniques. The goal of this study was to use some of the language modeling techniques that have proved beneficial for ASR in the past and to investigate whether they transfer to statistical machine translation. In particular, this includes language models that make use of morphological and part-of-speech information, so-called factored language models. Factored Language Models A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework. Assuming that each word w can be decomposed into k features, i.e. w ≡ f 1:K , a trigram model can be defined as p(f 1:K 1 , f 1:K 2 , ..., f 1:K T ) ≈ T t=3 p(f 1:K t |f 1:K t−1 , f 1:K t−2 ) (3) Each word is dependent not only on a single stream of temporally preceding words, but also on additional parallel streams of features. This representation can be used to provide more robust probability estimates when a particular word n-gram has not been observed in the training data but its corresponding feature combinations (e.g. stem or tag trigrams) has been observed. FLMs are therefore designed to exploit sparse training data more effectively. However, even when a sufficient amount of training data is available, a language model utilizing morphological and POS information may bias the system towards selecting more fluent translations, by boosting the score of hypotheses with e.g. frequent POS combinations. In FLMs, word feature information is integrated via a new generalized parallel backoff technique. In standard Katz-style backoff, the maximum-likelihood estimate of an n-gram with too few observations in the training data is replaced with a probability derived from the lower-order (n − 1)gram and a backoff weight as follows: p BO (w t |w t−1 , w t−2 ) (4) = d c p M L (w t |w t−1 , w t−2 ) if c > τ α(w t−1 , w t−2 )p BO (w t |w t−1 ) otherwise where c is the count of (w t , w t−1 , w t−2 ), p M L denotes the maximum-likelihood estimate, τ is a count threshold, d c is a discounting factor and α(w t−1 , w t−2 ) is a normalization factor. During standard backoff, the most distant conditioning variable (in this case w t−2 ) is dropped first, followed by the second most distant variable etc., until the unigram is reached. This can be visualized as a backoff path (Figure 1(a) ). If additional conditioning variables are used which do not form a temporal sequence, it is not immediately obvious in which order they should be eliminated. In this case, several backoff paths are possible, which can be summarized in a backoff graph (Figure 1 is also possible to choose multiple paths and combine their probability estimates. This is achieved by replacing the backed-off probability p BO in Equation 2 by a general function g, which can be any non-negative function applied to the counts of the lower-order n-gram. Several different g functions can be chosen, e.g. the mean, weighted mean, product, minimum or maximum of the smoothed probability distributions over all subsets of conditioning factors. In addition to different choices for g, different discounting parameters can be selected at different levels in the backoff graph. One difficulty in training FLMs is the choice of the best combination of conditioning factors, backoff path(s) and smoothing options. Since the space of different combinations is too large to be searched exhaustively, we use a guided search procedure based on Genetic Algorithms (Duh and Kirchhoff, 2004) , which optimizes the FLM structure with respect to the desired criterion. In ASR, this is usually the perplexity of the language model on a held-out dataset; here, we use the BLEU scores of the oracle 1-best hypotheses on the development set, as described below. FLMs have previously shown significant improvements in perplexity and word error rate on several ASR tasks (e.g. (Vergyri et al., 2004) ). W 1 t W − 2 t W − 3 t W − t W 1 t W − 2 t W − t W 1 t W − t W (a) F 1 F 2 F 3 F F F 1 F 2 F F 1 F 3 F F 2 F 3 F F 1 F F 3 F F 2 F (b) Baseline System We used a fairly simple baseline system trained using standard tools, i.e. GIZA++ (Och and Ney, 2000) for training word alignments and Pharaoh (Koehn, 2004) for phrase-based decoding. The training data was that provided on the ACL05 Shared MT task website for 4 different language pairs (translation from Finnish, Spanish, French into English); no additional data was used. Preprocessing consisted of lowercasing the data and filtering out sentences with a length ratio greater than 9. The total number of training sentences and words per language pair ranged between 11.3M words (Finnish-English) and 15.7M words (Spanish-English). The development data consisted of the development sets provided on the website (2000 sentences each). We trained our own word alignments, phrase table, language model, and model combination weights. The language model was a trigram model trained using the SRILM toolkit, with modified Kneser-Ney smoothing and interpolation of higher-and lowerorder ngrams. Combination weights were trained using the minimum error weight optimization procedure provided by Pharaoh. We use a two-pass decoding approach: in the first pass, Pharaoh is run in N-best mode to produce N-best lists with 2000 hypotheses per sentence. Seven different component model scores are collected from the outputs, including the distortion model score, the first-pass language model score, word and phrase penalties, and bidirectional phrase and word translation scores, as used in Pharaoh (Koehn, 2004 ). In the second pass, the N-best lists are rescored with additional language models. The resulting scores are then combined with the above scores in a log-linear fashion. The combination weights are optimized on the development set to maximize the BLEU score. The weighted combined scores are then used to select the final 1-best hypothesis. The individual rescoring steps are described in more detail below. Language Models We trained two additional language models to be used in the second pass, one word-based 4-gram model, and a factored trigram model. Both were trained on the same training set as the baseline system. The 4-gram model uses modified Kneser-Ney smoothing and interpolation of higher-order and lower-order n-gram probabilities. The potential advantage of this model is that it models n-grams up to length 4; since the BLEU score is a combination of n-gram precision scores up to length 4, the integration of a 4-gram language model might yield better results. Note that this can only be done in a rescoring framework since the first-pass decoder can only use a trigram language model. For the factored language models, a feature-based word representation was obtained by tagging the text with Rathnaparki's maximum-entropy tagger (Ratnaparkhi, 1996) and by stemming words using the Porter stemmer (Porter, 1980) . Thus, the factored language models use two additional features per word. A word history of up to 2 was considered (3gram FLMs). Rather than optimizing the FLMs on the development set references, they were optimized to achieve a low perplexity on the oracle 1-best hypotheses (the hypotheses with the best individual BLEU scores) from the first decoding pass. This is done to avoid optimizing the model on word combinations that might never be hypothesized by the firstpass decoder, and to bias the model towards achieving a high BLEU score. Since N-best lists differ for different language pairs, a separate FLM was trained for each language pair. While both the 4-gram language model and the FLMs achieved a 8-10% reduction in perplexity on the dev set references compared to the baseline language model, their perplexities on the oracle 1-best hypotheses were not significantly different from that of the baseline model. N-best List Rescoring For N-best list rescoring, the original seven model scores are combined with the scores of the secondpass language models using the framework of discriminative model combination (Beyerlein, 1998) . This approach aims at an optimal (with respect to a given error criterion) integration of different information sources in a log-linear model, whose combination weights are trained discriminatively. This combination technique has been used successfully in ASR, where weights are typically optimized to minimize the empirical word error count on a heldout set. In this case, we use the BLEU score of the N-best hypothesis as an optimization criterion. Optimization is performed using a simplex downhill method known as amoeba search (Nelder and Mead, 1965) Results The results from the first decoding pass on the development set are shown in Conclusions We have demonstrated improvements in BLEU score by utilizing more complex language models in the rescoring pass of a two-pass SMT system. We noticed that FLMs performed worse than wordbased 4-gram models. However, only trigram FLM were used in the present experiments; larger improvements might be obtained by 4-gram FLMs. The weights assigned to the second-pass language models during weight optimization were larger than those assigned to the first-pass language model, suggesting that both the word-based model and the FLM provide more useful scores than the baseline language model. Finally, we observed that the overall improvement represents only a small portion of the possible increase in BLEU score as indicated by the oracle results, suggesting that better language models do not have a significant effect on the overall system performance unless the translation model is improved as well. Acknowledgements This work was funded by the National Science Foundation, Grant no. IIS-0308297. We are grateful to Philip Koehn for assistance with Pharaoh.",
         "1966857",
         "2a05c9c5373a3e1e01b8161e6687b960ab3d2ff5",
         "55",
         "https://aclanthology.org/W05-0821",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Kirchhoff, Katrin  and\nYang, Mei",
         "Improved Language Modeling for Statistical Machine Translation",
         "125--128",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "kirchhoff-yang-2005-improved",
         null,
         null
        ],
        [
         "7",
         "R13-1046",
         "We improve upon a previous line of work for parsing web data, by exploring the impact of different decisions regarding the training data. First, we compare training on automatically POS-tagged data vs. gold POS data. Secondly, we compare the effect of training and testing within sub-genres, i.e., whether a close match of the genre is more important than training set size. Finally, we examine different ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways (sentence length, perplexity). In general, we find that approximating the in-domain data has a positive impact on parsing.",
         "We improve upon a previous line of work for parsing web data, by exploring the impact of different decisions regarding the training data. First, we compare training on automatically POS-tagged data vs. gold POS data. Secondly, we compare the effect of training and testing within sub-genres, i.e., whether a close match of the genre is more important than training set size. Finally, we examine different ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways (sentence length, perplexity). In general, we find that approximating the in-domain data has a positive impact on parsing. Introduction and Motivation Parsing data from the web is notoriously difficult, as parsers are generally trained on news data (Petrov and McDonald, 2012) . The problem, however, varies greatly depending upon the particular piece of web data: what is often termed web data is generally a combination of different sub-genres, such as Facebook posts, Twitter feeds, YouTube comments, discussion forums, blogs, etc. The language used in such data does not follow standard conventions in various respects (see Herring, 2011) : 1) The data is edited to varying degrees, with Twitter on the lower end and professional emails and blog on the upper end of the scale. 2) The sub-genres often display characteristics of spoken language, including sentence fragments and colloquialisms. 3) Some web data, especially social media data, typically contains a high number of emoticons and acronyms such as LOL. At the same time, there is a clear need to develop basic NLP technology for a variety of types of web data. To perform tasks such as sentiment analysis (Nakagawa et al., 2010) or information extraction (McClosky et al., 2011) , it helps to partof-speech (POS) tag and parse the data, as a step towards providing a shallow semantic analysis. We continue our work (Khan et al., 2013) on dependency parsing web data from the English Web Treebank (Bies et al., 2012) . We previously showed that text normalization has a beneficial effect on the quality of a parser on web data, that we can further improve the parser's accuracy by a simple, n-gram-based parse revision method, and that having a balanced training set of out-ofdomain and in-domain data provides the best results when parsing web data. The current work extends this previous work by more closely examining the data given as input for training the parser. Specifically, we take the following directions: 1. All previous experiments were carried out on gold part of speech (POS) tags. Here, we investigate using a POS tagger trained on outof-domain data, thus providing a more realistic setting for parsing web data. We specifically test the impact of training the parser on automatic POS tags (section 4). 2. The web data provided in the English Web Treebank (EWT) is divided into five different sub-genres: 1) answers to questions, 2) emails, 3) newsgroups, 4) reviews, and 5) weblogs. Figure 1 shows examples from the different sub-genres. So far, we used the whole set across these genres, which raises questions about whether a closer match of the genre is more important than the data size, and we thus investigate parsing results within  each sub-genre, and whether adding easy-toparse data to training improves performance for the difficult sub-genres (section 5). 3. Finally, from our previous work, we know that combining the EWT training set with sentences from the Penn Treebank is beneficial. However, we do not know how to best select the out-of-domain sentences. Should they be drawn randomly; should they match in size; should the sentences match in terms of parsing difficulty (cf. perplexity)? We explore different ways to match the in-domain data (section 6). Related Work There is a growing body of work on parsing web data, as evidenced by the 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012) . There have been many techniques employed for improving parsing models, including normalizing the potentially ill-formed text (Foster, 2010; Gadde et al., 2011; Øvrelid and Skjaerholt, 2012) and training parsers on unannotated or reannotated data, e.g., self-training or uptraining, (e.g., Seddah et al., 2012; Roux et al., 2012; Foster et al., 2011b,a) . Less work has gone into investigating the impact of different genres or on specific details of the sentences given to the parser. Indeed, Petrov and McDonald (2012) mention that for the shared task, \"[t]he goal was to build a single system that can robustly parse all domains, rather than to build several domain-specific systems.\" Thus, parsing results were not obtained by genre. However, Roux et al. (2012) demonstrated that using a genre classifier, in order to employ specific sub-grammars, helped improve parsing performance. Indeed, the quality and fit of data has been shown for in-domain parsing (e.g. Hwa, 2001) , as well as for other genres, such as questions (Dima and Hinrichs, 2011) . One common, well-documented ailment of web parsers is the effect of erroneous tags on POS accuracy. Foster et al. (2011a,b) , e.g., note that propagation of POS errors is a serious problem, especially for Twitter data. Researchers have thus worked on improving POS tagging for web data, whether by tagger voting (Zhang et al., 2012) or word clustering (Owoputi et al., 2012; Seddah et al., 2012) . There are no reports about the impact of the quality of POS tags for trainingi.e., whether worse, automatically-derived tags might be an improvement over gold tags-though Søgaard and Plank (2012) note that training with predicted POS tags improves performance. Researchers have trained parsers using additional data which generally fits the testing domain, as mentioned above. There has been less work, however, on extracting specific types of sentences which fit the domain well. Bohnet et al. (2012) noticed a problem with parsing fragments and so extracted longer NPs to include in training as standalone sentences. From a different perspective, Søgaard and Plank (2012) weight sentences in the training data rather than selecting a subset, to better match the distribution of the target domain. In general, identifying sentences which are similar to a particular domain is a concept familiar in active learning (e.g., Mirroshandel and Nasr, 2011; Sassano and Kurohashi, 2010) , where dissimilar sentences are selected for hand-annotation to improve parsing. Experimental Setup Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012) . The EWT is comprised of approx. 16 000 sen-tences from weblogs, newsgroups, emails, reviews, and question-answers. Note that our data sets are different from the ones in Khan et al. (2013) since in the previous work we had removed sentences with POS labels AFX and GW. To create training and test sets, we broke the data into the following sets: • WSJ training: sections 02-22 (42 009 sent.) • WSJ testing: section 23 (2 416 sent.) • EWT training: 80% of the data, taking the first four out of every five sentences (13 298 sent.) • EWT testing: 20% of the data, taking every fifth sentence (3 324 sent.) The two corpora were converted from PTB constituency trees into dependency trees using the Stanford dependency converter (de Marneffe and Manning, 2008). 1 Since the EWT uses data that shows many of the characteristics of non-standard language, we decided to normalize the spelling of the EWT training and the test set. For the normalization, we reduce all web URLs to a single token, i.e., each web URL is replaced with the place-holder URL. Similarly, all emoticons are replaced by a single marker EMO. Repeated use of punctuation, e.g., !!!, is reduced to a single punctuation token. POS Tagger We use TnT (Brants, 2000) , a Markov model POS tagger using a trigram model. It it is fast to train and has a state-of-the-art model for unknown words, using a suffix trie of hapax legomena. Parser We use MSTParser (McDonald and Pereira, 2006) , 2 a freely-available parser that reaches stateof-the-art accuracy in dependency parsing for English. MST is a graph-based parser which optimizes its parse tree globally (McDonald et al., 2005) , using a variety of feature sets, i.e., edge, Evaluation For parser evaluation, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS), the percentage of dependencies which are attached correctly or attached and labeled correctly (Kübler et al., 2009) . Parser evaluation is carried out with MSTParser's evaluation module. For POS tagger evaluation, we report accuracy based on TnT's evaluation script. Significance testing was performed using the CoNLL 2007 shared task evaluation using Dan Bikel's Randomized Parsing Evaluation Comparator. 3 The effect of POS tagging We here explore the effect of POS tagging on parsing web data, to see how closely the conditions for training should match the conditions for testing. However, first we need to gauge the effect of using the TnT POS tagger out of domain. For this reason, we conducted a set of experiments, training and testing TnT in different conditions. The results are shown in table 1. They show that TnT reaches an accuracy of 96.7% when trained and tested on the WSJ. This corroborates findings by Brants (2000) . When we train TnT on EWT training data, running it on the EWT testing data delivers an accuracy of 94.28%, already 2-3% below performance on news data. However, note that the EWT is much smaller than the full WSJ. In contrast, if we train TnT on WSJ and then use it for POS tagging EWT data, we only reach an accuracy of 88.73%. Even if we balance the source and target domain data, which proved beneficial in our previous experiments on parsing (Khan et al., 2013) , we reach an accuracy of 93.48%, well below the in-domain tagging result for the EWT. This means that in contrast to parsing, the POS tagger requires less training data and profits more Given this degree of error in tagging, a parser trained with similar noise in POS tags may outperform one which is trained on gold tags. Thus, we run TnT on the training data, using a 10-fold split of the training set: each tenth of the training corpus is tagged using a POS tagger trained on the other 9 folds. Then we use the combination of all the automatically POS tagged folds and insert those POS tags into the gold standard dependency trees before we train the parser. The three conditions for POS tagging are shown in table 2. The first point to note is the impact of switching from gold to automatic POS tags: testing on TnT tags results in a degradation of about 4.5-5.5% in LAS, as compared to gold standard POS tags in the test set, consistent with typical drops in performance (e.g., Rehbein et al., 2012) . More to the point for our purposes, we see in table 2 that training a parser on automaticallyassigned POS tags outperforms a parser trained on gold POS tags. LAS increases from 77.69% to 78.54%. This supports the notion that training data should match testing closely. However, it also shows that we need to investigate methods for improving POS tagger accuracy. The effect of domain As mentioned, the EWT contains subcorpora from five different genres, and, while they share many common features (misspellings, unknown words), they have many unique properties, as illustrated in the examples in figure 1 . In terms of sentence length, domains such as weblogs lend themselves more easily to longer, more well-edited sentences, matching news data better. Reviews, on the other hand, often have shorter sentences-similar to, e.g., email greetings. Run-ons are common across genres, but we see them here in the answer and news sub-genres. The example for the answer sub-corpus shows some of the difficult challenges faced by a parser, as it contains a declarative sentence embedded within the question, where the final word (please) attaches back to the question. To gauge the effect of different sub-genres, we trained and tested the parser within each subgenre. In order to concentrate on the differences in parsing, we used gold POS tags for these experiments. Results for the five individual sub-corpora are given in the first five rows of table 3. It is noteworthy that there is nearly a 5% difference in LAS between the best sub-genre (EWT email ) and the worst (EWT answer ). We also show various properties of the sub-corpora, including number of tokens (Tokens), the average sentence length (Sen-Len), and the number of finite verbal roots (Fin-Root) 4 in training; and also the percentage of unknown word tokens in the test corpus, as compared to the training corpus (Unk.) In general, emails and reviews fare the best, likely due to a combination of shorter sentences (11.84 and 14.58, respectively) and text that tends to follow grammatical conventions. Blogs and newsgroups are in the middle, with longer, harderto-parse sentences (18.17 and 22.07, respectively) and higher levels of unknown words in testing (12.2% and 10.2%), but being consistently fairly well-edited. While it might be surprising that the results for these two sub-genres are lower than emails and reviews, note that the training for both domains is significantly lower, on the order of 10,000 words less than the other corpora. It is possible that with more data, these well-edited domains would see improved parser performance. On the lower end of the parsing spectrum is the domain of answers, which is a curious trend. There is nearly as much training data as with emails and reviews, and the average sentence length is comparable. If we look at the number of non-finite sentence roots-as a way to approximate the number of non-fragment sentences-it is nearly identical to the email sub-genre. We suspect that the fragments are not as systematic as greetings and that users may post replies quickly, leading to less well-formed text, but this deserves future consideration. Given the poor performance on the answer domain and the higher performance of the parser on Table 3 : The effect of domain on parser performance, using gold POS tags (** = sig. at the 0.01 level, testing all conditions below the line, as compared to the first row Train=EWT answer ) emails, we decided to see whether parsing could be improved by adding data to the small answer training set 1) from the domain that is easiest to parse: emails, 2) from the news domain because of its similar average sentence length, and 3) from the blog domain because it has the longest sentences. We compare these configurations with one where we add the same number of sentences, but sampled from all four remaining domains (balanced) and one where we add all the training data from all other genres (rest). We see a clear improvement for all settings, in comparison with using only the answer data for training. The best results are obtained by using all other genres as additional training data, showing that the size of the training set is the most important variable. The results also show that the sampling from all remaining sub-genres results in higher parsing accuracy than just using the easiest to parse data set, illustrating that we should not look for data which is generally easy to parse, but data which is the best fit for the test data. The effect of sentence selection In our previous work (Khan et al., 2013) , we showed that we obtain the best results when we use a balanced training corpus with the same number of sentences from the EWT and the WSJ. On the one hand, these results show that in-domain data is critical for the success of the parser; on the other hand, out-of-domain data is important to increase the size of the training set. It is thus important to find a good balance between using more training data and not overpowering the indomain data. This leads to the question of whether it is possible to choose sentences from an out-of- domain data set that are similar to the sentences in the target domain rather than just selecting a portion of consecutive sentences. In other words, can we identify sentences from the WSJ that will have the best impact on a parser for web data? In the first set of experiments, we investigate simple heuristics to choose a good set of training sentences from the WSJ: In the first experiment, we use the full WSJ (EWT+WSJ). Then we restrict the WSJ part to match the number of sentences from the EWT (EWT+WSJSent). However, since WSJ sentences are longer on average than EWT sentences, we repeat the experiment but choose the WSJ subset so that it matches the number of words in the EWT training set (EWT+WSJToken). Finally, we choose the WSJ sentences so that they match the distribution of sentence lengths in EWT (EWT+WSJDist). For example, if EWT has 100 sentences with 10 words, we select 100 sentences of length 10 from the WSJ. All of these experiments are again carried out with gold POS tags. The results of these experiments are shown in the first two parts of table 4. The results for the selection methods show that selecting the WSJ part based on the number of words results in the lowest parsing accuracy. Choosing the WSJ part based on the number of sentences or the distribution of sentence length results in the same unlabeled accuracy (UAS) of 86.34%, as compared to 86.26% for the word based selection. However, the selection based on the number of sentences results in a higher labeled accuracy of 83.83%, as opposed to 83.73% for the distribution of sentence length. We suspect that the random selection of sentences gives more variety, which is beneficial for training. However, note that the difference in the number of words in the training set across these three methods is minimal: they vary only by 41 words. In a second set of experiments, we decided to use a more informed method for choosing similar sentences: perplexity. Thus, we trained a language model on the (stemmed) words of the test set based on a 5-gram word model, and then calculated perplexity for each sentence in the WSJ, normalized by the length of the sentence. We used the CMU-Cambridge Statistical Language Modeling Toolkit 5 for calculating perplexity. Perplexity should give an approximation of distance between sentences in the two corpora. We experimented with different selection strategies: 1. Low Perplexity (LowP): We select the sentences with the lowest perplexity, i.e., the most similar ones to the test set; we restricted the number of sentences from the WSJ to match the size of the EWT training set. 2. All Low Perplexity (AllLowP): Here, we also selected sentences with low perplexity, but this time used all sentences below the median, i.e. half the WSJ sentences. 3. Low Perplexity close to the median (Med-LowP): Here, we investigate the effect of choosing sentences that are less similar to the test sentences: we select the same number of sentences as with LowP, but this time from the median down. In other words, the sentences with the lowest perplexity, i.e., the most similar sentences, are excluded. This is based on the assumption that if the chosen sentences are too similar, it will not have much effect on the trained model. 4. Mid-range Perplexity (MidP): In this set, we choose sentences that are even less similar to the test sentences. We again choose the same number of sentences as in the EWT training set, but half of them from the median and down and half from the median up. The results are in the final four rows of table 4. Interestingly, the best-performing method adds low-perplexity data to training. Thus, selecting data which is more similar to the domain helps the most. Furthermore, once the data is farther away, it starts to harm parsing performance, as can be seen in the (albeit minimal) difference between the EWT+LowP and EWT+AllLowP models. Summary and Outlook Exploring the parsing of web data, we have investigated different decisions that go into the training data, demonstrating how the better the fit of the training data to the testing data-in properties ranging from the nature of the POS tags to which sentences go into the data-the better performance the parser will have. We first compared training on automatically POS-tagged data vs. gold POS tag data, showing that performance improves by automatically tagging the training data. Next, we compared the effect of training and testing within sub-genres and saw that features such as sentence length have a strong effect. Finally, we examined ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways, and we found that matching training sentences to a language model improves parsing. In short, fitting the training data to the in-domain data, in even fairly superficial ways, has a positive impact on parsing results. There are several directions to take this work. First, the sentence selection methods, for example, can be combined with self-training techniques to not only increase the training data size, but to only add sentences which fit the test domain well. Secondly, the work on understanding sub-genres of web parsing deserves more thorough treatment in the future to tease apart which components are most problematic (e.g., sentence fragments), how they can be automatically identified, and how the parser can be adjusted to accommodate them.",
         "7403488",
         "778382f4099a5d478c1f68e1bb46fc7c26013a5c",
         "11",
         "https://aclanthology.org/R13-1046",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Khan, Mohammad  and\nDickinson, Markus  and\nK{\\\"u}bler, Sandra",
         "Towards Domain Adaptation for Parsing Web Data",
         "357--364",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "khan-etal-2013-towards",
         null,
         null
        ],
        [
         "8",
         "W05-0820",
         "Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis.",
         "Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis. Statistical machine translation is currently the dominant paradigm in machine translation research. Annual competitions are held for Chinese-English and Arabic-English by NIST (sponsored by the US military funding agency DARPA), which creates a forum to present and compare novel ideas and leads to steady progress in the field. One of the advantages of statistical machine translation is that the currently applied methods are fairly language-independent. Building a new machine translation system for a new language pair is not much more than a matter of running a training process on a training corpus of parallel text (a text in one language paired with a translation in another). It is therefore possible to hold a competition where research groups have only a few weeks to build machine translation systems for language pairs that they have not previously worked on. We effectively demonstrated this with our shared task. For instance, seven teams built Finnish-English machine translation systems, a language pair that was certainly not of their immediate concern before. In contrast to the bigger NIST competition, we wanted to keep the barrier of entry as low as possible. We provided not only training data from the Europarl corpus (Koehn, 2005) , but also additional resources: sentence and word alignments, the decoder Pharaoh 1 (Koehn, 2004b) , and a language model, so that participation was feasible even as a graduate level class project. Using about 15 million words of translated text, participants were asked to build a phrase-based statistical machine translation system. The focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided -for more on phrase-based statistical machine translation, refer to Koehn et al. (2003) . The participants' systems were compared by how well they translated 2000 previously unseen test sentences from the same domain. The shared task operated within an extremely short timeframe. The workshop and hence the shared task was accepted on February 22, 2005 and announced on March 3. The official test data was made available on April 3, results were due one week later. Despite this tight schedule, eleven research groups participated and built a total of 32 machine translation systems for the four language pairs. Goals When setting up this competition, we were motivated by a number of goals. We set out to: Create a platform to demonstrate the effectiveness of novel ideas: The research community is easily balkanized, where different groups work on different data sets and under different conditions, so that it becomes often hard to assess, how effective a novel method is. By creating an environment with common test and training sets, language model, preprocessing, and even decoder, the effect of other model choices can be more easily demonstrated. Work on new language pairs, new problems: Different language pairs pose different challenges. We picked Finnish-English and German-English for the special problems of rich morphology, word order, which are a challenge to current phrase-based SMT methods. Enable more researchers to get engaged in SMT research: One of our main goals with providing as many resources as possible was to keep the barrier of entry low. Participants could use the word alignment and other resources and focus on phrase extraction. We hoped to attract researchers that are relatively new to the field. We were satisfied to learn that many entries are by graduate students working on their own. Promote and create free resources: Academic research thrives on freely available resources. The field of statistical machine translation has been blessed with a long tradition of freely available software tools -such as GIZA++ (Och and Ney, 2003) -and parallel corpora -such as the Canadian Hansards 2 . Following this lead, we made word alignments and a language model available for this competition in addition to our previously published resources (Europarl and Pharaoh). The competition created resources as well. Most teams agreed to share system output and their model files. You can download them from the competition web site 3 . Promote work on European language pairs: Finally, we wanted to promote work on European languages. The increasing economic and political ties within the European Union create a huge need for translation services. We would like to see researchers rise to the challenge of creating high quality machine translation systems to fill these needs. We are very grateful for the strong participation, especially by researchers who are relatively new to the field. Rules of Engagement We set up a machine translation competition for four language pairs. We chose Spanish-English and French-English, because many researchers would be familiar with these languages. We chose German-English for its special problems with word order (such as nested constructions and split verb groups) and morphology. Finally, we picked Finnish-English for the rich agglutinative morphology of Finnish. Statistical machine translation systems are typically trained on sentence-aligned parallel corpora. We selected Europarl 4 , a freely available parallel corpus in eleven languages. In addition, we also made a word alignment available, which was derived using a variant of the current default method for word alignment -Och and Ney (2003)'s refined method. Figure 1 details some properties of the parallel corpora. The training corpus is most of the Europarl corpus, only the text of sessions from last quarter of the year 2000 was reserved for testing. The corpus has the size of roughly 15 million English words in 700,000 sentences -these numbers differ for each of the four parallel corpora due to the different number of discarded sentences during sentence alignment and after enforcing a 40 word length limit for sentences. The number of foreign words differs even more dramatically. The effect of Finnish morphology manifests itself in a low number of words (just over 11 million), but a high number of distinct words (more than 5 times as many as in the English half). The test corpus consists of 2000 sentences aligned across all five languages. Note that the output of each system is compared against the same English references for all source languages. The number of total words, distinct words, and words not seen in the training data reflects again the morphology effect. For researchers willing to create their own word alignment, we suggested the use of GIZA++ 5 , an implementation of the IBM word-based machine translation models, which also assisted the creation of the provided word alignments. We trained a language model on the English part Spanish-English French-English Finnish of the Europarl corpus using the SRI language modeling toolkit (Stolke, 2002) . Finally, we suggested the use of Pharaoh (Koehn, 2004b) , a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year's NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004) ), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005) , additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3-10. We allowed late submissions up to April 17. Results Eleven teams from eight institutions in Europe and North America participated, see Figure 2 for a complete list. The figure also indicates, if a team used the Pharaoh decoder (eight teams), the provided language model (seven teams) and the provided word alignment (four did, three of those with additional preprocessing or additional data). Translation performance was measured using the BLEU score (Papineni et al., 2002) , which measures n-gram overlap with a reference translation. In our case, we only used a single reference translation, since the test set was taken from a held-out portion of the Europarl corpus. On the other hand we used a relatively large number of test sentences to guarantee that the BLEU results are stable despite the fact that we used only one reference translation for each sentence. Shared tasks like this one, of course, bring out the competitive spirit of participants and can draw criticisms about being a horse race. From an outside perspective, however, it is far more interesting to learn which methods and ideas proved to be successful, than who won the competition. Taking stock of the results -see Figure 3 -one observes a very packed field at the top. While the participants from the University of Washington produced the best translations for every single language pair, the distance to many other participant scores ID Team Pharaoh is within a BLEU percentage point or two. As one might have expected, the scores are best for Spanish and French, and worst for Finnish. Figure 4 shows some typical output of the submitted systems. The proceedings to the workshop include detailed system descriptions of all participants. Novel phrase extraction approaches were proposed, along with better preprocessing, language modeling, rescoring, and other ideas. We are certain that better performance can be achieved by combining some of the methods used by different participants. And hence, we would like to pose the challenge to the research community to build and test better systems using the provided resources. We will gladly list additional results on the competition web site. Survey Following the end of the competition, we sent out a questionnaire to the participants. One of the questions what they would like to see different in a potential future competition. We listed four potential changes: 70% of the respondends checked translation from English, 50% checked out of domain test data, 40% checked more language pairs, 0% checked fewer language pairs. Additional suggestions were: alternatives to the BLEU scoring method (maybe human judgment by participants themselves), transitive translation using pivot languages, translation of resource-poor languages, and more time to prepare for the task. Outlook Given the short timeframe, one should view the system performances (albeit very competitive with the state of the art) as a baseline effort on the task of open domain text translation between European languages. We hope that future researchers will use the provided environment as a test bed for their machine translation systems. We will continue to publish any scores reported to us. Since we placed much of the systems' output online, the interested reader may be inspired to more closely explore the quality and shortcomings. Even some of the model files have been made available, so it is even possible to download and install some of the systems. Spanish-English Reference We know all too well that the present Treaties are inadequate and that the Union will need a better and different structure in future , a more constitutional structure which clearly distinguishes the powers of the Member States and those of the Union . Input Spanish Sabemos muy bien que los Tratados actuales no bastan y que , en el futuro , será necesario desarrollar una estructura mejor y diferente para la Unión Europea , una estructura más constitucional que también deje bien claras cuáles son las competencias de los Estados miembros y cuáles pertenecen a la Unión . Best system (Spanish-English) we all know very well that the current treaties are not enough and that , in the future , it will be necessary to develop a structure better and different for the european union , a structure more constitutional also make it clear what the competences of the member states and what belongs to the union . Worst System (Spanish-English) we know very well that the current treaties not enough and that , in the future , will be necessary develop a better structure and different to the european union , a structure more constitutional that also be well clear the powers of the member states and what belong to the union . Input French Nous savons très bien que les Traités actuels ne suffisent pas et qu ' il sera nécessaire à l ' avenir de développer une structure plus efficace et différente pour l ' Union , une structure plus constitutionnelle qui indique clairement quelles sont les compétences des états membres et quelles sont les compétences de l ' Union . Best system (French-English) we know very well that the current treaties are not enough and that it will be needed in the future to develop a structure more effective and different for the union , a structure more constitutional which clearly indicates what are the competence of member states and what are the powers of the union . Input Finnish Tiedämme oikein hyvin , että nykyiset perustamissopimukset eivät ole riittäviä ja että tulevaisuudessa on tarpeen kehittää unionille parempi ja toisenlainen rakenne , siis perustuslaillisempi rakenne , jossa mys ilmaistaan selkeämmin , mitä jäsenvaltioiden ja unionin toimivaltaan kuuluu Best system (Finnish-English) we know very well that the existing founding treaties do not need to be developed for the union and a different structure , therefore perustuslaillisempi structure , which also expresses clearly what the member states and the union 's competence is not sufficient and that better in the future . Input German Uns ist sehr wohl bewusst , dass die geltenden Verträge unzulänglich sind und künftig eine andere , effizientere Struktur für die Union entwickelt werden muss , nämlich eine stärker konstitutionell ausgeprägte Struktur mit einer klaren Abgrenzung zwischen den Befugnissen der Mitgliedstaaten und den Kompetenzen der Union . Best system (German-English) the union must be developed , with a major institutional structure with a clear demarcation between the powers of the member states and the competences of the union is well aware that the existing treaties are inadequate and in the future , a different , more efficient structure for us .",
         "9271055",
         "5025a4b2b724867f7b6b24f3e9253a61b76a3406",
         "83",
         "https://aclanthology.org/W05-0820",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Koehn, Philipp  and\nMonz, Christof",
         "Shared Task: Statistical Machine Translation between {E}uropean Languages",
         "119--124",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "koehn-monz-2005-shared",
         null,
         null
        ],
        [
         "9",
         "R13-1047",
         "In this work, we present a new task for testing compositional distributional semantic models. Recently, there has been a spate of research into how distributional representations of individual words can be combined to represent the meaning of phrases. Vecchi et al. (2011) have shown that some compositional models, including the additive and multiplicative models of Mitchell and Lapata (2008; 2010)   and the linear map-based model of Baroni and Zamparelli (2010) , can be applied to detect semantically anomalous adjectivenoun combinations. We extend their experiments and apply these models to the combinations extracted from texts written by learners of English. Our work contributes to the field of compositional distributional semantics by introducing a new test paradigm for semantic models and shows how these models can be used for error detection in language learners' content word combinations.",
         "In this work, we present a new task for testing compositional distributional semantic models. Recently, there has been a spate of research into how distributional representations of individual words can be combined to represent the meaning of phrases. Vecchi et al. (2011) have shown that some compositional models, including the additive and multiplicative models of Mitchell and Lapata (2008; 2010)   and the linear map-based model of Baroni and Zamparelli (2010) , can be applied to detect semantically anomalous adjectivenoun combinations. We extend their experiments and apply these models to the combinations extracted from texts written by learners of English. Our work contributes to the field of compositional distributional semantics by introducing a new test paradigm for semantic models and shows how these models can be used for error detection in language learners' content word combinations. Introduction Vector-based (distributional) models are widely used for representing the meaning of single words. They rely on the assumption that word meaning can be learned from the linguistic environment and can be approximated by a word's distribution across contexts. Words are represented as vectors in a high-dimensional space, with vector dimensions encoding word co-occurrence with contextual elements -other words within a local window, words linked by specific dependencies to the target word, and so forth. Distributional models provide a clear basis for interpreting word meaning, as well as a simple means for measuring semantic similarity. These properties have been exploited in many NLP tasks, including automatic thesaurus extraction (Grefenstette, 1994) , word sense induction (Schütze, 1998) and disambiguation (McCarthy et al., 2004) , collocation extraction (Schone and Jurafsky, 2001) and others. In contrast to single words, the distribution of phrases cannot be used as a reliable approximation of their meaning, as phrase vectors are much sparser. Irrespective of the size of the corpus considered, some content word combinations will remain unattested as a consequence of their Zipf-like distributions. For example, Vecchi et al. (2011) have shown that both semantically acceptable and semantically deviant word combinations will be absent from large English corpora. A promising alternative is to use compositional models which combine distributional vectors for the component words in some way, for example, using a direct vector combination function (Kintsch, 2001; Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) or linear transformations on vectors (Baroni and Zamparelli, 2010) . In spite of the spate of recent work in this area, the question of how to combine word representations is far from answered. Compositional models can be assessed by their ability both to provide a solid theoretical basis for meaning composition and to represent composite meaning for relevant practical tasks. Promising results have been shown with such models on similarity detection and paraphrase ranking (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) , adjectivenoun vector prediction (Baroni and Zamparelli, 2010) and semantic anomaly detection (Vecchi et al., 2011) . Of these tasks, the latter appears to be particularly challenging since it addresses the ability of compositional models to account for linguistic productivity. No corpus can effectively sample all possible content word combinations. On the other hand, some corpus-attested word combinations may appear semantically deviant when considered out of context (for example, when they are used metaphorically). Vecchi et al. (2011) have focused on unattested adjective-noun (AN) combinations and noted that if a combination does not occur in a corpus, it may be due to various reasons including data sparsity as well as nonsensicality. The task of distinguishing between the two cases is challenging. Vecchi et al. use the following examples: (1) a. blue rose b. residential steak Whereas both may well be unattested in a corpus, the concept of blue rose is perfectly conceivable while that of residential steak is nonsensical and only interpretable in specifically-constructed discourse contexts. Vecchi et al. argue that there should be a detectable difference between the model-generated representations for the semantically deviant combinations and those for the acceptable ones, and assess compositional models by their ability to capture this difference. Vecchi et al. have created a set of corpus-unattested AN combinations, annotated them as semantically acceptable or deviant, and applied the additive (add) and multiplicative (mult) models of Mitchell and Lapata (2008) and adjective-specific linear maps (alm) of Baroni and Zamparelli (2010) . Given that promising results have been obtained in their experiments, we propose that a useful extension of this task is to test the compositional models on errors in content word combinations extracted from texts written by learners of English. This task provides a natural setting for testing semantic models on genuine examples and is a potential practical application for such models. Language learners' errors are diverse, but many of them can naturally be explained in terms of nonproductive, semantically anomalous combination of content words (Leacock et al., 2010) . Learners may lack robust intuitions about words' selectional preferences and subtle differences in meaning, so they may confuse near-synonyms, overuse words with broad meaning, and otherwise choose words inappropriately. Consider the following examples extracted from our data: (2) a. These examples illustrate that learner errors can often be explained by confusions stemming from similar meaning (2a) or form (2b). When a word combination appears to be nonsensical as in 2c, the words chosen might still be related to the appropriate ones in the learner's mental lexicon. We recognise that although error detection in learners' content word combinations is a natural extension to semantic anomaly detection, it also poses additional difficulties that semantic models might not be able to deal with. For example, some erroneous word combinations may not be completely devoid of compositional meaning, while violating language conventions. However, semantic models might still be able to capture some of these conventions. Another challenge is that some expressions cannot be unambiguously classified as either correct or incorrect, as their interpretation depends on the context of use: best moment (2d) is appropriate when used to denote a short period of time, but it is often incorrectly used by learners instead of best time. To make our work comparable with previous work on semantic anomaly, we investigate AN combinations extracted from texts written by nonnative speakers of English, and apply the add, mult and alm models of semantic composition. The main contributions of this work are to show that error detection in content word combinations provides a natural testbed and useful application for the compositional distributional models, and that the results obtained on this task provide a more natural estimate of the models' performance than ones based on artificially constructed examples. If the compositional distributional models can distinguish between correct and incorrect content word combinations, these models can then be used for writing or pedagogical assistance. To the best of our knowledge, this is the first attempt to handle learner errors in the choice of content words using compositional distributional semantics. Plan of the paper. We overview related work on error detection and discuss the three models of semantic composition in Section 2. Section 3 presents the data and experimental setup. We discuss the results of our experiments in Section 4 and conclude in Section 5. 2012). Such errors are more frequent, but they are also more systematic which makes them easier to detect. Function words constitute a closed class, so the set of possible corrections is also limited. By comparison, errors in content word combinations pose a bigger challenge. Since content words primarily express meaning rather than encode syntax, detection and correction of such errors depend on a system's ability, in the limit, to recognise the communicative intent of the writer. Moreover, the set of possible corrections is much larger than for function words. Previous work has either focused on correction alone assuming that errors are already detected (Liu et al., 2009; Dahlmeier and Ng, 2011) , or has reformulated the task as writing improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al., 2008; Yi et al., 2008) . In the former case error detection, which is a difficult task in itself, is not addressed, while in the latter case it is integrated into that of suggesting alternatives according to some metric (for example, frequency or mutual information). In some cases, a database of typical errors in word combinations is collected from learner texts and suggestions are only made for these errorprone combinations. Otherwise suggestions will be made for many acceptable phrases. In this work, we treat error detection in the choice of content words as an independent task and assess the ability of compositional distributional models to discriminate incorrect from correct AN combinations -a frequent source of error in learner texts. Composition by Component-wise Operations In the additive and multiplicative compositional models of Mitchell and Lapata (2008; 2010) , the components of the composite vector are obtained by component-wise operations applied to the word vectors. If c is a word combination vector and a and b are word vectors, then c's i-th component is the sum of the i-th components of a and b for the add model: c i = a i + b i (1) and the product of the corresponding components for the mult model: c i = a i b i (2) An advantage of using these models is that they provide a clear and simple interpretation of vector composition, requiring no training or tuning. They have also been shown to be promising models of composition in a number of NLP tasks, including semantic anomaly detection (Vecchi et al., 2011) . However, the principal weakness of these models is that they use commutative operations, and therefore fail to represent the difference in the grammatical function of the component words, their order, and \"headedness\". For example, these models would produce the same composite vectors for component vector and vector component. In addition, the add model does not take \"incompatibility\" of constituent vectors along individual dimensions into account. If one vector has a high value in its i-th dimension while another vector has 0, the composed vector will receive the high value from the first input vector, even though, intuitively, this dimension should get 0 or near-0 value. This problem does not arise with the mult model. On the other hand, the mult model is heavily biased towards dimensions with high values in both input vectors (Baroni et al., 2012) . Distributional Functions and Linear Maps The adjective-specific linear maps of Baroni and Zamparelli (2010) take the grammatical functions of the words within a combination into account. Focusing on AN combinations, they try to model the fact that adjectives modify nouns and the resulting combination is nominal. They note that the meaning of nouns can be represented with their distributional vectors, but the meaning of attributive adjectives cannot be fully captured by their distribution alone: for example, new in new friend is not the same as new in new shoes. The meaning of the adjective new is defined through its application to the denotations of the nouns. Therefore, Baroni and Zamparelli (2010) suggest treating adjectives as distributional functions that map between semantic vectors representing nouns to ones representing AN combinations. Within this approach, adjectives are represented with weight matrices. The composition is defined by matrix-by-vector multiplication as follows: f (noun) = def F × a = b (3) where F is the matrix representing an adjective and encoding function f, which maps the input noun vector a to the output AN vector b. The ij-th cell of the matrix contains the weight determining how much the component corresponding to the jth context element in the noun vector contributes to the value assigned to the i-th context element in the AN vector (Baroni et al., 2012) . These weights are estimated separately for each adjective from all corpus-observed noun-AN vector pairs using (multivariate) partial least squares regression. 3 Experimental Setup Test Data We have extracted a set of AN combinations from the publicly available CLC-FCE dataset (Yannakoudakis et al., 2011) , a subset of the Cambridge Learner Corpus (CLC), 1 which is a large corpus of texts produced by English language learners sitting Cambridge Assessment's examinations. 2  These texts have been manually errorcoded (Nicholls, 2003) . Using the error annotation, we have divided extracted ANs into two subsets -correctly used ANs and those that are annotated with error codes due to inappropriate choice of an adjective or/and noun. 3 For the ANs that are used correctly in some contexts and incorrectly in others we use the most frequent annotation from the data. Our test set contains 4681 correct and 530 incorrect combinations. In contrast to Vecchi et al. (2011) , who have used a limited set of constituent adjectives and nouns and an approximately equal number of semantically acceptable and deviant combinations, our test set is more skewed towards correct combinations and consists of a wider range of constituent words. It also includes ANs occurring in the BNC 4 -3294 of the correct test ANs and 256 of the incorrect ones are corpus-attested. The set of corpus-attested ANs annotated as incorrect in our data includes lowfrequency combinations from the BNC, as well as combinations whose error-annotation depends on context. We believe that this test set reflects practical applications of semantic anomaly detection more closely. Semantic Space Construction In constructing the semantic space we follow the procedure outlined in Vecchi et al. (2011) . We populate the semantic space with a large number of distributional vectors for the target elements -constituent nouns and adjectives from the test ANs, and the most frequent nouns and adjectives from a corpus of English as well as AN combinations of these words. To estimate the frequency rankings, we use a concatenation of two wellformed English corpora -the 100M word BNC and the Web-derived 2B word ukWaC corpus. 6  The semantic space is represented by a matrix encoding word co-occurrences, with the rows representing the target elements and the columns representing a set of 10K context words consisting of 6,590 nouns, 1,550 adjectives and 1,860 verbs most frequent in the combined corpus. The ijth cell of the original matrix contains a sentenceinternal co-occurrence count of the i-th target element with the j-th context word. The raw sentence-internal co-occurrence counts from the original matrix have been transformed into Local Mutual Information scores (Baroni and Zamparelli, 2010; Evert, 2005) . An interesting research question is how much data are needed to obtain reliable word cooccurrence counts. We estimate the word cooccurrence statistics using the BNC only, and leave it for future research to explore the impact of estimating them from larger corpora, for example, the ukWaC or the concatenated corpus mentioned above. We lemmatise, tag and parse the data with the RASP system (Briscoe et al., 2006; Andersen et al., 2008) , and extract all statistics at the lemma level. The target elements are selected as follows: we first select the 4K adjectives and 8K nouns which are most frequent in the concatenated corpus. In each case, we exclude the top 50 most frequent words since those may have too general meanings. Next, we extract the constituent adjectives and nouns from our test data and populate the semantic space with the words not yet contained in it. As a result, our semantic space contains 8,364 nouns. Since we aim at investigating AN behaviour in a highly-populated semantic space, we add more AN combinations to that. We select 218 very frequent adjectives (occurring more than 100K but We perform all operations on vectors in the full semantic space, using a 76,053 × 10K matrix. We leave it for future research to perform dimensionality reduction (for example, using Singular Value Decomposition) and to compare the results with the ones reported here. Composition Methods For the add and mult models, the AN vectors are obtained by component-wise addition and multiplication without normalisation. For the alm model, the weight coefficients are estimated with multivariate partial least squares regression using the R pls package (Mevik and Wehrens, 2007) , using the leave-one-out training regime. This model is computationally expensive since a separate weight matrix must be learned for each adjective and since we use the non-reduced semantic space. Therefore, for the experiments presented here we limit the number of test adjectives to 38. The selected adjectives are, on the one hand, frequently misused by language learners, and, on the other, have a manageable number of training examples. The reduced set of test ANs consists of 347 combinations. The number of latent variables used by the training algorithm depends on the number of available noun-AN training pairs. We have gradually changed this number from 3 to 20 depending on the adjective and the number of available training pairs with the aim of keeping the independentvariable-to-training-item ratio stable. However, we have not optimised this number and leave it for future research. Measures of Semantic Anomaly Once the composite vectors are obtained, the next question is how to distinguish between the vectors for correct and anomalous combinations. Vecchi et al. (2011) propose three simple measures for distinguishing between the two sets of vectors: 1. Vector Length (VLen): they hypothesise that vectors for anomalous ANs are shorter than those for acceptable ones. Since the distributional vectors encode word occurrence, words that do not \"match\" semantically should have their co-occurrence counts distributed differently along the dimensions, and their composition is expected to have many near-0 values. 2. Cosine with the Noun Vector (CosN): they hypothesise that in nonsensical ANs the meaning of the input nouns is degraded and their model-generated vectors are situated further away from the original noun vectors. For example, since a big dog is still a dog and an *extensive dog is less clearly so, in the semantic space the vector for big dog would be closer to that of dog than the vector for *extensive dog to dog. Semantically deviant ANs are expected to have lower cosine between their vectors and the original noun vectors. Density of the AN Neighbourhood (Dens): it is hypothesised that deviant ANs will have fewer close neighbours and be more \"isolated\" in the semantic space. This is measured by the average cosine with the top 10 nearest neighbours, which is assumed to be lower for anomalous ANs. We hypothesise that some cues alternative to the ones already proposed may also be effective: The models can be assessed by their ability to place the AN vector in the neighbourhood populated by similar words and combinations. We measure this as the proportion of nearest neighbours containing same constituent words as in the tested ANs. Results We use the measures described above and compute the difference between the mean values for the correct and incorrect model-generated ANs. We apply the unpaired t-test, assuming a twotailed distribution, to assess the statistical significance of the difference between these values. In Tables 1 to 3 we report p values estimating statistical significance at the 0.05 level, and statistical significance is marked with an asterisk ( * ). We assume that there might be a difference between the corpus-attested and corpus-unattested Vecchi et al. (2011) . We report the results on the full set of test ANs, as well as on each of the two subgroups separately. Our goals are to: • comparatively evaluate performance of the three composition models; • assess the appropriateness of the proposed metrics; • investigate models' performance on the corpus-attested and corpus-unattested combinations. Comparative Performance of the Models Of the three composition models, the mult model (Table 2 ) shows the best results overall. The alm model (Table 3 ) shows statistically significant difference between the model-generated vectors for the correct and incorrect combinations with the cosines and component overlap, but it does not detect the difference on the corpusunattested subset with any of the metrics. The add model (Table 1 ) shows statistically significant differences only with the cosine measures on the corpus-unattested subset. The poor performance of this model may be due to its weaknesses outlined in Section 2.2. Also, Baroni and Zamparelli (2010) note that normalisation may help improving its performance. Appropriateness of the Metrics Cosines to the original input vectors show promising results with all three models. In contrast to the results reported by Vecchi et al. (2011) With COver, the alm model, followed by the mult model, produce sensible results. Table 4 shows the top 3 nearest neighbours found by the models for the correct AN bad intention and the incorrect * bad information. The latter is annotated as incorrect since its meaning is quite vague and a possible correction is inaccurate information. Note that only the alm model is able to discriminate between the correct and the incorrect word combinations suggesting sensible nearest neighbours for bad intention and less sensible ones for *bad information. Attested vs Unattested Combinations Our results show that the models perform differently on the two subsets and somewhat better on corpus-attested ANs. However, the results also confirm that appropriate models and metrics can be found to distinguish between correct and incorrect ANs in both subsets. Conclusion In this paper we have introduced a new task on which compositional distributional semantic models can be tested. Our results support the hypothesis that semantic models can be applied to detect errors in the choice of content words by English language learners. The original contribution of our paper is to show how compositional and distributional semantics can be linked to error detection to provide a solution to a practical task. Our results suggest that with the metrics considered it is easier to detect the difference between the model-generated vectors for the correct and incorrect word combinations with the multiplicative model. On the other hand, qualitative analysis suggests that the adjective-specific linear maps of Baroni and Zamparelli (2010) are superior, since they place the model-generated vectors in semantically sensible neighbourhoods. We plan to investigate further whether the use of a bigger corpus for collecting word co-occurrence statistics provides more reliable counts, and whether dimensionality reduction and/or normalisation of the models improves the results. We also plan to apply the alm model to a larger number of examples. Some other models such as the ones by Erk and Padó (2008) and Thater et al. (2010) which take selectional preferences and context into account may yield better results on this task, and we plan to test this experimentally in the future. Finally, since these models can discriminate between correct and anomalous combinations, the next step is to incorporate them into an error detection classifier. Acknowledgments We are grateful to Cambridge ESOL, a division of Cambridge Assessment, and Cambridge University Press for supporting this research and for granting us access to the CLC for research purposes. We would like to thank Helen Yannakoudakis, Øistein Andersen and the anonymous reviewers for their valuable comments.",
         "2465888",
         "268c6aa93f799472085c1efb976e18878ba12a97",
         "13",
         "https://aclanthology.org/R13-1047",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Kochmar, Ekaterina  and\nBriscoe, Ted",
         "Capturing Anomalies in the Choice of Content Words in Compositional Distributional Semantic Space",
         "365--372",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "kochmar-briscoe-2013-capturing",
         null,
         null
        ],
        [
         "10",
         "W05-0823",
         "This work discusses translation results for the four Euparl data sets which were made available for the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\". All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model.",
         "This work discusses translation results for the four Euparl data sets which were made available for the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\". All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003) . Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002) . The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mariño (2002) , and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This model is described in section 2. Translation results from the four source languages made available for the shared task (es: Spanish, fr: French, de: German, and fi: Finnish) into English (en) are presented and discussed. The paper is structured as follows. Section 2 describes the bilingual n-gram translation model. Section 3 presents a brief overview of the whole SMT procedure. Section 4 presents and discusses the shared task results and other interesting experimentation. Finally, section 5 presents some conclusions and further work. Bilingual N-gram Translation Model As already mentioned, the translation model used here is based on bilingual n-grams. It actually constitutes a language model of bilingual units which are referred to as tuples (de Gispert and Mariño, 2002) . This model approximates the joint probability between source and target languages by using 3grams as it is described in the following equation: p(T, S) ≈ N n=1 p((t, s) n |(t, s) n−2 , (t, s) n−1 ) (1) where t refers to target, s to source and (t, s) n to the n th tuple of a given bilingual sentence pair. Tuples are extracted from a word-to-word aligned corpus according to the following two constraints: first, tuple extraction should produce a monotonic segmentation of bilingual sentence pairs; and second, the produced segmentation is maximal in the sense that no smaller tuples can be extracted without violating the previous constraint (Crego et al., 2004) . According to this, tuple extraction provides a unique segmentation for a given bilingual sentence pair alignment. Figure 1 illustrates this idea with a simple example. Two important issues regarding this translation model must be mentioned. First, when extracting tuples, some words always appear embedded into tuples containing two or more words, so no translation probability for an independent occurrence of such words exists. To overcome this problem, the tuple 3-gram model is enhanced by incorporating 1-gram translation probabilities for all the embedded words (de Gispert et al., 2004) . Second, some words linked to NULL end up producing tuples with NULL source sides. This cannot be allowed since no NULL is expected to occur in a translation input. This problem is solved by preprocessing alignments before tuple extraction such that any target word that is linked to NULL is attached to either its precedent or its following word. SMT Procedure Description This section describes the procedure followed for preprocessing the data, training the models and optimizing the translation system parameters. Preprocessing and Alignment The Euparl data provided for this shared task (Euparl, 2003) was preprocessed for eliminating all sentence pairs with a word ratio larger than 2.4. As a result of this preprocessing, the number of sentences in each training set was slightly reduced. However, no significant reduction was produced. In the case of French, a re-tokenizing procedure was performed in which all apostrophes appearing alone were attached to their corresponding words. For example, pairs of tokens such as l ' and qu ' were reduced to single tokens such as l' and qu'. Once the training data was preprocessed, a wordto-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000) . As an approximation to the most probable alignment, the Viterbi alignment was considered. Then, the intersection and union of alignment sets in both directions were computed for each training set. Feature Function Computation The considered translation system implements a total of five feature functions. The first of these models is the tuple 3-gram model, which was already described in section 2. Tuples for the translation model were extracted from the union set of alignments as shown in Figure 1 . Once tuples had been extracted, the tuple vocabulary was pruned by using histogram pruning. The same pruning parameter, which was actually estimated for Spanish-English, was used for the other three language pairs. After pruning, the tuple 3-gram model was trained by using the SRI Language Modeling toolkit (Stolcke, 2002) . Finally, the obtained model was enhanced by incorporating 1-gram probabilities for the embedded word tuples, which were extracted from the intersection set of alignments. Table 1 presents the total number of running words, distinct tokens and tuples, for each of the four training data sets. The second feature function considered was a target language model. This feature actually consisted of a word 3-gram model, which was trained from the target side of the bilingual corpus by using the SRI Language Modeling toolkit. The third feature function was given by a word penalty model. This function introduces a sentence length penalization in order to compensate the sys-tem preference for short output sentences. More specifically, the penalization factor was given by the total number of words contained in the translation hypothesis. Finally, the fourth and fifth feature functions corresponded to two lexicon models based on IBM Model 1 lexical parameters p(t|s) (Brown et al., 1993) . These lexicon models were calculated for each tuple according to the following equation: p lexicon ((t, s) n ) = 1 (I + 1) J J j=1 I i=0 p(t i n |s j n ) (2) where s j n and t i n are the j th and i th words in the source and target sides of tuple (t, s) n , being J and I the corresponding total number words in each side of it. The forward lexicon model uses IBM Model 1 parameters obtained from source-to-target alignments, while the backward lexicon model uses parameters obtained from target-to-source alignments. Decoding and Optimization The search engine for this translation system was developed by Crego et al. (2005) . It implements a beam-search strategy based on dynamic programming and takes into account all the five feature functions described above simultaneously. It also allows for three different pruning methods: threshold pruning, histogram pruning, and hypothesis recombination. For all the results presented in this work the decoder's monotonic search modality was used. An optimization tool, which is based on a simplex method (Press et al., 2002) , was developed and used for computing log-linear weights for each of the feature functions described above. This algorithm adjusts the log-linear weights so that BLEU (Papineni et al., 2002) is maximized over a given development set. One optimization for each language pair was performed by using the 2000-sentence development sets made available for the shared task. Shared Task Results Table 2 presents the BLEU scores obtained for the shared task test data. Each test set consisted of 2000 sentences. The computed BLEU scores were case insensitive and used one translation reference. es -en fr -en de -en fi -en 0.3007 0.3020 0.2426 0.2031 As can be seen from Table 2 the best ranked translations were those obtained for French, followed by Spanish, German and Finnish. A big difference is observed between the best and the worst results. Differences can be observed from translation outputs too. Consider, for example, the following segments taken from one of the test sentences: es-en: We know very well that the present Treaties are not enough and that , in the future , it will be necessary to develop a structure better and different for the European Union... fr-en: We know very well that the Treaties in their current are not enough and that it will be necessary for the future to develop a structure more effective and different for the Union... de-en: We very much aware that the relevant treaties are inadequate and , in future to another , more efficient structure for the European Union that must be developed... fi-en: We know full well that the current Treaties are not sufficient and that , in the future , it is necessary to develop the Union better and a different structure... It is evident from these translation outputs that translation quality decreases when moving from Spanish and French to German and Finnish. A detailed observation of translation outputs reveals that there are basically two problems related to this degradation in quality. The first has to do with reordering, which seems to be affecting Finnish and, specially, German translations. The second problem has to do with vocabulary. It is well known that large vocabularies produce data sparseness problems (Koehn, 2002) . As can be confirmed from Tables 1 and 2 , translation quality decreases as vocabulary size increases. However, it is not clear yet, in which degree such degradation is due to monotonic decoding and/or vocabulary size. Finally, we also evaluated how much the full feature function system differs from the baseline tuple 3-gram model alone. In this way, BLEU scores were computed for translation outputs obtained for the baseline system and the full system. Since the English reference for the test set was not available, we computed translations and BLEU scores over de-velopment sets. Table 3 presents the results for both the full system and the baseline. 1   Table 3 : Baseline-and full-system BLEU scores (computed over development sets). language pair baseline full es -en 0.2588 0.3004 fr -en 0.2547 0.2938 de -en 0.1844 0.2350 fi -en 0.1526 0.1989 From Table 3 , it is evident that the four additional feature functions produce important improvements in translation quality. Conclusions and Further Work As can be concluded from the presented results, performance of the translation system used is much better for French and Spanish than for German and Finnish. As some results suggest, reordering and vocabulary size are the most important problems related to the low translation quality achieved for German and Finnish. It is also evident that the bilingual n-gram model used requires the additional feature functions to produce better translations. However, more experimentation is required in order to fully understand each individual feature's influence on the overall loglinear model performance. Acknowledgments This work has been funded by the European Union under the integrated project TC-STAR -Technology and Corpora for Speech to Speech Translation -(IST-2002-FP6-506738, http://www.tc-star.org). The authors also want to thank José A. R. Fonollosa and Marta Ruiz Costa-jussà for their participation in discussions related to this work.",
         "8303276",
         "bd500b912faec09be996f76ab49e33eb5a8b5f8f",
         "22",
         "https://aclanthology.org/W05-0823",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Banchs, Rafael E.  and\nCrego, Josep M.  and\nde Gispert, Adri{\\`a}  and\nLambert, Patrik  and\nMari{\\~n}o, Jos{\\'e} B.",
         "Statistical Machine Translation of {E}uparl Data by using Bilingual N-grams",
         "133--136",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "banchs-etal-2005-statistical",
         null,
         null
        ],
        [
         "11",
         "2009.mtsummit-posters.23",
         "Rule based machine translation methods require a set of sophisticated transfer rules for good accuracy. To manually build such a bilingual resource, one requires many man-years of work performed by linguistic specialists. This cost is too high, especially in case of less represented language pairs, such as Hungarian and Japanese. This paper proposes a simple and robust method to automatically build a large coverage transfer rule set for the Hungarian-Japanese language pair. Our method uses a small parsed bilingual corpus and a bilingual dictionary of the selected languages. We concentrate on accurately inducing the most frequent target language translation rules from all instances of a source language rule. We achieved good accuracy especially for low level rules, which are especially important in case of agglutinative languages.",
         "Rule based machine translation methods require a set of sophisticated transfer rules for good accuracy. To manually build such a bilingual resource, one requires many man-years of work performed by linguistic specialists. This cost is too high, especially in case of less represented language pairs, such as Hungarian and Japanese. This paper proposes a simple and robust method to automatically build a large coverage transfer rule set for the Hungarian-Japanese language pair. Our method uses a small parsed bilingual corpus and a bilingual dictionary of the selected languages. We concentrate on accurately inducing the most frequent target language translation rules from all instances of a source language rule. We achieved good accuracy especially for low level rules, which are especially important in case of agglutinative languages. Introduction Creating a set of transfer rules for a rule-based or pattern-based system could take many man-years of work (Prószéky and Tihanyi, 2002) ; we attempt to simplify this process by automatically generating these rules in form of transfer rules, including word-level rule correspondences, such as inflection and conjugation rules. This is particularly crucial with agglutinative languages, such as Hungarian or Japanese. Both languages manifest a high degree of verb and adjective inflection, governed by grammatical rules for which bilingual transfer rule implementation can be very costly. Moreover, both languages present specific linguistic features that are again very costly to organize in a bilingual environment. For example, Hungarian has one of the most grammatical cases, estimated to be between 17 and 24 (László, 1977) (Table 1 ). In Japanese particles before the respective nouns are used instead of grammatical cases. We attempt to generate the transfer rules using a small or medium sized parallel corpus and a bilingual dictionary, concentrating mainly on wordlevel, inflectional correspondences. Because of the limited bilingual resources, our transfer rules will be incorporated into a rule based machine translation framework for assimilation purposes. Thus we target the most simple and general transfer rules that cover most of the language. In this stage we do not attempt to create rules for grammatical exceptions or idiomatic expressions. This paper is structured as follows: first we discuss the most significant related studies, after which we focus on the problems of current translation template generation methods, followed by a brief description of our method. Finally we evaluate our method and conclude with our findings. Related work There are numerous relatively successful examples of shallow translation template extraction methods for closely related languages (Altintas and Güvenir, 2003; Cicekli, 2005) . Initial in-depth structure alignment methods attempt to identify complex, hierarchical structures such as phrase structures (Kaji et al., 1992) or dependency structures (Watanabe et al., 2000) . Other methods include the Translation Template Learner (TTL) algorithm, which analyzes similarities and differences between translation pairs (Cicekli and Güvenir, 2003; Öz and Cicekli, 1998; Ong et al., 2007) . Most of these heuristic methods attempt to generate transfer rules from each sentence pair. With distant languages they notoriously fail, because after recognizing partial matches, these methods estimate that the remaining, unmatched fragments are also equivalent, producing many erroneous, useless and even contradictory results. As a possible solution to the drawbacks of the pure statistical machine translation (weak on reordering; lack of target language fluency), syntactic approaches were proposed that work with traditional statistic models: syntax-based statistical machine translation (Yamada and Knight, 2001) ; string-based (Galley et al., 2004; Galley et al., 2006) ; tree-based (Lin, 2004; Liu et al., 2006) forest-based (Mi et al., 2008) ; forest pruning based systems (Mi et al., 2008) . These methods perform better than the non-statistical ones, but require large bilingual corpora. One other major problem of statistical methods is their difficulty in applicability with agglutinative languages. For example, in Hungarian one noun can have more than 2000 possible forms (combi-nations of number, case, number or person of grammatical possessors or possessed, etc), thus simply collecting enough statistical data is an enormous task. Lemmatizers could facilitate this task, but because of the complexity of the inflection rules and the high number of exceptions from the rule, efficient Hungarian lemmatizers are not yet available. Proposed method In order to achieve high precision, our method analyzes all instances of a certain rule, attempting to extract the most frequent, and thus the most suitable transfer rules. During this process, it looks for the most general rule as possible, subcategorizing or exemplifying only when needed. Our method follows a bottom-to-top mechanism, looking to identify not only general translation templates, but also partial rules, or frequent subsequences of a certain pattern, mainly targeting inflections and conjugations. Resource details To generate the transfer rules, the proposed method uses an automatically generated bilingual dictionary (Varga and Yokoyama, 2007) and a parsed bilingual corpus. There is no known large digital bilingual corpus for Hungarian and Japanese, therefore we needed to improvise with a small, manually created corpus using bilingual language books for Japanese or Hungarian learners that has only about 5000 sentence pairs. Although the sentences are short, it might be suitable for transfer rule extraction, since its data is grammatically rich and well prepared due to its initial educational purpose. For Hungarian we used MetaMorph (Prószéky and Novák, 2005) and for Japanese Cabocha (Kudo and Matsumoto, 2000) parsers. Our method's bilingual corpus requires a specific format. To ensure robustness, we opted for label-bracketed phrase structure rules, since these retain a relatively detailed syntax of the language. For example, the format for the Japanese sentence 夜はオペラに行った [Yoru ha opera ni itta 'Last night I went to the opera'] is (S (PP (N 夜) (Part は)) (VP (NP (N オペラ)(Part に)) (V 行った)))). Both parsers needed minor modifications to accommodate the output. Moreover, the Japanese parser had to be adapted to the Yamada-grammar (Moriyama, 2000) , so that one word should represent one concept, similarly with Hungarian. To be able to generate low-level inflection rules, we extended both Hungarian and Japanese parses with additional, optional information for inflected or conjugated words. The additional information are: (1) information about the inflection or conjugation and (2) the stem of the inflected or conjugated word to be easily identified from the dictionary. For example, the additional information for the Japanese verb 行った [itta; 'went'], becomes (V<2p> 行った<行く>). The bracket after the part-of-speech (POS) information represents the grammatical category of the inflection (2: time; p: past, hence <2p>: past tense), while the bracket after the inflected word represents the stem of the word. Transfer rule generation Our method is composed of two steps: first we generate the language models for each language; next the transfer rules are generated. Both steps are entirely automated. Step 1: Language model generation In this step we are looking to build the language model of the two languages. We compute every sentence, saving all partial rules. We can distinguish four types of rules: 1. head rules: rules where the parent is the sentence itself. The number of terminal rules is equal with the number of words that the sentence contains (ex: V→sleep); 4. regular rules: every rule that is not head, lexical or terminal rule (ex: NP→Adj+NP). We count the frequency of each rule, saving also the sentences from which they were generated. Head, lexical and regular rules need to be solved (transfer rules need to be generated to each of them). We consider a rule to be solved, when there is a correspondence with it in the target language. The transfer rules for the terminal rules are the bilingual dictionaries, and thus they are already solved. Step 2: Transfer rule generation In this step we build the transfer rules between the two languages. Using a recursive algorithm (solve_rule) we build the transfer rule candidates followed by a noisy rule elimination process (clean_candidates). This is performed twice, once as Hungarian and once as Japanese set as source language. solve_rule() solve_rule() solve_rule() solve_rule() For each source head rule (S=s→children(s)), we retrieve all S[] instances (retrieve_instance) in which this rule appears (line 7). If there are S'[] children that are not solved (not_solved), we attempt to solve its children's rules (line 9). For example, in the case of S→PP+VP as a Japanese head rule, an instance where this rule appears is the PP(夜は)+VP(オペラに行った) [yoru ha + opera ni itta 'Last night + I went to the opera'] sentence. Since the PP→N(夜)+Part(は) and VP→PP( オ ペ ラ に )+V( 行 っ た ) are not solved yet, the algorithm attempts to solve these first (line 9). When and if these two rules will be solved, the parent rule (S→PP+VP) will be reattempted. If all children are solved, transfer rule candidates (generate_candidates) are generated (line 13) using all translations of the rule (re-trieve_translation) (line 11). identify_match identify_match identify_match identify_match() () () () If a source rule that is investigated has only solved children, we retrieve all instances of the rule, to- gether with their target language correspondences (TS[]). These target language correspondences are whole sentences, we need to identify which parts of these sentences correspond with the source rule. All rules are identified by their children's rules; therefore if the rule in question is a lexical rule, the identification is done using the bilingual dictionary. The stemmed expression is vital in this case. If no information about the stem is available, there is a risk that the word will not be retrieved from the dictionary. If the rule is a head rule, the identification is done using the already solved rules, while with regular rules both resources are needed. In case of lexical rules, we look up each lexical category's instance (stemmed word, if it is available) from the lexical rule and mark the eventual correspondences. After all such correspondences are marked, we investigate the lowest level phrasal categories in the target language, counting how many identified instances it has in its sub-tree. The node or nodes with the maximum value are selected together with the sub-tree(s) as a transfer rule candidate (Figure 1 ). For example, to identify the s 1 →s 2 +s 3 lexical rule from the t 1 →t 2 +t 3 subtree, the s 2 →w 4 and s 3 →w 5 terminal rules are looked up using the dictionary. The process can have multiple scenarios: the words correspond to the same sub-tree (scenario1), or to different subtree (scenario2). In the latter case, multiple candidates (t 2 →t 4 +t 5 and t 3 →t 6 +t 7 ) are saved. In case of head and regular rules the only difference is the number of children's children. While with lexical rules this was one (one instance for each PoS), for non-lexical rules this is generally at least two (Figure 2 ). If no correspondences are found, no transfer rule candidate is returned. instantiate instantiate instantiate instantiate() () () () In case of lexical rules there are cases when the translations are not registered in the dictionary. In these cases, assuming that the dictionaries are correct, these words have a grammatical, rather than a lexical function. In this case the corresponding lexical category is instantiated, being replaced by its instance. Instantiation is performed also when inflection or conjugation information are available.  For example, if our initial s 1 →s 2 (w 1 )+s 3 (w 2 ) rule's w 2 word did not have any correspondence, the rule becomes s 1 →s 2 (w 1 )+w 2 . For example, in case of the Japanese PP→N+Part, there is no regular rule for a noun plus a particle, therefore the method correctly makes the judgment that the particle needs to be instantiated and new rules are generated for each instance (Table 2 ). clean_ca clean_ca clean_ca clean_candidates ndidates ndidates ndidates() () () () If there are unmatched instances in the second language and their translation can not be found in the first language's rule, the transfer rule candidate is deleted. For example, none of the translations of japán (Japanese person; Japanese language) from example#4 could be found in the Japanese rule, thus the transfer rule was considered erroneous. On the other hand, definite articles (a, az) (English: the) also don't have translations in the Japanese rule, but it they have no translation in the dictionary either (there is no corresponding Japanese translation), therefore they were allowed. The remaining candidates are grouped by their common nodes and are saved with three values: total nr of candidates; total nr of transfer rule instances; nr of instances for the current rule. Since we do not use any thresholds within our method, these three numbers indicate the confidence level of the transfer rule. For example, for PP→N+は [ha] only one transfer rules could be generated: NP→DET+N<2s>. The corresponding values are (7, 2, 2). Evaluation For evaluation, we fragmented our corpus into 5 fragments. First we randomly separated 100 sentences that we used these as our evaluation data. Next we randomly separated 4 training corpora of 100, 500, 1000 and 2000 sentence pairs, to analyze the score differences across various sized corpora. Due to the small size of the available Hungarian-Japanese corpus, performing BLEU score was not adequate, since not enough statistical data was available. Instead, we performed automatic recall evaluation and a manual adequacy evaluation to evaluate our method. We used the rules whose number of instances for the current rule is at least 2. Recall evaluation We investigated to what percentage our method's output rules (R o ) manage to cover the training data's phrase structure rules (R T ). We performed a weighted recall evaluation, weighting each rule (r) with its frequency (frequency(r)) in the training corpus. Because of the instantiation feature many new rules are generated that are not part of the training data's phrase structure rules, during evaluation only we added these new rules to the training data. ( ( ) 100 ⋅ = ∑ ∑ ∈ ∈ T O R r R r r frequency r frequency recall (1) ) We analyzed the Japanese coverage, separately evaluating the head, regular and lexical rules. Lexical rules performed best, improving rapidly from 47.71% to 64.23% when the training data increased from 100 to 2000 sentence pairs (Table 3 ). Head rules performed worst, with only up to a third of them managing to move up the parse trees all the way to the root. Adequacy evaluation We automatically simulated a basic rule based machine translation system with 100 Japanese sentences whose rule head has a transfer rule. These sentences were randomly selected from the test data of our bilingual corpus. Our simple machine translation system exhaustively applied all suitable transfer rules, also performing lexical transfer, based on the bilingual dictionary. During this process, all intermediate data (lexical, regular and head rules) was separately saved. As a result, multiple Hungarian translations became available for a single Japanese sentence. Next, all Hungarian translations, together with the reference retrieved from the bilingual corpus, were manually checked by 3 independent, Hungarian native speakers. We used a 5 to 1 scoring criteria, where 5 is a perfect, 1 is a totally wrong output sentence. The interpretation of the intermediate scores was left to the judgment of each evaluator. We separately evaluated the head, regular and lexical rules. We performed the same evaluation on four training corpora: 100, 500, 1000 and 2000 samples. Assuming that a language model would correctly identify the best translation, we considered only the best scoring Hungarian translation for each Japanese source. Lexical items scored best, since understandably the errors on the lower level reflected within the regular and head rules as well. However, with the increase in size of the training data, the accuracy of the lexical rules increased faster than the other two types of rules. We could not observe any major difference in behaviour between the regular and head rules (Table 4 ). Discussions Our method showed its biggest weakness during recall evaluation. Many rules could not be identified in the transfer rules, especially the ones which direct over larger sub-trees. There are two major reasons for this: linguistic differences and resource issues. Regarding precision, the once recalled rules showed a surprising accuracy, especially lexical ones. Precision problems can be mainly attributed to resource issues. Linguistic differences Our first observation is that the biggest reasons for the low recall value are the linguistic differences between Hungarian and Japanese. Syntax is different, sentence construction is also different; therefore one sub-tree in a certain language does not necessarily match another sub-tree in the other parse. Other linguistic differences, such as expression of pronouns or the sentence topic manifest differently across languages, our method does not always generate the proper transfer rule in these cases. For example, with the 私[watashi]+は [ha] (me, myself) sub-tree rarely has any correspondence in Hungarian, since the agent (pronoun in this case) is expressed within the verb. Resource issues There are two types of resource issues. The first problem concerns the parsers and dictionary that we used. The parsers do not have a perfect accuracy, the noise produced by them reflected in the recall and accuracy results. The methodology of the parsers itself is different, with sub-trees not always matching a sub-tree in the other language, even when both parsers performed correctly. Our bilingual dictionary is noisy, because it was automatically generated using a pivot language (Varga and Yokoyama, 2007) . No manual cleaning was performed in order to raise its recall or accuracy; many translations could not be identified. The second problem concerns our corpus. Precision scores with smaller training data were low, because many erroneous transfer rules were generated besides the correct ones, but with the increase of the training data the frequency of correct rules also climbed rapidly. However, even with our biggest training data (2000 sentence pairs) the recall and precision values were not very high, but it is promising that from the second largest training data (1000 sentence pairs) the relative recall increase was between 3%-11%, the relative adequacy increase between 4%-13%. This significant increase shows that with an average sized corpus much better results can be achieved. Conclusions We presented a transfer rule generating method that uses a parsed bilingual corpus and a bilingual dictionary as resources. Although our biggest aim is low-cost in this research, during bilingual corpus acquisition we found ourselves in a contradictory situation: to generate low-cost transfer rules, we needed to manually create a small bilingual corpus. However, the cost of creating this corpus is insignificant when we think of the costs that a transfer rule system would require. As a compromise between having a small or medium sized corpus with noisy parses and the desire to achieve a good performance, we didn't handle grammatical exceptions or idiomatic expressions. As a result, with a small corpus we managed to achieve medium recall and good precision, with basic conjugation and inflectional rules being highly accurate. We showed that with the minimal increase in size of the bilingual corpus, overall adequacy, together with recall can quickly increase.",
         "34218208",
         "2e4ffd53b290960a18679c8d6f3db4e03bbacdc8",
         "3",
         "https://aclanthology.org/2009.mtsummit-posters.23",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Varga, Istv{\\'a}n  and\nYokoyama, Shoichi",
         "Transfer rule generation for a {J}apanese-{H}ungarian machine translation system",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "varga-yokoyama-2009-transfer",
         null,
         null
        ],
        [
         "12",
         "R13-1048",
         "We present an incremental dependency parser which derives predictions about the upcoming structure in a parse-as-youtype mode. Drawing on the inherent strong anytime property of the underlying transformation-based approach, an existing system, jwcdg, has been modified to make it truly interruptible. A speed-up was achieved by means of parallel processing. In addition, MaltParser is used to bootstrap the search which increases accuracy under tight time constraints. With these changes, jwcdg can effectively utilize the full time span until the next word becomes available which results in an optimal quality-time trade-off.",
         "We present an incremental dependency parser which derives predictions about the upcoming structure in a parse-as-youtype mode. Drawing on the inherent strong anytime property of the underlying transformation-based approach, an existing system, jwcdg, has been modified to make it truly interruptible. A speed-up was achieved by means of parallel processing. In addition, MaltParser is used to bootstrap the search which increases accuracy under tight time constraints. With these changes, jwcdg can effectively utilize the full time span until the next word becomes available which results in an optimal quality-time trade-off. Introduction Users prefer incremental dialogue systems to their non-incremental counterparts (Aist et al., 2007) . For a syntactic parser to contribute to an incremental dialogue system or any other incremental NLP application, it also needs to work incrementally. However, parsers usually operate on whole sentences only and few parsers exist that are capable of incremental parsing or are even optimized for it. This paper focuses on using a parser as part of an incremental pipeline that requires timely response to natural language input. In such a scenario, delay imposed by a parser's lookahead is more severe than delay caused by parsing speed since the parsing speed is capped by the user's input speed. Depending on the input method, the maximum typing speed varies between 0.75 seconds per word (qwerty keyboard) and 6 seconds per word (mobile 12-key multi-tap) (Arif and Stuerzlinger, 2009) and is usually lower if the sentence has to be phrased while typing. In such a scenario, the objective of the parser is to yield high quality results and produce them as soon as they are needed by a subsequent component. It is rarely known beforehand when the next word will be available for processing. Therefore, in an incremental pipeline a) computation should continue until the next word occurs if this might contribute to a better result, and b) a new word should be included immediately to avoid delays. A parser which works pull-based, i.e. processes one prefix until it is deemed finished and then pulls the next word, is insufficient under conditions, since it would require to determine the time used for processing before the processing can even start. Either the estimated processing time will be too short, violating a), or it will be too long, violating b). In contrast, push-based architectures can meet both requirements since the processing of the prefix can be interrupted when new input is available. Beuck et al. (2011) showed that Weighted Constraint Dependency Grammar-based parsers are capable of producing high-quality incremental results but neglected the processing time needed for each increment. In this paper, we will use jwcdg 1 , a reimplementation of the WCDG parser written in Java. jwcdg uses a transformation-based algorithm. It comes equipped with a strong anytime capability, causing the quality of the results to depend on the processing time jwcdg is allowed to consume. We will show that jwcdg can produce high quality results even if only granted fairly low amounts of processing time. Incremental Predictive Dependency Parsing Dependency parsing assigns every word to another word or NIL as its regent and the resulting edges are annotated with a label. If dependency analyses are used to describe the syntactic structure of sentence prefixes, different amounts of prediction can be provided. The interesting cases are those where either the regent or a dependent is not yet part of the sentence prefix. If the regent of a word w is not yet available, the parser can make a prediction about where and how w should be attached. One possibility is to simply state that the regent of w lies somewhere in the future without giving any additional information. This can be modelled by attaching w to a generic nonspec node (Daum, 2004) . Beuck et al. (2011) call this minimal prediction. However, it is usually possible to predict more: The existence of upcoming words can be anticipated and w can then be attached to one of these words. Of course, most of the time it will not be possible to predict exact words but abstract pseudo-words can be used instead that stand for a certain type such as nouns or verbs. Beuck et al. (2011) call these pseudo-words virtual nodes and the approach of using virtual nodes structural prediction (because the virtual nodes accommodate crucial aspects of the upcoming structure of the sentence). A virtual node can be included into an analysis to represent words that are expected to appear in later increments. As an example, \"Peter drives a red\" can be analyzed as Peter drives a red [nonspec] Su bj Det Ad j using minimal prediction or as Peter drives a red [VirtNoun] Su bj Det Ad j Obja using structural prediction. Minimal prediction leads to disconnected analyses while structural prediction allows for connected analyses which resemble the structure of whole sentences. Tn this case, the analysis includes the information that the regent of \"red\" is the object of \"drives\", which is missing in the analysis using minimal prediction. Challenges in Incremental Predictive Parsing The key difference between non-incremental and incremental parsing is the uncertainty about the continuation of the sentence. If a prediction about upcoming structure is being made, there is no guarantee that this prediction will be accurate. Using a beam of possible prefixes, as done in Input: \"Peter ...\" Make Initial Analysis Internal Analysis Peter--XY--> -1 Transform Partial Analysis Increment: \"... drives ...\" Extend Analysis Peter--XY--> 1 drives--S--> 0 Transform Internal Analysis Partial Analysis 2010 ), is a strategy to deal with this uncertainty. It guarantees that each new analysis is an extension of an old one. With this approach, the whole beam becomes incompatible with the observed sentence continuation if no fitting prediction is contained in the beam. Thus, such sentences can not be parsed. Minimal prediction, as another option, largely abstains from prediction. This allows for monotonous extensions even without a beam since the analysis of a prefix will not be incompatible with the continuation of the sentence. This approach is used by MaltParser (Nivre et al., 2007) . A transformation-based parser, finally, can also deal with non-monotonic extensions. In contrast to beam search, only a single analysis is generated for each prefix and there is no guarantee that the analysis of a prefix p n is a monotonic extension of p n−1 . Because the analysis of p n−1 is only used to initialize the transformation process, the search space is not restricted by the results that were obtained in former prefixes although they still guide the analysis. WCDG Parsing In the Weighted Constraint Dependency Grammar formalism, a grammar is used to judge the quality of analyses. The grammar consists of constraints that are evaluated on one or more edges of an analysis. If a constraint is violated, a penalty is computed. Constraints can incorporate more edges into their computation than the edges they are evaluated on (McCrae et al., 2008) . They can traverse the current analysis by using special predicates. This way, a constraint evaluated on one edge could, for example, check if that edge is adjacent to an edge with certain properties. If a constraint uses such predicates, it is considered context sensitive. In the WCDG formalism, the best analysis of a sentence is defined by ba = arg max a∈Analyses c∈Conflicts(a) penalty(c) where the conflicts are the parts of an analysis that stand in conflict with the grammar and penalty(c) is the penalty that the grammar assigns to the conflict c. The Frobbing Algorithm jwcdg tries to find an analysis by transforming a given one until it cannot be improved further. The algorithm employed for this purpose is called frobbing (Foth et al., 2000) . Frobbing consists of two phases: first, the problem is initialized. In this phase, all possible edges are constructed and the constraints defined for a single edge are evaluated on them. An initial analysis is constructed using the best-scored edge for every word, which is repeatedly transformed in the second phase. Frobbing is described as pseudocode in Algorithm 1. A set of conflicts (constraints that are violated on specific edges) is computed and the most severe of them is attacked by transforming the analysis (attackConflict, line 7). This either results in a (not necessarily better) analysis that does not have this conflict or in the insight that the conflict cannot be removed (line 12). The algorithm repeatedly tries to remove the most severe conflict. If in this process a new best analysis is found, it is marked as the new starting point. If the conflict can not be removed, the algorithm tracks back to the starting point. The procedure can be interrupted at any time and the best analysis that was found up to that point will be returned. In its incremental mode, jwcdg works as depicted in Figure 1 . The new word is added to the previous analysis using the best edge. The frobbing algorithm is then run until no better result can be found or the parser is interrupted. To allow for prediction, either a nonspec node (Daum, 2004) or a set of virtual nodes (Beuck et al., 2011) is added to the set of words. This way, all changes regarding incrementality are completely transparent to the frobbing algorithm, only the constraints in the grammar need to be aware of virtual nodes and nonspec. MaltParser is a shift-reduce based parser. It uses a classifier to determine locally optimal actions from a set of possible actions of a parsing algorithm. The classifier is trained using manually annotated sentences. MaltParser can use several different parsing algorithms. In this paper, the 2planar algorithm described in Gómez-Rodríguez and Nivre (2010) will be used, which is able to produce non-projective analyses. Related Work While MaltParser's processing is fast compared to jwcdg, a lookahead of one word already translates into a delay of one to three seconds, depending on the input speed of the user. Beuck et al. (2011) showed that, at least for German, Malt- Parser suffers from a fairly small decrease in accuracy if only features from the next two words instead of three are used. However, since the PoS tags are needed and a PoS tagger needs lookahead as well to achieve good accuracy, a lookahead of at least three words is needed for the whole taggerparser pipeline to achieve a high accuracy. The effect of this delay is illustrated in Figure 2 . In addition, MaltParser is not capable of producing structural predictions. Parallelizing jwcdg Among the two phases of frobbing, only the second one can be interrupted. Therefore, initialization needs to be faster than the shortest time limit we would like to impose. Initialization is mostly concerned with evaluating constraints on all edges that come from or point towards the new word. To make this judgement faster, the code has been changed so that it can be done in parallel, using worker threads instead of a sequential constraint evaluation. Table 1 shows the time needed to construct new edges and judge them while parsing a subset of the NEGRA corpus (Brants et al., 2003) . 2 Although the median time is already relatively good in the non-parallelized case, its maximum amounts to almost two seconds. Parallelization brings most benefits for the more complex initializations: the time needed for the 3rd quartile scales nearly linear up to eight cores. More than sixteen cores yield no further improvement. With the parallelized initialization, jwcdg can spend more time on transforming analyses. In addition, it is able to perform anytime parsing with a lower bound of about 200 ms per word on current hardware. The heart of the frobbing algorithm is the at-tackConflict method which, given an analysis a and a conflict c, systematically tries all changes of edges that are part of c. It then returns the best resulting analysis that does not violate the constraints constr (c). Since these transformations all work independently, they have been parallelized in the same manner as the initialization. The result can be seen in Table 2 . While the introduction of parallelized code causes a small overhead, using two cores already provides a noticeable benefit. The parallelized code can benefit from up to 32 cores, yet the overhead of managing more cores results in a sub-linear speedup. These optimizations have a noticeable impact on parsing performance under time pressure: With a time limit of two seconds per word, jwcdg base scores an unlabeled accuracy of 72.54% for the final analyses, while jwcdg parallel scores 76.29%. jwcdg base only reaches this accuracy with a time limit of four seconds. Unless otherwise noted, all evaluations have been carried out on sentences 18602 to 19601 of the NEGRA corpus. MaltParser as a Predictor for jwcdg When facing a tight time limit, jwcdg has only very little time to improve an analysis by transforming it. Therefore, with tighter time limits a good initial attachment becomes more important and a method which provides frobbing with a good initial analysis could help to achieve drastically better results. Foth and Menzel (2006) showed that WCDG can be augmented by trainable components to raise the accuracy of WCDG. The output of these predictors was converted into constraints to help WCDG finding a good analysis. One of the components was a shift-reduce parser modeled after Nivre (2003) , which was the first description of the MaltParser architecture. Although the shift-reduce parser was relatively simple compared to Malt-Parser and had a labeled accuracy of only 80.7 percent, it helped to raise the accuracy of WCDG from 87.5 to 89.8 percent. This approach has later been used by Khmylko et al. (2009) to integrate MST-Parser (McDonald et al., 2006 ) (which does not work incrementally) as an external data source for WCDG. We integrated MaltParser in a similar way. MaltParser consumes the input from left to right and, if using the 2planar algorithm, constructs edges as soon as possible: An edge can only be created between the word on top of the stack and the current input word. This means that every edge has to be constructed as soon as the second word number of threads used of the edge is the current input word. As soon as that word gets shifted onto the stack, the creation of the edge would no longer be possible. The parser works monotonically since edges are only added to but never removed from the set of edges. This means that all decisions by the parser are final; if word a is not attached to word b, we can be sure that a will never be attached to b in subsequent analyses. As a corollary, if MaltParser has an accuracy of X percent on whole sentences, the probability that a newly created edge is correct will also be X percent. As a delay is not acceptable for our application scenario, we will use MaltParser and the TnT tagger (Brants, 2000) without lookahead despite their inferior accuracy in that mode 3 . An Interface Between MaltParser and jwcdg A predictor (MaltPredictor) for jwcdg has been implemented that uses a newly written incremental interface for MaltParser so that the regents and labels predicted by MaltParser can be accessed by constraints as soon as they become available. MaltPredictor uses the PoS-tags that are provided 3 both were trained on sentences 1000 to 18000 of the negra corpus by the tagger predictor. Each time a new word w is pushed to jwcdg, MaltPredictor forwards w together with its PoS-tag onto MaltParsers input queue and runs MaltParser's algorithm until a shift operations occurs. With this shift operation, w is consumed from the input queue. If the sentence is marked as being finished, MaltParser is run until it has fully parsed the sentence. MaltPredictor then reads the state of MaltParser and stores for each word the regent it has been assigned to by Malt-Parser. If Maltparser did not assign a regent to a word, this fact is also stored. Since -as already mentioned -MaltParser works eagerly (i. e. constructs every edge as soon as possible), the regent of such a word must either lie in the future or be the root node. The three constraints that are used for accessing MaltParser's analyses are depicted as pseudocode in Figure 3 . If only these constraints and the tagger constraint (which selects the PoS-tag for every word) are used as a grammar, jwcdg will parse sentences exactly as MaltParser does. This way, jwcdg acts as an incremental interface to Malt-Parser. The first two constraints are only applicable if MaltPredictor has made a prediction for the word in question. The first constraint checks whether prediction_exists(word) -> regent_of(word) = predicted_regent(word) prediction_exists(word) -> label_of(word) = predicted_label(word) not prediction_exists(word) -> (regent(word) is virtual or regent(word) is nonspec or regent(word) is NIL or word is virtual) Figure 3 : Constraints for incorporating Malt-Parser's results into jwcdg the regent of a word is the one that has been selected by MaltParser. The second constraint checks that the predicted label matches the label of the edge in the analysis given that a label has been predicted. The third constraint is not as straightforward as the other two: Since we know that MaltParser creates edges as soon as possible and we know that MaltParser has not created an edge with this word as dependent, either the regent lies in the future (i. e. it should be a virtual node in jwcdg's analysis) or the regent is NIL (as MaltParser does not explicitly create edges to NIL while parsing). In the other possible case, the dependent of the current edge is a virtual node. In this case MaltParser cannot possibly predict an attachment. A parameter tuning on sentences 501 to 1000 of the NEGRA corpus has shown that a penalty of 0.9 works best for these constraints. When the input sentence is going to be extended, the new word is attached using the edge that violates the least unary, non context-sensitive constraints. The MaltParser constraints are unary and not context sensitive and therefore jwcdg will use the edge predicted by MaltParser if no other constraints prevent it from doing so. Comparison of MaltParser and jwcdg To compare MaltParser and jwcdg, the richer prediction of jwcdg has to be transformed into minimal prediction. In this mode, every attachment of a word to a virtual node is considered correct if the word is attached to an upcoming word in the gold standard. The two accuracies that are used for evaluation are initial attachment accuracy (how often is the newest word attached correctly?) and the final accuracy (how many attachments are correct in the parse trees for the whole sentences?). Figure 4 shows the accuracy for initial attachment and final accuracy as a function of a given time limit. Since MaltParser does not use an anytime algorithm, its results are the same for all time limits 4 . Note that the labeled initial attachment score is fairly low because MaltParser can not predict labels for edges that have nonspec as the regent. When ignoring labels, Maltparser's initial attachment accuracy is higher than its final accuracy since it does not change edges it has created (therefore the accuracy cannot rise) and words can be counted as correct initially but wrong in the final analysis: If the correct regent of a word lies somewhere in the future, the initial decision to not attach it is counted as correct. If it is later attached to a wrong word, it will be counted as wrong in the final accuracy. As can be seen, jwcdg outperforms MaltParser in most aspects when given enough time. The only exception is the unlabeled attachment score for the initial attachment. Here, however, one has to keep in mind that jwcdg produces more informative predictions with virtual nodes, which is not honored in this evaluation. Enhancing jwcdg with MaltParser jwcdg malt has been derived from jwcdg parallel by adding the MaltParser constraints discussed before to the grammar. The results (Figure 4 ) show that jwcdg malt has a considerably higher initial accuracy than jwcdg parallel , more than ten percentage points for a time limit of one second. The final accuracy is noticeably better as well. This shows that Malt-Parser's output helps jwcdg find a good initial analysis which can then be optimized by jwcdg. Evaluation on Additional Corpora The evaluations discussed so far have been carried out on the NEGRA corpus. NEGRA consists of newspaper texts and thus represents a very specific kind of text. However, most sentences from other text types (e.g. chat) are shorter and have a lower structural complexity. This section tries to measure the impact of these differences between different data sources. The results shown in Figure 5 confirm the assumption that jwcdg's accuracy increases when being evaluated only on short sentences. This difference could be due to the syntactic simplicity of short sentences and the shorter initialization time needed for short increments leaving more time for the transformation. In addition, jwcdg malt already approaches its best result with a time limit of four seconds. Evaluation on the creg-109 Corpus Since interactive Computer-Assisted Language Learning could be an interesting application domain for an incremental parser, an additional evaluation has been conducted on the creg-109 corpus, a set of 109 sentences of the corpus described in Meurers et al. (2010) . The creg-109 corpus \"consists of answers to German reading comprehension questions written by American college stu-dents learning German\" (Meurers et al., 2010) . Figure 6 shows the accuracy of the different parser versions on this corpus. The results have a different pattern than the ones for the NEGRA corpus: jwcdg parallel has nearly the same initial attachment accuracy as jwcdg malt and even slightly outperforms it in the final score. In addition to that, the result does not improve much with a time limit of more than two seconds. Both phenomena could be due to the lesser syntactic complexity of the sentences so that the MaltParser's analyses are not so beneficial for guiding jwcdg. Conclusion We have shown that it is possible to gain parse-asyou-type speed with good accuracy using a combination of different incremental approaches to dependency parsing. Our solution based on a parallelized version of jwcdg benefits from using up to 32 cores. In contrast to other approaches, which have to make algorithmic refinements, jwcdg can take advantage from the advances in processing speed due to its anytime property. MaltParser turned out to be a good predictor that helps jwcdg to produce good analyses earlier.",
         "464028",
         "e477ded457dc559b21d4f7f127e02aedcf58d76b",
         "4",
         "https://aclanthology.org/R13-1048",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "K{\\\"o}hn, Arne  and\nMenzel, Wolfgang",
         "Incremental and Predictive Dependency Parsing under Real-Time Conditions",
         "373--381",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "kohn-menzel-2013-incremental",
         null,
         null
        ],
        [
         "13",
         "W05-0822",
         "This paper describes the participation of the Portage team at NRC Canada in the shared task 1 of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task.",
         "This paper describes the participation of the Portage team at NRC Canada in the shared task 1 of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task. Introduction The rapid growth of the Internet has led to a rapid growth in the need for information exchange among different languages. Machine Translation (MT) and related technologies have become essential to the information flow between speakers of different languages on the Internet. Statistical Machine Translation (SMT), a data-driven approach to producing translation systems, is becoming a practical solution to the longstanding goal of cheap natural language processing. In this paper, we describe Portage, a statistical phrase-based machine translation system, which we evaluated on all different language pairs that were provided for the shared task. As Portage is a very 1 http://www.statmt.org/wpt05/mt-shared-task/ new system, our main goal in participating in the workshop was to test it out on different language pairs, and to establish baseline performance for the purpose of comparison against other systems and against future improvements. To do this, we used a fairly standard configuration for phrase-based SMT, described in the next section. Of the language pairs in the shared task, French-English is particularly interesting to us in light of Canada's demographics and policy of official bilingualism. We therefore divided our participation into two parts: one stream for French-English and another for Finnish-, German-, and Spanish-English. For the French-English stream, we tested the use of additional data resources along with hand-coded rules for translating numbers and dates. For the other streams, we used only the provided resources in a purely statistical framework (although we also investigated several automatic methods of coping with Finnish morphology). The remainder of the paper is organized as follows. Section 2 describes the architecture of the Portage system, including its hand-coded rules for French-English. Experimental results for the four pairs of languages are reported in Section 3. Section 4 concludes and gives pointers to future work. Portage Portage operates in three main phases: preprocessing of raw data into tokens, with translation suggestions for some words or phrases generated by rules; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. (A fourth postprocessing phase was not needed for the shared task.) Preprocessing Preprocessing is a necessary first step in order to convert raw texts in both source and target languages into a format suitable for both model training and decoding (Foster et al., 2003) . For the supplied Europarl corpora, we relied on the existing segmentation and tokenization, except for French, which we manipulated slightly to bring into line with our existing conventions (e.g., converting l ' an into l' an). For the Hansard corpus used to supplement our French-English resources (described in section 3 below), we used our own alignment based on Moore's algorithm (Moore, 2002) , segmentation, and tokenization procedures. Languages with rich morphology are often problematic for statistical machine translation because the available data lacks instances of all possible forms of a word to efficiently train a translation system. In a language like German, new words can be formed by compounding (writing two or more words together without a space or a hyphen in between). Segmentation is a crucial step in preprocessing languages such as German and Finnish texts. In addition to these simple operations, we also developed a rule-based component to detect numbers and dates in the source text and identify their translation in the target text. This component was developed on the Hansard corpus, and applied to the French-English texts (i.e. Europarl and Hansard), on the development data in both languages, and on the test data. Decoding Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P(t|s). Our model for P(t|s) is a log-linear combination of four main components: one or more trigram language models, one or more phrase translation models, a distortion model, and a word-length feature. The trigram language model is implemented in the SRILM toolkit (Stolcke, 2002) . The phrase-based translation model is similar to the one described in (Koehn, 2004) , and relies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings. s To set weights on the components of the loglinear model, we implemented Och's algorithm (Och, 2003) . This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell's algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations. Unfortunately, our implementation of this algorithm converged only very slowly to a satisfactory final nbest list, so we used two different ad hoc strategies for setting weights: choosing the best values encountered during , with the exception of a ch as the ability to decode either w ards. translarent language pairs of the sha d t hared t the iterations of Och's algorithm (French-English), and a grid search (all other languages). To perform the actual translation, we used our decoder, Canoe, which implements a dynamicprogramming beam search algorithm based on that of Pharaoh (Koehn, 2004) . Canoe is input-output compatible with Pharaoh few extensions su back ards or forw Rescoring To improve raw output from Canoe, we used a rescoring strategy: have Canoe generate a list of nbest translations rather than just one, then reorder the list using a model trained with Och's method to optimize BLEU score. This is identical to the final pass of the algorithm described in the previous section, except for the use of a more powerful loglinear model than would have been feasible to use inside the decoder. In addition to the four basic features of the initial model, our rescoring model included IBM2 model probabilities in both directions (i.e., P(s|t) and P(t|s)); and an IBM1-based feature designed to detect whether any words in one language seemed to be left without satisfactory tions in the other language. This missing-word feature was also applied in both directions. Experiments on the Shared Task We conducted experiments and evaluations on Portage using the diffe re ask. In addition to the provided data, a set of 6,056,014 sentences extracted from Hansard corpus, the official record of Canada's parliamentary debates, was used in both French and English languages. This c guage and translation models for use in decoding and rescoring. The development test data was split into two parts: The first part that includes 1,000 sentences in each language with reference translations into English served in the optimization of weights for both the decoding and rescoring models. In this study, number of n-best lists was set to 1,000. The second part, which includes 1,000 sentences in each language with referenc used in the evaluation of the performance of the translation models. Experiments on the French-English Task Our goal for this language pair was to conduct experiments on hniques: 1 indicates the method, the second column gives results for decoding with Canoe only, and the third column for decoding and rescoring with Canoe. For comparison between the four methods, there was an improvement in terms of BLEU scores when using two language models and two translation models generated from Europarl and Hansard corpora; however, parsing numbers and dates had a negative impact on the ranslation els. of increased trade within North merica but also functions as a good counterpoint for French-English. ble 1. BLEU scores for the French-English test sentences A noteworthy feature of these results is that the improvement given by the out-of-domain Hansard corpus was very slight. Although we suspect that somewhat better performance could have been achieved by better weight optimization, this result clearly underscores the importance of matching training and test domains. A related point is that our number and date translation rules actually caused a performance drop due to the fact that they were optimized for typographical conventions prevalent in Hansard, which are quite different from those used in Europarl. Our best result ranked third in the shared WPT05 French-English task , with a difference of 0.74 in terms of BLEU score from the first rank participant, and a difference of 0.67 in terms o BLEU score from the second ranked participant. Experiments on other Pairs of Languages The WPT05 workshop provides a good opportunity to achieve our benchmarking goals with corpora that provide challenging difficulties. German and Finnish are languages that make considerable use of compounding. Finnish, in addition, has a particularly complex morphology that is organized on principles that are quite different from any in English. This results in much longer word forms each of which occurs very infrequently. Our original intent was to propose a number of possible statistical approaches to analyzing and splitting these word forms and improving our results. Since none of these yielded results as good as the baseline, we will continue this work until we understand what is really needed. We also care very much about translating between French and English in Canada and plan to spend a lot of extra effort on difficulties that occur in this case. Translation between Spanish and English is also becoming more mportant as a result ble 2 BLEU scores for the Finnish-English, German-English and Spanish-English test sentences To establish our baseline, the only preprocessing we did was lowercasing (using the provided tokenization). Canoe was run without any special settings, although weights for distortion, word penalty, language model, and translation model were optimized using a grid search, as described above. Rescoring was also done, and usually resulted in at least an extra BLEU point. Our final results are shown in Table 2 . Ranks at the shared WPT05 Finnish-, German-, and Spanish-English tasks were assigned as second, third and fourth, with differences of 1.06, 1.87 ter s of BLEU sc Conclusion We have reported on our participation in the shared task of the ACL 2005 Workshop on Building and Using Parallel Texts, conducting evaluations of Portage, our statistical machine translation system, on all four language pairs. Our best BLEU scores for the French-, Finnish-, German-, and Spanish-English at this stage were 29.5, 20.95, 23.21 and 29.08 , respectively. In total, eleven teams took part at the shared task and most of them submitted results for all pairs of languages. Our results distinguished the NRC team at the third, second, third and fourth ranks with slight differences with the first ranked participants. A major goal of this work was to evaluate Portage at its first stage of implementation on different pairs of languages. This evaluation has served to identify some problems with our system in the areas of weight optimization and number and date rules. It has also indicated the limits of using out-ofdomain corpora, and the difficulty of morphologically complex languages like Finnish. Current and planned future work includes the exploitation of comparable corpora for statistica machine transl knowledge, and better features for nbest rescoring.",
         "1289925",
         "dd91cae2486eb393f524c3cc32006991568e8ba5",
         "47",
         "https://aclanthology.org/W05-0822",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Sadat, Fatiha  and\nJohnson, Howard  and\nAgbago, Akakpo  and\nFoster, George  and\nKuhn, Roland  and\nMartin, Joel  and\nTikuisis, Aaron",
         "{PORTAGE}: A Phrase-Based Machine Translation System",
         "129--132",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "sadat-etal-2005-portage",
         null,
         null
        ],
        [
         "14",
         "R13-1049",
         "The Unit Graphs (UGs) framework is a graph-based knowledge representation (KR) formalism that is designed to allow for the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory Combinatorial Dictionary of the Meaning-Text Theory (MTT). This paper introduces the UGs framework, and overviews current published outcomes. It first introduces rationale of this new formalism: neither semantic web formalisms nor Conceptual Graphs can represent linguistic predicates. It then overviews the foundational concepts of this framework: the UGs are defined over a UG-support that contains: i) a hierarchy of unit types which is strongly driven by the actantial structure of unit types, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. On these foundational concepts and on the definition of UGs, this paper finally overviews current outcomes of the UGs framework: the definition of a deep-semantic representation level for the MTT, representation of lexicographic definitions of lexical units in the form of semantic graphs, and two formal semantics: one based on UGs closure and homomorphism, and one based on model semantics.",
         "The Unit Graphs (UGs) framework is a graph-based knowledge representation (KR) formalism that is designed to allow for the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory Combinatorial Dictionary of the Meaning-Text Theory (MTT). This paper introduces the UGs framework, and overviews current published outcomes. It first introduces rationale of this new formalism: neither semantic web formalisms nor Conceptual Graphs can represent linguistic predicates. It then overviews the foundational concepts of this framework: the UGs are defined over a UG-support that contains: i) a hierarchy of unit types which is strongly driven by the actantial structure of unit types, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. On these foundational concepts and on the definition of UGs, this paper finally overviews current outcomes of the UGs framework: the definition of a deep-semantic representation level for the MTT, representation of lexicographic definitions of lexical units in the form of semantic graphs, and two formal semantics: one based on UGs closure and homomorphism, and one based on model semantics. Introduction The Meaning-Text Theory (MTT) is a theoretical dependency linguistics framework for the construction of models of natural language. As such, its goal is to write systems of explicit rules that express the correspondence between meanings and texts (or sounds) in various languages (Kahane, 2003) . From semantic representations to surface phonologic representations, seven different levels of linguistic representation are supposed for each set of synonymous utterances. Thus, two times six modules containing transformation rules are used to transcribe representations of a level into representations of an adjacent level. The main constituent of the MTT is the dictionary model where lexical units are described, which is called the Explanatory Combinatorial Dictionary (ECD) (Mel'čuk, 2006) . As for any community of interest, linguists and lexicographers of the MTT framework produce knowledge. Knowledge Representation (KR) is an area of artificial intelligence that deals with recurrent needs that emerge with such knowledge production. The aim of this paper is to introduce the Unit Graphs KR formalism that is designed to allow for the representation, manipulation, query, and reasoning over dependency structures, rules and lexicographic knowledge of the ECD. The rest of this paper is organized as follows. We will first introduce rationale of this new KR formalism ( §2), then the fundamental concepts of the UGs framework ( §3), implications for the MTT, lexicographic definitions and application to a specific MTT lexicographic edition project ( §4), and finally two approaches to assign UGs with logical semantics, so as to enable reasoning in the UGs framework ( §5). Rationale: Representation of Valency-based Predicates Most past or current projects that aimed at implementing the ECD did so in a lexicographic perspective. One important example is the RE-LIEF project (Lux-Pogodalla and Polguère, 2011) , which aims at representing a lexical system graph named RLF (Polguère, 2009) , where lexical units are interlinked by paradigmatic and syntagmatic links of lexical functions (Mel'čuk, 1996) . In the RELIEF project, the description of Lexical Functions is based on a formalization proposed by Kahane and Polguère (2001) . Moreover, lexicographic definitions start to be partially formalized in the RELIEF project using the markup type that has been developed in the Definiens project (Barque and Polguère, 2008; Barque et al., 2010) . One exception is the proprietary linguistic processor ETAP-3 that implements a variety of ECD for Natural Language Processing (Apresian et al., 2003; Boguslavsky et al., 2004) . Linguistic knowledge are asserted, and transformation rules are directly formalized in first order logic. Adding to these formalization works, our goal is to propose a formalization from a knowledge engineering perspective, compatible with standard KR formalisms. The term formalization here means not only make non-ambiguous, but also make operational, i.e., such that it supports logical operations (e.g., knowledge manipulation, query, reasoning). We thus adopt a knowledge engineering approach applied to the domain of the MTT. At first sight, two existing KR formalisms seem interesting for this job: semantic web formalisms (e.g., RDF 1 , RDFS 2 , OWL 3 , SPARQL 4 ), and Conceptual Graphs (CGs) (Sowa, 1984; Chein and Mugnier, 2008) . Both of them are based on directed labelled graph structures, and some research has been done towards using them to represent dependency structures and knowledge of the ECD (OWL in (Lefranc ¸ois and Gandon, 2011; Boguslavsky, 2011) , CGs at the conceptual level in (Bohnet and Wanner, 2010) ). Yet Lefranc ¸ois (2013) showed that neither of these KR formalisms can represent valency-based predicates, therefore lexicographic definitions. One crucial issue is the following: in RDFS, OWL and the CGs, there is a strong distinction between concept types and relations. Yet, a linguistic predicate may be considered both as a concept type as it is instantiated in dependency structures, and as a relation as its instances may link other instances. The simple semantic representation illustrated on figure 1 thus cannot be represented with these formalisms unless we use reification of n-ary rela-tions. But then these formalisms lack logical semantics to reason with such relations. ( Peter ) ( try ) ( push ) ( cat ) 1 2 1 2 Figure 1 : Semantic representation of sentence Peter tries to push the cat. As the CGs formalism is the closest to the semantic networks, the following choice has been made to overcome these issues: Modify the CGs formalism basis, and define transformations to syntaxes of Semantic Web formalisms for sharing and querying knowledge. As we are to represent linguistic units of different nature (e.g., semantic units, lexical units, grammatical units, words), term unit has been chosen to be used in a generic manner, and the result of this adaptation is thus the Unit Graphs (UGs) framework. Fundamental Concepts of the UGs Framework First, for a specific Lexical Unit L, Mel'čuk (2004, p.5 ) distinguishes considering L in language (i.e., in the lexicon), or in speech (i.e., in an utterance). KR formalisms and the UGs formalism also do this distinction using types. In this paper and in the UGs formalism, there is thus a clear distinction between units (e.g., semantic unit, lexical unit), which will be represented in the UGs, and their types (e.g., semantic unit type, lexical unit type), which are roughly classes of units that share specific features. It is those types that will specify through their so-called actancial structure (Mel'čuk, 2004) how their instances (i.e., units) are to be linked to other units in a UG. Hierarchy of Unit Types The core of the UGs framework is a structure called hierarchy of unit types and noted T , where unit types and their actantial structure are described. This structure is thoroughly described in (Lefranc ¸ois, 2013; Lefranc ¸ois and Gandon, 2013b) and studied in (Lefranc ¸ois and Gandon, 2013d). Whether they are semantic, lexical or grammatical, unit types are assigned a set of Actant Slots (ASlots), and every ASlot has a so-called Actant Symbol (ASymbol) which is chosen in a set denoted S T . S T contains numbers for the semantic unit types, and other \"classical\" symbols for the other levels under consideration (e.g, roman numerals I to VI for the Deep Syntactic actants). The set of ASlots of a unit type t is represented by the set α α α(t) of ASymbols these ASlots have. Moreover, • some ASlots are obligatory, they form the set α α α 1 (t) of Obligatory Actant Slots (OblASlots); • other are prohibited, they form the set α α α 0 (t) of Prohibited Actant Slots (ProASlots); • the ASlots that are neither obligatory nor prohibited are said to be optional, they form the set α α α ? (t) of Optional Actant Slots (Op-tASlots). Finally, every unit type t has a signature function ς ς ς t that assigns to every ASlot of t a unit type, which characterises units that fill such a slot. The set of unit types is then pre-ordered 5 by a specialization relation , and for mathematical reasons as one goes down the hierarchy of unit types the actantial structure of unit types may only become more and more specific: (i) some ASlot may appear, be optional a moment, and at some points become obligatory or prohibited; (ii) the signatures may only become more specific. Hierarchy of Circumstantial Symbols UGs include actantial relations, which are considered of type predicate-argument and are described in the hierarchy of unit types. Now UGs also include circumstantial relations which are considered of type instance-instance. Example of such relations are the deep syntactic representation relations ATTR, COORD, APPEND of the MTT, but we may also use such relations to represent the link between a lexical unit and its associated surface semantic unit for instance. Circumstantial relations are labelled by symbols chosen in a set of so-called Circumstantial Symbols (CSymbols), denoted S C , and their categories and usage are described in a hierarchy denoted C, that has been formally defined in (Lefranc ¸ois and Gandon, 2013a). Unit Graphs UGs are defined over a so-called support, S def = (T , C, M) where T is a hierarchy of unit types, C 5 A pre-order is a reflexive and transitive binary relation. is a hierarchy of CSymbols, and M is a set of unit identifiers. A UG G defined over a support S is a tuple denoted G def = (U, l, A, C, Eq), where U is the set of unit nodes, l is a labelling mapping over U that associate every unit node with a unit type and one or more unit identifiers, A and C are respectively actantial and circumstantial triples, and Eq is a set of asserted unit node equivalences. Unit nodes are illustrated by rectangles with their label written inside, actantial triples are illustrated by double arrows, circumstantial triples are illustrated by simple arrows, and asserted unit node equivalences are illustrated by dashed arrows. For instance, figure 1 is a semantic representation of sentence Peter tries to push the cat. in which units are typed by singletons and ASymbols are numbers, in accordance with the MTT. Figure 2 is a simplified deep syntactic representation of Peter is gently pushing the cat. In this figure unit nodes u 2 and u 4 are typed by singletons, and only unit node u 2 is not generic and has a marker: {P eter}. P is composed of (u 1 , I, u 2 ) and (u 1 , II, u 3 ), where I and II are ASymbols. C is composed of (u 1 , ATTR, u 4 ) where ATTR is a CSymbol. In the relation Eq there is (u 1 , u 1 ), (u 2 , u 2 ), and (u 3 , u 3 ). {PUSH,present, progressive} UGs so defined are the core dependency structures of the UGs mathematical framework. MAN:P eter {CAT,def } GENTLY u 1 u 2 u 3 u 4 I II ATTR Unit Graphs and the Meaning-Text Theory 4.1 A Deep-Semantic Representation Level As the unit types hierarchy T is driven by the actantial structure of unit types, and as semantic ASymbols are numbers, the pre-order over unit types at the semantic level represents a specialization of the actantial structure, and not of meanings. For instance, the french lexical unit INSTRUMENT (en: instrument) has a Semantic ASlot 1 that corresponds to the activity for which the instrument is designed. Now PEIGNE (en: comb) has a stricter meaning than INSTRUMENT, and also two Semantic ASlots: 1 correspond to the person that uses the comb, and 2 is a split variable 6 that corresponds either to the hair or to the person that is to be combed. Then semantic unit type ( peigne ) cannot be more specific than ( instrument ) in the hierarchy of unit types because the signature of its ASlot 1 is not more specific than the signature of the ASlot 1 of ( instrument ) , i.e., ς ς ς ( peigne ) (1) = ( person ) ( activity ) = ς ς ς ( instrument ) (1). In fact, the meaning of ASlot 1 is not the same for ( instrument ) and ( peigne ) . Lefranc ¸ois and Gandon (2013b) therefore introduced a deeper level of representation to describe meanings: the deep semantic level, and defined the deep and surface semantic unit types and their actantial structure. The Deep Semantic Unit Type (DSemUT) associated with a Lexical Unit Type (LexUT) L is denoted / L \\ . So that the ASlots of deep semantic unit types convey meanings, the set of ASsymbols that is used to symbolize ASlots at this level is a set of lexicalized semantic roles (e.g., agent, combedhair, combedperson). For instance the DSemUT / instrument \\ associated with the LexUT INSTRUMENT may have an ASlot arbitrarily symbolized activity, which would be inherited by the DSemUT / peigne \\ . Then / peigne \\ also introduces three new ASlots: one arbitrarily symbolized possessor that corresponds to the ASlot 1 of ( peigne ) , and two arbitrarily symbolized combedhair, and combedperson that correspond to the ASlot 2 of ( peigne ) . Actually, one may need to introduce a new ASymbol every time a Semantic ASlot that conveys a new meaning is introduced. The set of semantic roles thus cannot be bound to a small set of universal semantic roles. Lexicographic Definitions It is at the deep semantic representation level that one may represent the actual meaning of a LexUT L. The lexicographic definition of L corresponds to the definition of its associated DSe-mUT / L \\ , which is roughly an equivalence between two deep semantic UGs. Unit type definitions have been formally defined in (Lefranc ¸ois and Gandon, 2013a), and the definition of / L \\ is a triple D / L \\ def = (D − / L \\ , D + / L \\ , κ) , where (roughly): • D − / L \\ represents only a central unit node typed with / L \\ , and some other unit nodes that fill some of the ASlots of / L \\ ; • D + / L \\ is a UG called the expansion of / L \\ , • there is no circumstantial triple in these two UGs because circumstantials must not be part of the lexicographic definition of a LexUT. • κ is a mapping from the unit nodes of D − / L \\ to some unit nodes of D + / L \\ . Figure 3 is an example of lexicographic definition of PEIGNE: an instrument that a person X uses to untangle the hair Y 1 of a person Y 2 . Intuitively, a definition corresponds to two reciprocal rules. If there is the defined PUT in a UG then one may infer its definition, and vice versa. A set of unit type definitions D may thus be added to the unit types hierarchy. / peigne \\ / person \\ / hair \\ / person \\ possessor combedperson combedhair / instrument \\ / person \\ / untangle \\ / person \\ / hair \\ Lefranc ¸ois et al. ( 2013 ) illustrated how the UGs framework may be used to edit lexicographic definitions in the RELIEF lexicographic edition project (Lux-Pogodalla and Polguère, 2011) . Lexical Units are assigned a semantic label that may be considered as a deep semantic unit type and to which one may assign an actantial structure. A lexicographer may then manipulate nodes in a graphical user interface so as to little by little construct a deep semantic UG that represents the decomposition of the DSemUT associated with the defined LexUT. A prototype web application has been developed, and a demonstration is available online: http://wimmics.inria.fr/doc/ video/UnitGraphs/editor1.html. We currently lead an ergonomic study in partnership with actors of the RELIEF project in order to enhance the workflow of this prototype. Reasoning in the Unit Graphs Framework The prime decision problem of the UGs framework is the following: Considering two UGs G and H defined over the same support S, does the knowledge of G entails the knowledge of H ? Reasoning with UGs-Homomorphisms Lefranc ¸ois and Gandon (2013a) proposed to use the notion of UGs homomorphism to define this entailment problem. There is a homomorphism from a UG H to a UG G if and only if there is a homomorphism from the underlying oriented labelled graphs of H to that of G. Now one need to define the notion of knowledge of a UG. In fact, the UGs framework makes the open-world assumption, which means that a UG along with the support on which it is defined represents explicit knowledge, and that additional knowledge may be inferred. Consider the UG G = (U, l, A, C, Eq) defined over the support S illustrated in figure 4a . Some knowledge in G is implicit: 1. two unit nodes u 1 and u 2 share a common unit marker M ary, so one may infer that they represent the same unit. (u 1 , u 2 ) may be added to Eq. 2. every unit type is a subtype of the prime universal unit type , so one could add to all the types of unit nodes in G. 3. there are two unit nodes v 1 and v 2 that fill the same ASlot activity of the unit node typed / instrument \\ . So one may infer that v 1 and v 2 represent the same unit. Said otherwise, (v 1 , v 2 ) may be added to Eq. 4. one may recognize the expansion of / peigne \\ as defined in figure 3 , so this type may be made explicit in the unit node typed / instrument \\ . Each of the rules behind these cases explicit knowledge in G. More generally, Lefranc ¸ois and Gandon (2013a) listed a set of rules which defines the axiomatization of the UGs semantics. The process of applying this set of rules on a UG G until none of them has any effect is called closing G. Figure 4b illustrates the closure of G, where all of the inferable knowledge has been made explicit. The notion of entailment may hence be defined as follows: G entails H, noted G h H, if and only if there is a homomorphism from H to the closure of G. Lefranc ¸ois and Gandon (2013a) illustrated problematic cases where the closure is infinite for finite UGs. If that occurs it makes the closure undecidable, along with the entailment problem. We are currently working of the definition of restrictions of the unit types hierarchy and the set of definitions in order to ensure that any UG has a finite closure. Model Semantics for the UGs framework Another approach to defining the entailment problem has been presented in (Lefranc ¸ois and Gandon, 2013c), using model semantics based on relational algebra. The model of a support S = (T , C, M) is a couple M = (D, δ), where D is a set called the domain of M , and δ is denoted the interpretation function. In order to deal with the problem of prohibited and optional ASlots, D contains a special element denoted • that represents nothing, plus at least one other element, and must be such that: • M is a model of T ; • M is a model of C; • for all unit identifier m ∈ M, the interpretation of m is an object of the domain D except for the special nothing element; Lefranc ¸ois and Gandon (2013c) listed the different equations that the interpretation function must satisfy so that a model is a model of a unit types hierarchy and of a CSymbols hierarchy. A model of a UG G is a model of the support on which it is defined, augmented with an assignment function β, which is a mapping from the set of unit nodes of G to the domain D. Such a model needs to satisfy a list of equations so that it may be said to satisfy the unit graph G. Then the notion of entailment is defined as classically done with model semantics: Let H and G be two UGs defined over a support S. G entails H, or H is a semantic consequence of G, noted G m H, if and only if for any model (D, δ) of S and for any assignment β G such that (D, δ, β G ) satisfies G, then there exists an assignment β H of the unit nodes in H such that (D, δ, β H ) satisfies H. There are multiple directions of research for the reasoning problem. • the definition of the model semantics of the UGs shall be completed so as to take lexicographic definitions into account. • one need to define algorithms to construct a model that satisfy a UG, and to check the entailment of a UG by another. • such algorithms may lead to an infinite domain. A condition over the unit types hierarchy and the lexicographic definitions must be found so as to ensure that the model is decidable for a finite UG. • are the two entailment relations h and m equivalent ? / instrument \\ / person \\ :Mary / do \\ / untangle \\ / person \\ :Mary / hair \\ activity activity agent object partof u 1 u 2 v 2 v 1 (a) Incomplete deep semantic representation G { / peigne \\ , / instrument \\ , } { / person \\ , }:Mary { / untangle \\ , / do \\ , } { / untangle \\ , / do \\ , } { / person \\ , }:Mary { / hair \\ , } activity Conclusion We thus introduced rationale of the new Unit Graphs Knowledge Representation formalism that is designed to formalize, in a knowledge engineering perspective, the dependency structures, the valency-based predicates, and lexicographic definitions in the ECD. The strong coherence in the unit types hierarchy justifies the introduction of a deep semantic representation level that is deeper than the MTT semantic level, and in which one may represent the lexicographic definitions. Finally, two different logical semantics have been provided for UGs and the prime entailment decision problem has been defined in two ways. More research is needed to determine if these two decision problems are equivalent, and what their complexity is. There are other longer-term directions of research for the Unit Graphs framework: We are working on a syntax based on semantic web standards for the different objects of the framework. Like WordNet today, the linguistic knowledge written with that syntax could be shared and queried on the web of linked data 7 . This would support their use as a highly structured lexical resource by consumers of the linked data cloud. Rules have already been defined in the UGs framework. Let G DSem be a deep semantic UG, we need algorithms to select and apply correspondence rules to transcribe G DSem to a surface semantic UG G SSem for instance. We are working on defining generic rules to formally represent semantic derivations. This is a first step towards representing Lexical Functions that play a very important role in the MTT. Finally, the design of the Unit Graphs framework is a first step towards Natural Language Processing applications. Future work include (semiautomatically) populating this model with linguistic data, and formulating classical NLP tasks in terms of UGs, such as machine translation, question answering, text summarization, and so on.",
         "14330089",
         "6a32b0e9b257dca89a95b8e18444fdbd329da232",
         "4",
         "https://aclanthology.org/R13-1049",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Lefran{\\c{c}}ois, Maxime  and\nGandon, Fabien",
         "Rationale, Concepts, and Current Outcome of the Unit Graphs Framework",
         "382--388",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "lefrancois-gandon-2013-rationale",
         null,
         null
        ],
        [
         "15",
         "W09-1312",
         "In this paper we present an extractive system that automatically generates gene summaries from the biomedical literature. The proposed text summarization system selects and ranks sentences from multiple MEDLINE abstracts by exploiting gene-specific information and similarity relationships between sentences. We evaluate our system on a large dataset of 7,294 human genes and 187,628 MEDLINE abstracts using Recall-Oriented Understudy for Gisting Evaluation (ROUGE), a widely used automatic evaluation metric in the text summarization community. Two baseline methods are used for comparison. Experimental results show that our system significantly outperforms the other two methods with regard to all ROUGE metrics. A demo website of our system is freely accessible at http://60.195.250.72/onbires/summary.jsp.",
         "In this paper we present an extractive system that automatically generates gene summaries from the biomedical literature. The proposed text summarization system selects and ranks sentences from multiple MEDLINE abstracts by exploiting gene-specific information and similarity relationships between sentences. We evaluate our system on a large dataset of 7,294 human genes and 187,628 MEDLINE abstracts using Recall-Oriented Understudy for Gisting Evaluation (ROUGE), a widely used automatic evaluation metric in the text summarization community. Two baseline methods are used for comparison. Experimental results show that our system significantly outperforms the other two methods with regard to all ROUGE metrics. A demo website of our system is freely accessible at http://60.195.250.72/onbires/summary.jsp. Introduction Entrez Gene is a database for gene-centric information maintained at the National Center for Biotechnology Information (NCBI). It includes genes from completely sequenced genomes (e.g. Homo sapiens). An important part of a gene record is the summary field (shown in Table 1 ), which is a small piece of text that provides a quick synopsis of what is known about the gene, the function of its encoded protein or RNA products, disease associations, genetic interactions, etc. The summary field, when available, can help biologists to understand the target gene quickly by compressing a huge amount of knowledge from many papers to a small piece of text. At present, gene summaries are generated manually by the National Library of Medicine (NLM) curators, a time-and labor-intensive process. A previous study has concluded that manual curation is not sufficient for annotation of genomic databases (Baumgartner et al., 2007) . Indeed, of the 5 million genes currently in Entrez Gene, only about 20,000 genes have a corresponding summary. Even in humans, arguably the most important species, the coverage is modest: only 26% of human genes are curated in this regard. The goal of this work is to develop and evaluate computational techniques towards automatic generation of gene summaries. To this end, we developed a text summarization system that takes as input MEDLINE documents related to a given target gene and outputs a small set of genic information rich sentences. Specifically, it first preprocesses and filters sentences that do The protein encoded by this gene is a receptor for interleukin 20 (IL20), a cytokine that may be involved in epidermal function. The receptor of IL20 is a heterodimeric receptor complex consisting of this protein and interleukin 20 receptor beta (IL20B). This gene and IL20B are highly expressed in skin. The expression of both genes is found to be upregulated in Psoriasis. Table1 . Two examples of human-written gene summaries not include enough informative words for gene summaries. Next, the remaining sentences are ranked by the sum of two individual scores: a) an authority score from a lexical PageRank algorithm (Erkan and Radev, 2004 ) and b) a similarity score between the sentence and the Gene Ontology (GO) terms with which the gene is annotated (To date, over 190,000 genes have two or more associated GO terms). Finally, redundant sentences are removed and top ranked sentences are nominated for the target gene. In order to evaluate our system, we assembled a gold standard dataset consisting of handwritten summaries for 7,294 human genes and conducted an intrinsic evaluation by measuring the amount of overlap between the machine-selected sentences and human-written summaries. Our metric for the evaluation was ROUGE 1 , a widely used intrinsic summarization evaluation metric. Related Work Summarization systems aim to extract salient text fragments, especially sentences, from the original documents to form a summary. A number of methods for sentence scoring and ranking have been developed. Approaches based on sentence position (Edmundson, 1969) , cue phrase (McKeown and Radev, 1995) , word frequency (Teufel and Moens, 1997) , and discourse segmentation (Boguraev and Kennedy, 1997) have been reported. Radev et al. (Radev et al., 2004 ) developed an extractive multidocument summarizer, MEAD, which extracts a summary from multiple documents based on the document cluster centroid, position and firstsentence overlap. Recently, graph-based ranking methods, such as LexPageRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004 ), 1 http://haydn.isi.edu/ROUGE/ have been proposed for multi-document summarization. Similar to the original PageRank algorithm, these methods make use of similarity relationships between sentences and then rank sentences according to the \"votes\" or \"recommendations\" from their neighboring sentences. Lin and Hovy (2000) first introduced topic signatures which are topic relevant terms for summarization. Afterwards, this technique was successfully used in a number of summarization systems (Hickl et al., 2007, Gupta and Nenkova et al., 2007) . In order to improve sentence selection, we adopted the idea in a similar way to identify terms that tend to appear frequently in gene summaries and subsequently filter sentences that include none or few such terms. Compared with newswire document summarization, much less attention has been paid to summarizing MEDLINE documents for genic information. Ling et al. (Ling et al., 2006 and 2007) presented an automatic gene summary generation system that constructs a summary based on six aspects of a gene, such as gene products, mutant phenotype, etc. In their system, sentences were ranked according to a) the relevance to each category (namely the aspect), b) the relevance to the document where they are from; and c) the position where sentences are located. Although the system performed well on a small group of genes (10~20 genes) from Flybase, their method relied heavily on high-quality training data that is often hard to obtain in practice. Yang et al. reported a system (Yang et al., 2007 and 2009 ) that produces gene summaries by focusing on gene sets from microarray experiments. Their system first clustered gene set into functional related groups based on free text, Medical Subject Headings (MeSH ® ) and Gene Ontology (GO) features. Then, an extractive summary was generated for each gene following the Edmundson paradigm (Edmundson, 1969) (Hersh and Bhupatiraju, 2003) . Many teams approached the task as a sentence classification problem using GeneRIFs in the Entrez database as training data (Bhalotia et al., 2003; Jelier et al., 2003) . This task has also been approached as a single document summarization problem (Lu et al., 2006) . The gene summarization work presented here differs from the TREC task in that it deals with multiple documents. In contrast to the previously described systems for gene summarization, our approach has three novel features. First, we are able to summarize all aspects of gene-specific information as opposed to a limited number of predetermined aspects. Second, we exploit a lexical PageRank algorithm to establish similarity relationships between sentences. The importance of a sentence is based not only on the sentence itself, but also on its neighbors in a graph representation. Finally, we conducted an intrinsic evaluation on a large publicly available dataset. The gold standard assembled in this work makes it possible for comparisons between different gene summarization systems without human judgments. Method To determine if a sentence is extract worthy, we consider three different aspects: (1) the number of salient or informative words that are frequently used by human curators for writing gene summaries; (2) the relative importance of a sentence to be included in a gene summary; (3) the gene-specific information that is unique between different genes. Specifically, we look for signature terms in handwritten summaries for the first aspect. Ideally, computer generated summaries should resemble handwritten summaries. Thus the terms used by human curators should also occur frequently in automatically generated summaries. In this regard, we use a method similar to Lin and Hovy (2000) to identify signature terms and subsequently use them 2 http://ir.ohsu.edu/genomics/ to discard sentences that contain none or few such terms. For the second aspect, we adopt a lexical PageRank method to compute the sentence importance with a graph representation. For the last aspect, we treat each gene as having its own properties that distinguish it from others. To reflect such individual differences in the machinegenerated summaries, we exploit a gene's GO annotations as a surrogate for its unique properties and look for their occurrence in abstract sentences. Our gene summarization system consists of three components: a preprocessing module, a sentence ranking module, and a redundancy removal and summary generation module. Given a target gene, the preprocessing module retrieves corresponding MEDLINE abstracts and GO terms according to the gene2pubmed and gene2go data provided by Entrez Gene. Then the abstracts are split into sentences by the MEDLINE sentence splitter in the LingPipe 3 toolkit. The sentence ranking module takes these as input and first filters out some non-informative sentences. The remaining sentences are then scored according to a linear combination of the PageRank score and GO relevance score. Finally, a gene summary is generated after redundant sentences are removed. The system is illustrated in Figure 1 and is described in more detail in the following sections. Signature Terms Extraction There are signature terms for different topic texts (Lin and Hovy, 2000) . For example, terms such as eat, menu and fork that occur frequently in a corpus may signify that the corpus is likely to be We use the Pearson's chi-square test (Manning and Schütze, 1999) to extract topic signature terms from a set of handwritten summaries by comparing the occurrence of terms in the handwritten summaries with that of randomly selected MEDLINE abstracts. Let R denote the set of handwritten summaries and R denote the set of randomly selected abstracts from MEDLINE. The null hypothesis and alternative hypothesis are as follows: 0 H : ( | ) ( | ) i i P t R p P t R = = 1 1 2 H : ( | ) ( | ) i i P t R p p P t R = ≠ = The null hypothesis says that the term i t appears in R and in R with an equal probability and i t is independent from R . In contrast, the alternative hypothesis says that the term i t is correlated with R . We construct the following 2-by-2 contingency ij ij i j ij O E X E = − = ∑ where ij O is the observed frequency and ij E is the expected frequency. In our experiments, the significance level is set to 0.001, thus the corresponding chi-square value is 10.83. Lexical PageRank Scoring The lexical PageRank algorithm makes use of the similarity between sentences and ranks them by how similar a sentence is to all other sentences. It originates from the original PageRank algorithm (Page et al., 1998) that is based on the following two hypotheses: (1) A web page is important if it is linked by many other pages. (2) A web page is important if it is linked by important pages. The algorithm views the entire internet as a large graph in which a web page is a vertex and a directed edge is connected according to the linkage. The salience of a vertex can be computed by a random walk on the graph. Such graph-based methods have been widely adapted to such Natural Language Processing (NLP) problems as text summarization and word sense disambiguation. The advantage of such graph-based methods is obvious: the importance of a vertex is not only decided by itself, but also by its neighbors in a graph representation. The random walk on a graph can imply more global dependence than other methods. Our PageRank scoring method consists of two steps: constructing the sentence graph and computing the salience score for each vertex of the graph. Let { |1 } i S s i N = ≤ ≤ be the sentence collection containing all the sentences to be summarized. According to the vector space model (Salton et al., 1975) Taking each sentence as a vertex, and the similarity score as the weight of the edge between two sentences, a sentence graph is constructed. The graph is fully connected and undirected because the similarity score is symmetric. The sentence graph can be modeled by an adjacency matrix M , in which each element corresponds to the weight of an edge in the graph. Thus [ ] ij N N M × = M is defined as: , || || || || 0, i j i j ij s s if i j s s M otherwise ⋅ ⎧ ≠ ⎪ ⋅ = ⎨ ⎪ ⎩ We normalize the row sum of matrix M in order to assure it is a stochastic matrix such that the PageRank iteration algorithm is applicable. The normalized matrix is: 1 1 , 0 0, N N ij ij ij j j ij M M if M M otherwise = = ⎧ ≠ ⎪ = ⎨ ⎪ ⎩ ∑ ∑ . Using the normalized adjacency matrix, the salience score of a sentence i s is computed in an iterative manner: 1 (1 ) ( ) ( ) N i j j i j d score s d score s M N = − = ⋅ ⋅ + ∑ where d is a damping factor that is typically between 0.8 and 0.9 (Page et al., 1998) . If we use a column vector p to denote the salience scores of all the sentences in S , the above equation can be written in a matrix form as follows: [ ( 1 ) ] T p d d p = ⋅ + − ⋅ ⋅ M U where U is a square matrix with all elements being equal to 1/ N . The component (1 ) d − ⋅U can be considered as a smoothing term which adds a small probability for a random walker to jump from the current vertex to any vertex in the graph. This guarantees that the stochastic transition matrix for iteration is irreducible and aperiodic. Therefore the iteration can converge to a stable state. In our implementation, the damping factor d is set to 0.85 as in the PageRank algorithm (Page et al., 1998) . The column vector p is initialized with random values between 0 and 1. After the algorithm converges, each component in the column vector p corresponds to the salience score of the corresponding sentence. This score is combined with the GO relevance score to rank sentences. GO Relevance Scoring Up to this point, our system considers only geneindependent features, in both sentence filtering and PageRank-based sentence scoring. These features are universal across different genes. However, each gene is unique because of its own functional and structural properties. Thus we seek to include gene-specific features in this next step. The GO annotations provide one kind of genespecific information and have been shown to be useful for selecting GeneRIF candidates (Lu et al., 2006) . A gene's GO annotations include descriptions in three aspects: molecular function; biological process; and cellular component. For example, the human gene AANAT (gene ID 15 in Entrez Gene) is annotated with the GO terms in Table 4 . GO ID GO term GO:0004059 aralkylamine N-acetyltransferase activity GO:0007623 circadian rhythm GO:0008152 metabolic process GO:0008415 acyltransferase activity GO:0016740 transferase activity Table 4 . GO terms for gene AANAT The GO relevance score is computed as follows: first, the GO terms and the sentences are both stemmed and stopwords are removed. For example, the GO terms in Table 4 are processed into a set of stemmed words: aralkylamin, N, acetyltransferas, activ, circadian, rhythm, metabol, process, acyltransferas and transferas. Second, the total number of occurrence of the GO terms appearing in a sentence is counted. Finally, the GO relevance score is computed as the ratio of the total occurrence to the sentence length. The entire process can be illustrated by the following pseudo codes: Redundancy Removal A good summary contains as much diverse information as possible for a gene, while with as little redundancy as possible. For many well-studied genes, there are thousands of relevant papers and much information is redundant. Hence it is necessary to remove redundant sentences before producing a final summary. We adopt the diversity penalty method (Zhang et al., 2005; Wan and Xiao, 2007) for redundancy removal. The idea is to penalize the candidate sentences according to their similarity to the ones already selected. The process is as follows: (1) Initialize two sets, A φ = , { | 1, 2,..., } i B s i K = = containing all the extracted sentences; (2) Sort the sentences in B by their scores in descending order; (3) Suppose i s is the top ranked sentence in B , move it from B to A . Then we penalize the remaining sentences in B as follows: For each sentence j s in B , j i ≠ sim s s is the similarity between i s and j s . (4) Repeat steps 2 and 3 until enough sentences have been selected. Results and Discussion Evaluation Metrics Unlike the newswire summarization, there are no gold-standard test collections available for evaluating gene summarization systems. The two previous studies mentioned in Section 2 both conducted extrinsic evaluations by asking human experts to rate system outputs. Although it is important to collect direct feedback from the users, involving human experts makes it difficult to compare different summarization systems and to conduct large-scale evaluations (both studies evaluated nothing but a small number of genes). In contrast, we evaluated our system intrinsically on a much larger dataset consisting of 7,294 human genes, each with a preexisting handwritten summary downloaded from the NCBI's FTP site 5 . The handwritten summaries were used as reference summaries (i.e. a gold standard) to compare with the automatically generated summaries. Although the length of reference summaries varies, the majority of these summaries contain 80 to 120 words. To produce a summary of similar length, we decided to select five sentences consisting of about 100 words. For the intrinsic evaluation of a large number of summaries, we made use of the ROUGE metrics that has been widely used in automatic evaluation of summarization systems (Lin and Hovy, 2003; Hickl et al., 2007) . It provides a set of evaluation metrics to measure the quality of a summary by counting overlapping units such as n-grams or word sequences between the generated summary and its reference summary. We computed three ROUGE measures for each summary, namely ROUGE-1 (unigram based), ROUGE-2 (bigram based) and ROUGE-SU4 (skip-bigram and unigram) (Lin and Hovy, 2003) . Among them, ROUGE-1 has been shown to agree most with human judgments (Lin and Hovy, 2003) . However, as biomedical concepts usually contain more than one word (e.g. transcription factor), ROUGE-2 and ROUGE-SU4 scores are also important for assessing gene summaries. Determining parameters for best performance The two important parameters in our system -the linear coefficient α for the combination of Page-Rank and GO scores and the diversity penalty degree factor ω in redundancy removal -are investigated in detail on a collection of 100 randomly selected genes. First, by setting α to values from 0 to 1 with an increment of 0.1 while holding ω steady at 0.7, we observed the highest ROUGE-1score when α was 0.8 (Figure 2 ). This suggests that the two scores (i.e. PageRank and GO score) complement to each other and that the PageRank score plays a more dominating role in the summed score. Next, we variedω gradually from 0 to 5 with an increment of 0.25 while holding α steady at 0.75.The highest ROUGE-1 score was achieved when ω was 1.3 (Figure 3 ). For ROURE-2, the best performance was obtained when α was 0.7 and ω was 0.5. In order to balance ROUGE-1 and ROUGE-2 scores, we set α to 0.75 and ω to 0.7 for the remaining experiments. Comparison with other methods Because there are no publicly available gene summarization systems, we compared our system with two baseline methods. The first is a well known publicly available summarizer -MEAD (Radev et al., 2004) . We adopted the latest version of MEAD 3.11 and used the default setting in MEAD that extracts sentences according to three features: centroid, position and length. As shown in Table 5 , our system significantly outperformed the two baseline systems in all three ROUGE measures. Furthermore, larger performance gains are observed in ROUGE-2 and ROUGE-SU4 than in ROUGE-1. This is because many background words (e.g. gene, protein and enzyme) also appeared frequently as unigrams in randomly selected summaries. In Figure 4 , we show that the majority of the summaries have a ROUGE-1 score greater than 0.4. Our further analysis revealed that almost half summaries with a low score (smaller than 0.3) either lacked sufficient relevant abstracts, or the reference summary was too short or too long. In either case, only few overlapping words can be found when comparing the generated gene summary with the reference. The statistics for low ROUGE-1 score are listed in Table 6 . We also note that almost half of the summaries that have low ROUGE-1 scores were due to other causes: mostly, machine generated summaries differ from human summaries in that they describe different functional aspects of the same gene product. Take the gene TOP2A (ID: 7153) for example. While both summaries (handwritten and machine generated) focus on its encoded protein DNA topoisomerase, the handwritten summary describes the chromosome location of the gene whereas our algorithm selects statements about its gene expression when treated with a chemotherapy agent. We plan to investigate such differences further in our future work. Results on various summary length Figure 5 shows the variations of ROUGE scores as the summary length increases. At all lengths and for both ROUGE-1 and ROUGE-2 measures, our proposed method performed better than the two baseline methods. By investigating the scores of different summary lengths, it can be seen that the advantage of our method is greater when the summary is short. This is of great importance for a summarization system as ordinary users typically prefer short content for summaries. In this paper we have presented a system for generating gene summaries by automatically finding extract-worthy sentences from the biomedical literature. By using the state-of-the-art summarization techniques and incorporating gene specific annotations, our system is able to generate gene summaries more accurately than the baseline methods. Note that we only evaluated our system for human genes in this work. More summaries are available for human genes than other organisms, but our method is organism-independent and can be applied to any other species. This research has implications for real-world applications such as assisting manual database curation or updating existing gene records. The ROUGE scores in our evaluation show comparable performance to those in the newswire summarization (Hickl et al., 2007) . Nonetheless, there are further steps necessary before making our system output readily usable by human curators. For instance, human curators are generally in favor of sentences presented in a coherent order. Thus, information-ordering algorithms in multi-document summarization need to be investigated. We also plan to study the guidelines and scope of the curation process, which may provide additional important heuristics to further refine our system output. ",
         "16527070",
         "01ad43b5665e8f56dae779d445ed5c67ea34790e",
         "17",
         "https://aclanthology.org/W09-1312",
         "Association for Computational Linguistics",
         "Boulder, Colorado",
         "2009",
         "June",
         "Proceedings of the {B}io{NLP} 2009 Workshop",
         "Jin, Feng  and\nHuang, Minlie  and\nLu, Zhiyong  and\nZhu, Xiaoyan",
         "Towards Automatic Generation of Gene Summary",
         "97--105",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "jin-etal-2009-towards",
         null,
         null
        ],
        [
         "16",
         "Y18-1058",
         "In this paper we proposed the use of effective features and transfer leaning to improve the accuracies of neural-network-based models for accurate semantic role labeling (SRL) of Japanese, which is an aggregated language. We first reveal that the final morphemes in each argument, which have not been discussed in previous work on English SRL are effective features in determining semantic role labels in Japanese. We then discuss the possibility of using large-scale training corpora annotated with different semantic labels from the target semantic labels by transfer learning on CNN, 3-LNN, and GRU models. The experimental results of Japanese SRL on the proposed models indicate that all of the neural-network-based models performed better with transfer learning as well as using the feature vectors of final moprhemes in each argument. 1 In CoNLL 2009 (Hajič et al., 2009), annotated corpora with semantic roles in multiple languages including Japanese were used; however, most semantic tags in the Japanese corpus were case-marker-based relations, which are different from semantic roles annotated in PropBank. 2",
         "In this paper we proposed the use of effective features and transfer leaning to improve the accuracies of neural-network-based models for accurate semantic role labeling (SRL) of Japanese, which is an aggregated language. We first reveal that the final morphemes in each argument, which have not been discussed in previous work on English SRL are effective features in determining semantic role labels in Japanese. We then discuss the possibility of using large-scale training corpora annotated with different semantic labels from the target semantic labels by transfer learning on CNN, 3-LNN, and GRU models. The experimental results of Japanese SRL on the proposed models indicate that all of the neural-network-based models performed better with transfer learning as well as using the feature vectors of final moprhemes in each argument. 1 In CoNLL 2009 (Hajič et al., 2009), annotated corpora with semantic roles in multiple languages including Japanese were used; however, most semantic tags in the Japanese corpus were case-marker-based relations, which are different from semantic roles annotated in PropBank. 2 Introduction Several studies on semantic role labeling (SRL) have been conducted, mainly for English texts with a wide coverage from revealing syntactic and grammatical features that impact SRL decisions (Gildea and Jurafsky, 2002) to end-to-end models without syntactic inputs. This is because current rich language resources are related to annotated corpora with semantic roles such as PropBank (Kingsbury et al., 2002) , FrameNet (Baker et al., 1998) , and shared tasks of CoNLL 2005 (Carreras and Màrquez, 2005) , 2009 (Hajič et al., 2009 ), and 2012 (Pradhan et al., 2012) . Instead of English SRL, most studies (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011; Hayashibe et al., 2011; Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) on SRL for Japanese texts, the grammar of which is quite different from English, have been focused on detecting three types of case-marker-based labels, i.e., nominative, accusative, and dative. This is because large-scale corpora annotated with these three labels have been developed (Kyoto Corpus (Kawahara et al., 2002) and NAIST Text Corpus (Iida et al., 2007) ) and widely used 1 . There are, however, annotated corpora with semantic tags corresponding to semantic role labels (EDR corpus 2 , BCCWJ-PT (Takeuchi et al., 2015) and Japanese FrameNet (Ohara et al., 2011) ) or semantic categories (GDA corpus 3 ) that contain semantic relations as cause and location. Thus, by constructing a Japanese SRL system using these resources, we can discuss the effective approaches and models for a situation without standard semantic-role-labeled corpora as well as effective features for Japanese SRL. We address the following two issues: what is an effective grammatical feature for Japanese SRL, and what is a model when there are no standard labeled corpora for Japanese SRL. We first argue that the last two morphemes of arguments have an effect on determining SRL. This is because Japanese grammar allows scrambling (Saito, 1989) ; thus, case markers and functional expressions located at the last part of arguments contribute to the selection of semantic relations. We then propose three neural-network-based models, i.e., a convolutional network (CNN), 3-layer neural-network (3-LNN) and GRU, by applying transfer learning for using different semantic-role-labeled corpora. With transfer learning, using annotated corpora with different tag sets from the target semantic role labels is expected. We conducted experiments on the Japanese SRL accuracy of the proposed models that involved using the BCCWJ Predicate-Thesaurus (BCCWJ-PT) corpus, which is annotated with dependencies for target verbs in BCCWJ (Maekawa et al., 2014) . The reasons of using BCCWJ-PT are 1) most of the semantic role labels correspond to the well-known categories of Agent, Theme, Goal, and Recipient, which are also used in other resources (e.g., Verb-Net and FrameNet); and 2) the semantic role labels and their verb senses (i.e., semantic frames) are defined with example sentences in the Predicate Thesaurus (PT), which are freely available on the Internet 4 . The experimental results indicate that the last two morphemes of arguments sufficiently boosted the SRL performance of our neural-network-based models as well as a baseline, i.e., an SVM-based model. We also reveal that transfer learning improved the accuracy of recognizing labels annotated in BCCWJ-PT using different semantic tags annotated in GDA. Characteristics of Japanese SRL There are two main characteristics of Japanese SRL, i.e., Japanese grammatical features and language resources for SRL. Japanese Grammatical Features for SRL In Japanese syntax, the case markers located in the final part of each phrase, play important roles in determining the semantic relations (i.e., semantic roles) to the predicate. Not only the case markers but also certain functional multi-morphemes, attached to the final part of each phrase have an effect on determining the semantic relations to the predicate. The example sentence in Figure 1 shows !\"#$%&\"'#(&)(*'$$% +&'($!,%(&)-.%/ 0#1!)$'(2! \"*!&3%1#(3!1%($! !\"#$%&\"'(&)*#+&$',$ -./0('*123$ 4\"54&.*67889:;*678< <(&$-./0('$%\"=$4\"54&.&#$#+&$',$!\"#$%&\"'(&) Fig. 1 : Example of functional multi-morphemes that the different functional morphemes ni-yotte, no-tame-ni, and de 5 can designate the same type of semantic relation, i.e., \"Cause\" to the verb kyanseru (cancel). Also, certain functional morphemes as well as case markers can provide different semantic relations depending on their contexts. Unfortunately, there is no standard dictionary of functional morphemes 6 nor morphological analyzers that can detect these functional morphemes 7 . Thus, we do not currently have language resources that can help determine the possible semantic relations that functional morphemes provide 8 . With this background, we take the last two morphemes in each argument as a simple grammatical feature to take into account the functional morphemes and case markers of arguments in Japanese, as in a previous study (Ishihara and Takeuchi, 2016) . Language Resources of Japanese SRL Several language resources have been constructed on internal semantic relations in predicates and their arguments for Japanese. The language resource EDR provides English and Japanese parsed corpora containing about 400,000 examples, which are annotated with semantic tags and original sense tags defined in the EDR dictionary. The semantic tags not only contain semantic role labels, such as Agent and Object, but also semantic relations 5 Most of the functional suffixes are written in Hiragana． 6 There was a study on the collection of Japanese suffixes (Matsuyoshi et al., 2007) ; however, there are still no registered functional morphemes such as to-issyoni (with). 7 For example, the Japanese morphological analyzer MeCab (http://taku910.github.io/mecab/) with the IPA dictionary does not extract the functional suffix no-tame-ni. 8 The functional morphemes are annotated in CoNLL2009 not as features but as semantic role tags (i.e., target of disambiguations). Thus, the semantic role tags in the Japanese corpus of CoNLL 2009 might be different from those for English SRL. between main and subordinate clauses, e.g., Cooccurrence and Sequence; however, these similar semantic role tags are not connected to predicate concepts, such as frames, as defined in PropBank or FrameNet. Thus, we do not use EDR as the target SRL corpus. Unlike the EDR corpus, the JFN (Ohara et al., 2003) was constructed in the same structure of English FrameNet, however, JFN is a closed corpus, and several essential information such as total amount of the annotated sentences and number of lexicons are not revealed. Thus, it does not seem to be easy to use the JFN corpus. The GDA corpus has 37,000 sentences that are annotated with 100 semantic tags that contain not only semantic role tags but discourse-related, syntactic, and grammatical tags. This can be regarded as a sufficient amount; however, the semantic role tags are not related to frames for predicates nor are lexical frames provided. Instead of the above semantic role-related corpora in Japanese, BCCWJ-PT contains 64 semantic role labels that are based on the analysis of semantic relations between predicates and arguments for about 5,000 sentences. The semantic role labels are also connected to frames defined in the PT, which is freely published on the Web. This framework of tags and frames is the same as PropBank and FrameNet. We therefore use BCCWJ-PT as the target corpus of Japanese SRL. Task Definition To focus on the impact of how features and learning models can recognize semantic roles in Japanese texts, we apply the simple SRL task, i.e., SRL systems identify the semantic role labels for a pair of an argument and head verb that are correctly extracted in the preprocessing step. Figure 2 shows an example of a partial sentence annotated with semantic roles in BCCWJ-PT. The BCCWJ-PT corpus has not only semantic role labels but also annotated tags of morphemes, the target predicate, and its arguments; thus, we convert the tagged texts into the target data, which are separated into target semantic role labels and their features. Figure 3 shows an example of the target data converted from the annotated data in Figure 2 . In the example features in Figure 3 , basic forms as well as surface forms of the verbs are stored for normalization as the predicate features. The basic forms and/or the surface forms are selected as the predicate features depending on the models of neural networks. Proposed Models and Approaches Previous work has shown that neural-networkbased approaches models are powerful for English SRL (He et al., 2017) as well as Japanese case-marker disambiguation and anaphora detection (Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) ; however, neural-network-based models have various tuning parameters such as network structure, hyper parameters, and several methods of avoiding over fitting. Therefore, it is not easy to determine the cause-and-effect relations between models and the final results. Thus, we used simple neuralnetwork architectures to focus on clarifying what input features are effective, which learning models are powerful, and how different labeled corpora can be effective for the target label set. As described above, several corpora annotated with different sets of semantic role labels for Japanese have been constructed, but the amount of labeled examples for each annotated corpus is limited. Thus, we applied transfer learning, which uses corpora annotated with different semantic labels from the target labels. Because of the flexibility of the neural-network structures, transfer learning can be easily implemented by changing the network structure. In the following sections, we give details of the three proposed models, their input features, and the methods of transfer learning. 3-LNN Our 3-LNN model is applied to determine semantic role labels by varying the following different input feature vectors to determine the effectiveness of the input feature vector. (1) Bag-of-Words (BOW) for morphemes in an argument and its head verb (Asahara, 2018) that are made from 1 billion corpus of Japanese using fasttext 9 . These vectors are expected to determine the tendency between semantic role tags and the combination of a content morpheme and verb. (3) Feature vector (2) with a BOW of the last two morphemes in an argument. This added feature is expected to capture functional suffixes in arguments, as described in Section 2.1. For example, feature vector (1) for the first data in Figure 3 is a BOW vector of granturisumo, 4, o, and kau. In feature vector (2), the skip-gram vectors of the content morpheme-head, i.e., 4 and its verb kau, are added to feature vector (1). In feature vector (3), a BOW vector of the two morphemes 4 and o is added to feature vector (2). Regarding the network structure and tuning methods, ReLU (Nair and Hinton, 2010) is applied to the non-linear function of the intermediate units, and softmax is used as the function of the units at the final layer. To obtain better accuracy for test data, we applied a dropout method for the intermediate units with 50%; and used Adam as an optimization method with the recommended setting 10 . CNN Instead of using BOW, we take certain sequential characteristics in verb-argument features with a simple CNN. Our CNN is a simple structure that consists of an input layer, convolution layer, pooling layer, and output layer (Figure 4 ). The input of the CNN is a sequence of feature morphemes in verb-argument order. The nwjc2vec was used to convert the input morphemes to the skip-gram vectors with 200 dimensions. We define the three types of filters that take into account three, four, and five successive morphemes. The number of filters for each type is 128.  After the convolution layer, a pooling layer with max pooling is applied, then a fully connected layer is applied to the connection to the final output layer. In addition to the simple network, we also use a variation model in which the input features defined in the 3-LNN model are added into the previous final layer, as mentioned in Section 6. !\"#$%%&'((!\"#)&*\"+ !!! \"#'*&\")((!\"#$,$+-( !!! !\"#'$%,$((!\"#$\"$-* !!! \"#'***+((!\"#--*&' !!! \"#)-+-$((!\"#$&,&% !!! \"#$%%'&((!\"#)%)' !!! \"#''$**((!\"#)+-+$ !!! ./ GRU To focus on the sequential feature of arguments and verbs, we use our GRU model, which is a type of recurrent neural network. Figure 5 shows the structure of how we apply our GRU model for SRL. Input feature morphemes are converted to dense vectors with nwjc2vec, the final GRU state is applied to a dense layer, then the final output is obtained. !\"#$%&\"'(&)* ! ! ! !\"# !\"# !\"# ! ! ! $%& To observe how the order and inflection of input morphemes can contribute to the accuracy of identifying semantic role labels, four types of input features are defined in Table 1 . The slash in the examples denotes a delimiter of morphemes. The GRU model takes a verb with the final position in v1, but a verb or verbs at the first position in the other features. Feature vectors v3 and v4 have a surface form of the verb. Transfer Learning As described in Section 2.2, the amount of the target corpus, BCCWJ-PT, is limited; thus, we apply a method of using other annotated corpora whose semantic labels are even different from that of the target corpus, which is called transfer learning. The basic procedure of transfer learning is as follows: 1) a neural network is trained using an annotated corpus of different semantic tags from the target corpus; 2) the units at the final layer of the neural network are replaced with new units for the target semantic tags; and 3) all the weights in the neural work are trained using the target corpus 11 . Transfer learning is applied to the three proposed neural-network-based models. Baseline Model We apply an SVM-based model that has a linear kernel to the SRL task as a baseline model. For multi-class categorization, we use the one-versusrest method. The input feature is the BOW, which is the same as case (1) used in the 3-LNN model. Experiments on Japanese SRL Accuracy Experimental Setup The BCCWJ-PT corpus was the target corpus containing 5069 sentences, 64 semantic role labels for 548 verb types 12 . We extracted 10,390 instances of the target data whose format is shown in Figure 3 , i.e., combinations of an argument and its verb. The target data were divided into the three parts: training (65%), development (5%), and test (30%). The GDA corpus containing 100 semantic tags, which are different from those in BCCWJ-PT, was selected as the corpus for transfer learning. We extracted 82,892 instances whose format is the same as that in Figure 3 . The instances of GDA were divided into development (85%), training (5%), and test (10%) data. Table 2 shows the details of the data sets used in the experiments. The top five most frequent semantic role labels contained in the BCCWJ-PT data are listed in Table 3 . The semantic role labels of Theme and Agent were most frequent, which indicates the same tendency shown in PropBank (Palmer et al., 2005) , which is an English semantic-role-labeled corpus. We applied the proposed models to the training data for training parameters and applied them to the test data for evaluating their SRL accuracy. The SRL accuracy of the SVM model was evaluated with 5-fold cross validation of all target data. This indicates that the SVM model is advantageous compared to our neural-network-based models, because the amount of training data is larger than that of our neural-network-based models. The SRL accuracies of our models and the SVM model were evaluated based on the instances of the test data using the following accuracy formula: accuracy = #instances correctly estimated labels #total instances (1) In the SVM model, evaluation on test data in one hold was conducted with the above accuracy formula. The final accuracy of the SVM model was then an average evaluated based on all the test data in 5-fold cross validation. The mini-batch size of all three proposed neuralnetwork-based models was set to 100, and the number of training iterations, i.e., epochs, was determined using the accuracy of the development data. To avoid overfitting to the training data, we used 20 epochs on the BCCWJ-PT data because almost all the proposed models converged in 20 epochs. Experimental Results Table 4 lists that experimental results of the SRL accuracies of the proposed and baseline models using only the BCCWJ-PT data. All three neural-network-based models outperformed the SVM model regarding SLR accuracy. When we look at the effectiveness of the features in the SVM and 3-LNN models, the skip-gram vectors significantly improved the accuracies of the both models. Adding a BOW of the last two morphemes in each argument further improved their accuracies. For the GRU results, v2 showed the best performance among the other feature sets. This indicates that the verb should come first in the input sequence by comparing to v1; and the base form of the verbs must be more effective than the inflected form compared to the results of v3 and v4. The SRL accuracy of the GRU model, however, was inferior to that of the 3-LNN model. This indicates that the GRU model currently does not seem to fully use contextual information. The best SRL accuracy of all the models was that of our CNN model. Compared to the 3-LNN model, the convolution and pooling structure contributes to improve 0.015 points. The CNN model with only using the base feature vector conv did not perform as well as the 3-LNN model with the BOW + skip + two feature vector. This indicates that the manually designed feature vectors, i.e., skip + two with BOW, work well to obtain the characteristics of the semantic role labels. Table 5 shows the results of incorporating transfer learning, i.e., using the GDA data for training the initial values of the weights. All three proposed neural-network-based models improved their accuracies with transfer learning, but the increase in the accuracies differed depending on the model. The 3-LNN model had the most improvement with transfer learning and showed the best accuracy among all the models. The CNN model improved in accuracy, but was not as effective as the 3-LNN model. The accuracy of the GRU model also increased 0.011 points within the maximum score, but the best accuracy of the GRU model was lower than those of the other models. According to the effects of the training epochs in the GDA data, too many training epochs for the GDA data will decrease the SLR accuracy with BCCWJ-PT for all three models. This indicates that neural-network-based models would have caused overfitting to the GDA data if the models were trained with too many iterations. Thus, we need to stop the training in GDA data with a small number of iterations. Table 5 shows that the best training epoch for the 3-LNN and CNN models is only ten iterations, which would be the best for obtaining the initial weights towards learning the final BCCWJ-PT data. Discussions The results of transfer learning in Table 5 indicate that transfer learning contributed to the improvement in the SRL accuracies of our neural-networkbased models, but the best accuracy score of 0.67 is not so different from 0.665 with the CNN model without transfer learning even though the GDA data are about ten times larger than the BCCWJ-PT data. The role of transfer learning is to obtain better initial weights in neural-network-based models than randomized initial weights. In transfer learning, all the units in the final layer for GDA tags are discarded; however, some of the tags are almost the same as the semantic role tags defined in BCCWJ-PT, such as Agent, Theme, and Goal. Therefore, we must consider how we can use the similar semantic tags in transfer learning. As described in Section 2.1, the last two morphemes in each argument are defined to capture functional suffixes that have an effect on determining its semantic role of the argument. The experimental results listed in Table 4 reveal that the multi-word functional suffix i.e., two feature vector, improves the accuracy of the GRU model as well as those of the SVM and 3-LNN models. The three feature vectors v2, v3, v4 outperformed v1 in terms of SRL accuracy. Since v1 is only the case in which a verb comes at the end, the other case markers come last. In the GRU model, the final layer of the GRU loated at the final positions of a time sequence determines the label. Therefore, the last two morphemes located at the final positions of a time sequence are naturally taken into account in v2, v3, and v4. The accuracies of all the models show that the last morphemes have a positive effect on determining the semantic role labels in Japanese. Related Work Several SRL studies have been conducted mainly in English because of existing high-quality language resources such as FrameNet and PropBank as well as shared tasks of semantic role labels such as in CoNLL-2005 (Carreras and Màrquez, 2005) , 2009 (Hajič et al., 2009 ) and 2012 (Pradhan et al., 2012) . In the early stage of SRL investigation, statistical modeling and effective features for SRL have been studied. Gildea and Jurafsky (2002) revealed that several syntactic features, such as parse tree path, phrase type, and voice, can improve the accuracy of a statistical learning model. More detailed features were studied by Surdeanu et al. (2003) and Xue and Palmer (2004) . Toutanova et al. (2008) showed effective combinations of statistical joint models with rich features. Syntactic features are powerful; however, parse errors will decrease the accuracies of SRL systems. Thus Zhou and Xu (2015) proposed an end-toend SRL model using bi-directional long shortterm memory (LSTM) without any syntactic features. Roth and Lapata (2016) used dependency information on LSTM. The dependency path is convenient, but He et al. ( 2017 ) revealed that higher accuracies on neural-network-based SRL models could be obtained if correct parsed information is available. Most studies on Japanese SRL (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011; Hayashibe et al., 2011; Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) have been focused on recognizing three types of case-marker-based semantic roles with anoaphra resolutions. Taira et al. (2008) have shown that the detailed noun categories of nominals in arguments improve the accuracy of statistical models for recognizing the three case-markers. Imamura et al. (2009) proposed effective grammatical features such as dependency path, phrase positions, and several detailed characteristics. Sassano and Kurohashi 2011 and Hnagyo et al. 2013 proposed models to use large-scale case frames to provide selectional preference between a head noun in an argument and its predicate. Ouchi et al. (2015) , Shibata et al. (2016) , Ouchi et al. (2017) and Matsubayashi and Inui (2018) proposed neural-network-based models. These studies are focused on anaphora resolution, i.e., detecting arguments for a predicate without dependency relations and recognizing their semantic roles. Thus, these studies discussed how to incorporate the effectiveness of multiple predicates. For SRL in Japanese, Ishihara and Takeuchi (2015) revealed that the last morphemes in an argument are effective on a linear-chain CRFs for determining 64 semantic roles for BCCWJ-PT. Thus, the effective grammatical features as well as approaches on neural-network-based models for Japanese SRL are required to be studied. Conclusion We propose three neural-network-based models and described the effective features and methods for Japanese SRL. We revealed that the last two morphemes in an argument, the dense morpheme vector concatenated with a head noun morpheme and its predicate, and bag-of-morphemes in an argument, are effective for our 3-LNN and CNN models. We conducted experiments on Japanese SRL with BCCWJ-PT containing 64 semantic roles, which was different from most previous studies, which focused on 3 semantic roles. We applied transfer learning using GDA, which has different semantic role tags from BCCWJ-PT. After pre-training the weights using the GDA data, the neural network models were trained on the BCCWJ-PT data. The experimental results indicate that transfer learning improved the accuracies of all three proposed neural network models compared to the cases without transfer learning. We are planning more detailed analyses of the combinations of features and neural network models on BCCWJ-PT.",
         "201642107",
         "f0b536df92c287b556a482f1480e52764bf1202c",
         "1",
         "https://aclanthology.org/Y18-1058",
         "Association for Computational Linguistics",
         "Hong Kong",
         "2018",
         "1{--}3 December",
         "Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",
         "Okamura, Takuya  and\nTakeuchi, Koichi  and\nIshihara, Yasuhiro  and\nTaguchi, Masahiro  and\nInada, Yoshihiko  and\nIizuka, Masaya  and\nAbo, Tatsuhiko  and\nUeda, Hitoshi",
         "Improving {J}apanese semantic-role-labeling performance with transfer learning as case for limited resources of tagged corpora on aggregated language",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "okamura-etal-2018-improving",
         null,
         null
        ],
        [
         "17",
         "R13-1050",
         "We are interested in a graph-based Knowledge Representation formalism that would allow for the representation, manipulation, query, and reasoning over dependency structures, and linguistic knowledge of the Explanatory and Combinatorial Dictionary in the Meaning-Text Theory framework. Neither the semantic web formalisms nor the conceptual graphs appear to be suitable for this task, and this led to the introduction of the new Unit Graphs framework. This paper first introduces the foundational concepts of this framework: Unit Graphs are defined over a support that contains: i) a hierarchy of unit types which is strongly driven by their actantial structure, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. Then, this paper provides all of these objects with a model semantics that enables to define the notion of semantic consequence between Unit Graphs.",
         "We are interested in a graph-based Knowledge Representation formalism that would allow for the representation, manipulation, query, and reasoning over dependency structures, and linguistic knowledge of the Explanatory and Combinatorial Dictionary in the Meaning-Text Theory framework. Neither the semantic web formalisms nor the conceptual graphs appear to be suitable for this task, and this led to the introduction of the new Unit Graphs framework. This paper first introduces the foundational concepts of this framework: Unit Graphs are defined over a support that contains: i) a hierarchy of unit types which is strongly driven by their actantial structure, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. Then, this paper provides all of these objects with a model semantics that enables to define the notion of semantic consequence between Unit Graphs. Introduction We are interested in the ability to reason over dependency structures and linguistic knowledge of the Explanatory and Combinatorial Dictionary (ECD), which is the lexicon at the core of the Meaning-Text Theory (MTT) (Mel'čuk, 2006) . Some formalisation works have been led on the ECD. For instance Kahane and Polguère, 2001) proposed a formalization of Lexical Functions, and the Definiens project (Barque and Polguère, 2008; Barque et al., 2010) aims at formalizing lexicographic definitions with genus and specific differences for the TLFi 1 . Adding to these formalization works, the goal of the Unit Graphs formalism is to propose a formalization from a knowledge engineering perspective, compatible with standard Knowledge Representation (KR) formalisms. The term formalization here means not only make nonambiguous, but also make operational, i.e., such that it supports logical operations (e.g., knowledge manipulation, query, reasoning). We thus adopt a knowledge engineering approach applied to the domain of the MTT. At first sight, two existing KR formalisms seemed interesting for representing dependency structures: semantic web formalisms (RDF/S, OWL, SPARQL), and Conceptual Graphs (CGs) (Sowa, 1984; Chein and Mugnier, 2008) . Both formalisms are based on directed labelled graph structures, and some research has been done towards using them to represent dependency structures and knowledge of the lexicon (OWL in (Lefranc ¸ois and Gandon, 2011; Boguslavsky, 2011) , CGs at the conceptual level in (Bohnet and Wanner, 2010) ). Yet Lefranc ¸ois, 2013) showed that neither of these KR formalisms can represent linguistic predicates. As the CG formalism is the closest to the semantic networks, the following choice has been made (Lefranc ¸ois, 2013): Modify the CGs formalism basis, and define transformations to the RDF syntax for sharing, and querying knowledge. As we are to represents linguistic units of different nature (e.g., semantic units, lexical units, grammatical units, words), term unit has been chosen to be used in a generic manner, and the result of this adaptation is thus the Unit Graphs (UGs) framework. The valency-based predicates are represented by unit types, and are described in a structure called the unit types hierarchy. Unit types specify through actant slots and signatures how their instances (i.e., units) may be linked to other units in a UG. Unit Graphs are then defined over a support that contains: i) a hierarchy of unit types which is strongly driven by their actantial structure, ii) a hierarchy of circumstantial sym-bols, and iii) a set of unit identifiers. Apart from giving an overview foundational concepts of the UGs framework, the main goal of this paper is to answer the following research question: What semantics can be attributed to UGs, and how can we define the entailment problem for UGs ? The rest of this paper is organized as follows. Section 2 overviews the UGs framework: the hierarchy of unit types ( §2.1), the hierarchy of circumstantial symbols ( §2.2), and the Unit Graphs ( §2.3). Then, section 3 provides all of these mathematical objects with a model, and finally the notion of semantic consequence between UGs is introduced ( §3.4). 2 Background: overview of the Unit Graphs Framework For a specific Lexical Unit L, (Mel'čuk, 2004, p.5) distinguishes considering L in language (i.e., in the lexicon), or in speech (i.e., in an utterance). KR formalisms and the UGs formalism also make this distinction using types. In this paper and in the UGs formalism, there is thus a clear distinction between units (e.g., semantic unit, lexical unit), which will be represented in the UGs, and their types (e.g., semantic unit type, lexical unit type), which are roughly classes of units for which specific features are shared. It is those types that specify through actant slots and signatures how their instances (i.e., units) are to be linked to other units in a UG. Hierarchy of Unit Types Unit types and their actantial structure are described in a structure called hierarchy, that specifies how units may, must, or must not be interlinked in a UG. Definition 2.1. A hierarchy of unit types is denoted T and is defined by a tuple: T def = (T D , S T , γ γ γ, γ γ γ 1 , γ γ γ 0 , C A , ⊥ A , {ς ς ς t } t∈T ) This structure has been thoroughly described in (Lefranc ¸ois and Gandon, 2013a; Lefranc ¸ois, 2013) . Let us overview its components. T D is a set of declared Primitive Unit Types (PUTs). This set is partitioned into linguistic PUTs of different nature (e.g., deep semantic, semantic, lexical). S T is a set of Actant Symbols (ASymbols). γ γ γ (resp1. γ γ γ 1 , resp2. γ γ γ 0 ) assigns to every s ∈ S T its radix 2 (resp1. obligat 3 , resp2. prohibet 4 ) unit type γ γ γ(s) (resp1. γ γ γ 1 (s), resp2. γ γ γ 0 (s)) that introduces (resp1. makes obligatory, resp2. makes prohibited) an Actant Slot (ASlot) of symbol s. The set of PUTs is denoted T and defined as the disjoint union of T D , the ranges of γ γ γ, γ γ γ 1 and γ γ γ 0 , plus the prime universal PUT and the prime absurd PUT ⊥ (eq. 1). T def = T D • ∪ γ γ γ(S T ) • ∪ γ γ γ 1 (S T ) • ∪ γ γ γ 0 (S T ) • ∪{⊥, } (1) T is then pre-ordered by a relation which is computed from the set C A ⊆ T 2 of asserted PUTs comparisons. t 1 t 2 models the fact that the PUT t 1 is more specific than the PUT t 2 . Then a unit type has a set (that may be empty) of ASlots, whose symbols are chosen in the set S T . Moreover, ASlots may be obligatory, prohibited, or optional. The set of ASlots (resp1. obligatory ASlots, resp2. prohibited ASlots, resp3. optional ASlots) of a PUT is thus defined as the set of their symbols α α α(t) ⊆ S T (resp1. α α α 1 (t), resp2. α α α 0 (t), resp3. α α α ? (t)). The set of ASlots (resp1. obligatory ASlots, resp2. prohibited ASlots) of a PUT t ∈ T is defined as the set of ASymbol whose radix (resp1. obligat, resp2. prohibet) is more general or equivalent to t, and the set of optional ASlots of a PUT t is the set of ASlots that are neither obligatory nor prohibited. The number of ASlots of a PUT is denoted its valency. {ς ς ς t } t∈T , the set of signatures of PUTs, is a set of functions. For all PUT t, ς ς ς t is a function that associates to every ASlot s of t a set of PUT ς ς ς t (s) that characterises the type of the unit that fills this slot. Signatures participate in the specialization of the actantial structure of PUTs, which means that if t 1 t 2 and s is a common ASlot of t 1 and t 2 , the signature of t 1 for s must be more specific or equivalent than that of t 2 . Hence t 1 t 2 implies that the actancial structure of t 1 is more specific than the actantial structure of t 2 . Now a unit type may consist of several conjoint PUTs. We introduce the set T ∩ of possible Conjunctive Unit Types (CUTs) over T as the power-set 5 of T. The set ⊥ A is the set of declared absurd CUTs that can not be instantiated. The definition of the actancial structure of PUTs is naturally extended to CUTs as follows: α α α ∩ (t ∩ ) def = t∈t ∩α α α(t) (2) α α α ∩ 1 (t ∩ ) def = t∈t ∩α α α 1 (t) (3) α α α ∩ 0 (t ∩ ) def = t∈t ∩α α α 0 (t) (4) α α α ∩ ? (t ∩ ) def = α α α ∩ (t ∩ ) − α α α ∩ 1 (t ∩ ) − α α α ∩ 0 (t ∩ ) (5) ς ς ς ∩ t ∩ (s) def = t∈t ∩ |s∈α α α(t) ς ς ς t (s) (6) Finally the pre-order over T is extended to a pre-order ∩ over T ∩ as defined by Lefranc ¸ois and Gandon, 2013a). Lefranc ¸ois and Gandon, 2013b) proved that in the hierarchy of unit types, if t ∩ 1 ∩ t ∩ 2 then the actantial structure of t ∩ 1 is more specific than that of t ∩ 2 , except for some degenerated cases. Thus as one goes down the hierarchy of unit types, an ASlot with symbol s is introduced by the radix {γ γ γ(s)} and first defines an optional ASlot for any unit type t ∩ more specific than {γ γ γ(s)}, as long as t ∩ is not more specific than the obligat {γ γ γ 1 (s)} (resp. the prohibet {γ γ γ 0 (s)}) of s. If that happens, the ASlot becomes obligatory (resp. prohibited). Moreover, the signature of an ASlot may only become more specific. Hierarchy of Circumstantial Symbols Unit types specify how unit nodes are linked to other unit nodes in the UGs. As for any slot in a predicate, one ASlot of a unit may be filled by only one unit at a time. Now, one may also encounter dependencies of another type in some dependency structures: circumstantial dependencies (Mel 'čuk, 2004) . Circumstantial relations are considered of type instance-instance contrary to actantial relations. Example of such relations are the deep syntactic representation relations ATTR, COORD, AP-PEND of the MTT, but we may also define other such relations to represent the link between a lexical unit and its sense for instance. We thus introduce a finite set of so-called Circumstantial Symbols (CSymbols) S C which is a set of binary relation symbols. In order to classify S C in sets and subsets, we introduce a partial order C over S C . C is the reflexo-transitive closure of a set of asserted comparisons C S C ⊆ T 2 . 5 The powerset of X is the set of all subsets of X: 2 X Finally, to each CSymbol is assigned a signature that specifies the type of units that are linked through a relation having this symbol. The set of signatures of CSymbol {σ σ σ s } s∈S C is a set of couples of CUTs: {(domain(s), range(s))} s∈S C . As one goes down the hierarchy of PUTs, we impose that the signature of a CSymbol may only become more specific (eq. 7). s 1 s 2 ⇒ σ σ σ(s 1 ) ∩ σ σ σ(s 2 ) (7) We may hence introduce the hierarchy of CSymbols: Definition 2.2. The hierarchy of CSymbols, denoted C def = (S C , C S C , T , {σ σ σ s } s∈S C ), Definition of Unit Graphs (UGs) The UGs represent different types of dependency structures. Parallel with the Conceptual Graphs, UGs are defined over a so-called support. Definition 2.3. A UGs support is denoted S def = (T , C, M) and is composed of a hierarchy of unit types T , a hierarchy of circumstantial symbols C, and a set of unit identifiers M. Every element of M identifies a specific unit, but multiple elements of M may identify the same unit. In a UG, unit nodes that are typed and marked are interlinked by dependency relations that are either actantial or circumstantial. Definition 2.4. A UG G defined over a UGsupport S is a tuple denoted G def = (U, l, A, C, Eq) where U is the set of unit nodes, l is a labelling mapping over U , A and C are respectively actantial and circumstantial triples, and Eq is a set of asserted unit node equivalences. Let us detail the components of G. U is the set of unit nodes. Every unit node represents a specific unit, but multiple unit nodes may represent the same unit. Unit nodes are typed and marked so as to respectively specify what CUT they have and what unit they represent. The marker of a unit node is a set of unit identifiers for mathematical reasons. The set of unit node markers is denoted M ∩ and is the powerset 5 of M. If a unit node is marked by ∅, it is said to be generic, and the represented unit is unknown. On the other hand, if a unit node is marked {m 1 , m 2 }, then the unit identifiers m 1 and m 2 actually identify the same unit. l is thus a labelling mapping over U that assigns to each unit node u ∈ U a couple l(u) = (t ∩ , m ∩ ) ∈ T ∩ × M ∩ of a CUT and a unit node marker. We denote t ∩ = type(u) and m ∩ = marker(u). A is the set of actantial triples (u, s, v) ∈ U × S T × U . For all a = (u, s, v) ∈ A, the unit represented by v fills the ASlot s of the unit represented by u. We denote u = governor(a), s = symbol(a) and v = actant(a). We also denote arc(a) = (u, v). C is the set of circumstantial triples (u, s, v) ∈ U × S C × U . For all c = (u, s, v) ∈ C, the unit represented by u governs the unit represented by v with respect to s. We denote u = governor(c), s = symbol(c) and v = circumstantial(c). We also denote arc(c) = (u, v). Eq ⊆ U 2 is the set of so-called asserted unit node equivalences. For all (u 1 , u 2 ) ∈ U 2 , (u 1 , u 2 ) ∈ Eq means that u 1 and u 2 represent the same unit. The Eq relation is not an equivalence relation over unit nodes 6 . We thus distinguish explicit and implicit knowledge. UGs so defined are the core dependency structures of the UGs mathematical framework. On top of these basic structures, one may define for instance rules and lexicographic definitions. Due to space limitation we will not introduce such advanced aspects of the UGs formalism, and we will provide a model to UGs defined over a support that does not contain definitions of PUTs. 3 Model Semantic for UGs Model of a Support In this section we will provide the UGs framework with a model semantic based on a relational algebra. Let us first introduce the definition of the model of a support. Definition 3.1 (Model of a support). Let S = (T , C, M) be a support. A model of S is a couple M = (D, δ). D is a set called the domain of M that contains a special element denoted • that represents nothing, plus at least one other element. δ is denoted the interpretation function and must be such that: • M is a model of T ; • M is a model of C; 6 An equivalent relation is a reflexive, symmetric, and transitive relation. • ∀m ∈ M, δ(m) ∈ D \\ •; This definition requires the notion of model of a unit types hierarchy, and model of a CSymbols hierarchy. We will sequentially introduce these notions in the following sections. Model of a Hierarchy of Unit Types The interpretation function δ associates with any PUT t ∈ T a relation δ({t}) of arity 1 + valency(t) with the following set of attributes (eq. 8): • a primary attribute denoted 0 (0 / ∈ S T ) that provides {t} with the semantics of a class; • an attribute for each of its ASlot in α α α(t) that provides {t} with the dual semantics of a relation. ∀t ∈ T, δ({t}) ⊆ D 1+valency(t) with attributes {0} ∪ α α α(t) (8) Every tuple r of δ({t}) can be identified to a mapping, still denoted r, from the attribute set {0} ∪ α α α(t) to the universe D. r describes how a unit of type {t} is linked to its actants. r(0) is the unit itself, and for all s ∈ α α α(t), r(s) is the unit that fills ASlot s of r(0). If r(s) = •, then there is no unit that fills ASlot s of r(0). A given unit may be described at most once in δ({t}), so 0 is a unique key in the interpretation of every PUT: ∀t ∈ T, ∀r 1 , r 2 ∈ δ({t}), r 1 (0) = r 2 (0) ⇒ r 1 = r 2 (9) must be the type of every unit, except for the special nothing element •, and ⊥ must be the type of no unit. As the projection π 0 δ({t}) on the main attribute 0 represents the set of units having type {t}, equations 10 and 11 model these restrictions. π 0 δ({ }) = D \\ •; (10) δ({⊥}) = ∅ (11) The ASlot s of the obligat γ γ γ 1 (s) must be filled by some unit, but no unit may fill ASlot s of the prohibet γ γ γ 0 (s). As for every s ∈ α α α(t), the projection π s δ({t}) represents the set of units that fill the ASlot s of some unit that has type t, equations 12 and 13 model these restrictions. ∀s ∈ S T , • / ∈ π s δ({γ γ γ 1 (s)}); (12) ∀s ∈ S T , π s δ({γ γ γ 0 (s)}) = {•}; (13) Now if a unit i ∈ D is of type {t 1 } and t 1 is more specific than t 2 , then the unit is also of type {t 2 }, and the description of i in δ({t 2 }) must correspond to the description of i in δ({t 1 }). Equivalently, the projection of δ({t 1 }) on the attributes of δ({t 2 }) must be a sub-relation of δ({t 2 }): ∀t 1 t 2 , π {0}∪α α α(t 2 ) δ({t 1 }) ⊆ δ({t 2 }) (14) The interpretation of a CUT is the join of the interpretation of its constituting PUTs, except for ∅ which has the same interpretation as { }, and asserted absurd CUTs t ∩ ∈ ⊥ A that contain no unit. ∀t ∩ ∈ T ∩ \\ ∅ − ⊥ A , δ(t ∩ ) = t∈t ∩ δ({t}) (15) δ(∅) = δ({ }) (16) ∀t ∩ ∈ ⊥ A , δ(t ∩ ) = ∅ (17) Finally, for every unit of type {t} and for every ASlot of t, the unit that fills ASlot s must be either nothing, or a unit of type ς ς ς t (s): ∀t ∈ T, ∀s ∈ α α α(t), π s δ({t}) \\ • ⊆ π 0 δ(ς ς ς t (s)) (18) We may now define the model of a unit type hierarchy. Definition 3.2. Let be a unit types hierarchy D, δ ) such that the interpretation function δ satisfies equations 8 to 18. T = (T D , S T , γ γ γ, γ γ γ 1 , γ γ γ 0 , C A , ⊥ A , {ς ς ς t } t∈T ). A model of T is a couple M = ( Model of a Hierarchy of Circumstantial Symbols So as to be also a model of a CSymbols hierarchy, the interpretation function δ must be extended and further restricted as follows. The interpretation function δ associates with every CSymbol s ∈ S C a binary relation δ(s) with two attributes : gov which stands for governor, and circ which stands for circumstantial. Parallel with binary relations in the semantic model of the CGs formalism, if a CSymbol s 1 is more specific than another CSymbol s 2 , then the interpretation of s 1 must be included in the interpretation of s 2 . ∀s 1 , s 2 ∈ S C , s 1 C s 2 ⇒ δ(s 1 ) ⊆ δ(s 2 ) (20) Finally, the type of the units that are linked through a CSymbol s must correspond to the signature of s. Model Satisfying a UG and Semantic Consequence Now that the model of a support is fully defined, we may define the model of a UG. A model of a UG is a model of the support on which it is defined, augmented with an assignment mapping over unit nodes that assigns to every unit node an element of D. Definition 3.4 (Model of a UG). Let G = (U, l, A, C, Eq) be a UG defined over a support S. A model of G is a triple (D, δ, β) where: • (D, δ) is a model of S; • β, called an assignment, is a mapping from U to D. So as to satisfy the UG, the assignment β must satisfy a set of requirements. First, if a unit node u ∈ U has a marker m ∈ marker(u), then the assignment of u must correspond to the interpretation of m. ∀u ∈ U, ∀m ∈ marker(u), β(u) = δ(m) (23) Then, the assignment of any unit node u must belong to the set of units that have type type(u). ∀u ∈ U, β(u) ∈ π 0 δ(type(u)) (24) For every actantial triple (u, s, v) ∈ A, and as {γ γ γ(s)} is the CUT that introduces a ASlot s, the interpretation δ({γ γ γ(s)}) must reflect the fact that the unit represented by v fills the actant slot s of the unit represented by u. ∀(u, s, v) ∈ A, π 0,s δ({γ γ γ(s)}) = {(β(u), β(v))} (25) Similarly, for every circumstantial triple (u, s, v) ∈ C, the interpretation of s must reveal the fact that the unit represented by v depends on the unit represented by u with respect to s. ∀(u, s, v) ∈ C, (β(u), β(v)) ∈ δ(s) (26) Finally, if two unit nodes are asserted to be equivalent, then the unit they represent are the same and their assignment must be the same. ∀(u 1 , u 2 ) ∈ Eq, β(u 1 ) = β(u 2 ) (27) We may now define the notion of satisfaction of a UG by a model. Conclusion We thus studied how to formalize, in a knowledge engineering perspective, the dependency structures and the valency-based predicates. We gave an overview of the foundational concepts of the new graph-based Unit Graphs KR formalism. The valency-based predicates are represented by unit types, and are described in a unit types hierarchy. Circumstantial relations are another kind of dependency relation that are described in a hierarchy, and along with a set of unit identifiers these two structures form a UGs support on which UGs may be defined. We then provided these foundational structures with a model, in the logical sense, using a relational algebra. We dealt with the problem of prohibited and optional actant slots by adding a special nothing element • in the domain of the model, and listed the different equations that the interpretation function must satisfy so that a model satisfies a UG. We finally introduced the notion of semantic consequence, which is a first step towards reasoning with dependency structure in the UGs framework. We identify three future directions of research. • We did not introduce the definition of PUTs that are to model lexicographic definitions in the ECD and shall be included to the support. The definition of the model semantics of the UGs shall be completed so as to take these into account. • A UG represents explicit knowledge that only partially define the interpretations of unit types, CSymbols, and unit identifiers. One need to define algorithms to complete the model, so as to check the entailment of a UG by another. • We know from ongoing works that such an algorithm may lead to an infinite domain. A condition over the unit types hierarchy must be found so as to ensure that the model is decidable for a finite UG.",
         "10247031",
         "c167d01bc826e954295b153102c23c0afd054ff6",
         "3",
         "https://aclanthology.org/R13-1050",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Lefran{\\c{c}}ois, Maxime  and\nGandon, Fabien",
         "The Unit Graphs Framework: Foundational Concepts and Semantic Consequence",
         "389--395",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "lefrancois-gandon-2013-unit",
         null,
         null
        ],
        [
         "18",
         "W05-0825",
         "In this paper, we present a phrase extraction algorithm using a translation lexicon, a fertility model, and a simple distortion model. Except these models, we do not need explicit word alignments for phrase extraction. For each phrase pair (a block), a bilingual lexicon based score is computed to estimate the translation quality between the source and target phrase pairs; a fertility score is computed to estimate how good the lengths are matched between phrase pairs; a center distortion score is computed to estimate the relative position divergence between the phrase pairs. We presented the results and our experience in the shared tasks on French-English.",
         "In this paper, we present a phrase extraction algorithm using a translation lexicon, a fertility model, and a simple distortion model. Except these models, we do not need explicit word alignments for phrase extraction. For each phrase pair (a block), a bilingual lexicon based score is computed to estimate the translation quality between the source and target phrase pairs; a fertility score is computed to estimate how good the lengths are matched between phrase pairs; a center distortion score is computed to estimate the relative position divergence between the phrase pairs. We presented the results and our experience in the shared tasks on French-English. Introduction Phrase extraction becomes a key component in today's state-of-the-art statistical machine translation systems. With a longer context than unigram, phrase translation models have flexibilities of modelling local word-reordering, and are less sensitive to the errors made from preprocessing steps including word segmentations and tokenization. However, most of the phrase extraction algorithms rely on good word alignments. A widely practiced approach explained in details in (Koehn, 2004) , (Och and Ney, 2003) and (Tillmann, 2003) is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words. With this refined word alignment, the phrase extraction for a given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries. In (Vogel et al., 2004) , they treat phrase alignment as a sentence splitting problem: given a source phrase, find the boundaries of the target phrase such that the overall sentence alignment lexicon probability is optimal. We generalize it in various ways, esp. by using a fertility model to get a better estimation of phrase lengths, and a phrase level distortion model. In our proposed algorithm, we do not need explicit word alignment for phrase extraction. Thereby it avoids the burden of testing and comparing different heuristics especially for some language specific ones. On the other hand, the algorithm has such flexibilities that one can incorporate word alignment and heuristics in several possible stages within this proposed framework to further improve the quality of phrase pairs. In this way, our proposed algorithm is more generalized than the usual word alignment based phrase extraction algorithms. The paper is structured as follows: in section 2, The concept of blocks is explained; in section 3, a dynamic programming approach is model the width of the block; in section 4, a simple center distortion of the block; in section 5, the lexicon model; the complete algorithm is in section 6; in section 7, our experience and results using the proposed approach. Blocks We consider each phrase pair as a block within a given parallel sentence pair, as shown in Figure 1 . The y-axis is the source sentence, indexed word by word from bottom to top; the x-axis is the target sentence, indexed word by word from left to right. The block is defined by the source phrase and its projection. The source phrase is bounded by the start and the end positions in the source sentence. The projection of the source phrase is defined as the left and right boundaries in the target sentence. Usually, the boundaries can be inferred according to word alignment as the left most and right most aligned positions from the words in the source phrase. In this paper, we provide another view of the block, which is defined by the centers of source and target phrases, and the width of the target phrase. Phrase extraction algorithms in general search for the left and right projected boundaries of each source phrase according to some score metric computed for the given parallel sentence pairs. We present here three models: a phrase level fertility model score for phrase pairs' length mismatch, a simple center-based distortion model score for the divergence of phrase pairs' relative positions, and a phrase level translation score to approximate the phrase pairs' translational equivalence. Given a source phrase, we can search for the best possible block with the highest combined scores from the three models. Length Model: Dynamic Programming Given the word fertility definitions in IBM Models (Brown et al., 1993) , we can compute a probability to predict phrase length: given the candidate target phrase (English) e I 1 , and a source phrase (French) of length J, the model gives the estimation of P (J|e I 1 ) via a dynamic programming algorithm using the source word fertilities. Figure 2 shows an example fertility trellis of an English trigram. Each edge between two nodes represents one English word e i . The arc between two nodes represents one candidate non-zero fertility for e i . The fertility of zero (i.e. generating a NULL word) corresponds to the direct edge between two nodes, and in this way, the NULL word is naturally incorporated into this model's representation. Each arc is A path φ I 1 through the trellis represents the number of French words φ i generated by each English word e i . Thus, the probability of generating J words from the English phrase along the Viterbi path is: P (J|e I 1 ) = max {φ I 1 ,J= I i=1 φ i } I i=1 P (φ i |e i ) (1) The Viterbi path is inferred via dynamic programming in the trellis of the lower panel in Figure 2 : φ[j, i] = max        φ[j, i − 1] + log P N U LL (0|e i ) φ[j − 1, i − 1] + log P φ (1|e i ) φ[j − 2, i − 1] + log P φ (2|e i ) φ[j − 3, i − 1] + log P φ (3|e i ) where P N U LL (0|e i ) is the probability of generating a NULL word from e i ; P φ (k = 1|e i ) is the usual word fertility model of generating one French word from the word e i ; φ[j, i] is the cost so far for generating j words from i English words e i 1 : e 1 , • • • , e i . After computing the cost of φ[J, I], we can trace back the Viterbi path, along which the probability P (J|e I 1 ) of generating J French words from the English phrase e I 1 as shown in Eqn. 1. With this phrase length model, for every candidate block, we can compute a phrase level fertility score to estimate to how good the phrase pairs are match in their lengthes. Distortion of Centers The centers of source and target phrases are both illustrated in Figure 1 . We compute a simple distortion score to estimate how far away the two centers are in a parallel sentence pair in a sense the block is close to the diagonal. In our algorithm, the source center f j+l j of the phrase f j+l j with length l + 1 is simply a normalized relative position defined as follows: f j+l j = 1 |F | j =j+l j =j j l + 1 (2) where |F | is the French sentence length. For the center of English phrase e i+k i in the target sentence, we first define the expected corresponding relative center for every French word f j using the lexicalized position score as follows: e i+k i (f j ) = 1 |E| • (i+k) i =i i • P (f j |e i ) (i+k) i =i P (f j |e i ) (3) where |E| is the English sentence length. P (f j |e i ) is the word translation lexicon estimated in IBM Models. i is the position index, which is weighted by the word level translation probabilities; the term of I i=1 P (f j |e i ) provides a normalization so that the expected center is within the range of target sentence length. The expected center for e i+k i is simply a average of e i+k i (f j ): e i+k i = 1 l + 1 j+l j =j e i+k i (f j ) (4) This is a general framework, and one can certainly plug in other kinds of score schemes or even word alignments to get better estimations. Given the estimated centers of Lexicon Model Similar to (Vogel et al., 2004) , we compute for each candidate block a score within a given sentence pair using a word level lexicon P (f |e) as follows: P (f j+l j |e i+k i ) = j ∈[j,j+l] i ∈[i,i+k] P (f j |e i ) k + 1 • j / ∈[j,j+l] i / ∈[i,i+k] P (f j |e i ) |E| − k − 1 6 Algorithm Our phrase extraction is described in Algorithm 1. The input parameters are essentially from IBM Model-4: the word level lexicon P (f |e), the English word level fertility P φ (φ e = k|e), and the center based distortion P ( e i+k i | f j+l j ). Overall, for each source phrase f j+l j , the algorithm first estimates its normalized relative center in the source sentence, its projected relative center in the target sentence. The scores of the phrase length, center-based distortion, and a lexicon based score are computed for each candidate block A local greedy search is carried out for the best scored phrase pair (f j+l j , e i+k i ). In our submitted system, we computed the following seven base scores for phrase pairs: P ef (f j+l j |e i+k i ), P f e (e i+k i |f j+l j ), sharing similar function form in Eqn. 5. P ef (f j+l j |e i+k i ) = j i P (f j |e i )P (e i |e i+k i ) = j i P (f j |e i ) k + 1 (5) We compute phrase level relative frequency in both directions: P rf (f j+l j |e i+k i ) and P rf (e i+k i |f j+l j ). We compute two other lexicon scores which were also used in (Vogel et al., 2004) : S 1 (f j+l j |e i+k i ) and S 2 (e i+k i |f j+l j ) using the similar function in Eqn. 6: S(f j+l j |e i+k i ) = j i P (f j |e i ) (6) In addition, we put the phrase level fertility score computed in section 3 via dynamic programming to be as one additional score for decoding. Algorithm 1 A Generalized Alignment-free Phrase Extraction score the phrase pair (f j+l j , e i+k i ), where score = P ( e | f )P (l|e i+k i )P (f j+l j |e i+k i ) 14: add top-n {(f j+l j , e i+k i )} into PhraseSet. Experimental Results Our system is based on the IBM Model-4 parameters. We train IBM Model 4 with a scheme of 1 7 2 0 h 7 3 0 4 3 using GIZA++ (Och and Ney, 2003) . The maximum fertility for an English word is 3. All the data is used as given, i.e. we do not have any preprocessing of the English-French data. The word alignment provided in the workshop is not used in our evaluations. The language model is provided by the workshop, and we do not use other language models. The French phrases up to 8-gram in the development and test sets are extracted with top-3 candidate English phrases. There are in total 2.6 million phrase pairs 1 extracted for both development set and the unseen test set. We did minimal tuning of the parameters in the pharaoh decoder (Koehn, 2004) settings, simply to balance the length penalty for Bleu score. Most of the weights are left as they are given: [ttable-limit]=20, [ttable-threshold]=0.01, 1 Our phrase table is to be released to public in this workshop 1 , setting s 1 was our submission without using the inverse relative frequency of P rf (e i+k i |f j+l j ). s 2 is using all the seven scores. Discussions In this paper, we propose a generalized phrase extraction algorithm towards word alignment-free utilizing the fertility model to predict the width of the block, a distortion model to predict how close the centers of source and target phrases are, and a lexicon model for translational equivalence. The algorithm is a general framework, in which one could plug in other scores and word alignment to get better results.",
         "13293973",
         "bb746564a712ee90541b6b5649aaa06fffa03dce",
         "25",
         "https://aclanthology.org/W05-0825",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Zhao, Bing  and\nVogel, Stephan",
         "A Generalized Alignment-Free Phrase Extraction",
         "141--144",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "zhao-vogel-2005-generalized",
         null,
         null
        ],
        [
         "19",
         "W09-1315",
         "We introduce a controlled natural language for biomedical queries, called BIOQUERYCNL, and present an algorithm to convert a biomedical query in this language into a program in answer set programming (ASP)-a formal framework to automate reasoning about knowledge. BIOQUERYCNL allows users to express complex queries (possibly containing nested relative clauses and cardinality constraints) over biomedical ontologies; and such a transformation of BIOQUERYCNL queries into ASP programs is useful for automating reasoning about biomedical ontologies by means of ASP solvers. We precisely describe the grammar of BIOQUERYCNL, implement our transformation algorithm, and illustrate its applicability to biomedical queries by some examples.",
         "We introduce a controlled natural language for biomedical queries, called BIOQUERYCNL, and present an algorithm to convert a biomedical query in this language into a program in answer set programming (ASP)-a formal framework to automate reasoning about knowledge. BIOQUERYCNL allows users to express complex queries (possibly containing nested relative clauses and cardinality constraints) over biomedical ontologies; and such a transformation of BIOQUERYCNL queries into ASP programs is useful for automating reasoning about biomedical ontologies by means of ASP solvers. We precisely describe the grammar of BIOQUERYCNL, implement our transformation algorithm, and illustrate its applicability to biomedical queries by some examples. Introduction The rapid increase in the popularity and usage of Web leads researchers to store data and make it publicly available in many ways. In particular, to facilitate access to its desired parts, it is stored in a structured form, like ontologies. These ontologies can be queried with an SQL-like formal query language. However, since these ontologies have been developed for and widely used by people that lacks the necessary knowledge in a formal query language, a simpler and more commonly known language is needed to represent queries. A natural language is the perfect answer, but ambiguities in its grammar and vocabulary make it difficult to automate reasoning about queries in natural language. Therefore, to represent queries, we consider a middle ground between these two options: a Controlled Natural Language (CNL). A CNL is a subset of a natural language, with a restricted grammar and vocabulary, that overcomes the ambiguity of natural languages. Since we consider queries in a specific domain, namely biomedicine, and over specific sources of information, namely biomedical ontologies, a CNL designed and developed for reasoning about biomedical ontologies is sufficient to represent biomedical queries. Essentially, a CNL is a formal language but with a look of a natural language. Therefore, compared to a natural language, a CNL can be easily converted to some other formalisms. This allows us to use automated reasoners, specifically developed for such formalisms, to find answers to queries expressed in a CNL. One such formalism is Answer Set Programming (ASP) (Baral, 2003) . ASP is a new knowledge representation and reasoning paradigm which supports representation of defaults, constraints, preferences, aggregates, etc., and provides technologies that allow us to automate reasoning with incomplete information, and to integrate other technologies, like description logics reasoners and Semantic Web technologies. For instance, in (Bodenreider et al., 2008) , the authors illustrate the applicability and effectiveness of using ASP to represent a rule layer that integrates relevant parts of some biomedical ontologies in RDF(S)/OWL, and to compute answers to some complex biomedical queries over these ontologies. Although CNLs are appropriate for expressing biomedical queries, and methods and technologies of ASP are appropriate for automated reasoning about biomedical ontologies, there is no algorithm to convert a CNL biomedical query into a program. In (Bodenreider et al., 2008) , biomedical queries are represented as programs in ASP; however, these programs are constructed manually. However, manually constructing ASP programs to represent biomedical queries is not only time consuming but also requires expertise in ASP. This prevents automating the whole process of computing an answer to a query, once it is given in a CNL. In this paper, we design and develop a CNL (called BIOQUERYCNL) for expressing biomedical queries over some ontologies, and introduce an algorithm to convert a biomedical query expressed in this CNL into a program in ASP. The idea is to automatically compute an answer to the query using methods of (Bodenreider et al., 2008) , once the user types the query. This idea is illustrated in Figure 1 . Similar approaches of using a CNL for querying ontologies have been investigated in various studies. For instance, (Bernstein et al., 2005) considers queries in the controlled natural language, Attempto Controlled English (ACE) (Attempto, 2008) , and transforms them into queries in PQL (Klein and Bernstein, 2004) to be evaluated by a query engine. (Bernstein et al., 2006) presents a system that guides the user to write a query in ACE, and translates the query into SPARQL to be evaluated by the reasoner of JENA (Jena, 2008) . On the other hand, (Kaufmann et al., 2006) transforms a given natural language query to a SPARQL query (using the Stan-ford Parser and WORDNET) to be evaluated by a reasoner like that of JENA. Our work is different from these studies in two ways: we consider queries over biomedical ontologies (thus different forms of queries, and vocabulary), and we transform a query into an ASP program to automate reasoning over a rule layer presented in ASP. Transformations of natural language sentences into ASP has been studied in (Baral et al., 2008) and (Baral et al., 2007) . In (Baral et al., 2008) , the authors introduce methods to transform some simple forms of sentences into ASP using Lambda Calculus. In (Baral et al., 2007) , the authors use C&C tools (CC, 2009) to parse the some forms of natural language input, and perform a semantic analysis over the parser output using BOXER (Boxer, 2009) , to do reasoning in ASP. Our work is different in that we consider a CNL to express queries, and introduce a different method for converting CNL to a program in ASP, via Discourse Representation Structures (DRS) (Kamp, 1981) . In the rest of the paper, first we briefly discuss ASP with some examples (Section 2). Then we define the grammatical structure of BIOQUERYCNL and give some examples (Section 3). Next, we introduce our algorithm for transforming a BIO-QUERYCNL query into an ASP program and explain it by an example (Section 4). We conclude with a discussion of challenges related to the implementation of our algorithm (Section 5) and other related problems that we are working on (Section 6). Answer Set Programming Answer Set Programming (ASP) (Lifschitz, 1999; Marek and Truszczyński, 1999; Niemelä, 1999; Baral, 2003) is a new knowledge representation and reasoning paradigm which supports representation of defaults, constraints, preferences, aggregates, etc., and provides technologies that allow us to automate reasoning with incomplete information, and to integrate other technologies, like description logics reasoners and Semantic Web technologies. In ASP, knowledge is represented as a \"program\" (a finite set of \"rules\") whose meaning is captured by its models (called \"answer sets\" (Gelfond and Lifschitz, 1988) ). Answer sets for a program can be computed by \"answer set solvers\" such as DLV (DLV, 2009) . Consider for instance the program: gene_gene(''ADRB1'',''CHRM5''). gene_gene(''CHRM1'',''CHRM5''). chain(X,Y) :-gene_gene(X,Y). chain(X,Y) :-gene_gene(Y,X). chain(X,Y) :-gene_gene(X,Z), chain(Z,Y). The first rule expresses that the gene ADRB1 interacts with the gene CHRM5. The second rule expresses that the gene CHRM1 interacts with the gene CHRM5. The third, the fourth, and the fifth rules express a chain of such interactions. In a rule containing :-, the left-hand-side of :-is called the head of the rule, the right-hand-side is called the body of the rule. Such a rule p :-q, r. is read as \"p if q and r\". Here the head atom is p, and the body atoms are q and r. The answer set for this program describes that there is a chain of interactions between CHRM1 and CHRM5, ADRB1 and CHRM5, and ADRB1 and CHRM1. As mentioned above, the language of ASP is expressive enough to represent defaults, constraints, preferences, aggregates, etc.. For instance, the rule treats_2diseases(R) :- #count{D:treats(R,D)}>=2, drug(R). describes drugs R that treat at least 2 diseases. A Controlled Natural Language for Biomedical Queries We introduce a controlled natural language, called BIOQUERYCNL, to express biomedical queries, whose grammar is shown in Table 1 . This grammar should be considered in connection with the given biomedical ontologies. The italic words in the grammar, for instance, represent the information extracted from the related ontologies. We call these italic words ontology functions; the detailed description of these functions are given in Table 2 . With BIOQUERYCNL, the users can ask simple queries, queries with nested relative clauses (with any number of conjunctions and disjunctions), and queries with cardinalities. Some sample queries are given below. (Q1) Which symptoms are alleviated by the drug Epinephrine? (Q2) What are the side-effects of the drugs that treat the disease Asthma? (Q3) What are the genes that are related to the disease Asthma and are targeted by the drug Epinephrine? (Q4) What are the symptoms of the diseases that are related to the gene ADRB1 or that are treated by the drug Epinephrine? (Q5) Which genes are targeted by at least 2 drugs and are related to at most 3 diseases? BIOQUERYCNL is a subset of Attempto Controlled English (ACE) (Attempto, 2008) , which can represent a wide range of queries (Fuchs et al., 2008) , specialized for biomedical ontologies. Converting Controlled Natural Language Queries to Programs We have implemented an algorithm, QUERY, presented in Algorithm 1, that obtains an ASP rule Head ← Body from a query Q expressed in BIO-QUERYCNL, via transforming Q into a DRS. We will explain the main steps of the QUERY algorithm by an example, considering query (Q4). (Kamp, 1981) -a variant of the first-order logic that is used for the dynamic interpretation of natural language and systematic translation of natural language into logical form -without any ambiguity, using tools like Attempto Parsing Engine (APE). APE converts ACE text to DRS by an approach similar to (Blackburn and Bos, 2005) , as explained in (Fuchs et al., 2008) . For instance, APE transforms query (Q4) into the following DRS: QUERY → YESNOQUERY | WHQUERY QUESTIONMARK YESNOQUERY → DODOESQUERY | ISAREQUERY WHQUERY → WHATQUERY | WHICHQUERY DODOESQUERY → [ Do | Does ] T ype() Instance(T ) PREDICATERELATION ISAREQUERY → [ Is | Are ] T ype() Instance(T ) V erb(T ) WHATQUERY → What BE T ype() that PREDICATERELATION WHATQUERY → What BE OFRELATION that PREDICATERELATION WHATQUERY → What BE OFRELATIONINSTANCE that PREDICATERELATION WHICHQUERY → Which T ype() PREDICATERELATION OFRELATION → N oun(T ) of T ype() OFRELATIONINSTANCE → N oun(T ) of T ype() Instance(T ) PREDICATERELATION → ACTIVERELATION (CONNECTOR (that)? PREDICATERELATION)* PREDICATERELATION → PASSIVERELATION (CONNECTOR (that)? PREDICATERELATION)* ACTIVERELATION → V erb(T, T ) T ype() Instance(T ) ACTIVERELATION → V erb(T, T ) GENERALISEDQUANTOR PositiveNumber T ype() PASSIVERELATION → BE V erb(T , T ) by T ype() Instance(T ) PASSIVERELATION → BE V erb(T , T ) by GENERALISEDQUANTOR PositiveNumber T ype() BE → is | are CONNECTOR → and | or GENERALISEDQUANTOR → at least | at most | more than | less than | exactly QUESTIONMARK → ? Table 2 : The Ontology Functions T ype() returns the type information the ontologies keep, ex. gene, disease, drug Instance(T ) returns instances of the type T , ex. Asthma for type disease V erb(T ) returns the verbs related to the type T , ex. approve for type drug V erb(T, T ) returns the verbs where type T is the subject and type T is the object, ex. drug treat disease N oun(T ) returns the nouns that are related to the type T , ex. symptom for type disease Note that the DRS consists of two kinds of expressions. The lines with a list of uppercase letters, like [E,F,G], describe the domain of the DRS; each uppercase letter is a referent. The rest of the DRS describe the conditions about the domain. The DRS above contains some predefined predicates, such as object, property, predicate, query, etc.. All the nouns, adjectives, verbs, modifiers, etc. are represented with one of them. For instance, • object describes objects and the relevant forms of nouns denoting them (like \"diseases\") • predicate describes relations that are pro-duced by different forms of verbs (like \"treated\"), • relation describes relations that are produced by of-constructions (like \"symptoms of disease\"), • query describes the form of the query and the objects that the query is referring to. Ontologies represent relations between concepts. A rule layer over ontologies introduce further concepts integrating them. ASP takes into account relevant concepts and relations to answer a given query about these ontologies. In the biomedical queries we consider, the concepts and instances are represented with object and the relations between these concepts are represented with predicate and relation. The query is also important in terms of the type of information the user asks for. Constructing the Head and the Body Atoms Once the corresponding DRS is obtained from a given BIOQUERYCNL query, the head and the body atoms are constructed by analyzing the conditions in the DRS, as described in Algorithms 2 and 3. The HEAD algorithm is about the query predicate, which refers to objects or relations that are asked for in the given query. By following the referents, starting from the one mentioned in query, the algorithm finds out the type of the information that is asked for in the given query. Consider, for instance, query (Q4). The referent mentioned in query(A,what) is A. It is mentioned in predicate(B,be,A,C)-1, and here it denotes an object with referent C. Now let's find where C is mentioned: in object(C,symptoms,countable,na,eq,1)-1 to denote symptoms. Therefore, the query asks for symptoms. Based on this information, Algorithm 2 returns the head of the ASP rules as follows: what_be_symptoms(SYM1) The BODY algorithm analyzes the predicate and the relation predicates. These two predicates describe relations between objects described by the object predicates. The algorithm starts from the predicate and the relation predicates, and then, by following the referents, it returns the body atoms of the ASP rule. For instance, Algorithm 3 returns the following body atoms for query (Q4): symptoms_of_diseases(symptom_SYM1, disease_DIS1) diseases_be_related_to_gene(disease_DIS1, gene_''ADRB1'') drug_treated_diseases(drug_''Epinephrine'', disease_DIS1) These body atoms are given to POSTPROCESSING step, to produce bodies of the ASP rules. Constructing the ASP Rules POSTPROCESSING is the last step of the QUERY algorithm. At this step, first the number of rules is determined, and then the body atoms are placed in the bodies of these rules. In ASP, a conjunctive query can be represented by a rule. However, disjunctive queries are represented by several rules with same head but different bodies. For instance, query (Q4) is a disjunctive query (a disjunction of two queries), so there will be two rules representing this query: what_be_symptoms(SYM1) :- symptoms_of_diseases(symptom_SYM1, disease_DIS1), diseases_be_related_to_gene(disease_DIS1, gene_''ADRB1''). what_be_symptoms(SYM1) :-drug_treated_diseases(drug_''Epinephrine'', disease_DIS1), symptoms_of_diseases(symptom_SYM1, disease_DIS1). Next, the predicate names in the bodies of these rules are matched with the names of the already defined predicates in ontologies or in the rule layer over these ontologies. After matching the predicate names, the parameters of the predicates may have to be reordered. The matching of the predicates very much depends on the user interface (UI). If UI enforces users to use a specific grammar and lexicon while forming the query, then the matching can be done with an easy table look-up method. If the UI allows more flexibility of constructing a query, then the matching algorithm should use some basic Natural Language Processing methods and similarity metrics to find the most probable matching. After matching the predicates, the ordering of the parameters can be done easily. The Object := REFERSTO(Ref ) // e.g., A refers to \"genes\" GEN E1 7: Head := CONCAT(QuestionWord, Predicate, Object, Ref ) // e.g., what be genes(GEN E1) 8: end if 9: return Head returns the body predicates with the parameters. In these parameters, the type and the instance names are kept together. Thus, ordering of those parameters are done just by using the type information. After the ordering is done, the type information part is removed from the parameters. For instance, after matching the predicates, we get the following ASP rule for query (Q4). With an ASP rule layer over ontologies, and this ASP program, an ASP solver, like DLVHEX (DLVHEX, 2009) , returns an answer to query (Q4). For instance, consider the ASP rule layer, and the gene, disease, drug ontologies of (Bodenreider et al., 2008) . The ontologies of (Bodenreider et al., 2008) are obtained from the ontologies PHAR-MGKB (PharmGKB, 2008) , UNIPROT (UniProt, 2008) , GENE ONTOLOGY (GO) (GeneOntology, 2008) , GENENETWORK database (GeneNetwork, 2008) , DRUGBANK (DrugBank, 2008) , and the Medical Symptoms and Signs of Disease web page (MedicalSymptomsSignsDisease, 2008) . With this rule layer and the ontologies, and the ASP program above, the following is a part of the answer DLVHEX finds to the query above: noisy breathing faster breathing shortness of breath coughing chest tightness wheezing Another Example The algorithm discussed above returns the following ASP program for query (Q5): which_genes(GN1) :-2<=#count{DRG1:drug_gene(DRG1,GN1)}, #count{DIS1:disease_gene(DIS1,GN1)}<=3. Since query (Q5) contains cardinality constraints, the ASP program uses the aggregate #count. More examples of biomedical queries, and the ASP programs generated by our program can be seen at http://people.sabanciuniv.edu/ esraerdem/bioquery-asp/bionlp09/ . Implementational Issues We have implemented the algorithms explained above in PERL. We have used Attempto Parsing Engine APE to convert a given BIOQUERYCNL query into a DRS. Since BIOQUERYCNL is about biomedical ontologies, we provided APE some information about biomedical concepts, such as gene, drug, and words that represent relations between these concepts such as treat, target etc.. However, providing such information is not sufficient to convert all BIOQUERYCNL biomedical queries into programs, mainly due to specific instances of these concepts (consider, for instance, various drug names that appear in ontologies). One way to deal with this problem is to extract from the ontologies all instances of each concept and provide them to APE as an additional lexicon. This may not be the perfect solution since this process has to be repeated when an instance is added to the ontology. An alternative way can be enforcing the user to enter Subject := REFERSTO(SubRef ) // e.g., A refers to \"genes\" GEN E1 // e.g., genes targeted by drugs(GEN E1, DRG1) 13: end for 14: for each relation R do // e.g., symptoms of diseases(SY M 1, DIS1) 20: end for 21: return Body the concept name just before the instance (like \"the drug Epinephrine\") in the query. This is how we deal with instance names, in the current version of our implementations. However, such BIOQUERYCNL queries are not in the language of APE; so, with some preprocessing, we rewrite these queries in the correct syntax for APE. Conclusion We have designed and developed a Controlled Natural Language (CNL), called BIOQUERYCNL, to represent biomedical queries over some ontologies, and provided a precise description of its grammatical structure. We have introduced an algorithm to convert queries in BIOQUERYCNL to a program in Answer Set Programming (ASP). The idea is to compute answers to these queries automatically, by means of automated reasoners in ASP, over biomedical ontologies in RDF(S)/OWL and a rule layer in ASP integrating these ontologies. Our algorithm can handle various forms of simple/complex disjunc-tive/conjunctive queries that may contain (nested) relative clauses and cardinality constraints. We have implemented this algorithm in PERL, and tried it with the ASP rule layer, and the ontologies of (Bodenreider et al., 2008) . One essential part of the overall system is an intelligent user interface that allows a user to enter biomedical queries in BIOQUERYCNL. Design and implementation of such a user-interface is a part of our ongoing work. Acknowledgments Thanks to Tobias Kuhn for his help with ACE. This work is supported by the Scientific and Technological Research Council of Turkey (TUBITAK) grant 108E229.",
         "14985007",
         "8c3a4fc75d23259d5bd413d0a632713a39ed39c5",
         "24",
         "https://aclanthology.org/W09-1315",
         "Association for Computational Linguistics",
         "Boulder, Colorado",
         "2009",
         "June",
         "Proceedings of the {B}io{NLP} 2009 Workshop",
         "Erdem, Esra  and\nYeniterzi, Reyyan",
         "Transforming Controlled Natural Language Biomedical Queries into Answer Set Programs",
         "117--124",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "erdem-yeniterzi-2009-transforming",
         null,
         null
        ],
        [
         "20",
         "R13-1051",
         "Information extraction systems automatically extract structured information from machine-readable documents, such as newswire, web, and multimedia. Despite significant improvement, the performance is far from perfect. Hence, it is useful to accurately estimate confidence in the correctness of the extracted information. Using the Knowledge Base Population Slot Filling task as a case study, we propose a confidence estimation model based on the Maximum Entropy framework, obtaining an average precision of 83.5%, Pearson coefficient of 54.2%, and 2.3% absolute improvement in F-measure score through a weighted voting strategy.",
         "Information extraction systems automatically extract structured information from machine-readable documents, such as newswire, web, and multimedia. Despite significant improvement, the performance is far from perfect. Hence, it is useful to accurately estimate confidence in the correctness of the extracted information. Using the Knowledge Base Population Slot Filling task as a case study, we propose a confidence estimation model based on the Maximum Entropy framework, obtaining an average precision of 83.5%, Pearson coefficient of 54.2%, and 2.3% absolute improvement in F-measure score through a weighted voting strategy. Introduction Despite significant progress in recent years, Information Extraction (IE) technologies are still far from completely reliable. Errors result from the fact that language itself is ambiguous as well as methodological and technical limitations (Gandrabur et al., 2006) . Therefore, evaluating the probability that the extracted information is correct can contribute to improve IE system performance. Confidence Estimation (CE) is a generic machine learning rescoring approach for measuring the probability of correctness of the outputs, and usually adds a layer on top of the baseline system to analyze the outputs using additional information or models (Gandrabur et al., 2006) . There is previous work in IE using probabilistic and heuristic methods to estimate confidence for extracting fields using a sequential model, but to the best of our knowledge, this work is the first probabilistic CE model for the multi-stage systems employed for the Knowledge Base Population (KBP) Slot Filling task (Section 2). The goal of Slot Filling (SF) is to collect information from a corpus of news and web documents to determine a set of predefined attributes (\"slots\") for given person and organization entities (Ji et al., 2011a ) (Section 3). Many existing methodologies have been used to address the SF task, such as Distant Supervision (Min et al., 2012) and Question Answering (Chen et al., 2010) , and each method has its own strengths and weaknesses. Many current KBP SF systems actually consist of several independent SF pipelines. The system combines intermediate responses generated from different pipelines into final slot fills. Since these intermediate outputs may be highly redundant, if confidence values can be associated, it will definitely help re-ranking and aggregation. For this purpose, we require comparable confidence values from disparate machine learning models or different slot filling strategies. Robust probabilistic machine learning models are capable of accurate confidence estimation because of their intelligent handling of uncertainty information. In this paper, we use the Maximum Entropy (MaxEnt) framework (Berger et al., 1996) to automatically predict the correctness of KBP SF intermediate responses (Section 4). Results achieve an average precision of 83.5%, Pearson's r of 54.2%, and 2.3% absolute improvement in final F-measure score through a weighted voting system (Section 5). Related Work Confidence estimation is a generic machine learning approach for measuring confidence of a given output, and many different CE methods have been used extensively in various Natural Language Processing (NLP) fields (Gandrabur et al., 2006) . Gandrabur and Foster (2003) and Nguyen et al. (2011) investigated the use of machine learning approaches for confidence estimation in machine translation. Agichtein (2006) showed Expectation-Maximization algorithms to estimate the confidence for partially supervised relation extraction. White et al. (2007) described how a maximum entropy model can be used to generate confidence scores for a speech recognition engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic summarization outputs. Many approaches for confidence estimation have also been explored and implemented in other NLP research areas. There are also many previous confidence estimation studies in IE, and most of these have been in the Active Learning literature. Thompson et al. (1999) proposed a rule-based extraction method to compute confidence. Scheffer et al. (2001) utilized hidden Markov models to measure the confidence in an IE system, but they only estimated the confidence of singleton tokens. Culotta and McCallum (2004) 's work is the most relevant to our work, since they also utilized a machine learning model to estimate the confidence values for IE outputs. They estimated the confidence of both extracted fields and entire multi-field records mainly through a linear-chain Conditional Random Field (CRF) model, but their case studies are not as complicated and challenging as slot filling, since SF systems need to handle difficult crossdocument coreference resolution, sophisticated inference, and also other challenges (Min and Grishman, 2012) . Furthermore, to the best of our knowledge, there is no previous work in confidence estimation for the KBP slot filling task. KBP Slot Filling Task Definition The Knowledge Base Population (KBP) track, organized by U.S. National Institute of Standards and Technology (NIST)'s Text Analysis Conference (TAC), aims to promote research in discovering information about entities and augmenting a Knowledge Base (KB) with this information (Ji et al., 2010) . KBP mainly consists of two tasks: Entity Linking, linking names in a provided document to entities in the KB or NIL; and Slot Filling (SF), extracting information about an entity in the KB to automatically populate a new or existing KB. As a new but influential IE evaluation, Slot Filling is a challenging and practical task (Min and Grishman, 2012) . The Slot Filling task at KBP2012 provides a large collection of 3.7 million newswire articles and web texts as the source corpus, and an initial KB derived from the Wikipedia infoboxes. In such a large corpus, some information can be highly redundant. Given a list of person (PER) and organization (ORG) entity names (\"queries\"), SF systems retrieve the documents about these entities in the corpus and then fill the required slots with correct, non-redundant values. Each query consists of the name of the entity, its type (PER or ORG), a document (from the corpus) in which the name appears, its node ID if the entity appears in the provided KB, and the slots which need not be filled. Along with each slot fill, the system should also provide the ID of the document that justifies this fill. If the system does not extract any information for a given slot, the system just outputs \"NIL\" without any document ID. The task defines a total of 42 slots, 26 for person entities and 16 for organization entities. Some slots are single-valued, like \"per:date of birth\", which can only accept at most a single value, while the other slots, for example \"org:subsidiaries\", are list-valued, which can take a list of values. Since the overall goal is to augment an existing KB, the redundancy in list-valued slots must be detected and avoided, requiring a system to identify different but equivalent strings. Such as, both \"United States\" and \"U.S.\" refer to the same country. More information can be found in the task definition (Ji et al., 2010) . Baseline System Description We use a slot filling system that has achieved highly competitive results (ranked top 2) at the KBP2012 evaluation as our baseline. Like most SF systems, our system has three basic components: Document Retrieval, Answer Extraction, and Response Combination. Our SF system starts by retrieving relevant documents based on a match to the query name or the results of query expansion. where the best answer is selected for each singlevalued slot and non-redundant fills are generated for list-valued slots. More details about our KBP Slot Filling system can be found in the system description paper (Min et al., 2012) . Confidence Estimation Model Experiments We have collected and merged the previous three years' KBP SF evaluation data, which consists of a total of 280 queries, and Table 1 lists the number of person and organization queries as well as the number of intermediate responses from each year. There are in total 31878 intermediate responses generated by 6 different pipelines from our SF system. We trained our CE model and measured the confidence values through a 10-fold crossvalidation, so that each fold randomly contains 14 person queries and 14 organization queries with their associated intermediate responses. Then for each iteration, the CE model is trained on 9 folds and approximates the confidence values in the remaining fold, and it assigns the probability of each intermediate response being correct as confidence. Voting Systems To evaluate the reliability of confidence values generated by this model, we used the weighted voting method to investigate the relationship between the confidence values and the performance. Baseline Voting System Our baseline SF system applies a basic plurality voting to combine all intermediate responses to generate the final response submission. This voting system simply counts the frequencies of each response entity, which is a unique response tuple in the form <Query ID, Slot Name, Response Fill>. For a single-valued slot of a query, the response with the highest count is returned as the final response fill. For the list-valued slots, all non-redundant responses are returned as the final response fills. In this basic voting system, each intermediate response contributes equally. Weighted Voting System Weighted voting is based on the idea that not all the voters contribute equally. Instead, voters have different weights concerning the outcome of an election. In our experiment, voters are all of intermediate responses generated by all pipelines, and the voters' weights are their confidence values. We set a threshold τ in this weighted voting system, where those intermediate responses with Category Feature Description Response Features slot name The slot name slot response length The conjunction of the length of R and the slot name name response slot The slot requires a name as the response Pipeline Features pipeline name The name of pipeline which generates R pipeline precision The Precision of the pipeline which generates R pipeline recall The Recall of the pipeline which generates R pipeline fmeasure The F-measure of the pipeline which generates R Local Features sent contain QR S contains both original Q and R sent contain ExQR S contains both co-referred Q or expanded Q and R dpath length The length of shortest dependency path between Q and R in S shortest dpath The shortest dependency path between Q and R in S NE boolean R is a person or organization name in S NE margin The difference between the log probabilities of this name R and the second most likely name n-gram Tri-gram context window associated with part-of-speech tags containing Q or R genre The supporting document is a newswire or web document Global Features query doc num The number of documents retrieved by Q response doc num The number of documents retrieved by R co-occur doc num The number of documents retrieved by the co-occurrences of Q and R cond prob givenQ The conditional probability of R given Q cond prob givenR The conditional probability of Q given R mutual info The Point-wise Mutual Information (PMI) of Q and R Table 2 : Features of Confidence Estimation Model confidences that are lower than τ would be eliminated. For each response entity, this weighted voting system simply sums all the weights of the intermediate responses that support this response entity as its weight. Then for a single-valued slot of a query, it returns the response with the highest weight as the final slot fill, while it returns all nonredundant responses as the final slot fills for the list-valued slots. The maximum confidence ψ of supporting intermediate responses is used as the final confidence for that slot fill. We also set a threshold η (optimized on a validation data set), where the final slot fills with confidence ψ lower than η would not be submitted finally. Results Table 3 compares the results of this weighted voting system (with τ = 0, η = 0.17) and the baseline voting system, where the responses were judged based only on the answer string, ignoring the document ID. As we can see, the weighted voting system achieves 2.3% absolute improvement in F-measure over the baseline, at a 99.8% confi- Figure 1 summarizes the results of this weighted voting system with different threshold τ settings. When τ is raised, Precision continuously increases to around 1, while Recall gradually decreases to 0. In addition to improving overall performance, the confidence estimates can be used to convey to the user of slot filling output our confidence in individual slot fills. After the intermediate responses are combined by the above weighted voting system (setting τ and η as 0), we divide the range of confidence values (0 to 1) into 10 equal intervals (0 to 0.1, 0.1 to 0.2, and so on) and categorize these Table 4 shows the Pearson's r and average precision results for all intermediate responses, where RANKED ranks the responses based on their confidence values; RANDOM assigns confidence values uniformly at random between 0 and 1; WORSTCASE ranks all incorrect responses above all correct ones. Applying the features separately, we find that slot response length and response doc num are the best predictors of correctness. dpath length (the length of the shortest dependency path between query and response) is also a significant contributor. Among the features, only NE margin seeks to directly estimate the confidence of a pipeline component, and it makes only a minimal contribution to the result. Overall this shows that confidence can be predicted quite well from features of the query and response, their appearance in the corpus, and prior IE system performance, without modeling the confidence of individual pipeline components. Conclusion We have presented our Maximum Entropy based confidence estimation model for information extraction systems. The effectiveness of this model has been demonstrated in the challenging Knowledge Base Population Slot Filling task, where a weighted voting system achieves 2.3% absolute improvement in F-measure score based on the confidence estimates. A strong correlation between the confidence estimates in KBP slot fills and the correctness has also been proved by obtaining an average precision of 83.5% and Pearson's r of 54.2%. In the future, further experiments are planned to investigate more elaborate models, explore more interesting feature sets, and study the contribution of each feature through a more detailed and thorough analysis.",
         "11868924",
         "66b33c252aca322d12526c14c56d49af494be455",
         "7",
         "https://aclanthology.org/R13-1051",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Li, Xiang  and\nGrishman, Ralph",
         "Confidence Estimation for Knowledge Base Population",
         "396--401",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "li-grishman-2013-confidence",
         null,
         null
        ],
        [
         "21",
         "W05-0826",
         "We describe the Spanish-to-English LDV-COMBO system for the Shared Task 2: \"Exploiting Parallel Texts for Statistical Machine Translation\" of the ACL-2005 Workshop on \"Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond\". Our approach explores the possibility of working with alignments at different levels of abstraction, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on Word-Net which is used for unknown words.",
         "We describe the Spanish-to-English LDV-COMBO system for the Shared Task 2: \"Exploiting Parallel Texts for Statistical Machine Translation\" of the ACL-2005 Workshop on \"Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond\". Our approach explores the possibility of working with alignments at different levels of abstraction, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on Word-Net which is used for unknown words. Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. Many other authors have tried to do so. See (Och and Ney, 2000) , (Yamada and Knight, 2001) , (Koehn and Knight, 2002) , (Koehn et al., 2003) , (Schafer and Yarowsky, 2003) and (Gildea, 2003) . Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al., 1993) . Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit. Apart from redefining the scope of the alignment unit, we may use different degrees of linguistic annotation. We introduce the general concept of data view, which is defined as any possible representation of the information contained in a bitext. We enrich data view tokens with features further than lexical such as PoS, lemma, and chunk label. As an example of the applicability of data views, suppose the case of the word 'plays' being seen in the training data acting as a verb. Representing this information as 'plays V BZ ' would allow us to distinguish it from its homograph 'plays N N S ' for 'plays' as a noun. Ideally, one would wish to have still deeper information, moving through syntax onto semantics, such as word senses. Therefore, it would be possible to distinguish for instance between two realizations of 'plays' with different meanings: 'he P RP plays V BG guitar N N ' and 'he P RP plays V BG basketball N N '. Of course, there is a natural trade-off between the use of data views and data sparsity. Fortunately, we hava data enough so that statistical parameter estimation remains reliable. System Description The LDV-COMBO system follows the SMT architecture suggested by the workshop organizers. First, training data are linguistically annotated for the two languages involved (See subsection 2.1). 10 different data views have been built. Notice that it is not necessary that the two parallel counterparts of a bitext share the same data view, as long as they share the same granularity. However, in all our experiments we have annotated both sides with the same linguistic information. See token descriptions: (W) word, (WL) word and lemma, (WP) word and PoS, (WC) word and chunk label, (WPC) word, PoS and chunk label, (Cw) chunk of words (Cwl), chunk of words and lemmas, (Cwp) chunk of words and PoS (Cwc) chunk of words and chunk labels (Cwpc) chunk of words, PoS and chunk labels. By chunk label we refer to the IOB label associated to every word inside a chunk, e.g. 'I B−N P declare B−V P resumed I−V P the B−N P session I−N P of B−P P the B−N P European I−N P Parliament I−N P . O '). We build chunk tokens by explicitly connecting words in the same chunk, e.g. '(I) N P (declare resumed) V P (the session) N P (of) P P (the European Parliament) N P '. See examples of some of these data views in Table 1 . Then, running GIZA++, we obtain token alignments for each of the data views. Combined phrasebased translation models are built on top of the Viterbi alignments output by GIZA++. See details in subsection 2.2. Combo-models must be then postprocessed in order to remove the additional linguistic annotation and split chunks back into words, so they fit the format required by Pharaoh. Moreover, we have used the Multilingual Central Repository (MCR), a multilingual lexical-semantic database (Atserias et al., 2004) , to build a wordbased translation model. We back-off to this model in the case of unknown words, with the goal of improving system recall. See subsection 2.3. Data Representation In order to achieve robustness the same tools have been used to linguistically annotate both languages. The SVMTool 1 has been used for PoS-tagging (Giménez and Màrquez, 2004) . The Freeling 2 package (Carreras et al., 2004) has been used for lemmatizing. Finally, the Phreco software by (Carreras et al., 2005) has been used for shallow parsing. No additional tokenization or pre-processing steps other than case lowering have been performed. Special treatment of named entities, dates, numbers, 1 The SVMTool may be freely downloaded at http://www.lsi.upc.es/˜nlp/SVMTool/ . 2 Freeling Suite of Language Analyzers may be downloaded at http://www.lsi.upc.es/˜nlp/freeling/ currency, etc., should be considered so as to further enhance the system. Building Combined Translation Models Because data views capture different, possibly complementary, aspects of the translation process it seems reasonable to combine them. We consider two different ways of building such combo-models: LPHEX Local phrase extraction. To build a separate phrase-based translation model for each data view alignment, and then combine them. There are two ways of combining translation models: MRG Merging translation models. We work on a weighted linear interpolation of models. These weights may be tuned, although a uniform weight selection yields good results. Additionally, phrase-pairs may be filtered out by setting a score threshold. noMRG Passing translation models directly to the Pharaoh decoder. However, we encountered many problems with phrasepairs that were not seen in all single models. This obliged us to apply arbitrary smoothing values to score these pairs. GPHEX Global phrase extraction. To build a single phrased-based translation model from the union of alignments from several data views. In its turn, any MRG operation performed on a combo-model results again in a valid combo-model. In any case, phrase extraction 3 is performed as depicted by (Och, 2002) . Using the MCR Outer knowledge may be supplied to the Pharaoh decoder by annotating the input with alternative translation options via XML-markup. We enrich every unknown word by looking up every possible translation for all of its senses in the MCR. These are scored by relative frequency according to the number of senses that lexicalized in the same manner. Let w f , p f be the source word and PoS, and w e be the target word, we define a function  Scount(w f , p f , w e ) which counts the number of senses for (w f , p f ) which can lexicalize as w e . A translation pair is scored as: score(w f , p f |w e ) = Scount(w f , p f , w e ) (w f ,p f ) Scount(w f , p f , w e ) (1) Better results would be expected working with word sense disambiguated text. We are not at this point yet. A first approach could be to work with the most frequent sense heuristic. Experimental Results Data and Evaluation Metrics We have used the data sets and language model provided by the organization. No extra training or development data were used in our experiments. We evaluate results with 3 different metrics: GTM F 1 -measure (e = 1, 2), BLEU score (n = 4) as provided by organizers, and NIST score (n = 5). Experimenting with Data Views Table 2 presents MT results for the 10 elementary data views devised in Section 2. Default parameters are used for λ tm , λ lm , and λ w . No tuning has been performed. As expected, word-based views obtain significatively higher results than chunk-based. All data views at the same level of granularity obtain comparable results. In is performed. We refer to the W model as our baseline. In this view, only words are used. The 5W-MRG and 5W-GPHEX models use a combination of the 5 word-based data views, as in MRG and GPHEX, respectively. The 5C-MRG and 5C-GPHEX system use a combination of the 5 chunk based data views, as in MRG and GPHEX, respectively. The 10-MRG system uses all 10 data views combined as in MRG. The 10-GPHEX/MRG system uses the 5 word based views combined as in GPHEX, the 5 chunk based views combined as in GPHEX, and then a combination of these two combo-models as in MRG.  It can be seen that results improve by combining several data views. Furthermore, global phrase extraction (GPHEX) seems to work much finer than local phrase extraction (LPHEX). Table 4 shows MT results after optimizing λ tm , λ lm , λ w , and the weights for the MRG operation, by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) . Observe that tuning the system improves the performance considerably. The λ w parameter is particularly sensitive to tuning. Even though the performance of chunk-based models is poor, the best results are obtained by combinining the two levels of abstraction, thus proving that syntactically motivated phrases may help. 10-MRG and 10-GPHEX models achieve a similar performance. The 10-MRG-best W N system corresponds to the 10-MRG model using WordNet. The 10-MRGsub W N system is this same system at the time of submission. Results using WordNet, taking into account that the number of unknown 4 words in the development set was very small, are very promising. Conclusions We have showed that it is possible to obtain better phrase-based translation models by utilizing alignments built on top of different linguistic data views. These models can be robustly combined, significantly outperforming all of their components in isolation. We leave for further work the experimentation of new data views such as word senses and semantic roles, as well as their natural porting and evolution from the alignment step to phrase extraction and decoding. Acknowledgements This research has been funded by the Spanish Ministry of Science and Technology (ALIADO TIC2002-04447-C02). Authors are thankful to Patrik Lambert for providing us with the implementation of the Simplex Method used for tuning.",
         "81127",
         "c6a3e8627dd17293efa7c49d426befb52e0263fa",
         "16",
         "https://aclanthology.org/W05-0826",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Gim{\\'e}nez, Jes{\\'u}s  and\nM{\\`a}rquez, Llu{\\'\\i}s",
         "Combining Linguistic Data Views for Phrase-based {SMT}",
         "145--148",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "gimenez-marquez-2005-combining",
         null,
         null
        ],
        [
         "22",
         "2009.mtsummit-posters.22",
         "We have developed a web site called Minna no Hon'yaku (\"Translation for Everyone by Everyone\"), which hosts online volunteer translators. Its core features are (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor QRedit. Translators who use QRedit daily reported an up to 30 per cent reduction of the overall translation time. As of 3 July 2009, there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan.",
         "We have developed a web site called Minna no Hon'yaku (\"Translation for Everyone by Everyone\"), which hosts online volunteer translators. Its core features are (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor QRedit. Translators who use QRedit daily reported an up to 30 per cent reduction of the overall translation time. As of 3 July 2009, there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan. Introduction Online volunteer translators, who are involved in translating online electronic documents in their free time, translate a variety of documents every day, such as blogs, Wikipedia articles, open source software manuals, documents on nongovernmental organization (NGO) activities, and so on. These translations are read for pleasure, for practical purposes, for language learning, and many other reasons. Needless to say, volunteer translators contribute a great deal to the sharing and spreading of information around the world. Consequently, supporting their activities is a very important research issue. Volunteer translators translate a large number of documents everyday. However, they lack proper translation support tools (Abekawa and Kageura, 2007a) . Thus, providing a good supporting environment should be of great assistance in improving volunteer translators' efficiency and increasing the level of enjoyment they experience in translating. This is the motivation for our work in this paper. 2 Hosting volunteer translators Abekawa and Kageura (2007a) have developed a translation aid editor, QRedit, which has been experimentally provided to a limited number of volunteer translators. They report that QRedit is very effective for aiding their work, as described in Section 8. Based on the success of this translation aid editor, we have developed a web site called Minna no Hon'yaku (MNH, \"Translation for Everyone by Everyone\") where everyone uses QRedit and other translation support tools. In addition, documents translated on MNH are open to the public because these translations are assigned open licenses such as Creative Commons licenses. A screenshot of MNH is shown in Figure 1 . Currently, MNH hosts volunteer translators who translate Japanese (English) documents into English (Japanese), as QRedit currently only supports Japanese-English and English-Japanese translation directions. We plan to extend QRedit and MNH to other language pairs in our future work. We also plan to make QRedit and the software system of MNH open source. There are three reasons why we have developed MNH. First, we were inspired by the success of hosting services for open source software such as sourceforge.net. These services help engi- Our hope is that hosting services for volunteer translators could have a similar impact on the creation and distribution of translations. Second, hosting volunteer translators enables them to use translation support tools without installation efforts (or fees) because support tools such as QRedit are built-in functions of MNH (and everyone uses MNH for free). This situation sharply contrasts to the usual situation of volunteer translators in which they do not use translation aid systems for a variety of reasons, such as the fact that they are too expensive for personal use. (Abekawa and Kageura, 2007a) . Third, hosting volunteer translators means that their documents are saved and published on MNH. These translations are shared by translators and used to make a parallel corpus. Translators can search this parallel corpus for expressions translated by other translators. This is an important benefit of hosting volunteer translators, for if a large number of translators use MNH, a large parallel corpus will become available on the site. Consequently, translators will be able to share a rich parallel corpus for their translation activities. We opened MNH to the public on April 8 2009. We also asked several volunteer translation groups to use MNH as their translation platform in order to get feedback for improving MNH. We are now running MNH to see if MNH can attract and host many volunteer translators. 1 1 Currently, MNH only has a Japanese interface. We plan to In the following sections, we describe the details of MNH. MNH core features MNH has the following four core features: (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor QRedit. Blog-like look and feel We designed the look and feel of MNH to be similar to those of standard blogs, to make it as easy as possible for a wide variety of volunteer translators to use. For example, the top page of MNH (Figure 1 ) lists new translations, active translators, translations of Wikipedia articles, and so on. The three lines at the top part of the page list tags that are assigned by translators to their translations. By clicking on these tags, the corresponding translations are listed in the top page. The box in the upper right corner is the search box for documents, tags, translators, and so on. MNH also provides a place where translators can ask and answer questions on any topic. This mechanism is very useful for sharing translation experiences. When a translator, taro, logins to MNH, he is delivered to his personal page, as shown in Figure 2 , where he edits his translations. To translate a new document, he enters the URL of the document into the long box at the top of the page and clicks the button on the right side. This launches the translation aid editor QRedit, which he uses to translate that document. The details of QRedit are described in Section 7. When he finishes his translation, the translation is saved to his personal space. He may open his translation to the public after assigning a proper license to the document. Open translations are shared by translators (and others) as described in Section 5. Translators also share term lists which they enter into MNH. A screenshot of a term list is shown in Figure 3 . These terms are treated as public domain contents as described in the terms of service of MNH. Thus, anyone can use these term lists for any purpose. add an English interface by the end of this summer. Attribution This license lets others distribute, remix, tweak, and build upon the translators work, even commercially, as long as they credit the translator for the original creation. Attribution Share Alike This license lets others remix, tweak, and build upon the translator's work even for commercial reasons, as long as they credit the translator and license their new creations under the identical terms. Attribution Non-Commercial This license lets others remix, tweak, and build upon the translator's work non-commercially, and although their new works must also acknowledge the translator and be non-commercial, they don't have to license their derivative works on the same terms. Attribution Non-Commercial Share Alike This license lets others remix, tweak, and build upon the translator's work non-commercially, as long as they credit the translator and license their new creations under identical terms. Translators can also use other licenses that are similar to the Creative Commons licenses listed above. In this way, translators can legally share their translations on MNH. These shared translations are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003) . Currently, MNH has a simple bilingual concordancer as shown in Figure 4 . We plan to extend this concordancer in our future work because a bilingual concordancer is a very important tool for translation (Macklovitch et al., 2009) . High quality, comprehensive language resources Dictionaries and the web are the two main language resources that online volunteer translators use during translation. MNH, in cooperation with Sanseido, provides the \"Grand Concise English Japanese Dictionary\" (Sanseido, 2006) to translators. It has about 360,000 entries and is socially accepted as a standard and comprehensive dictionary. Consequently, translators will not need to use other dictionaries in most cases. We also plan to incorporate a comprehensive Japanese-English dictionary of names, collected from the web and consisting of about 400,000 entries (Sato, 2009) , in order to handle person names that are not covered in usual bilingual dictionaries. MNH also provides seamless access to the web, because the web is a very important resource for checking factual information. 2 For example, MNH provides a dictionary that was made from the English Wikipedia. This enables translators to reference Wikipedia articles during the translation process as if they are looking up dictionaries. MNH also provides a seamless connection to web searches as described in the next section. 7 Translation aid editor: QRedit About QRedit QRedit is a translation aid system which is designed for volunteer translators working mainly online (Abekawa and Kageura, 2007a) . Volunteer translators involved in translating online documents have a variety of backgrounds. Some are professional translators, some translate documents about topics they are interested in, while others translate as a part of their NGO activities. They nevertheless share a few basic characteristics: (1) They are native-speakers of the target language (TL). (2) Most of them do not have a native-level command in the source language (SL). (3) They do not use a translation aid system or machine translation (MT) system. (4) They want to reduce the burden involved in the process of translation. (5) They spend a great deal of time looking up reference sources. (6) The smallest basic unit of translation is the paragraph and \"at a glance\" readability of the SL text is very important. The philosophy of QRedit reflects these characteristics and can be summarized as follows: (R1) To reduce the time it takes for translators to do what they are currently doing, rather than to add new functions; (R2) To provide sufficient information sources so that translators do not have to look elsewhere; (R3) To provide information to facilitate decisionmaking by translators; (R4) To provide information that triggers translators' creativity; (R5) To make the interface as simple as possible. These requirements stem from the requests made by volunteer translators (Abekawa and Kageura, 2007a) . QRedit was designed to meet these requirements. For example, it uses the high quality, comprehensive language resources described in Section 6, based on R2. It also provides a seamless connection to web searches, based on R4. Further, based on R1 and R3, QRedit does not provide an MT function, for the following reasons: (1) In terms of the quality of translation, MT results are far behind human translations for Japanese-English (or English-Japanese) translation. As a result, using MT results does not contribute to the reduction of the overall translation time.  (2) Given input texts, MT systems produce output translations without human intervention. This situation contradicts R3. Even if MT outputs are good translations, for now most translators are not ready to accept MT results as reliable, because they do not regard computers as communication partners. Thus, they want to decide on translations and control translation process by themselves. Finally, we made the interface of QRedit as simple as possible to meet R5. In the following sections, we introduce major features of QRedit. Automatic word lookup When a URL of an SL text is input into QRedit, it loads the corresponding text into the left-hand panel, as shown in Figure 5 . (Users can also copy-andpaste the SL text.) Then, QRedit automatically looks up all words in the SL text using the dictionaries described in Section 6. When a user clicks on an SL word, its translation candidates are displayed in a pop-up window. The user can paste a translation candidate into the right-hand panel, which is used for writing the translation, as described in Section 7.3. Idiom lookup In addition to the single word lookup method, QRedit has a flexible multi-word unit lookup function (Takeuchi et al., 2007) . For example, QRedit automatically looks up the dictionary entry \"with one's tongue in one's cheek\" for the expression \"He said that with his big fat tongue in his big fat cheek\" or \"head screwed on right\" for \"head screwed on wrong\". This function is a major advantage of QRedit compared to other MT systems or computer-aided translation (CAT) systems. Indeed, this function has not been realized in any English-Japanese MT system we have checked, and while some CAT systems realize similar functions through approximate matching, they do not specifically target the look-up of idioms with their variations. The lack of flexible idiom lookup in other systems does not mean that this function is not needed. In fact, there is a pressing need for it. The importance of this function derives from two factors: (a) many translators, even experienced ones, have relatively less knowledge of idioms than of words; and (b) some idioms may not be identified as such by translators, because they make sense without an idiomatic interpretation. This leads to translation mistakes. The flexible multi-word unit lookup function of QRedit helps translators identify idioms, and thus reduce translation mistakes. Stratified term emphasis Because idioms are often missed by translators, QRedit notifies translators of the existence of idioms or terms that have special meanings. To notify users of the existence of such terms, it is good to highlight the terms with text decoration. But too many highlighted terms reduces the readability of source language texts dramatically. This is a serious problem, because the richer the reference sources become, the greater the number of candidates for notification. To resolve this problem, QRedit adopts a stratified term emphasis method, which distinguishes three user awareness levels depending on the type and nature of the reference unit, or the candidate term for notification. These awareness levels are reflected in the way the reference units are displayed, such as a change of background color or underlining. These levels are decided according to a four criteria: \"composition,\" \"difficulty,\" \"specialty\" and \"resource type.\" See Figure 5 for an example and refer to (Abekawa and Kageura, 2007b) for details of this method. Lookup that doesn't disturb translation When users click on a term in the left-hand panel, its translation candidates are displayed in a pop-up window, as shown in Figure 5 . This action does not affect keyboard operation. That is, users can still write their translations continuously without moving the mouse cursor back to the right-hand panel, as whatever they do with the mouse, the keyboard cursor always stays in this panel. This function helps translators concentrate on translation, as the interviews in Section 8 show. If translators had to reset the cursor every time they looked up the dictionaries, it would break the rhythm of their work. QRedit helps them maintain their rhythm and focus on their translation activities. Functions associated with lookup Translators can call up the following four functions for each term and each translated candidate in a popup window, as shown in Figure 5 . • Paste Paste a term or translation term to the righthand panel. This helps the translator avoid misspelling of long named entities or numerical representations. • Detailed lookup Display complete dictionary entries for the term: attributes (domain, nuance, etc.), relative information (derivative words, antonyms, etc.), and example sentences. These entries are displayed in another window because of the large amount of information. • Web search Display search results from a web search engine. This help translators confirm in which context the term is used or how to use the term. • Register term Register a term to user's term list. Note that users can look up registered terms in QRedit. Lookup by keyboard To display a pop-up window, translators click on terms with their mouse. But mouse operation can be an obstacle to efficient input of TL texts. Therefore, in QRedit, translators can look up terms by using only their keyboards. While inputting TL texts, translators can activate a keyboard incremental search method. Figure 6 shows an example of an incremental search when the user has input the first character 's'. Customization The environment for inputting TL texts can greatly impact the efficiency of translation work. In addition, an optimal environment differs from translator to translator. So, a flexible customization method that meets the needs of various translators is required. In QRedit, users can customize the following factors in their translation input environments. • Placement of the SL area [left, right, top, bottom] • Width or height of the SL area • Placement of translation candidate display [inside, left, right] of the SL area • Synchronized scroll [both directions, source→target, target→source, none] Figure 7 shows an example of a customized QRedit window. (See Figure 5 for a default window.) User response As of 3 July, 2009 -three months after we made MNH and QRedit publicly available -there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan. As quantitatively evaluating the benefit of using translation aid systems is technically a difficult task (cf. (Macklovitch, 2006) ), and as we are dealing with volunteer translators who are not working on a \"time is money\" basis but rather wish to reduce the subjective burden of translation, we carried out qualitative evaluations through e-mail and phone interviews with three users. They are trial users who have been using QRedit since before it was made public, thus have enough experience to give informed judgment on the system. According to them, without QRedit, the division of time for the different elements of translation is roughly: 10 to 30 per cent for dictionary lookup, 10 to 40 per cent for searching the web to obtain information, 20 to 50 per cent for draft translation, and 0 to 10 per cent for revision and editing. All in all, around one fourth to half of the time is used for dictionary and web lookup. Translator A, a novice translator with two years' experience, said that she feels it takes her an average of 20 per cent and a maximum of 30 per cent less time to complete translations using QRedit, with the clear additional benefit that the quality of draft translations is improved because she can concentrate on context/paragraph reading rather than dictionary lookup in making draft translations. Translator B, a middle level translator with three years' experience, also reported an average 25 per cent to 30 per cent reduction in the overall translation time, with the same effect on quality as translator A. Translator C, who is an expert translator, reported that she prefers the environment she is familiar with for translating easier texts, but can reduce translation time by 20 per cent when dealing with other texts. She said that QRedit enables her to tackle a wider variety of texts than before. We have not yet been able to obtain responses from group users, because they have only started using QRedit recently and thus have not accumulated sufficient experience to give an informed judgment on the system. We also have not been able to evaluate the usability of MNH because it is still under development and we are now improving its usability based on comments and suggestions from users. Related work Related work can be roughly classified into three types, i.e. projects that aim at translation and publishing translated documents; work that is concerned with hosting translated documents, often multilingually; and work that is addressed at aiding translators and translation communities. There are too many joint or collaborative online translation projects to mention here. GlobalVoices Online 4 is perhaps one of the most well known, along with TUP 5 (Translators United for Peace). Most projects do not provide translation aid facilities or collaborative working environments. They are rather projects defined by interested groups of people, using existing facilities. An example of the second category is Yakushite.net (Shimohata et al., 2001) . It provides a collaborative translation environment in which users can use MT for translation, while contributing to collaborative terminology augmentation for the improvement of MT. Except for providing the MT engine, the translation aid functions are weak. Worldwide Lexicon (McConnell, 2007) is another example. Within the project a variety of mechanisms are provided that facilitate the sharing of translated documents world wide, with which one can (i) detect translated texts, if there are any; (ii) translate by oneself; (iii) subscribe to an RSS feed for translation; and (iv) use machine translation. The system, however, does not provide rich facilities to aid human translations (step (ii)). As such, the system is more a hosting service rather than a translation aid system. We are also witnessing the rapid growth of Wikibased platforms to facilitate collaborative translations, such as Traduwiki (related projects are listed in http://wiki-translation.com/ tiki-index.php) and BEYtrans (Bey et al., 2008) . They provide functions that support collaboration among translators, as well as providing a translation memory function, thus having the features of translation aid systems as well as translation hosting functions. Turning our eyes to fully-fledged translation aid systems, there are several commercial and noncommercial systems. SDL Trados 6 is one of the most well known and widely used. SDL also developed Idiom WorldServer system 7 , an online multilingual document management system with translation memory functions. There are many other systems such as TransType (Macklovitch, 2006) and the free translation memory and terminology management system Omega-T 8 . Though it is now a little dated, Gow (2003) evaluates different translation aid systems. The functions that MNH provides are closer to those provided by Idiom WorldServer or Wiki-based collaborative translation aid systems, but MNH provides a high-quality bilingual dictionary and functions for seamless Wikipedia and web searches within the integrated translation aid editor QRedit, thus providing connections to the existing reference information infrastructure that online translators use. This reflects the fact that the main target of MNH is online documents available under the Creative Commons licenses. On the other hand, collaborative, project-oriented document management functions are weak in MNH, because MNH basically assumes use by individual translators or smaller groups. Implementing this functionality will be a future task. Conclusion We have developed a web site called Minna no Hon'yaku (MNH, \"Translation for Everyone by Everyone\"), which hosts online volunteer translators. Its core features are (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor; QRedit. Translators who use QRedit daily reported an up to 30 per cent reduction of their overall translation time. We are now running MNH to see if it can attract and host many volunteer translators. Please join MNH!",
         "14565746",
         "e12f5b1510741059062bce873e9083681ffa98eb",
         "9",
         "https://aclanthology.org/2009.mtsummit-posters.22",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Utiyama, Masao  and\nAbekawa, Takeshi  and\nSumita, Eiichiro  and\nKageura, Kyo",
         "Hosting Volunteer Translators",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "utiyama-etal-2009-hosting",
         null,
         null
        ],
        [
         "23",
         "2020.lrec-1.386",
         "As part of constructing the NINJAL Parsed Corpus of Modern Japanese (NPCMJ), a web-accessible language resource, we are adding frame information for predicates, together with two types of semantic role labels that mark the contributions of arguments. One role type consists of numbered semantic roles, like in PropBank, to capture relations between arguments in different syntactic patterns. The other role type consists of semantic roles with conventional names. Both role types are compatible with hierarchical frames that belong to related predicates. Adding semantic role and frame information to the NPCMJ will support a web environment where language learners and linguists can search examples of Japanese for syntactic and semantic features. The annotation will also provide a language resource for NLP researchers making semantic parsing models (e.g., for AMR parsing) following machine learning approaches. In this paper, we describe how the two types of semantic role labels are defined under the frame based approach, i.e., both types can be consistently applied when linked to corresponding frames. Then we show special cases of syntactic patterns and the current status of the annotation work.",
         "As part of constructing the NINJAL Parsed Corpus of Modern Japanese (NPCMJ), a web-accessible language resource, we are adding frame information for predicates, together with two types of semantic role labels that mark the contributions of arguments. One role type consists of numbered semantic roles, like in PropBank, to capture relations between arguments in different syntactic patterns. The other role type consists of semantic roles with conventional names. Both role types are compatible with hierarchical frames that belong to related predicates. Adding semantic role and frame information to the NPCMJ will support a web environment where language learners and linguists can search examples of Japanese for syntactic and semantic features. The annotation will also provide a language resource for NLP researchers making semantic parsing models (e.g., for AMR parsing) following machine learning approaches. In this paper, we describe how the two types of semantic role labels are defined under the frame based approach, i.e., both types can be consistently applied when linked to corresponding frames. Then we show special cases of syntactic patterns and the current status of the annotation work. Introduction Semantic role labeled data is important to capture predicate-argument sentence level meaning for NLP researchers, linguists, and language learners. For English, various kinds of annotated corpora with semantic roles and frames are provided (e.g., PropBank (Bonial et al., 2010) and FrameNet (Baker et al., 1998) ) and they are widely used. The numbered semantic roles (e.g., Arg0, Arg1, Arg2) proposed in PropBank are effective for adapting to various systems of semantic roles. Thus, the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) approach, which is based on PropBank numbered semantic roles and frames, has attracted a lot of attention from NLP researchers as a model for describing sentence-level semantics. AMR parsing models are studied with deep neural network models, e.g., (Zhang et al., 2019) using AMR resources for English 1 . In terms of Japanese language resources, several corpora containing annotated tags related to predicateargument information have been proposed (Kyoto (Kawahara et al., 2002) , NAIST (Iida et al., 2007) , EDR (EDR, 1995) , GDA (Hashida, 2005) , Japanese FrameNet (Ohara et al., 2006) , BCCWJ-PT (Takeuchi et al., 2015) ). However, numbered semantic roles have not been annotated for Japanese texts. Also, none of the annotated corpora that are reported in the literature are web accessible because of licensing restrictions 2 . 1 LDC2017T10 and LDC2014T12. 2 Japanese FrameNet is constructed on the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa et al., 2014) , from which it inherits release issues. Thus, the annotated sentences are not available on the website: http://sato.fm.senshu-u.ac.jp/frameSQL/jfn23/notes/index2.html (accessed 2019/11/22). In this research project, we annotate two types of semantic role labels and frames of predicates from the NINJAL Parsed Corpus of Modern Japanese (NPCMJ), which is a freely available web-accessible treebank (NINJAL, 2016) 3 . The semantic roles and frames are annotated based on a freely available web-accessible thesaurus of predicateargument structure for Japanese (PT) 4 . Utilizing the tree structure of the NPCMJ, we can focus directly on the problem of labeling for the semantic roles and frames of target predicates. This is because the identification of target predicates and their arguments can be automatically derived from the treebank following (Horn et al., 2018) . The first type of annotation for semantic roles is PropBankstyle, that is, numbered semantic roles. The second type involves names for semantic roles (e.g., Agent, Theme, Goal), which has been used in PT. The reason why we employ the conventional named semantic roles is that the named semantic roles are intuitively more understandable for humans compared to the numbered roles. Thus, the named roles are expected to be used in queries when language learners or linguists want to extract example sentences that, e.g., contain \"Experiencer\" arguments in the emotional sense, or \"Source\" arguments in the moving sense. Contributions of this paper are as follows: (1) For NLP researchers, linguists, and Japanese language learners, we propose a dual semantic role annotation that is composed of: (i) numbered semantic roles, and (ii) semantic roles with conventional names. (2) The proposed numbered semantic roles are framebased so that roles can be assigned consistently with regards to their corresponding frame. (3) We reveal that the frame-based semantic roles are suitable for Japanese because of dividing transitive and intransitive verbs. (4) We show that the current annotated predicates are about 10,000, and reveal highly promising accuracy results from a semantic role annotation system with a neural network model. Frame-Based Semantic Roles 2.1. Frame-based Semantic Roles Are Needed in an Agglutinative Language Such as Japanese With semantic role annotation, the aim is to fix the semantic relations between arguments and predicates in a manner that is abstracted away from differences between case markers and syntactic alternations. One of the difficulties of annotating semantic roles with names involves settling on a system of names, such as Agent, Theme, and Patient. Several different systems of semantic roles are proposed in previous studies (Fillmore, 1968; Palmer et al., 2010; Loper et al., 2007; Baker et al., 1998) . By contrast, the numbered semantic roles proposed in PropBank can absorb these differences, because numbered arguments can be consistently assigned in example sentences for the target verb sense. In Japanese, some verbs, e.g., 'hirak-u' (open), behave as both intransitive and transitive verbs without any intransitivizing or transitivizing suffix 5 . (5) a. Intransitive and transitive pairs similar to 'ak-u' and 'ak-eru' above are registered as different lexical units in morphological analyzers (e.g., MeCab 6 ) as well as Japanese dictionaries even though they share the same stem. However, they share the same semantic role set. This is why we need to assign the same semantic role set and frame (that is, frame-based semantic roles) to each verb. Thus, we define a frame that indicates a shared concept of predicates that belong to the frame, and then the frame unites a consistent semantic role set. Each verb meaning of polysemous verbs is fixed by adding example sentences. Since a polysemous verb has several meanings, each verb meaning is connected to a distinct frame. For example, the verb 'hirak-u', shown in (5), has another meaning as in (7). The meaning of 'hirak-u' in ( 7 ) is assigned to the development frame to which verbs such as 'kaitaku-su-ru' (cultivate) and 'kaihatsu-su-ru' (develop) are also assigned. PT: Repository of Semantic Roles and Frames for Japanese Predicates As a base repository of semantic roles and frames for Japanese predicates, we use PT. PT is composed of hierarchical frames for predicates and each frame indicates a shared meaning of predicates whose sense is designated with semantic-role-annotated example sentences. In previous work (Takeuchi et al., 2010) , PT was constructed for about 11,000 Japanese predicates, with about 23,000 annotated example sentences. The semantic roles in PT have conventional role names (e.g., Agent, Theme, Goal) 7 . Thus, by adding PropBank-style semantic roles to the example sentences in PT, we utilize PT as a repository of frames containing frame based semantic roles that are both numbered and named for Japanese predicates. Figure 1 shows an example of how the verb 'hirak-u' is registered in PT. As a polysemous verb, 'hirak-u' has open, develop, and start meanings. Each word meaning has an example sentence that is annotated with both numbered semantic roles and named semantic roles. The semantic roles are Arg1 and Theme in all of the example sentences 8 in Figure 1 . !\"#$%&'(# )*#+%&'*,%(#-. '/01,'/#\"0 2%\"'/0 2%\"'/0 $3'43,,,2%\"'/0 5\"67 82#-# )93-#3:#,3;#:9,(2#,$33\". 6#:<'43,,2%\"'/0 5\"67 82#-# )93-#3:#,&'*(%='(#9 (2#,>#9(#\"*':$. 2':'<'43,2%\"'/0 5\"67 82#-# )93-#3:#,9('\"(9,(2#,?*3>#\" 923;. @%??#\"#:(,-#':%:69 3?,;\"#$%&'(# !\"#$ %&'$(#)!*)+,',# 0; ,(3 ,A ,2 %# \"' \"& 2< 82#9'0\"09,3?,?\"'-#9 /'%('/090\"0 /'%2'(9090\"0 2%\"'/0 -#.#/!\"0#$, !12#%,)%&'$(# +,'3,)#$- +,'3, /%6<3090\"0 /'%(#:90\"0 2%\"'/0 BCD7 BEAF BEGG Figure 1: Example of how a polysemous verb is registered in PT, where PT is the repository of semantic roles and frames. Then each example sentence is linked to a frame in the thesaurus. For example, the meaning of 'hanaya o hirak-u' (someone starts the flower shop) is assigned to start 9 whose frame ID is 366. The start frame also contains 'kigyoosu-ru' (start new venture) and 'kaiten-su-ru' (open a shop). The structure of frames is a thesaurus. There is no multiple inheritance in the thesaurus of frames. Such structure is similar to a synset in WordNet. Frame-based numbered semantic roles are convenient to capture the same type of arguments with different named roles for different verbs in the same frame. For example, both predicate verbs 'oros-u' (get down) and 'ochi-ru' (fall) in ( 8 ) and ( 9 ) belong to the moving from frame. In the moving from frame, the mover of the moving event is annotated as Arg0 and Agent. This raises concern for (9), where it is 'chichi' (father) who is moved. For such a 8 Subjects (i.e., Arg0) are often omitted in Japanese, even when the verb is transitive. 9 To be exact, this is the change of state/start end/start frame. For simplicity, we often omit to write the hierarchy of the frame. case, 'chichi' is annoated with the Experiencer named semantic role because 'chichi' is moved without intending to be moved. According to the annotation guidelines of Prop-Bank (Bonial et al., 2010) , Experiencer is typically Arg0 in numbered roles. However, the numbered role Arg1 is usually assigned to the Patient argument, i.e. \"the argument which undergoes the change of state or is being affected by the action\". Consequently, 'chichi' is annotataed as Arg1 in (9), so as to be consistent in the meaning of something which moves, i.e., 'hon' (book) and 'chichi' (father) in the sentences. On the other hand, named semantic roles are helpful for understanding the meanings of arguments. The named semantic roles are annotated across frames, and thus the same type of arguments can be extracted, while arguments numbered Arg2 or with a higher number depend on each frame 10 . In ( 10 ) and ( 11 ), even though the frames are different, 'kangoshi' (nurse) and 'asshoo' (big win) are a complement of the argument with the accusative case marker. For the arguments, the named semantic role is Complement (ACC) that indicates a complement of the argument with accusative case. Framework of Annotation Task As mentioned in the previous section, the repository of semantic roles and frames (i.e., PT) is web-accessible, and has search functionality so annotators can look up example sentences in PT. The annotation task is carried out by annotators under the guidance of a supervisor. Figure 2 gives an overview of the annotation task. The procedure of annotation of semantic roles and frames on the NPCMJ is as follows: 1. Target predicates and their arguments are extracted by a program (Treebank Semantics) (Butler, 2019) . This works by converting constituency tree annotations into logic based meaning representations from which 10 From the standpoint of Construction Grammar (Goldberg, 1995) , the named semantic roles and the numbered semantic roles can be regarded as 'argument roles' and 'patient roles', respectively. The named semantic roles are annotated according to syntactic expressions of arguments for verbs, while the numbered semantic roles are annotated based on frames. predicate-argument information can be extracted and remapped back into the tree structures of the NPCMJ for identifying target predicates and their arguments. 2. Annotators select the target sentence, look up a target predicate for an example, find the most appropriate frame for the example, and then assign the frame ID number to the target predicate. 3. Then, annotators assign semantic roles to the arguments according to the examples in PT. 4. If annotators find cases that are missing from PT or annotators are not confident, annotators write a report. 5. The supervisor adds new examples to PT or has discussions with annotators according to their reports 11 . In step 2, annotators can see the dependencies of the target sentence on the NPCMJ server (Figure 3 ). Since the NPCMJ is a web-accessible treebank, annotators can also look at the tree view of the target sentence from the same webpage (Figure 4 ). As can be seen in Figure 2 , annotators input frame IDs and semantic roles directly into the NPCMJ server, then the annotated results can be checked by all the other annotators and the supervisor. The frames of PT were constructed on the basis of the Lexeed database (Fujita et al., 2006) . This database is a sense 11 Currently there are three annotators working on the project. Discussions with the supervisor are used to settle all cases where annotators are not confident. repository for a rule-based machine translation system. It follows that the frames of PT have a wide coverage for frequent predicates. However, there remain words and word meanings that have not been registered. Adding new words or example sentences to PT is limited to the supervisor only so as to maintain the consistency of the dictionary. A problem with this setup is that the operation of extending PT could become a bottleneck for the annotation. !\"\"#$%#%&'()& &*+,-.&) /0123#4%&&#5+67 !66$4+4$%) \"&-&6\"&689#\"+4+ +66$4+4&\"#\"+4+ /0123#)&%'&% !\"\"#%$.&)#+6\"#:%+,&) 0; <*+,-.&)#$:#)&,+64(8 %$.&)#+6\"#:%+,&) =$$7#>- )>-&%'()$%# ?&-$%4# >6+66$4+4&\" 8+)&) 0%$5.&, %&-$%4) \"()8>))($6) ;+)7#$:#+66$4+4$%) ;+)7#$:#)>-&%'()$% Examples of Complicated Annotations With the annotation task, we found syntactic and grammatical variations because, as a corpus, the NPCMJ is composed of texts from very diverse genres, such as white papers, novels, speech dictations, news papers, and so on. Here we show some complicated cases of semantic role annotations. Causative form As described in Section 2.1., some transitive verbs have corresponding intransitive verbs. Both verbs can take the causative form. In PT numbered arguments and named roles are assigned on the basis of active voice. Note that causative forms of intransitive verbs exhibit structure similar to transitive verbs. Passive form Japanese has several special grammatical forms in the passive voice. The first one is the adversative passive (Wierzbicka, 1979) , that is, the passive form for an intransitive verb. Light verbs Japanese often uses the light verb construction. For this construction, there is an argument that works as the predicate, and so this argument that provides the predicate content is withdrawn from being assigned a semantic role. For example, consider (18). ( 18 In ( 18 ), the target verb 'shi-ma-su' (do) is a light verb. The content of the action is expressed by the deverbal noun 'oshaberi' (chatting). For this light verb case, the Arg PRX tag is assigned to the content argument 'oshaberi o'. This follows the treatment proposed in the PropBank guidelines (Bonial et al., 2010) . However, the composed meaning of the light verb construction, i.e., the chat frame and the verb 'oshaberi-sur-u' (chat-do-PRS) are assigned to the light verb 'shi-ma-su'. Thus the role sets of the chat frame are applied to the arguments and adjuncts except for 'oshaberi o' (ArgM PRX). Current Annotation Results In this section we discuss the current state of annotation. First, we show the statistics of annotated numbers of sentences, target predicates, types of predicates, and semantic roles. Second, we apply the annotated corpus to a machine learning system to estimate roughly the quality of annotated semantic roles with comparison to previous studies. Currently we are completing the first round of annotation that we plan to review. Statistics of Annotated Data Preliminary Experiments of Semantic Role Labeling Using Deep Learning Model Motivation Previous work has constructed a semantic role labeling system with neural network models for Japanese (Okamura et al., 2019) . The target data is BCCWJ-PT, where data is annotated with the semantic roles defined in PT. While the sentences in BCCWJ-PT are different from those of the NPCMJ, we can roughly estimate the quality of the named semantic roles in the NPCMJ by comparing the accuracy of a semantic role labeling system using the NPCMJ to the case of using BCCWJ-PT. Semantic role labeling system The input of the semantic role labeling system is the target predicate and morpheme sequence of its argument (or adjunct), and then the output is a semantic role label of the argument. All of the morphemes are converted to d dimension vectors with nwjc2vec, which gives Japanese word vectors 13 . Let X be a vector sequence of an input morpheme sequence, that is, X = x 1 , . . . , x t , . . . , x T (x t ∈ R d ). Let S be output of a semantic role label for input X, the estimated semantic role label is defined by Formula (1). Ŝ = argmax j∈Sem p(S j |X) (1) Where, Sem is a set of semantic role labels, and S j (j = 1, . . . , Sem) is the jth semantic role label. Let y j be an output of the jth unit at the final output layer of the neural network model. Since the softmax function is used as a non-linear function at the output layer, y j can be a probability. y j = p(S j |X). (2) Then Ŝ can be estimated by using a neural network. As a neural network model, we apply bi-directional GRU with the max-pooling model (hereafter referred to as the bi-GRU model) to the semantic role labeling because the bi-GRU model gave the best performance in the previous study of Okamura et al. (2019) . Figure 5 shows the 13 http://nwjc-data.ninjal.ac.jp/ !\"#$%& !\"'\"#\" (\")))))))))))))) !\"%\" !$%$ *+ @$A6)=%\"&516) !2' \"B))))))))))))))) $C B%\"# DDC@6E)@$C)=%\"&516)C2(<6))DC@6E)F$C)$)!2'E -\"%0@6#6C)2()=@6)$%3&#6(=G$'H&(<= 8$%36=) 0%6'2<$=6 Figure 5 : Bi-directional GRU with max-pooling model architecture of the bi-GRU model. An input vector sequence X is applied to the input of bi-directional GRU, and then the max-pooling is applied to outputs of the GRU with time sequence direction. The final output y = [y 1 , . . . , y j , . . . , y Sem ] is obtained after applying a fullyconnected layer to the results of max-pooling. Experimental setup The number of hidden units of GRU is 256. The settings of the optimizer are set the same as in Okamura et al. (2019) . The annotated data is divided into 65%, 5%, and 30% for training, development, and test data, respectively. The performance is evaluated by the accuracy of the test data. Accuracy = # Estimated semantic roles are correct # All instances (3) Experimental results Table 4 shows the total accuracies of numbered and named semantic role labels. The accuracy of the third column shows the results for BCCWJ-PT (Okamura et al., 2019) . According to the accuracies of named semantic roles in Ta-NPCMJ BCCWJ-PT Numbered semantic roles 0.716 N/A Named semantic roles 0.667 0.702 Table 4 : Total accuracy of semantic roles ble 4, the accuracy of the model using NPCMJ is near to that of BCCWJ-PT. BCCWJ-PT was annotated with two annotators for each semantic role as well as being checked by a third annotator, and so the quality of annotated semantic roles is expected to be high. The above results indicate that for the current annotated semantic roles of the NPCMJ the consistency of the annotated tags is promising for a resource that is under development. Comparing numbered and named semantic roles in their accuracy, numbered semantic roles are higher than named semantic roles. The result can be considered to show that the numbered semantic roles are annotated more consistently than the named semantic roles. Comparing the accuracy of numbered semantic roles to the results for the shared task in English semantic role labeling tasks, we have to improve our semantic role labeling models. Discussions One of the difficulties of annotating semantic roles is to discriminate between arguments and adjuncts, while keeping frame consistency in PT. Arguments are the essential factors for the defined frame, while adjuncts are not core elements for a frame. In principle, adjuncts can be attached to any frame. As described in Section 2.1. we added Arg0, Arg1, etc. tags to the previously defined named semantic roles in PT for the frame repository. However, because of the lack of variation in the example sentences, we have found arguments to be missing. This applies especially to arguments that are inserted as parts of constructions. Consider (19), and the need for a create frame that contains the meaning of the verb 'kak-u' (write). For (19), we need to define two essential semantic roles, namely: Arg0 (Agent, writer) and Arg1 (Theme, written thing). ( The recipient 'imooto-ni' (sister DAT) must be part of a construction (Goldberg, 1995) . Thus, the recipient can be considered as an argument because the recipient appears depending on the create frame. We can also confirm the semantic role of the above 'recipient' case by looking at the corresponding examples in English PropBank and FrameNet. In the frameset write.01 of PropBank, the corresponding roles are defined as A2 (benefactive), that is an essential argument. In FrameNet, the frames are more detailed than our frames in PT. The meaning of the above case, i.e., 'write' in English, is assigned to the Contacting frame. In the Contacting frame, the recipient role is defined as the Addressee role that indicates a core role (i.e., an essential argument). Thus both English language resources offer an analysis that is the same as our analysis for the Japanese data. Next, consider (21) 14 , which is another example that is not registered in PT. Frame: create '(They) write it as \"Shomyo\" in the old kanji style' (5 wikipedia KYOTO 11) We are currently investigating whether the two phrases ('kyuujitai de' and 'shoomyoo to') are arguments and/or adjuncts for the create frame of PT. According to FrameNet, the verb 'write' can belong to the Statement, Text creation, Contacting and Spelling and pronouncing frames. The create in PT can partially correspond to the Text creation frame. However, it is the Spelling and pronouncing frame that seems to correspond to (21). In the Spelling and pronouncing frame, 'kyuujitai de' might correspond to Manner and 'shoomyoo to' to Formal realization; and both roles are defined as core roles. This is suggestive evidence that FrameNet has analyzed examples that can be expected to cover Japanese examples too, even though Japanese has different syntactic and grammatical characteristics when compared to English. Thus we think it will be helpful to refer to existing language resources, especially PropBank and FrameNet, in revising framesets in PT. Such comparisons are only possible because there are web-accessible frame data sets, notably, PropBank and FrameNet. Thus, we believe that providing the NPCMJ and the frame repository PT as web-accessible resources is essential for being able to construct reliable semantic role labeled language resources for Japanese and beyond. Conclusion This paper has described ongoing research of constructing an annotated corpus of semantic roles and frames as an addition to the NPCMJ, a Japanese web-accessible parsed corpus. The annotation task is coupled with the expansion of PT, that is, the repository of semantic roles and frames. We annotate two types of semantic roles, i.e., numbered and named semantic roles for the arguments. Both the numbered semantic roles and the named semantic roles are defined consistently with respect to frames. Then numbered semantic roles are expected to be used in NLP, and the named roles are to be used as tags for searching example sentences by language learners and linguists. In the annotated texts, we have found various kinds of syntactical and grammatical variations: e.g., adversative passives, alternations of cases, and collocations. We also applied part of the annotated corpus into a semantic role annotation model based on neural networks to evaluate how the annotated corpus can contribute to the statistical learning model approach. The results show that the accuracies of semantic role annotation systems are almost the same as the current quality of named semantic roles, for results near to the achievements of previous work. Thus, we can estimate that the quality of the annotated semantic roles are highly promising. What is currently annotated is the first part of a planned annotation cycle, and so all is due for review as we also continue to annotate new texts from the NPCMJ. The NPCMJ is planned to increase in size to 60,000 sentences. Acknowledgements A part of the research reported in this paper is supported by the NINJAL project \"Development of and Research with a parsed corpus of Japanese\" through the NINJAL collaborative research project \"Annotating Semantic Role Labels and Frames to NPCMJ\", and by JSPS KAKENHI (Grant Number JP 15H03210 and 19K00552) .",
         "218974487",
         "df3027b9d07d2bbfdb0739a9f0592b3d80e23ce7",
         "0",
         "https://aclanthology.org/2020.lrec-1.386",
         "European Language Resources Association",
         "Marseille, France",
         "2020",
         "May",
         "Proceedings of the 12th Language Resources and Evaluation Conference",
         "Takeuchi, Koichi  and\nButler, Alastair  and\nNagasaki, Iku  and\nOkamura, Takuya  and\nPardeshi, Prashant",
         "Constructing Web-Accessible Semantic Role Labels and Frames for {J}apanese as Additions to the {NPCMJ} Parsed Corpus",
         "3153--3161",
         null,
         null,
         null,
         null,
         null,
         "979-10-95546-34-4",
         "inproceedings",
         "takeuchi-etal-2020-constructing",
         "English",
         null
        ],
        [
         "24",
         "R13-1052",
         "We look into the problem of recognizing citation functions in scientific literature, trying to reveal authors' rationale for citing a particular article. We introduce an annotation scheme to annotate citation functions in scientific papers with coarse-to-fine-grained categories, where the coarse-grained annotation roughly corresponds to citation sentiment and the finegrained annotation reveals more about citation functions. We implement a Maximum Entropy-based system trained on annotated data under this scheme to automatically classify citation functions in scientific literature. Using combined lexical and syntactic features, our system achieves the F-measure of 67%.",
         "We look into the problem of recognizing citation functions in scientific literature, trying to reveal authors' rationale for citing a particular article. We introduce an annotation scheme to annotate citation functions in scientific papers with coarse-to-fine-grained categories, where the coarse-grained annotation roughly corresponds to citation sentiment and the finegrained annotation reveals more about citation functions. We implement a Maximum Entropy-based system trained on annotated data under this scheme to automatically classify citation functions in scientific literature. Using combined lexical and syntactic features, our system achieves the F-measure of 67%. Introduction Citations in scientific papers serve different purposes, from comparing one work to another to acknowledging the inventor of certain concepts. Recognizing citation functions is important for understanding the structure of a single scientific document as well as mining citation graphs within a document collection. Therefore, this task has attracted researchers from the fields of discourse analysis, sociology of science, and information sciences for decades (Teufel et al., 2006a) . Most of the existing research in this area focused on the analysis of citation sentiment, which has achieved good accuracy (see, e.g., (Teufel et al., 2006a) ). Citation sentiment analysis systems are usually able to identify positive, neutral, or negative opinions, but if we want to better understand the exact function of a citation, we need to know not only whether the authors like the citation, but also how the citation is used in a given context (Section 2). In this paper, we try to reveal citation functions more accurately than simply classifying citation sentiment. We first create a two level coarse-to-fine grained annotation scheme (Section 3). The coarse-level annotation corresponds roughly to sentiment categories, including POSITIVE, NEGATIVE, and NEUTRAL. The fine-grained annotation scheme provides a more detailed description of citation functions, such as Significant, which asserts the importance of an article or a work, and Discover, which acknowledges the original discoverer/inventor of a method or material. Using data annotated under this scheme, we train classifiers to determine citation functions, and experiment with features from lexical to syntactic levels (Section 4). We predict the finegrained citation function at 67% in F-measure in our experiments, which is at the same level as the coarse-grained citation sentiment classification (Section 5). Related Work The background for our work is in citation analysis. Applications of citation analysis include evaluating the impact of a published literature through a measurable bibliometric (Garfield, 1972; Luukkonen, 1992; Borgman and Furner, 2002) , analyzing bibliometric networks (Radev et al., 2009) , summarizing scientific papers (Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011) , generating surveys of scientific paradigms (Mohammad et al., 2009) , among others. Correctly and accurately recognizing citation functions is a cornerstone for these tasks. et al. (2006b) is the most related to ours. They proposed an annotation scheme for citation functions based on why authors cite a particular paper, following Spiegel-Rüsing (1977) . This scheme provides clear definition for some of the basic citation functions, such as Contrast, but mainly concerns the citations that authors compare to or build upon, ignoring the relationship between two cited works. Sometimes the relationship between two cited works is also meaningful and important, from which we can know more about the functions and influences of one cited work on other works. For example, the cited work may be utilized or applied by another cited work, which would be captured by Practical in our annotation scheme but considered as neutral under their scheme. In addition, their annotation scheme does not explicitly recognize milestone or standard work in a particular research field, while our annotation scheme does through the Significant function. We continue to use these basic functions, but try to expand their scheme by incorporating more functions, such as acknowledgement and corroboration, which reflects the attitude of the research community towards a citation. Regarding the automatic recognition of citation functions or citation categories, Teufel et al. (2006a) presented a supervised learning framework to classify citation functions mainly utilizing features from cue phrases. Athar (2011) explored the effectiveness of sentence structurebased features to identify sentiment polarity of citations. Dong and Schäfer (2011) proposed a four-category definition of citation functions following Moravcsik and Murugesan (1975) and a self-training-based classification model. Different from previous work that mainly classified citations into sentiment categories or coarse-grained functions, our scheme, we believe, is more finegrained. It is also worth noting that Teufel et al. (2006a) , Athar (2011) , and Dong and Schäfer (2011) all worked on citations in computational linguistics papers, but we investigate citations in biomedical articles. Annotation Our annotation scheme contains three general citation function categories POSITIVE, NEUTRAL, and NEGATIVE: POSITIVE citations reflect agreement, usage, or compatibility with cited work; NEUTRAL citations refer to related knowledge or background in cited work; and NEGATIVE citations show weakness of cited work. These three general categories are often used as citation sentiments in previous citation sentiment analysis work. We extend these categories by sorting them into smaller subcategories that reflect the functions of citations. POSITIVE (see + in Table 1 Two annotators are trained to perform the annotation. The articles we work on are from the open access subset of PubMed, which consists of articles from the biomedical domain. We require the annotators to mark citation functions, and point to textual evidence for assigning a particular function. Recognizing Citation Functions We use the Maximum Entropy (MaxEnt) model to classify all citations into the above citation function categories. We experiment with both surface and syntactic features. When parsing the context sentence, we replace each citation content with a <CITATION> symbol, in order to remove the contextual bias. Surface Features We capture n-grams, signal words collected by system developers, pronouns, negation words, and words related to formulae, graphs, or tables in the context sentence as surface level features. • N-Gram Features use both uni-grams of the context sentence and the tri-gram context window that contains the citation. • Signal Word Features check whether the text signals for a citation function (151 words/phrases in total, collected by system developers from dictionaries) appear in the context sentence. • Pronoun Features look for third-person pronouns and their positions in the context sentence. (135 words in total) appear in the context sentence with its scope. NNP • FGT Features fire if words or structures like formula, graph, or table appear in the context sentence. Syntactic Features We capture more generalized or long-distance information by taking advantages of syntactic features. The Part-of-Speech Features use Part-of-Speech (POS) tags adds generalizability to surface level signals, e.g., \"VERB with\" covers signals like \"experiment with\" and \"solve with\", which might indicate a Practical function. We use a combination of POS tags and words in a two-word context window around the <CITATION> as features. In Figure 1 , \"VBD DT\", \"identified DT\", and \"VBD the\" would be extracted. The Dependency Features use the dependency structure of the context sentence to capture grammatical relationships between a citation and its signal words regardless of the distance between them. We extract both dependency triples and dependency labels as features. In Figure 1 , if we extract dependency relations and labels attached to a <CITATION>, we would obtain \"NSUBJ identified CITATION\", \"NSUBJ\", and \"NSUBJ showed CITATION\" as dependency features. \"NSUBJ showed CITATION\" captures the long-distance relation between <CITATION> and a signal word \"showed\", which other features miss. Experiments From 91 annotated articles with total 6, 355 citation instances, we train our model and test the performance through a 10-fold cross-validation procedure, so that each fold randomly contains 9 (or 10) articles with their associated citation instances. 3 : Overall Performance Using Different Features: n-gram features (baseline), FGT features (fgt), signal word features (sig), negation features (neg), pronoun features (pron), dependency structure features (dep), and Part-of-Speech features (pos). Features Table 3 shows the overall performance in Precision (P), Recall (R), and F-measure (F1) by incorporating different feature sets, at a 99.8% confidence level according to the Wilcoxon Matched-Pairs Signed-Ranks Significance Test. If we randomly assign one of the citation function classes to each citation instance, the performance is only 3.8% in F-measure. In addition, a simple majority classifier assigns each citation with whichever class that is in the majority in the training set, also only obtaining F-measure of 42.2%. Our results clearly show that our MaxEnt system easily outperforms these two simple baseline classifiers. We report macro-average numbers over all citation functions, except for NEUTRAL:Neutral, which simply reflects that a work is cited without any particular information. We observe that surface features do not work well enough alone, as they cannot generalize beyond the signal knowledge observed in a relatively small training set. Syntactic features, on the other hand, can utilize linguistic knowledge to solve the problem, and lead to better results. We compare F-measure of coarse-grained sentiment classification and fine-grained citation func- 5 . We see that coarsegrained classification performs only slightly better. We suspect that each citation function in the POSITIVE category needs different signal information to identify, so a more fine-grained annotation scheme could lead to a stronger correlation between a class label and its signals. This can explain the close performance between these two paradigms, although citation function prediction is more informative and harder. We report performance and distribution in annotated data for each citation function in Table 4. Note that the numbers in the \"Distribution\" column does not sum to 1, because we omit the NEUTRAL:Neutral category that does not carry information and some categories (e.g., Negative) that are too few (e.g., less than 5) in the corpus. We see that some of the functions (such as Discover) can perform much better than others. The major reason for the difference in performance is the imbalance distribution of citation functions in the annotated corpus, which, in turn, results in the difference in prediction ability of our classifier. In the extreme case, our system fails to find any positive instance for some of the categories because of the scarcity of training examples. In order to mitigate this problem, we plan to perform more function-specific annotation to obtain more data on current scarce functions. Conclusion In this paper, we introduced the task of citation sentiment analysis and citation function classification, which aims to analyze the fine-grained utility of citations in scientific documents. We described an annotation scheme to annotate citation functions in scientific papers into fine-grained categories. We presented our Maximum Entropybased system to automatically classify the citation functions, explored the advantages of different feature sets, and confirmed the necessity of using syntactic features in our task, obtaining 67% of final F-measure score. For future work, we plan to explore more features and perform more citation function-specific annotation for scarce functions in the current annotated corpus. Furthermore, we will also apply our annotation scheme and classification method in scientific literature from different domains, as well as investigate more elaborate machine learning models and techniques. Acknowledgement Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20154. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government. Approved for Public Release; Distribution Unlimited.",
         "1329203",
         "2351bf32623c4391c07d0cb3f532e32aa0aa2b63",
         "30",
         "https://aclanthology.org/R13-1052",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Li, Xiang  and\nHe, Yifan  and\nMeyers, Adam  and\nGrishman, Ralph",
         "Towards Fine-grained Citation Function Classification",
         "402--407",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "li-etal-2013-towards",
         null,
         null
        ],
        [
         "25",
         "W05-0824",
         "Thanks to the profusion of freely available tools, it recently became fairly easy to built a statistical machine translation (SMT) engine given a bitext. The expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another. We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT sharedtask.",
         "Thanks to the profusion of freely available tools, it recently became fairly easy to built a statistical machine translation (SMT) engine given a bitext. The expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another. We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT sharedtask. Introduction Machine translation is nowadays mature enough that it is possible without too much effort to devise automatically a statistical translation system from just a parallel corpus. This is possible thanks to the dissemination of valuable packages. The performance of such a system may however greatly vary from one pair of languages to another. Indeed, there is no free lunch for system developers, and if a black box approach can sometimes be good enough for some applications (we can surely accomplish translation gisting with the French-English and Spanish-English systems we developed during this exercice), making use of the output of such a system for, let's say, quality translation is another kettle of fish (especially in our case with the Finnish-English system we ended-up with). We devoted two weeks to the SMT shared task, the aim of which was precisely to see how well systems can do across different language families. We began with a core system which is described in the next section and from which we obtained baseline performances that we tried to improve upon. Since the French-and Spanish-English systems produced output that were comprehensible enough 1 , we focussed on the two languages whose translations were noticeably worse: German and Finnish. For German, we tried to move around words in order to mimic English word order; and we tried to split compound words. This is described in section 4. For the Finnish/English pair, we tried to decompose Finnish words into smaller substrings (see section 5). In parallel to that, we tried to smooth a phrasebased model (PBM) making use of WORDNET. We report on this experiment in section 3. We describe in section 6 the final setting of the systems we used for submitting translations and their official results as computed by the organizers. Finally, we conclude our two weeks of efforts in section 7. The core system We assembled up a phrase-based statistical engine by making use of freely available packages. The translation engine we used is the one suggested within the shared task: PHARAOH (Koehn, 2004) which control the decoder. For acquiring a PBM, we followed the approach described by Koehn et al. (2003) . In brief, we relied on a bi-directional word alignment of the training corpus to acquire the parameters of the model. We used the word alignment produced by Giza (Och and Ney, 2000) out of an IBM model 2. We did try to use the alignment produced with IBM model 4, but did not notice significant differences over our experiments; an observation consistent with the findings of Koehn et al. (2003) . Each parameter in a PBM can be scored in several ways. We considered its relative frequency as well as its IBM-model 1 score (where the transfer probabilities were taken from an IBM model 2 transfer table). The language model we used was the one provided within the shared task. We obtained baseline performances by tuning the engine on the top 500 sentences of the development corpus. Since we only had a few parameters to tune, we did it by sampling the parameter space uniformly. The best performance we obtained, i.e., the one which maximizes the BLEU metric as measured by the mteval script 2 is reported for each pair of languages in Table 1 . Smoothing PBMs with WORDNET Among the things we tried but which did not work well, we investigated whether smoothing the transfer table of an IBM model (2 in our case) with WORDNET would produce better estimates for rare words. We adapted an approach proposed by Cao et al. (2005) for an Information Retrieval task, and computed for any parameter (e i , f j ) be-longing to the original model the following approximation: ṗ(e i |f j ) ≈ e∈E p wn (e i |e) × p n (e|f j ) where E is the English vocabulary, p n designates the native distribution and p wn is the probability that two words in the English side are linked together. We estimated this distribution by cooccurrence counts over a large English corpus 3 . To avoid taking into account unrelated but cooccurring words, we used WORDNET to filter in only the co-occurrences of words that are in relation according to WORDNET. However, since many words are not listed in this resource, we had to smooth the bigram distribution, which we did by applying Katz smoothing (Katz, 1997) : p katz (e i |e) = ċ(e i ,e|W,L) P e j c(e j ,e|W,L) if c(e i , e|W, L) > 0 α(e)p katz (e i ) otherwise where ċ(a, b|W, L) is the good-turing discounted count of times two words a and b that are linked together by a WORDNET relation, co-occur in a window of 2 sentences. We used this smoothed model to score the parameters of our PBM instead of the native transfer table. The results were however disappointing for both the G-E and S-E translation directions we tested. One reason for that, may be that the English corpus we used for computing the co-occurrence counts is an out-of-domain corpus for the present task. Another possible explanation lies in the fact that we considered both synonymic and hyperonymic links in WORDNET; the latter kind of links potentially introducing too much noise for a translation task. The German-English task We identified two major problems with our approach when faced with this pair of languages. First, the tendency in German to put verbs at the end of a phrase happens to ruin our phrase acquisition process, which basically collects any box of aligned source and target adjacent words. This can be clearly seen in the alignment matrix of figure 1 where the verbal construction could clarify is translated by two very distant German words könnten and erläutern. Second, there are many compound words in German that greatly dilute the various counts embedded in the PBM Figure 1 : Bidirectional alignment matrix. A cross in this matrix designates an alignment valid in both directions, while the symbol indicates an uni-directional alignment (for has been aligned with einen, but not the other way round). Moving around German words For the first problem, we applied a memory-based approach to move around words in the German side in order to better synchronize word order in both languages. This involves, first, to learning transformation rules from the training corpus, second, transforming the German side of this corpus; then training a new translation model. The same set of rules is then applied to the German text to be translated. The transformation rules we learned concern a few (five in our case) verbal constructions that we expressed with regular expressions built on POS tags in the English side. Once the locus e v u of a pattern has been identified, a rule is collected whenever the following conditions apply: for each word e in the locus, there is a target word f which is aligned to e in both alignment directions; these target words when moved can lead to a diagonal going from the target word (l) associated to e u−1 to the target word r which is aligned to e v+1 . The rules we memorize are triplets (c, i, o) where c = (l, r) is the context of the locus and i and o are the input and output German word order (that is, the order in which the tokens are found, and the order in which they should be moved). For instance, in the example of Figure 1 , the Verb Verb pattern match the locus could clarify and the following rule is acquired: (sie einen, könnten erläutern, könnten erläutern), a paraphrase of which is: \"whenever you find (in this order) the word könnten and erläutern in a German sentence containing also (in this order) sie and einen, move könnten and erläutern between sie and einen. A set of 124 271 rules have been acquired this way from the training corpus (for a total of 157 970 occurrences). The most frequent rule acquired is (ich herrn, möchte danken, möchte danken), which will transform a sentence like \"ich möchte herrn wynn für seinen bericht danken.\" into \"ich möchte danken herrn wynn für seinen bericht.\". In practice, since this acquisition process does not involve any generalization step, only a few rules learnt really fire when applied to the test material. Also, we devised a fairly conservative way of applying the rules, which means that in practice, only 3.5% of the sentences of the test corpus where actually modified. The performance of this procedure as measured on the development set is reported in Table 2 . As simple as it is, this procedure yields a relative gain of 7% in BLEU. Given the crudeness of our approach, we consider this as an encouraging improvement. Compound splitting For the second problem, we segmented German words before training the translation models. Empirical methods for compound splitting applied to German have been studied by Koehn and Knight (2003) . They found that a simple splitting strategy based on the frequency of German words was the most efficient method of the ones they tested, when embedded in a phrase-based translation engine. Therefore, we applied such a strategy to split German words in our corpora. The results of this approach are shown in Table 2 . Note: Both the swapping strategy and the compound splitting yielded improvements in terms of BLEU score. Only after the deadline did we find time to train new models with a combination of both techniques; the results of which are reported in the last line of Table 2 . The Finnish-English task The worst performances were registered on the Finnish-English pair. This is due to the agglutinative nature of Finnish. We tried to segment the Finnish material into smaller units (substrings) by making use of the frequency of all Finnish substrings found in the training corpus. We maintained a suffix tree structure for that purpose. We proceeded by recursively finding the most promising splitting points in each Finnish token of C characters F C 1 by computing split(F C 1 ) where: split(F j i ) =    |F j i | if j − i < 2 max c∈[i+2,j−2] |F c i |× split(F j c+1 ) otherwise This approach yielded a significant degradation in performance that we still have to analyze. Submitted translations At the time of the deadline, the best translations we had were the baselines ones for all the language pairs, except for the German-English one where the moving of words ranked the best. This defined the configuration we submitted, whose results (as provided by the organizers) are reported in Table 3 Conclusion We found that, while comprehensible translations were produced for pairs of languages such as French-English and Spanish-English; things did not go as well for the German-English pair and especially not for the Finnish-English pair. We had a hard time improving our baseline performance in such a tight schedule and only managed to improve our German-English system. We were less lucky with other attempts we implemented, among them, the smoothing of a transfer table with WORDNET, and the segmentation of the Finnish corpus into smaller units.",
         "9374715",
         "177bfa5b71e3e4fff964c4c96f08d971885b41a9",
         "3",
         "https://aclanthology.org/W05-0824",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Langlais, Philippe  and\nCao, Guihong  and\nGotti, Fabrizio",
         "{RALI}: {SMT} Shared Task System Description",
         "137--140",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "langlais-etal-2005-rali",
         null,
         null
        ],
        [
         "26",
         "W16-4705",
         "In this paper, we propose a method of augmenting existing bilingual terminologies. Our method belongs to a \"generate and validate\" framework rather than extraction from corpora. Although many studies have proposed methods to find term translations or to augment terminology within a \"generate and validate\" framework, few has taken full advantage of the systematic nature of terminologies. A terminology of a domain represents the conceptual system of the domain fairly systematically, and we contend that making use of the systematicity fully will greatly contribute to the effective augmentation of terminologies. This paper proposes and evaluates a novel method to generate bilingual term candidates by using existing terminologies and delving into their systematicity. Experiments have shown that our method can generate much better term candidate pairs than the existing method and give improved performance for terminology augmentation.",
         "In this paper, we propose a method of augmenting existing bilingual terminologies. Our method belongs to a \"generate and validate\" framework rather than extraction from corpora. Although many studies have proposed methods to find term translations or to augment terminology within a \"generate and validate\" framework, few has taken full advantage of the systematic nature of terminologies. A terminology of a domain represents the conceptual system of the domain fairly systematically, and we contend that making use of the systematicity fully will greatly contribute to the effective augmentation of terminologies. This paper proposes and evaluates a novel method to generate bilingual term candidates by using existing terminologies and delving into their systematicity. Experiments have shown that our method can generate much better term candidate pairs than the existing method and give improved performance for terminology augmentation. Introduction In this paper, we propose a new way of generating new bilingual multi-word term pairs for augmenting existing bilingual terminologies. There is growing demand for properly managed terminologies in many areas of society, e.g. in document authoring and management, in technical translation, in knowledge transfer and education, and in IR/NLP (Sager, 1990; Wright and Wright, 1997; Budin, 2008; Kockaert and Steurs, 2015) . With the constant introduction of new terms in many domains, timely augmentation and update of terminologies is critical for proper terminology management, and automatic assistance for this process is greatly needed (Kockaert and Steurs, 2015) . Many researchers have proposed various methods to augment terminologies automatically. As we will see in Section 2, these can be divided into two broad approaches, i.e. \"extraction from corpora\" approach and \"generate and validate\" approach. We focus on the latter approach, which fits better for augmenting or expanding existing terminologies, the task which is in strong demand in language industries but has not been much addressed from the NLP point of view. A term in a terminology of a domain represents a concept of that domain. Majority of terms are complex in most domains in most languages. These complex terms represent concepts analytically, with each constituent element representing an important feature of the concept. A terminology, i.e. the set of terms of a domain, represents the structure of concepts of that domain more or less systematically. Although the extent of systematicity differ from language to language and from domain to domain, new terms are generally formed systematically within the conceptual system of the domain. If we can take into account this aspect of term formation for generating term candidates in the task of augmenting terminologies, we would be able to develop an effective way of help augmenting existing terminologies. Against this backdrop, this paper proposes a new method of generating bilingual term candidates by taking advantage of the structural feature of terminology. The basic idea is as follows: define terminological network that reflects conceptual systematicity; identify \"motivated\" subnetworks within which term formation is supposed to be activated, and generate term candidates for each subnetwork. The rest of this paper is organised as follows. Section 2 looks at related work and places the present work in context. Section 3 explains our proposed method. Sections 4 and 5 introduce the experimental setup and the results, respectively. Section 6 summarises the results and discusses remaining issues. Related work Automatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010) , most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010) . The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010) . Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et al., 2006; Daille and Morin, 2008) , i.e. they generate term candidates in target language by translating constituent elements and validate their existence. These studies partly adopt the \"generate and validate\" framework. Sato et al. (2013) generated multi-word term pairs as bilingual term candidates by considering all possible pairs of constituent elements of terms in a terminology. The generated pairs are then validated by using web documents. Our method adopts this \"generate and validate\" framework. More specifically, we take Sato et al. (2013) as a point of departure as the aim of this work is the same as the present work, i.e. extending existing bilingual terminologies. The method proposed by Sato et al. (2013) takes advantage of a general tendency that if one term is a compound, a part of the term is a term and a part of the term can be changed. For example, if a terminological lexicon contains, \"linear programming\", \"linear optimization\", \"linear function\", \"convex programming\" and \"convex function\", they can expect that the term \"convex optimization\" exists, even if this term is not listed in the lexicon. They generate term candidates consisting of two constituents by defining head-modifier bipartite graph and interpolate missing edges. Figure 1 shows this idea graphically. The problems we identify with their method are (a) if applied straightforwardly, a huge number of bilingual term candidates are generated, and (b) the Kernighan-Lin algorithm they adopted (Kernighan and Lin, 1970) to partition head-modifier bipartite graph in order to reduce term candidates does not reflect systematic structure of terminologies. Following theoretical research in terminology (Sager, 1990; Kageura, 2002) , we understand that new terms are formed within the conceptual-terminological subsystem surrounding the new concepts. So our main task is concerned with consolidating these subsystems consisting of tightly-related or \"motivated\" terms/concepts within which new terms are formed. Structural nature of terminology Terminologies in most languages contain a substantial number of complex terms (Cerbah, 2000; Nomura and Ishii, 1989 ). Research has shown that complex terms tend to show conceptual relationships systematically, with each constituent element representing an important feature of concepts represented by terms (Felber, 1984; Sager, 1990; Kageura, 2002) . 2012 ) examined the systematic nature of terminologies by introducing terminological network, the vertices of which are terms and the edges of which consist of common constituent elements between terms. For instance, a putative terminology consisting of six terms, \"information\", \"information retrieval\", \"information extraction\", \"document retrieval\", \"document processing\", and \"information processing\" makes a network as shown in Figure 2 . Kageura and Abekawa (2007) applied partitive clustering over the terminological network to obtain sub-groups of terminologies. Asaishi and Kageura (2011) comparatively analysed the formal nature of terminological structure by defining terminological networks of English and Japanese bilingual terminologies of several domains. Iwai et al. (2016) have shown that there is a reasonable amount of cross-lingual correspondence between sub-groups of English and Japanese terms identified by using community detection algorithms over the terminological network. As stated above, to identify conceptual subsystems consisting of closely-related concepts in terminology constitutes an essential part of our method. Iwai et al. (2016) showed that meaningful conceptual subsystems can be identified and extracted by applying relevant network partition algorithms to terminological networks. Method of term candidate generation Starting from a given terminology, the method of term candidate generation we propose consists of two steps: 1. Dividing a terminology into subgroups each of which consists of terms representing closely related concepts; and 2. Generating bilingual term candidates by generating possible combinations of constituent elements of terms included in each subgroup. Figure 3 shows an outline of the method. While Sato et al. (2013) firstly considered all possible head-modifier pairs for all terms in terminology and then reduced the number of term candidates by applying Kernighan-Lin algorithm to the head-modifier bipartite graph, our method first consolidate subgroups of terms and generate term candidates for each subgroup separately. Note that this is not just a methodological alternative, but reflects theoretical understanding of how new terms are formed, as stated above. Identifying \"motivated\" sub-groups of terms We first construct terminological networks (Kageura, 2012) , and then apply partitive clustering or community detection algorithm to the network. This manipulation identifies motivated sub-groups of terms within a given terminology. As terms are formed within subsystems of concepts, this serves for reducing the number of generated term candidates while at the same time increasing the plausibility of candidates. After dividing terminological networks into sub-groups or clusters, we generated a head-modifier set for each cluster. The steps for this process are as follows: 1. Decompose each term into its constituent elements; 2. Generate terminological network with terms as vertices and common constituent elements as edges; 3. Divide the generated terminological network into clusters using a community detection algorithm. For step 1, we used MeCab 1 with UniDic 2 to decompose Japanese terms into constituent elements. For English terms, we decompose terms using spaces and other punctuations and then apply stemming and lemmatisation of constituent words using a lemmatiser 3 . Although POS taggers, such as Stanford POS Tagger 4 , are widely used for pre-processing English sentences or phrases, we used here the lemmatiser because (a) our aim is to extract semantically identical units by removing inflectional (and sometimes derivational) variations and (b) we do not need POS-information. Previous work has shown that approximately matching units can be extracted for English and Japanese terminologies by applying these pre-processing steps (Asaishi and Kageura, 2011) . For step 2, we used python igraph library 5 to generate terminology networks for English and Japanese. We removed functional words (symbols, numbers, prepositions and articles for English; symbols, numbers, particles and auxiliary verbs for Japanese) as they do not represent conceptual characteristics. For step 3, we adopted Potts spin glass algorithm to divide the terminology networks into clusters. Many community detection algorithms have been proposed (Clauset et al., 2004; Rosvall and Bergstrom, 2008; Raghavan et al., 2007; Blondel et al., 2008; Pons and Latapy, 2006; Newman, 2006) . After examining several commonly used methods, we decided to adopt Potts spinglass-based method (Reichardt and Bornholdt, 2006) , which works by solving the global optimization problem (Kirkpatrick, 1984) . Not only is this method reported to work well in several experiments, the underlying concept reflects nicely the task of extracting motivated sub-groups of terminologies (Kageura and Abekawa, 2007) . Generating bilingual term candidates After obtaining clusters on sub-groups of terms, we generated bilingual term candidates as follows: 1. Identify corresponding English and Japanese terms contained in each cluster. As English and Japanese clusters do not match completely (Iwai et al., 2016) , we generated term candidate pairs in three different ways in step 1: (a) based on Japanese clusters (Japanese), (b) based on English clusters (English), and (c) based on the intersections of Japanese and English clusters (mix). 2. Generate bilingual pairs of constituent elements (henceforth constituent pairs). This is carried out first by identifying single-word term pairs and then subtracting them from multi-word terms and making remaining elements as pairs recursively. 3. Generate head-modifier pairs for constituent elements of source language terms, as shown in Figure 4 . We identify head-modifier relations by identifying constituents on the left as modifiers and on the right as heads, as English (and Japanese, for that matter) complex terms are head final. We also assumed that if a term constitutes more than three words, two constituent elements can replace as one semantics unit. For example, we can consider that \"data\" is the modifier and \"processing 4 . However, we considered only head-modifier pairs by minimum unit in this time. We set English as source language for convenience of processing; there is no inherent technical reason for us to make the process directional in terms of languages. 4. Generate a bipartite graph based on the head-modifier pairs of the source language, as shown in Figure 5 . 5. Take the direct product of the head and modifier vertices to generate extended head-modifier pairs from that bipartite graph. 6. Create new bilingual term pairs by taking translations for each constituent elements of the headmodifier pairs using constituent pairs. The candidate term pairs generated through this process are then validated using web documents. 4 Experimental setup Seed terminologies For evaluation, we used two terminological dictionaries, i.e. one in the field of computer science (Aiso, 1993) and the other in the field of economics (Yuhikaku, 1986) . These are two of the five terminological dictionaries used in Sato et al. (2013) . Table 1 shows the number and ratio of terms by length in each terminology, i.e. single terms, terms with two constituents, terms with three constituents and terms with four or more constituents. \"Dom.\" stands for domain, \"Lang.\" stands for language, and \"T\" indicates the number of terms. From Table 1 , we can observe that these terminologies contain many complex terms. Terminological network and candidate generation We constructed terminological networks for English and for Japanese separately for these two datasets. Table 2 shows the quantitative nature of the terminological networks, in which N stands for the number of constituent elements, V the number of vertices, E the numbers of edges, and S the number of isolated terms. We can observe that each network consists of a single giant component (max subgraph) and several small components (others) including isolated vertices. We then extracted max subgraph and divided it into clusters. The number of clusters was set in two ways, i.e. 25 and 10. These numbers were decided heuristically, referring to the number of subdomains listed in handbooks and in academic societies. The number of candidates generated from these clusters is given in Table 3 , which also provides the number of candidates generated from the method by Sato et al. (2013) . Note that our method produces smaller number of term candidates. Collecting web documents for validation Web documents are collected separately for two languages and stored in a database. To avoid collecting irrelevant web pages, we used domain keywords (the name of the domain such as \"computer science\") together with individual terms for collecting documents. Web documents for computer science were collected in October and November 2014, by using terms and the domain keywords \"computer science\" (English) and \"情報科学\" (\"information science\" for Japanese) (see 3.1). Web documents for economics were collected at the end of December 2014, with domain keywords \"economics\" (English) and \"経済学\" (\"economics\" for Japanese). Table 4 shows the basic quantities of the collected documents. We extracted 200 pages randomly from the English data and manually checked the number of technical documents. The result is shown in Table 5 . Approximately 60 % of the documents were technical in both domains. Evaluation We evaluated our method in two ways. First, we compared our result with Sato et al. (2013) in terms of the number of retained candidates after validation. Second, to evaluate precision, we extracted top 100 candidates ranked according to (a) the sum of English and Japanese occurrences and (b) the Jaccard coefficient. Note that we do not make comparison between our approach and the approach of extracting terms from corpora, because their experimental setups are very different to each other. Comparison of the number of retained candidates after validation The candidate term pairs generated in six different ways (two cluster sizes of 10 and 25 by based on Japanese clusters, based on English clusters, and based on the intersections of Japanese and English clusters) were validated by 2 steps using the web documents (see 4.3). 1. Searching bilingual term candidates from collected web documents and retaining candidate pairs of which both English part and Japanese part occur at least once in the documents. 2. Calculating a Jaccard coefficient by using retained candidate pairs. In step 1, instead of using the web search directly, we first pool the web documents relevant to the two domain. It is to avoid repeatedly searching the web for every candidate pairs. In step 1, we validate English and Japanese terms separately, as we can assume that the candidates are aligned. However, it is still useful to validate the bilingual co-occurrences in the web documents. In order to observe that, we used Jaccard coefficient. Table 6 : The result of validation (filtering) Filtering by using collected web documents We first did the filtering by using collected web documents to reduce the number of generated bilingual term candidates. Candidate pairs of which both English part and Japanese part occur at least once in the corpus were retained as validated terms. Table 6 shows the result. The first line in each domain shows the number of validated candidates. The second line shows their percentage against the number of candidate pairs given in Table 3 . It shows that the number of terms retained after validation is generally larger in our methods than Sato et al. (2013) , with exceptions (\"mix\" for 10 clusters, and \"Ja\" and \"mix\" for 25 clusters in economics). In all cases, the ratio of retained candidates is much higher in our method than Sato et al. (2013) . These results indicate that our proposed method: • performs both more effectively in terms of computational cost and in terms of recall, assuming that the validated terms have roughly the same level of pairing precision and termhood precision; and • enables us to control the balance between recall and precision, by changing the number of clusters as well as the pairing methods. The first point indicates that our method successfully captures the conceptual subsystems/terminological subgroups within the dynamics of which new terms are formed. The second point shows that our method gives us applicational flexibility. Calculating Jaccard coefficient After filtering by collected web documents, we searched retained bilingual term candidates with search engine and calculated Jaccard coefficient by using the number of hit. In order to keep the comparison with Sato et al. ( 2013 ) sensible, we chose the validated candidates generated from \"mix\" for 25 clusters, as the number of validated terms in the two domains is close to that by Sato et al. (2013) (although \"Ja\" pairing for 10 clusters is the closest in economics, we chose the same setting for the two domains). Jacard coefficient is defined as: Jaccard(L1, L2) = H(L1) ∧ H(L2) H(L1) ∨ H(L2) = H(L1) ∧ H(L2) H(L1) + H(L2) − H(L1 ∧ L2) , where L1 and L2 indicate English and Japanese parts (or vice versa) of a candidate pair in our case, and H(x) is the number of documents in which they occur. If the number of hits is zero, the Jaccard coefficient is defined to be zero. In filtering by using collected web documents, the process retained candidate pairs that either English part or Japanese part occur. Therefore, it is considered that nonparallel candidate pairs are retained. By calculating Jaccard coefficient with the number of hit in search engine and retaining candidate pairs that Jaccard coefficient is positive, we finally extract candidate pairs that is validated parallel. We used Bing search API as search engine. Precision of top 100 candidates The top 100 candidates generated by \"mix\" for 25 clusters, ranked according to the sum of English and Japanese occurrences and to the Jaccard coefficient, were manually evaluated for each domain. The evaluation was carried out from two points of view, i.e. (a)whether the Japanese and English matches or not (pairing), and (b)whether the Japanese candidates can be regarded as a term in the domain in question (term). For (b), we also counted partial-terms (partial). The evaluation was carried out by one of the authors. Table 8 shows the result, together with the corresponding results given in Sato et al. (2013) (in bracket). Table 8 shows that except for \"pairing\" by Jaccard in computer science, our method is consistently better than Sato et al. (2013) in terms of precision as well. Conclusion and future work In this paper we proposed a method of augmenting existing bilingual terminological lexicon. We introduced a way of generating candidate term pairs which reflect the conceptual system/terminological group within which new terms are formed, by taking advantage of the \"motivated\" structure of terminologies. Compared with the method proposed so far, our method consistently shows higher performance, which indicates that our method succeeded in identifying, to a reasonable extent, the conceptual subsystem/terminological subgroups within which terms are formed. The method also has more applicational flexibility. We are currently addressing the following issues: • Extending our method so that it can generate and validate terms with more than three constituent elements. For example, if a term consists of more than three words, it is natural to decompose it into 2 words as one unit and the other one word from the point of semantic structure. In this way, we try to apply generating bilingual term candidates that consists of more than three words. • Improving the pairing module. As of now, we examined English as source language and Japanese as target language. However, we can consider reverse pattern in our proposed method. Directional property of language and correspondence of translation words are one of the points of that we need to address in the future. • Analysing non-validated candidates (error analysis). Now that it was shown that the proposed method can capture, to a reasonable extent, conceptual subsystem within which new terms are generated, it is important to analyse non-validated candidates to obtain further insights into candidate generation process. • Finding a way of suggesting reasonable number of clusters. As can be inferred from Tables 4 and  7 , the best number of clusters may differ from domain to domain. In addition, we are planning to extend our research into the following directions: • Applying our method to different language pairs. We are planning to apply our method to Chinese-English and Korean-English pairs. • Clarifying the difference between the \"generate and validate\" framework and extraction from parallel or comparable corpora. Although the comparison of these two approaches are difficult, because not only the theoretical assumption and the range of relevant applications but also the range of data which can be used differ greatly (the \"generate and validate\" approach in general can use wider variety of data as they are used for validation rather than sources from which terms are extracted), it would still be interesting to examine the relationship between these two approaches on the empirical basis. Acknowledgements This work is partly supported by JSPS Grant-in-Aid (A) 25240051 \"Archiving and using translation knowledge to construct collaborative translation training aid system.\"",
         "18622276",
         "b74bf37df909d2baf4b14f69a953aa5ccc2a9386",
         "3",
         "https://aclanthology.org/W16-4705",
         "The COLING 2016 Organizing Committee",
         "Osaka, Japan",
         "2016",
         "December",
         "Proceedings of the 5th International Workshop on Computational Terminology (Computerm2016)",
         "Iwai, Miki  and\nTakeuchi, Koichi  and\nKageura, Kyo  and\nIshibashi, Kazuya",
         "A Method of Augmenting Bilingual Terminology by Taking Advantage of the Conceptual Systematicity of Terminologies",
         "30--40",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "iwai-etal-2016-method",
         null,
         null
        ],
        [
         "27",
         "R13-1054",
         "Many sentiment-analysis methods for the classification of reviews use training and test-data based on star ratings provided by reviewers. However, when reading reviews it appears that the reviewers' ratings do not always give an accurate measure of the sentiment of the review. We performed an annotation study which showed that reader perceptions can also be expressed in ratings in a reliable way and that they are closer to the text than the reviewer ratings. Moreover, we applied two common sentiment-analysis techniques and evaluated them on both reader and reviewer ratings. We come to the conclusion that it would be better to train models on reader ratings, rather than on reviewer ratings (as is usually done).",
         "Many sentiment-analysis methods for the classification of reviews use training and test-data based on star ratings provided by reviewers. However, when reading reviews it appears that the reviewers' ratings do not always give an accurate measure of the sentiment of the review. We performed an annotation study which showed that reader perceptions can also be expressed in ratings in a reliable way and that they are closer to the text than the reviewer ratings. Moreover, we applied two common sentiment-analysis techniques and evaluated them on both reader and reviewer ratings. We come to the conclusion that it would be better to train models on reader ratings, rather than on reviewer ratings (as is usually done). Introduction There is a growing volume of product reviews on the web which help customers to make decisions when planning to travel or buying a product. Sentiment-analysis tools try to discover user opinions in these reviews by converting the text to numerical ratings. Building these tools requires a large set of annotated data to train the classifiers. Most developers compile a training and test corpus by collecting reviews from web sites on which customers post their reviews and give a star rating. They test and train their tools against these reviewer ratings assuming that they are an accurate measure of the sentiment of the review. However, when reading reviews and comparing them with the reviewer ratings there does not always seem to be a clear and consistent relation between these ratings and the text (cf. also Carrillo de Albornoz et al., 2011) . That is, from a reader's perspective, there is a discrepancy between what the reviewer expresses with the numerical rating and what is expressed in text. For example, the following hotel review was rated '7' (weakly positive), whereas possible guests probably would not go to the hotel after having read the review. The hotel seems rather outdated. The breakfast room is just not big enough to cope with the Sunday-morning crowds. This mismatch between the reviewer's rating and the review's sentiment may lead to problems. For example, reviews are often ranked according to their reviewer's ratings from highly positive to highly negative. If the review text is not in accordance with its ranking, the rankings may become ineffective. In the area of sentiment analysis and opinion mining the mismatch may lead to methodological problems. Testing and training of sentimentanalysis tools on reviewer ratings may lead to the wrong results if the mismatch between the ratings and the text proves to be a common phenomenon. We assume that one of the most important sources of this mismatch is the fact that the reviewer writes the review and, separately, rates the experience (i.e., with the book he read, with the hotel he stayed at, with a product he bought). Of course, both text and rating are based on the same experience but they do not necessarily express the same aspects of it. If we have a closer look at the hotel review above, the reviewer probably rates the hotel with a '7', because there may be some positive aspects which he does not mention in his review. We hypothesize that reader ratings which express the reader's perceptions of the sentiment of a text are a good alternative. As the reader's judgment is based solely on the text of the review, we assume that its rating is closer to the sentiment of the text than the reviewer's rating. In this study we investigate whether the observed mismatch between reviewer rating and the sentiment of the review is a common and whether reader ratings could be a more reliable measure of this sentiment than reviewer ratings. The next section presents related work. In section 3, the reliability of reviewer and reader ratings as a measure of a review's sentiment is further investigated by performing an annotation study. In section 4, we study the effect of the different types of ratings on the performance of two widely used sentiment-analysis techniques. Finally, we conclude with a discussion of our findings. Related Work There is a large body of work concerning sentiment analysis of customer reviews (Liu, 2012) . Most of these studies regard sentiment analysis as a classification problem and apply supervised learning methods where the positive and negative classes are determined by reviewer ratings. Studies propose additional annotations only when focusing on novel information which is not reflected in the user ratings (Toprak et al., 2010, Ando and Ishizaki, 2012) . The issue of a possible mismatch between reviewer ratings and review text is usually not addressed. Much attention is paid to the customer's (or reader's) perspective in studies in the area of business and social science. Mahony et al. (2010) and Ghose et al. (2012) study product reviews in relation to customer behavior. Their aim is to identify reviews which are considered helpful to customers and to know what kind of reviews affect sales. Their work is similar to ours because of the focus on the effect of the review text on the customer/reader, but they also include other types of information such as transaction data, consumer browser behavior and customer preferences. However, none of these studies focus on the relationship between reviewer rating and review text. As far as we know, Carrillo de Albornoz et al. ( 2011 ) is the only study which mentions the mismatch between rating and text. They ignore reviewer ratings and employ a new set of ratings for the training and testing of their system. From their work, however, it is neither clear to what extent the new ratings differ from the user ratings as they do not report inter-annotator agreement scores nor what the effect is of the different ratings on classifier performance. Reviewer and reader annotations To get a better understanding of the relationship between reviewer ratings, review text and reader ratings, we perform an annotation study which allows us to answer the following research questions: (1) To what extent are mismatches between reviewers' ratings and sentiments common? And (2) Can reader ratings be employed to measure review sentiment more reliably? Hotel review corpus For the annotation study we compiled a corpus of Dutch hotel reviews. The corpus consists of 1,171 reviews extracted from four different booking sites during the period 2010-2012. The reviews have been collected in such a way that they are evenly distributed among the following categories:  They are collected from different booking site like Tripadvisor.com, zoover.com, hotelnl.com and booking.com  They include most frequent text formats: pro-con (where boxes are provided for positive and negative remarks) and free text.  They include reviews on hotels from all over the world (although the majority is Dutch).  They include reviewer's ratings ranging from strong negative to strong positive Each review contains the following information:  Reviewer rating: a user rating given by the reviewer translated to a scale ranging from 0 to 10 (very negative to very positive) describing the overall opinion of the hotel customer.  Review text: a brief text describing the reviewer's opinion of the hotel.  Reader ratings: ratings of two readers on a scale ranging from 0 to 10. These ratings are described in more detail in the next section. Reader ratings and agreement scores Two annotators (R1 and R2), both native speakers of Dutch and with no linguistic background, added a reader rating to each review. They were asked to read the review and rate the text on a scale from 1-10 (very negative to very positive), answering the question whether the reviewer would advise them to choose the hotel, or not. They were asked to ignore their own preferences as much as possible. We measured the Pearson Correlation Coefficient (r) between the 10-point numerical rating scales of each annotator pair (R1, R2 and reviewer), regarding the reviewer (REV) also as an annotator. As correlation can be high without necessarily high agreement on absolute values, we also performed evaluations on categorical values. A 2-class evaluation was performed by translating 1 to 5 ratings to 'positive' and 6 to10 ratings to 'negative'; a 4class evaluation is performed by translating 1-3 ratings to 'strong negative', 4 to 5 ratings to 'weak negative', 6 to 7 ratings to 'weak positive' and 8 to 10 to 'strong positive'. Agreement was measured between each annotator pair in terms of percentage of agreement (%) and kappa agreement (κ). raters 1/10 2-class 4-class REV-R1 0.82 r 0.81 κ 0.90% 0.51 κ 0.63% REV-R2 0.83 r 0.82 κ 0.91% 0.53 κ 0.65% R1-R2 0.92 r 0.92 κ 0.96% 0.71 κ 0.78% Table 1 . Inter-annotator agreement. Table (1) shows that inter-annotator agreement is quite high between all raters, both when correlation is measured on the 10-point-scale (r >= 0.82) and when agreement is measured with the 2-class annotation sets (κ >= 0.81). Agreement on the 4 class annotations is much lower (κ >= 0.51) showing that polarity strength is difficult to annotate. However, given the purpose of this study, we are not interested in agreement as such. Our focus is on the differences in agreement between readers and reviewers. From that perspective it is interesting to note that, according to all measures, the reviewer is an outlier. Agreement between each individual reader and the reviewer (REV-R1 and REV-R2, respectively) is consistently lower than agreement between both readers (R1-R2). The differences already become important when measuring agreement on 2-class annotations, but even more prominent when measuring agreement on 4-class annotations. All observed differences ranging from 5 up to 15%, are statistically significant (p < 0.01). On the basis of these results, we can answer our research questions (cf. section 3). We infer that the observed mismatch between the sentiment of the review and reviewer rating is a relatively common phenomenon. With respect to at least 10% (cf. table 1, row 2, column 4) of the reviews (when reviews are categorized in 2 categories) up to approx. 37% (cf. table 1, row 1, column 6) of the reviews (when reviews are categorized in more fine-grained categories) readers do not agree with the reviewer. Secondly, the fact that readers have higher agreement with each other than with the reviewer confirms our hypothesis that reader ratings are a more accurate measure of the review's sentiment than reviewer ratings. Implications for sentiment analysis We investigated how automated sentiment analysis methods perform with the different sets of annotations by applying two widely used approaches to document-level sentiment classification. Classifier accuracy is measured against the three sets of ratings (R1, R2 and REV) we described in the previous section. The lexicon-based approach The first method is a lexicon-based approach which starts from a text which is lemmatized with the Dutch Alpino-parser 1 .The approach is similar to the \"vote-flip-algorithm\" proposed by Choi and Cardie (2008) . The intuition about this algorithm is simple: for each review the number of matched positive and negative words from the sentiment lexicon are counted. If polar words are preceded by a negator, their polarity is flipped; if polar words are preceded by an intensifier, their polarity is doubled. We then assign the majority polarity to the review. In the case of a tie (being zero or higher than zero), we assign neutral polarity. The sentiment lexicon used in this approach is an automatically derived general language sentiment lexicon obtained by WordNet propagation (Maks and Vossen, 2011) . The machine-learning approach The second method is a machine learning approach that also starts from a text that is lemmatized by the Dutch Alpino-parser. After lemmatization the text is transformed to a word-vector representation by applying Weka's StringToWord Vector with frequency representation (instead of binary). We used Weka's NaiveBayesMultinominal (NBM) classifier to classify the reviews. The NBM was chosen because our review texts are rather short (with an average of 68 words) and, according to Wang and Manning (2012) , NBM classifiers perform well on short snippets of Results on different types of ratings Results are evaluated against the whole set of 1,172 reviews (cf. table 2 'all'). As many approaches to sentiment analysis do not use the class of weak sentiment (Liu, 2012) , we also evaluated against a subset of strong negative (ratings 1 to 3) and strong positive (ratings 8 to 10) reviews (cf. table 2, 'strong'). Table ( 2) shows the classification results in terms of accuracy, obtained by the lexicon-based approach (LBA, row 1, 2, 3) and the machinelearning approach (NBM, row 4, 5, 6) . The results show that both approaches perform well against all ratings. Classification of the strong sentiment reviews seems considerably easier than classification of the whole review set. Interestingly, both sentiment analysis approaches appear to perform better on reader ratings than on reviewer ratings. The better performance holds across both selections of reviews and with both approaches. Differences are statistically significant (chi-square test, p<0.05) in all cases but the LBA approach on the whole dataset which is almost statistically significant. Discussion and Conclusions We performed an annotation study that showed that the observed mismatch between reviewer ratings and review's sentiment is a rather frequent phenomenon. Considerable part of the reviews (ranging from 9 to 37% depending on the granularity of the classification) is classi-fied by the reviewer in the wrong sentiment class. The annotation study also showed that reader ratings are a more accurate measure. We already expected reader ratings to be closer to the text because they are exclusively based on it. In addition, the annotation study shows that readers agree in their ratings and that the review's sentiment can be reliably annotated by readers. Our experiments in section 4 show that sentiment-analysis tools perform better with reader ratings than with reviewer ratings. This should probably not surprise us as sentiment analysis behaves like a reader whose only source of information is the review text. As such, this is a promising result. However, since reviewer ratings are widely available and come for free with the text, they will often be used to evaluate the tools. Likewise, training and fine-tuning will be done with reviewer ratings rather than with reader ratings. We think that researchers and system developers should be aware of the differences between reviewer and reader ratings and their effects on the system they develop. Recently, many sentiment analysis tools perform a more in-depth analysis identifying aspects of products (and services) and their sentiments (Liu, 2012) . Again, reviewer ratings are used to train and test these systems. In view of our findings, it seems advisable that researchers and system developers make the effort to collect a set of reader ratings and train and test their tools with them. The additional value of sentiment analysis should be sought in finding the sentiment of the text rather than in finding the sentiment of its writer. Acknowledgements This research is supported by the European Unions 7 th Framework Programme via the OpeNER (Open polarity enhanced Named Entity Recognition) Project (ICT-296451).",
         "7036291",
         "57ac89edb6a460880b4bb400e60e593f0271e88e",
         "22",
         "https://aclanthology.org/R13-1054",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Maks, Isa  and\nVossen, Piek",
         "Sentiment Analysis of Reviews: Should we analyze writer intentions or reader perceptions?",
         "415--419",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "maks-vossen-2013-sentiment",
         null,
         null
        ],
        [
         "28",
         "W05-0827",
         "Nowadays, most of the statistical translation systems are based on phrases (i.e. groups of words). In this paper we study different improvements to the standard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\" (ACL Workshop on Parallel Texts 2005).",
         "Nowadays, most of the statistical translation systems are based on phrases (i.e. groups of words). In this paper we study different improvements to the standard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task \"Exploiting Parallel Texts for Statistical Machine Translation\" (ACL Workshop on Parallel Texts 2005). Introduction Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), ẽ = argmax e P (e|f ) (1) If we use Bayes rule to reformulate the translation probability, we obtain, ẽ = argmax e P (f |e)P (e) (2) This translation model is known as the sourcechannel approach [1] and it consists on a language model P (e) and a separate translation model P (f |e) [5] . In the last few years, new systems tend to use sequences of words, commonly called phrases [8] , aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. The features functions, h m , are the system models (translation model, language model and others) and weigths, λ i , are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understanding task. It has the advantatge that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phrase-extraction algorythm in [11] . It also combines several interesting features and it reports an important improvement from the baseline. It is organized as follows. Section 2 introduces the baseline; the following section explains the modification in the phrase extraction; section 4 shows the different features which have been taken into account; section 5 presents the evaluation framework; and the final section shows some conclusions on the experiments in the paper and on the results in the shared task. Baseline The baseline is based on the source-channel approach, and it is composed of the following models which later will be combined in the decoder. The Translation Model. It is based on bilingual phrases, where a bilingual phrase (BP ) is simply two monolingual phrases (M P ) in which each one is supposed to be the translation of each other. A monolingual phrase is a sequence of words. Therefore, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [17] . During training, the system has to learn a dictionary of phrases. We begin by aligning the training corpus using GIZA++ [6] , which is done in both translation directions. We take the union of both alignments to obtain a symmetrized word alignment matrix. This alignment matrix is the starting point for the phrase based extraction. Next, we define the criterion to extract the set of BP of the sentence pair (f j2 j1 ; e i2 i1 ) and the alignment matrix A ⊆ J * I, which is identical to the alignment criterion described in [11] . BP (f J 1 , e I 1 , A) = {(f j2 j1 , e i2 i1 ) : ∀(j, i) A : j 1 ≤ j ≤ j 2 ↔ i 1 ≤ i ≤ i2 ∧∃(j, i) A : j 1 ≤ j ≤ j 2 ∧ i 1 ≤ i ≤ i2} The set of BP is consistent with the alignment and consists of all BP pairs where all words within the foreign language phrase are only aligned to the words of the English language phrase and viceversa. At least one word in the foreign language phrase has to be aligned with at least one word of the English language. Finally, the algorithm takes into account possibly unaligned words at the boundaries of the foreign or English language phrases. The target language model. It is combined with the translation probability as showed in equation (2) . It gives coherence to the target text obtained by the concatenated phrases. Phrase Extraction Motivation. The length of a M P is defined as its number of words. The length of a BP is the greatest of the lengths of its M P . As we are working with a huge amount of data (see corpus statistics), it is unfeasible to build a dictionary with all the phrases longer than length 4. Moreover, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [8] . X-length In our system we considered two length limits. We first extract all the phrases of length 3 or less. Then, we also add phrases up to length 5 if they cannot be generated by smaller phrases. Empirically, we chose 5, as the probability of reappearence of larger phrases decreases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments. For example, from the following sentence, Cuando el Parlamento Europeo , que tan frecuentemente insiste en los derechos de los trabajadores y en la debida protección social , (...) NULL ( ) When ( 1 ) the ( 2 ) European ( 4 ) Parliament ( 3 4 ) , ( 5 ) that ( 6 ) so ( 7 ) frequently ( 8 ) insists ( 9 ) on ( 10 ) workers ( 11 15 ) ' ( 14 ) rights ( 12 ) and ( 16 ) proper ( 19 ) social ( 21 ) protection ( 20 ) , ( 22 ) (...) where the number inside the clauses is the aligned word(s). And the phrase that we are looking for is the following one. los derechos de los trabajadores # workers ' rights which only could appear in the case the maximum length was 5. Phrase ranking Conditional probability P (f |e) Given the collected phrase pairs, we estimated the phrase translation probability distribution by relative frecuency. P (f |e) = N (f, e) N (e) (4) where N(f,e) means the number of times the phrase f is translated by e. If a phrase e has N > 1 possible translations, then each one contributes as 1/N [17] . Note that no smoothing is performed, which may cause an overestimation of the probability of rare phrases. This is specially harmful given a BP where the source part has a big frecuency of appearence but the target part appears rarely. For example, from our database we can extract the following BP : \"you # la que no\", where the English is the source language and the Spanish, the target language. Clearly, \"la que no\" is not a good translation of \"you\", so this phrase should have a low probability. However, from our aligned training database we obtain, P (f |e) = P (you|la que no) = 0.23 This BP is clearly overestimated due to sparseness. On the other, note that \"la que no\" cannot be considered an unusual trigram in Spanish. Hence, the language model does not penalise this target sequence either. So, the total probability (P (f |e)P (e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of the posterior [12] and lexical probabilities [1] [10] as additional features. Feature P (e|f ) In order to estimate the posterior phrase probability, we compute again the relative frequency but replacing the count of the target phrase by the count of the source phrase. P (e|f ) = N (f, e) N (f ) (5) where N'(f,e) means the number of times the phrase e is translated by f. If a phrase f has N > 1 possible translations, then each one contributes as 1/N. Adding this feature function we reduce the number of cases in which the overall probability is overestimated. This results in an important improvement in translation quality. IBM Model 1 We used IBM Model 1 to estimate the probability of a BP . As IBM Model 1 is a word translation and it gives the sum of all possible alignment probabilities, a lexical co-ocurrence effect is expected. This captures a sort of semantic coherence in translations. Therefore, the probability of a sentence pair is given by the following equation. P (f |e; M 1) = 1 (I + 1) J J j=1 I i=0 p(f j |e i ) (6) The p(f j |e i ) are the source-target IBM Model 1 word probabilities trained by GIZA++. Because the phrases are formed from the union of source-totarget and target-to-source alignments, there can be words that are not in the P (f j |e i ) table. In this case, the probability was taken to be 10 −40 . In addition, we have calculated the IBM −1 Model 1. P (e|f ; M 1) = 1 (J + 1) I I I=1 J j=0 p(e i |f j ) (7) Language Model The English language model plays an important role in the source channel model, see equation (2) , and also in its modification, see equation (3) . The English language model should give an idea of the sentence quality that is generated. As default language model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation (by using SRILM [16] ). The phrase penalty is a constant cost per produced phrase. Here, a negative weight, which means reducing the costs per phrase, results in a preference for adding phrases. Alternatively, by using a positive scaling factors, the system will favor less phrases. Word and Phrase Penalty Evaluation framework Corpus Statistics Experiments were performed to study the effect of our modifications in the phrases. The training material covers the transcriptions from April 1996 to September 2004. This material has been distributed by the European Parlament. In our experiments, we have used the distribution of RWTH of Aachen under the project of TC-STAR 1 . The test material was used in the first evaluation of the project in March 2005. In our case, we have used the development divided in two sets. This material corresponds to the transcriptions of the sessions from October the 21st to October the 28th. It has been distributed by ELDA 2 . Results are reported for Spanish-to-English translations. 1 http://www.tcstar.org/ 2 http://www.elda.org/ Experiments The decoder used for the presented translation system is reported in [2] . This decoder is called MARIE and it takes into account simultaneously all the 7 features functions described above. It implements a beam-search strategy. As evaluation criteria we use: the Word Error Rate (WER), the BLEU score [15] and the NIST score [3] . As follows we report the results for several experiments that show the performance of: the baseline, adding the posterior probability, IBM Model 1 and IBM1 −1 , and, finally, the modification of the phrases extraction. Optimisation. Significant improvements can be obtained by tuning the parameters of the features adequately. In the complet system we have 7 parameters to tune: the relatives frecuencies P (f |e) and P (e|f ), IBM Model 1 and its inverse, the word penalty, the phrase penalty and the weight of the language model. We applied the widely used algorithm SIMPLEX to optimise [9] . In Table 2 (line 5th), we see the final results. Baseline. We report the results of the baseline. We use the union alignment and we extract the BP of length 3. As default language model feature, we use the standard trigram with smoothing Kneser-Ney and interpolation. Also we tune the parameters (only two parameters) with the SIM-PLEX algorithm (see Table 2 ). Posterior probability. Table 2 shows the effect of using the posterior probability: P (e|f ). We use all the features but the P (e|f ) and we optimise the parameters. We see the results without this feature decrease around 1.1 points both in BLEU and WER (see line 2rd and 5th in Table 2 ). IBM Model 1. We do the same as in the paragraph above, we do not consider the IBM Model 1 and the IBM1 −1 . Under these conditions, the translation's quality decreases around 1.3 points both in BLEU and WER (see line 3th and 5th in Table 2 ). Modification of the Phrase Extraction. Finally, we made an experiment without modification of the phrases' length. We can see the comparison between: (1) the phrases of fixed maximum length of 3; and (2) including phrases with a maximum length of 5 which can not be generated by smaller phrases. We can see it in Table 2 (lines 4th and 5th). We observe that there is no much difference between the number of phrases, so this approach does not require more resources. However, we get slightly better scores. Shared Task This section explains the participation of \"Exploiting Parallel Texts for Statistical Machine Translation\". We used the EuroParl data provided for this shared task [4] . A word-to-word alignment was performed in both directions as explained in section 2. The phrase-based translation system which has been considered implements a total of 7 features (already explained in section 4). Notice that the language model has been trained with the training provided in the shared task. However, the optimization in the parameters has not been repeated, and we used the parameters obtained in the subsection above. We have obtained the results in the Table 3 . Conclusions We reported a new method to extract longer phrases without increasing the quantity of phrases (less than 0.5%). We also reported several features as P (e|f ) which in combination with the functions of the source-channel model provides significant improvement. Also, the feature IBM1 in combination with IBM1 −1 provides improved scores, too. Finally, we have optimized the parameters, and we provided the final results which have been presented in the Shared Task: Exploiting Parallel Acknowledgements The authors want to thank José B. Mariño, Adrià de Gispert, Josep M. Crego, Patrik Lambert and Rafael E. Banchs (members of the TALP Research Center) for their contribution to this work.",
         "12386445",
         "142a06b58eaa48bd4e53ade28dd175befe7a4124",
         "6",
         "https://aclanthology.org/W05-0827",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Ruiz Costa-juss{\\`a}, Marta  and\nFonollosa, Jos{\\'e} A. R.",
         "Improving Phrase-Based Statistical Translation by Modifying Phrase Extraction and Including Several Features",
         "149--154",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "ruiz-costa-jussa-fonollosa-2005-improving",
         null,
         null
        ],
        [
         "29",
         "2009.mtsummit-posters.17",
         "Most of the existing, easily available parallel texts to train a statistical machine translation system are from international organizations that use a particular jargon. In this paper, we consider the automatic adaptation of such a translation model to the news domain. The initial system was trained on more than 200M words of UN bitexts. We then explore large amounts of in-domain monolingual texts to modify the probability distribution of the phrase-table and to learn new task-specific phrase-pairs. This procedure achieved an improvement of 3.5 points BLEU on the test set in an Arabic/French statistical machine translation system. This result compares favorably with other large state-of-the-art systems for this language pair.",
         "Most of the existing, easily available parallel texts to train a statistical machine translation system are from international organizations that use a particular jargon. In this paper, we consider the automatic adaptation of such a translation model to the news domain. The initial system was trained on more than 200M words of UN bitexts. We then explore large amounts of in-domain monolingual texts to modify the probability distribution of the phrase-table and to learn new task-specific phrase-pairs. This procedure achieved an improvement of 3.5 points BLEU on the test set in an Arabic/French statistical machine translation system. This result compares favorably with other large state-of-the-art systems for this language pair. Introduction Adaptation of a statistical machine translation system (SMT) is a topic of increasing interest during the last years. Statistical (n-gram) language models are used in many domains and several approaches to adapt such models were proposed in the literature, for instance in the framework of automatic speech recognition. Many of these approaches were successfully used to adapt the language model of an SMT system. On the other hand, it seems more challenging to adapt the other components of an SMT system, namely the translation and reordering model. In this work we consider the adaptation of the translation model of a phrase-based SMT system. While rule-based machine translation rely on rules and linguistic resources built for that purpose, SMT systems can be developed without the need of any language-specific expertise and are only based on bilingual sentence-aligned data (\"bitexts\") and large monolingual texts. However, while monolingual data is usually available in large amounts and for a variety of tasks, bilingual texts are a sparse resource for most language pairs. Current parallel corpora mostly come from one domain (proceedings of the Canadian or European Parliament, or of the United Nations). This is problematic when SMT systems trained on such corpora are used for general translations, as the language jargon heavily used in these corpora is not appropriate for everyday life translations or translations in some other domain. This problem could be attacked by either searching for more in-domain training data, e.g. by exploring comparable corpora or the WEB, or by adapting the translation model to the task. In this work we consider translation model adaptation without using additional bilingual data. One can distinguish two types of translation model adaptation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007) , for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008) . This mixture approach has the advan-tage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005) . Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language (Snover et al., 2008) . Cross-lingual information retrieval was applied to find texts in the target language that are related to the domain of the source texts. However, it was difficult to get the alignments between the source and target phrases and an over-generalizing IBM1-style approach was used. Another direction of research is self-enhancing of the translation model. This was first proposed by (Ueffing, 2006) . The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing, 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008) . A somehow related approach was named lightlysupervised training (Schwenk, 2008) . In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. We could obtain small improvements in the BLEU score in a French/English translation system. Although this technique seems to be close to self enhancing as proposed by (Ueffing, 2006) , there is a conceptual difference. We do not use the test data to adapt the translation model, but large amounts of monolingual training data in the source language and we create a complete new model that can be applied to any test data without additional modification of the system. This kind of adapted system can be used in WEB service. In this paper, we use the same type of approach to adapt an generic Arabic/French translation system to the news domain. This task is interesting for several reasons: there is only a limited amount of in-domain bitexts available (about 1.2M words), but large amounts of out-of-domain bitexts (≈150M words of UN data) and both languages have a rich morphology. Usually, the Arabic source words are decomposed to detach pre-and suffixes. This helps to significantly reduce the size of the translation vocabulary and is reported to improve the translation quality. This morphological decomposition also results in many different and infrequent phrases which may lead to bad relative frequency estimates of the phrase translation probabilities. We are aiming in improving those estimates by using large amount of monolingual in-domain data. Finally, there seems to be a real need to translate between Arabic and French for the population in the Mediterranean area. This paper is organized as follows. In the next section we first describe the considered task and the available bilingual and monolingual resources. Section 3 describes the baseline SMT systems. The following section describe our adaptation technique. Results are summarized in section 5 and the paper concludes with a discussion and perspectives of this work. Task Description and resources In this paper, we consider the translation of news texts from Arabic into French. We are not aware of easily available aligned parallel corpora for this language pair. Fortunately, Arabic and French are both official languages of the United Nations. We crawled data from various sources of the United Nations over the period 1988-2008. This totals in almost 150M Arabic words. The Arabic and French texts were automatically sentence aligned. This amount of parallel texts is usually considered as more than sufficient to train an SMT system. Note however, that a particular jargon is used in the UN texts that is not appropriate for news-text translation. The French TRAMES 1 project considered the translation of Arabic Speeches to French. In the framework of this project, about 90h of Arabic TV and radio broadcast news were recorded, transcribed and translated into French. The sources are Orient, Qatar, BBC, Alarabiya, Aljazeera and Alalam. These high-quality domain specific bitexts of about 262k Arabic words were made available to us by the DGA. 2  Additional bilingual training data was obtained from the Project Syndicate WEB-site. 3 This data source is already used to build SMT systems to translate between European languages, in particular in the framework of the evaluations organized in junction with the workshops on statistical machine translation (Callison-Burch et al., 2007; Callison-Burch et al., 2008) . Some of the texts are also translated into Arabic. The scripts to access this WEBsite were kindly made available by P. Koehn. We crawled and aligned a total of 1.6M words. Note that these texts are not exactly broadcast news texts. The characteristics of the translation model training data is summarized in table 1. The number of words is given after tokenization. Arabic French Words Vocab Words Vocab DGA TRAMES 262k 30k 400k 18k News commentary 1.1M 67k 1.3M 41k UN 149M 712k 212M 420k The DGA also provided a test set that was created in the same way than the in-domain bitexts. Four high-quality reference translations are available. We randomly split this data into a development set for system tuning and an internal test set. The details of the development and test set are given in Table 2 . We are only aware of one other large Arabic/French news translation system, the one that was developed during the TRAMES project (Hasan and Ney, 2008) 3 Baseline system The baseline system is a standard phrase-based SMT system based on the the Moses SMT toolkit (Koehn et al., 2007) . It uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. It constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008) . 4 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. All the bitexts were concatenated. The parameters of Moses are tuned on the development data using the CMERT tool. Tokenization There is a large body of work in the literature showing that a morphological decomposition of the Arabic words can improve the word coverage and by these means the translation quality, see for instance (Habash and Sadat, 2006) . It is clear that such a decomposition is most helpful when the translation model training data is limited, but this is less obvious for tasks where several hundreds of millions of words of bitexts are available. Most of the published work is based on the freely available tools, like the Buckwalter transliterator and the MADA and TOKAN tools for morphological analysis from Columbia University. In this work, we compare two different tokenization of the Arabic source text: a full word mode and the morphological decomposition provided by the sentence analysis module of SYSTRAN's rulebased Arabic/English translation software. Sentence analysis represents a large share of the computations in a rule-based system. This process first applies decomposition rules coupled with a word dictionary. For words that are not known in the dictionary, the most likely decomposition is guessed. In general, all possible decompositions of each word are generated and then filtered in the context of the sentence. This step uses lexical knowledge and a global analysis of the sentences. This morphological decomposition drastically reduced the vocabulary of the Arabic bitexts: it was almost divided by two. The French texts were tokenized using the tools of the Moses suite. Punctuation and case were preserved. LM training In contrast to the translation model, many resources are available to train a LM optimized on French broadcast news texts. We used the French side of the bitexts, data form the European and the Canadian parliament, news-data crawled in the framework for the 2009 WMT evaluation, 5 other WEB data collected by ourselves and finally LDC's Gigaword corpus. Separate 4-gram back-off LMs were build on each data source with the SRI LM toolkit (Stolcke, 2002) coefficients with an EM procedure. The perplexities of these LMs are given in Table 3 . Translation model adaptation by lightly-supervised training The goal of this work is to adapt the translation model without using additional bilingual data. Instead we will use in-domain monolingual data in the source language. Usually it is relatively easy to obtain large collections of such data, in particular in the news domain as considered in this work. We use parts of the LDC Arabic Gigaword corpus, but more recent texts could be easily found on the Internet. These texts are translated by an initial, unadapted SMT system. We then need to filter the automatic translations in order to keep only the \"good ones\" for addition to the translation model training data. This selection could take advantage of word-based confidence scores (Ueffing, 2007) . We use the sentencelength normalized log-likelihoods of the decoder. These selected translations are used as additional indomain bitexts and the standard procedure to build a new SMT system is performed, i.e. word alignment with GIZA++, phrase extraction and tuning of the system parameters. Alternately, we could reuse the alignments established by the translation process since the Moses decoder is able to output the phrase and word alignments. This would speed up the process of creating the adapted SMT system since we skip the timeconsuming word alignment performed by GIZA++. It could also be that the decoder-induced word alignments are more appropriate than those performed by GIZA++. This was partially investigated in the framework of pivot translation to produce artificially bitexts in another language (Bertoldi et al., 2008) . Finally, instead of only using the 1-best translation we could also use the n-best list. LDC's Arabic and French Gigaword corpora are described in Table 4 . There is only one source that does exist in both languages: the AFP collection. It is likely that the Arabic and French texts partially cover the same facts, but they are usually not direct translations. 6 In fact, we were informed that journalists at AFP have the possibility to freely change the sentences when they report on a fact based on text already available in another language. Nevertheless, it can be expected that using these texts in the target language model helps the SMT system to produce good translations. This language model training data can be considered as some form of light supervision and we will therefore use the term lightly-supervised training (Schwenk, 2008) . This can be compared to the research in speech recognition where the same term was used for supervision that either comes from approximate transcriptions of the audio signal (closed captions) or related language model training data. Source Arabic French AFP 145M 570M APW -200M ASB 7M - HYT 175M - NHR 188M - UMH 1M - XIN 58M - In this work, we have processed the AFP Arabic text only, but the other texts (ASB, HYT, . . .) could be processed in the same manner. In fact it is an interesting question whether the availability of related or even \"comparable\" texts for the target language model is a necessary condition for our approach to work. All these issues will be explored in future research. Experimental Evaluation We first performed experiments using different amounts of bitexts to train the translation model and analyzed the benefits of the morphological decomposition of the Arabic words. The results are summarized in Table 5 . The TRAMES training corpus contains several lines with more than 100 words. These can't be processed by the GIZA++ tool and the were discarded. This was the case for about 6% of the data. In future research, we will try to split those lines into shorter sentences. As expected, the morphological decomposition of the Arabic words is very helpful when only a small amount of training data is available: using only the TRAMES and news-commentary in-domain data we observed an improvement of 4.6 BLEU points (first and second line in Table 5 ). Note that in this case we have actually less training data since the morphological decomposition leads to longer phrases out of which many are discarded by the 100 words limit of GIZA++. Somewhat surprisingly, the morphological decomposition still achieves a significant improvement of 1 BLEU point when more than 200M words of bitexts are available (last two lines in Table 5). Translation model adaptation The best system we were able to build with all human provided translations was used to translate all the AFP news texts from Arabic to French. The phrase table of whole AFP Gigaword corpus with such a large system is a computational challenge since it is impossible to charge the whole phrase table into memory. The Moses system supports two procedures to deal with this problem: filtering of the phrase table or binary on-disk phrase tables. Neither technique can be applied here. The phrase table is still too big in the first case and the binary representation of the whole phrase table occupies too much space on disk. This problem could be eventually approached with a distributed representation of the data structures. We finally adopted a combination of both techniques: the AFP corpus is split into parts with 50k lines (approximately 1.5M words), the phrase table is filtered for this data and then binarized. This made it possible to load the LM into memory and to have a process size of less then 20GB. The total translation time was more than 2700 hours. 7 These automatic translations were filtered according to the sentence-length normalized log-likelihood and the most likely ones were used as bitexts. Different amounts of data can be obtained by varying the threshold on the likelihood. The BLEU scores on the development and test data in function of the total size of the bitexts are shown in figure 1 . In these experiments we only use the in-domain humanprovided bitexts (TRAMES and news-commentary) -the UN data being replaced by the automatic translations. The best value on the development data was obtained for a total of 48M words of bitexts. The BLEU score on the development data is 45.44 and 43.68 on the test data respectively (see also table 6 ). This is an improvement of 3.5 BLEU points on both data sets. We analyzed the phrase table of the original system trained on all human provided data, including UN, and the one of the automatically adapted system. This is summarized in table 7. The original phrase table had 329M entries out of which 22.9M could be potentially applied on the test data. The phrase table of the adapted system on the other hand used only 700k out of a total of 8.6M phrases. It seems clear that the phrase table obtained by training on the UN data contains many entries that are not useful, or eventually even correspond to wrong translations. Surprisingly, the phrase table of the adapted system is not only substantially smaller, but even contains about 11% more entries (18029 with respect to 16263). All these entries correspond to new sequences of known words since lightlysupervised training cannot extend the source or target side vocabulary. We conjecture that this is particularly important with the morphological decomposition of the Arabic words. This decomposition reduces the vocabulary size of the source language, but produces on the other hand many possible se- Source: ‫ق.‬ ِ ‫ساب‬ ّ ‫ال‬ ‫ي‬ ّ ِ ‫راق‬ َ ِ ‫لع‬ َ ‫ا‬ ‫ِيس‬ ‫رئ‬ َ ‫ل‬ َ ‫ا‬ ّ ‫ضد‬ ِ ‫م‬ َ ‫ُه‬ ‫ت‬ ِ ‫حة‬ َ ِ ‫لئ‬ َ ِ ‫جيه‬ ِ ‫َو‬ ‫ت‬ ِ ‫ب‬ ‫ِيل‬ ‫َل‬ ‫ق‬ ُ ‫منذ‬ ُ ‫ت‬ َ ‫َأ‬ ‫َد‬ ‫ب‬ ‫ّة‬ ‫ِي‬ ‫راق‬ َ ِ ‫الع‬ ‫ة‬ ُ ‫م‬ َ َ ‫محك‬ َ ‫ل‬ َ ‫ا‬ Base: le tribunal irakien a commencé depuis peu par la direction du règlement des accusations contre l'ancien président irakien. Adapt: le tribunal irakien a commencé depuis peu une liste d'accusations contre l'ancien président irakien. Ref: La Cour irakienne a commencé à dresser la liste des inculpations de l'ancien président irakien. Source: ‫ِي‬ ‫ف‬ ‫ه‬ ّ ‫لل‬ َ ‫ا‬ ‫رام‬ َ ‫ِي‬ ‫ف‬ ً ‫شطا‬ ِ ‫َا‬ ‫ن‬ ً ‫يل‬ َ ‫ل‬ ‫ل‬ َ َ ‫َق‬ ‫ِعت‬ ‫ا‬ ‫ي‬ ّ ِ ‫ِيل‬ ‫رائ‬ َ ‫س‬ ِ ‫ال‬ ‫ش‬ َ ‫جي‬ َ ‫ال‬ ‫ن‬ ّ َ ‫أ‬ ‫ّة‬ ‫ِي‬ ‫ِيل‬ ‫رائ‬ َ ‫س‬ ِ ‫إ‬ ‫ة‬ ٌ ّ ‫ي‬ ِ ‫ر‬ َ ‫َسك‬ ‫ع‬ ‫ر‬ ُ ِ ‫صاد‬ َ ‫م‬ َ ‫َت‬ ‫َاد‬ ‫ف‬ َ ‫أ‬ ‫ن‬ َ ‫رو‬ ُ ‫ض‬ ّ ‫ح‬ َ ُ ‫ي‬ ‫ُوا‬ ‫ان‬ َ ‫ك‬ ‫رين‬ َ ‫خ‬ َ ‫آ‬ ‫ن‬ ِ ‫ي‬ َ ‫شط‬ ِ ‫َا‬ ‫ن‬ ‫ل‬ ُ ‫َا‬ ‫ِق‬ ‫اعت‬ ‫م‬ ّ َ ‫ت‬ ‫ما‬ َ َ ‫ك‬ ‫ّة‬ ‫ـي‬ ِ ‫َرب‬ ‫الغ‬ ِ ‫ّة‬ ‫ضف‬ ّ ‫ال‬ Base: De source militaire israélienne a indiqué que l'armée israélienne a arrêté dans la nuit militants à Ramallah en Cisjordanie ont été arrêtés autres militants qui ... Adapt: Selon des sources militaires israéliennes, l'armée israélienne a arrêté dans la nuit de militants à Ramallah, en Cisjordanie, a également été arrêté deux autres activistes qui ... Ref: Des sources militaires israéliennes ont indiqué que l'armée israélienne a arrêté de nuit un activiste à Ramallah en Cisjordanie, ainsi que deux autres activistes qui ... Source: ‫من.‬ َ َ ‫لي‬ َ ‫ا‬ ‫َة،‬ ‫حاف‬ َ ‫ص‬ ّ ‫ال‬ ‫ة‬ ُ َ ‫جول‬ َ ‫ي،‬ ِ ‫َار‬ ‫ُب‬ ‫الغ‬ ُ ‫مد‬ ّ ‫ح‬ َ ‫م‬ ُ Base: Mohammed du brouillard, le cycle de la presse, au Yémen. Adapt: Mohammed, une tournée de la presse le Yémen. Ref: Mohamed Al-Ghobari, tour de la presse, Yémen. Source:  . quences of tokens. It seems important to a include sequences of these tokens in the phrase table that appear in in-domain data. As a side effect, the smaller phrase-table of the adapted system also leads to a 40% faster translation. We compared the translations of the unadapted and adapted systems: the TER is about 30 in both directions, meaning that the outputs differ substantially. Some example translations are shown in figure 2. The adapted system clearly produces better output in these examples. There remain of course some errors in these sentences, but we argue that the quality is high enough for an human being to capture most of the meaning of the sentences. ‫راق.‬ َ ِ ‫الع‬ ‫ن‬ َ ‫م‬ ِ ‫َا‬ ‫ِه‬ ‫ُود‬ ‫جن‬ ُ ‫ب‬ ِ ‫سح‬ َ ‫ِي‬ ‫ف‬ ً ‫يضا‬ َ ‫أ‬ ‫لند‬ َ ‫َاي‬ ‫ت‬ ‫َت‬ ‫رع‬ َ ‫ش‬ َ ‫رى‬ َ ‫خ‬ ُ ‫أ‬ ٍ ‫ة‬ َ ‫جه‬ ِ ‫من‬ ِ Base: d' Conclusion Statistical machine translation is today used to rapidly create automatic translations systems for a variety of tasks. In principle, we only need aligned example translations and monolingual data in the target language. However, for many application domains and language pairs there are no appropriate in-domain parallel texts to train the translation model. On the other hand, large generic bitexts may be available. In this work we consider such a configuration: the translation of broadcast news texts from Arabic to French. We have a little more than 1M words of in-domain bitexts and about 150M words of generic bitexts from the United Nations. This system is automatically adapted to the news domain by using large amounts of monolingual texts, namely LDC's collection of Arabic AFP texts from 1994 to 2006. These texts were processed by the initial SMT system and the most reliable automatic translations were added to the bitexts and a new system was trained. By these means we achieved an improvement in the BLEU score of 3.5 points on the test set. This system actually uses less bitexts than the generic one since the generic UN bitexts are not used any more. An analysis of the created phrase table seems to indicate that the adaptation of the translation model leads to much smaller and more concise phrases. Our best system achieves a BLEU score of 43.68 on the test set which compares favorably with other large state-of-the-art systems for this language pair. The proposed algorithm is generic and could be applied to other language pairs and applications domains. We only need a good initial generic SMT system and in-domain monolingual texts in the source language.r It is interesting to compare our approach to self-learning as proposed in (Ueffing, 2006) Selflearning was applied to small amounts of test data only while we use several hundreds of million words of training data in the source language. We build a complete new phrase table instead of interpolating a small \"adapted phrase table\" with a generic one. Finally, self-learning was applied during the translation process and must be repeated for each new test data. This is computationally expensive and is difficult to use in on-line translation. The approach proposed in this paper applies translation model adaptation once and builds a new SMT system that can be then applied to any test data (ideally from the same domain). Several extensions of the proposed approach can be envisioned, namely improved confidence scores for filtering the most reliable translations, processing of n-best lists instead of using the most likely translation only, and reuse of the decoder-induced word alignments instead of rerunning GIZA++. We are currently working on these issues. Acknowledgments The Arabic/French corpus of broadcasts news as well as the corresponding test set were made available to us by the DGA. This work has been partially funded by the French Government under the project INSTAR (ANR JCJC06 143038) and the European Commission under the project EuromatrixPlus.",
         "28006881",
         "53d6c70d1a4871a8a8164f86d824313ef655a8b7",
         "41",
         "https://aclanthology.org/2009.mtsummit-posters.17",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Schwenk, Holger  and\nSenellart, Jean",
         "Translation Model Adaptation for an {A}rabic/{F}rench News Translation System by Lightly- Supervised Training",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "schwenk-senellart-2009-translation",
         null,
         null
        ],
        [
         "30",
         "R13-1053",
         "Translating from English, a morphologically poor language, into morphologically rich languages such as Persian comes with many challenges. In this paper, we present an approach to rich morphology prediction using a parallel corpus. We focus on the verb conjugation as the most important and problematic phenomenon in the context of morphology in Persian. We define a set of linguistic features using both English and Persian linguistic information, and use an English-Persian parallel corpus to train our model. Then, we predict six morphological features of the verb and generate inflected verb form using its lemma. In our experiments, we generate verb form with the most common feature values as a baseline. The results of our experiments show an improvement of almost 2.1% absolute BLEU score on a test set containing 16K sentences.",
         "Translating from English, a morphologically poor language, into morphologically rich languages such as Persian comes with many challenges. In this paper, we present an approach to rich morphology prediction using a parallel corpus. We focus on the verb conjugation as the most important and problematic phenomenon in the context of morphology in Persian. We define a set of linguistic features using both English and Persian linguistic information, and use an English-Persian parallel corpus to train our model. Then, we predict six morphological features of the verb and generate inflected verb form using its lemma. In our experiments, we generate verb form with the most common feature values as a baseline. The results of our experiments show an improvement of almost 2.1% absolute BLEU score on a test set containing 16K sentences. Introduction One of the main limitations of statistical machine translation (SMT) is the sensitivity to data sparseness, due to the word-based or phrased-based approach incorporated in SMT (Koehn et al., 2003) . This problem becomes severe in the translation from or into a morphologically rich language, where a word stem appears in many completely different surface forms. Therefore, morphological analysis is an important phase in the translation from or into such languages, because it reduces the sparseness of model. So, modeling rich morphology in machine translations (MT) has received a lot of research interest in several studies. In this paper, we present a novel approach to rich morphology prediction for Persian as target language. We focus on the verb conjugation as a highly inflecting class of words and an important part of morphological processing in Persian. Our model incorporates decision tree classifier (DTC) (Quinlan, 1986) , which is an approach to multistage decision making. In order to train DTC, we use both English and Persian linguistic information such as syntactic parse tree and dependency relations obtained from an English-Persian parallel corpus. Morphological features which we predict and use to generate the inflected form of verb are voice (VOC), mood (MOD), number (NUM), tense (TEN) , negation (NEG) and person (PER). Our proposed model can be used as a component to generate rich morphology for any kind of languages and MTs. The reminder of the paper is organized as follows: Section 2 briefly reviews some challenges in Persian verb conjugation, Section 3 presents our proposed approach to generate rich morphology, in Section 4 our experiments and results are presented, in Section 5 we cover conclusions and future work, and finally, in Section 6 we describe related works. Morphology Challenges of the Persian Verbs Verbs in Persian have a complex inflectional system (Megerdoomian, 2004) . This complexity appears in the following aspects: • Different verb forms • Different verb stems • Affixes marking inflections • Auxiliaries used in certain tenses Simple form and compound form are two forms used in Persian verbal system. Simple form is broken into two categories according to the stem used in its formation. Compound form refers to those that require an auxiliary to form a correct verb. Two stems are used to construct a verb: present stem and past stem. Each of which is used in creating of specific tenses. We cannot derive the two stems from each other due to different surface forms they usually have. Therefore, they treated as distinct characteristics of verbs. Several affixes are combined with stems to mark MOD, NUM, NEG and PER inflections. Auxiliaries are used to make a compound form in certain tenses to indicate VOC and TEN inflections, similar to HAVE and BE in Approach Our proposed approach is broken into two main steps: DTC training and Morphology prediction. Then we can generate a verb form using a finite state automaton (Megerdoomian, 2004) , if we are given the six morphological features of the verb. In the next subsections we describe these steps more precisely. DTC Training To make train and test set, we use an English-Persian parallel corpus containing 399K sentences Table 2 : Some statistics about the English-Persian parallel corpus (Mansouri and Faili, 2012) . (367K to train,16K to validate and 16K to test). More details about this corpus, which is used by Mansouri and Faili (2012) to build an SMT, are presented in Table 2 . Giza++ (Och and Ney, 2003) is used to word alignment. We only select such an alignment that is most probable to translate both from English to Persian and Persian to English among those assigned to each verb. With this heuristic we ignore a lot of alignments to produce a high quality data set. We selected 100 sentences randomly and evaluated the alignments manually, so that 27% recall and 93% precision were obtained. Then, we define a set of syntactic features on English side as DTC learning features. These features consist of several language-specific features such as English part-of-speech tag (POS) of the verb, dependency relationships of the verb and POS of subject of the verb. English is parsed using Stanford Parser (Klein and Manning, 2003) . After that, we can produce training data set by analyzing the Persian verb aligned to each English verb using (Rasooli et al., 2011) , in which two unsupervised learning methods have been proposed to identify compound verbs with their corresponding morphological features. The first one which is extending the concept of pointwise mutual information, uses a bootstraping method and the second one uses K-means clustering algorithm to detect compound verbs. However, as we have the verb, we only use their proposed method to determine VOC, MOD, NUM, TEN, NEG and PER for a given verb as our class labels. Also, we use their tool to extract the lemma of the verb (in Figure 1 \"Verb lemmatizer\" refers to this tool in which there is a lookup table to find the lemma of a verb). This lemma is used to generate an inflected verb form using FSA. phemes. Unlike these approaches, we predict morphological features like El Kholy and Habash (2012a and b) . Using our training data set, we build six language specific DTCs to predict each of the morphological features. Each DTC uses a subset of our feature set and predicts corresponding morphology feature independently. Then, we use a FSA to generate an inflected verb form using these six morphological features. Figure 1 , shows the general schema of verb generation process. Morphology Prediction Table 3 shows Correct Classification Ratio (CCR) of each DTC learned on our train data containing 178782 entries and evaluated on a test set containing more than 20k verbs. The most common feature value is used as our baseline for each classifier. The most improvement is achieved in the prediction of MOD and NUM. Others have high CCR but they also have very high baselines. Experiments In this section, we present the results of our experiments on a test set containing 16K sentences selected from an English-Persian parallel corpus. As the main goals of our experiments, we are interested in knowing the effectiveness of our approach to rich morphology prediction and the contribution each feature has. To do so, like Kholy and Habash (2012) , who use aligned sentence pair of reference translations (reference experiments) instead of the output of an MT system as input, we also perform reference experiments because they are golden in terms of word order, lemma choice and morphological features. Table 4 shows detailed n-gram BLEU (Papineni et al., 2002) precision (for n=1,2,3,4), BLEU and TER (Snover et al., 2006) scores for morphology generation using gold lemma with the most common feature values (LEM) as a baseline and other gold morphological features and their combinations as our reference experiments. In this experiment, we replace each sentence verb with predicted verb generated by FSA using gold lemma plus the most common feature values as a baseline. In comparison with the baseline used by El Kholy and Habash (2012) , this baseline is more stringent. As another baseline we have used a rule-based morphological analyzer which determines morphological features of the verb grammatically and generates inflected verb form (this rule-based morphological analyzer uses syntactic parse, POS tags and dependency relationships of English sentence). We use each gold feature separately to investigate the contribution each feature has. Finally, we combine gold features incrementally based on their CCR. Adding more features improve BLEU and TER scores. Since, there are some cases in which with the same morphological features it is possible to generate different but correct verb forms, the maximum BLEU score of 100 is hard to be reached even if we are given the gold features. So, the best result (97.90 of BLEU and 0.0114 of TER) could be considered as an upper bound for proposed approach. Note that, these results are obtained from our reference experiments in which a reference is duplicated and modified by our approach. In fact, there is no translation task here and a reference is evaluated by its modified version. We perform the same reference experiments on the same data using predicted features instead of the gold features. Table 5 reports the results of detailed n-gram BLEU precision, BLUE and TER scores. According to the results, our approach outperforms the baselines in all configurations. The best configuration uses all predicted features and shows an improvement of about 2.1% absolute BLEU score and 0.102% absolute TER against our first baseline. Also, in comparison with our second baseline, rule-based approach, we achieve improvements of about 1.6% absolute BLEU score and 0.103% absolute TER. Conclusions and Future Work In this paper we present a supervised approach to rich morphology prediction. We focus on verb inflections as a highly inflecting class of words in Persian, a morphologically rich language. Using different combination of morphological features to generate inflected verb form, we evaluate our approach on a test set containing 16K sentences and obtain better BLEU and TER scores compared with our baseline, morphology generation with lemma plus the most common feature values. Our proposed approach predicts each morphological feature independently. In the future, we plan to investigate how the features affect each other to present an order in which a predicted morphological feature is used as a learning feature for the next one. Furthermore, we also plan to use our approach as a post processing morphology generation to improve machine translation output. 6 Related Work In this section we introduce the main approaches to morphology generation. The first approach is based on factored models, an extension of phrased-based SMT model (Koehn and Hoang, 2007) . In this approach each word is annotated using morphology tags on morphologically rich side. Then, morphology generation is done based on the word level instead of phrase level, which is also the limitation of this approach. A similar approach is used by Avramidis and Koehn (2008) to translate from English into Greek and Czech. They especially focus on noun cases and verb persons. Mapping from syntax to morphology in factored model is used by Yeniterzi and Oflazer (2010) to improve English-Turkish SMT. Hierarchical phrase-based translation, an extension of factored translation model, proposed by Subotin (2011) to generate complex morphology using a discriminative model for Czech as the target laguage. Maximum entropy model is another approach used by Minkov et al. (2007) for English-Arabic and English-Russian MT. They proposed a postprocessing probabilistic framework for morphology generation utilizing a rich set of morphological knowledge sources. There are some similar approaches used by Toutanova et al. (2008) for Arabic and Russian as the target languages and by Clifton and Sarkar (2011) for English-Finnish SMT. In these approaches, the model of morphol-ogy prediction is an independent process of the SMT system. Segmentation is another approach that improves MT by reducing the data sparseness of translation model and increasing the similarity between two sides (Goldwater and McClosky, 2005; Luong et al., 2010; Oflazer, 2008) . This method analyzes morphologically rich side and unpacks inflected word forms into simpler components. Goldwater and McClosky (2005) showed that modifying Czech as the input language using 'pseudowords' improves the Czech-English machine translation system. Similar approaches are used by Oflazer (2008) for English to Turkish SMT, Luong et al. (2010) for translating from English into Finnish and Namdar et al. (2013) to improve Persian-English SMT. Recently, a novel approach to generate rich morphology is proposed by El Kholy and Habash (2012) . They use SMT to generate inflected Arabic tokens from a given sequence of lemmas and any subset of morphological features. They also have used their proposed method to model rich morphology in SMT (El Kholy and Habash, 2012) . Since we use lemma and the most common feature values as our baseline, the results of their experiments is somewhat comparable to ours. However, they use only lemma with no prediction as their baseline. So, our baseline is more stringent than the baseline used by El Kholy and Habash (2012) . Our work is conceptually similar to that of de Gispert and Marino (2008) , in which they incorporate a morphological classifier for Spanish verbs and define a collection of context dependent linguistic features (CDLFs), and predict each morphology feature such as PER or NUM. However, we use a different set of CDLFs and incorporate DTC to predict the morphology features of Persian verbs. Acknowledgment This work has been partially funded by Iran Telecom Research Center (ITRC) under contract number 9513/500.",
         "15414615",
         "fbf09b3df0673d93a182d3cbc99a56b7fc3c0b6d",
         "2",
         "https://aclanthology.org/R13-1053",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Mahmoudi, Alireza  and\nArabsorkhi, Mohsen  and\nFaili, Heshaam",
         "Supervised Morphology Generation Using Parallel Corpus",
         "408--414",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "mahmoudi-etal-2013-supervised",
         null,
         null
        ],
        [
         "31",
         "W05-0828",
         "We motivate our contribution to the shared MT task as a first step towards an integrated architecture that combines advantages of statistical and knowledge-based approaches. Translations were generated using the Pharaoh decoder with tables derived from the provided alignments for all four languages, and for three of them using web-based and locally installed commercial systems. We then applied statistical and heuristic algorithms to select the most promising translation out of each set of candidates obtained from a source sentence. Results and possible refinements are discussed.",
         "We motivate our contribution to the shared MT task as a first step towards an integrated architecture that combines advantages of statistical and knowledge-based approaches. Translations were generated using the Pharaoh decoder with tables derived from the provided alignments for all four languages, and for three of them using web-based and locally installed commercial systems. We then applied statistical and heuristic algorithms to select the most promising translation out of each set of candidates obtained from a source sentence. Results and possible refinements are discussed. Motivation and Long-term Perspective \"The problem of robust, efficient and reliable speech-to-speech translation can only be cracked by the combined muscle of deep and shallow processing approaches.\" (Wahlster, 2001) Although this statement has been coined in the context of VerbMobil, aiming at translation for direct communication, it appears also realistic for many other translation scenarios, where demands on robustness, coverage, or adaptability on the input side and quality on the output side go beyond today's technological possibilities. The increasing availability of MT engines and the need for better quality has motivated considerable efforts to combine multiple engines into one \"super-engine\" that is hopefully better than any of its ingredients, an idea pionieered in (Frederking and Nirenburg, 1994) . So far, the larger group of related publications has focused on the task of selecting, from a set of translation candidates obtained from different engines, one translation that looks most promising (Tidhar and Küssner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004) . But also the more challenging problem of decomposing the candidates and re-assembling from the pieces a new sentence, hopefully better than any of the given inputs, has recently gained considerable attention (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005) . Although statistical MT approaches currently come out as winners in most comparative evaluations, it is clear that the achievable quality of methods relying purely on lookup of fixed phrases will be limited by the simple fact that for any given combination of topic, application scenario, language pair, and text style there will never be sufficient amounts of pre-existing translations to satisfy the needs of purely data-driven approaches. Rule-based approaches can exploit the effort that goes into single entries in their knowledge repositories in a broader way, as these entries can be unfolded, via rule applications, into large numbers of possible usages. However, this increased generality comes at significant costs for the acquisition of the required knowledge, which needs to be encoded by specialists in formalisms requiring extensive training to be used. In order to push the limits of today's MT technology, integrative approaches will have to be developed that combine the relative advantages of both paradigms and use them to compensate for their disadvantages. In particular, it should be possible to turn single instances of words and constructions found in training data into internal representations that allow them to be used in more general ways. In a first step towards the development of integrated solutions, we need to investigate the relative strengths and weaknesses of competing systems on the level of the target text, i.e. find out which sentences and which constructions are rendered well by which type of engine. In a second step, such an analysis will then make it possible to take the outcomes of various engines apart and re-assemble from the building blocks new translations that avoid errors made by the individual engines, i.e. to find integrated solutions that improve over the best of the candidates they have been built from. Once this can be done, the third and final step will involve feed back of corrections into the individual systems, such that differences between system behaviour can trigger (potentially after manual resolution of unclear cases) system updates and mutual learning. In the long term, one would hope to achieve a setup where a group of MT engines can converge to a committee that typically disagrees only in truly difficult cases. In such a committee, remaining dissent between the members would be a symptom of unresolved ambiguity, that would warrant the cost of manual intervention by the fact that the system as a whole can actually learn from the additional evidence. We expect this setup to be particularly effective when existing MT engines have to be ported to new application domains. Here, a rule-based engine would be able to profit from its more generic knowledge during the early stages of the transition and could teach unseen correspondences of known words and phrases to the SMT engine, whereas the SMT system would bring in its abilities to apply known phrase pairs in novel contexts and quickly learn new vocabulary from examples. Collecting Translation Candidates Setting up Statistical MT In the general picture laid out in the preceding section, statistical MT plays an important role for several reasons. On one hand, the construction of a relatively well-performing phrase-based SMT system from a given set of parallel corpora is no more overly difficult, especially if -as in the case in this shared task -word alignments and a decoder are provided. Furthermore, once the second task in our chain will have been surmounted, it will be relatively easy to feed back building blocks of improved translations into the phrase table, which constitutes the central resource of the SMT system Therefore, SMT facilitates experiments aiming at dynamic and interactive adaptation, the results of which should then also be applicable to MT engines that represent knowledge in a more condensed form. In order to collect material for testing these ideas, we constructed phrase tables for all four languages, following roughly the procedure given in (Koehn, 2004 ) but deviating in one detail related to the treatment of unaligned words at the beginning or end of the phrases 1 . We used the Pharaoh decoder as described on http://www.statmt.org/wpt05/mt-sharedtask/ after normalization of all tables to lower case. Using Commercial Engines As our main interest is in the integration of statistical and rule-based MT, we tried to collect results from \"conventional\" MT systems that had more or less uniform characteristics across the languages involved. We could not find MT engines supporting all four source languages, and therefore decided to drop Finnish for this part of the experiment. We sent the texts of the other three languages through several incarnations of Systran-based MT Web-services 2 and through an installation of Lernout & Hauspie Power Translator Pro, Version 6.43. 3 1 We used slightly more restrictive conditions that resulted in a 5.76% reduction of phrase table size 2 The results were incomplete and different, but sufficiently close to each other so that it did not seem worthwhile to explore the differences systematically. Instead we ranked the services according to errors in an informal comparison and took for each sentence the first available translation in this order. 3 After having collected or computed all translations, we observed that in the case of French, both systems were quite sensitive to the fact that the apostrophes were formatted as separate tokens in the source texts (l ' homme instead of l'homme). We therefore modified and retranslated the French texts, but did not explore possible effects of similar transformations in the other languages. Heuristic Selection Approach We implemented two different ways to select, out of a set of alternative translations of a given sentence, one that looks most promising. The first approach is purely heuristic and is limited to the case where more than two candidates are given. For each candidate, we collect a set of features, consisting of words and word n-grams (n ∈ {2, 3, 4}). Each of these features is weighted by the number of candidates it appears in, and the candidate with the largest feature weight per word is taken. This can be seen as the similarity of each of the candidate to a prototypical version composed as a weighted mixture of the collection, or as being remotely related to a sentence-specific language model derived from the candidates. The heuristic measure was used to select \"favorite\" from each group of competing translations obtained from the same source sentence, yielding a fourth set of translations for the sentences given in DE, FR, and ES. A particularity of the shared task is the fact that the source sentences of the development and test sets form a parallel corpus. Therefore, we can not only integrate multiple translations of the same source sentence into a hopefully better version, but we can merge the translations of corresponding parts from different source languages into a target form that combines their advantages. This approach, called triangulation in (Kay, 1997) , can be motivated by the fact that most cases of translation for dissemination involve multiple target languages; hence one can assume that, except for the very first of them, renderings in multiple languages exist and can be used as input to the next step 4 . See also (Och and Ney, 2001) for some related empirical evidence. In order to obtain a first impression of the potential of triangulation in the domain of parliament debates, we applied the selection heuristics to a set of four translations, one from Finnish, the other three the result of the selections mentioned above. Results and Discussion The BLEU scores (Papineni et al., 2002) 1 . These results show that in each group of translations for a given source language, the statistical engine came out best. Furthermore, our heuristic approach for the selection of the best among a small set of candidate translations did not result in an increase of the measured BLEU score, but typically gave a score that was only slightly better than the second best of the ingredients. This somewhat disappointing result can be explained in two ways. Apparently, the selection heuristic does not give effective estimates of translation quality for the candidates. Furthermore, the granularity on which the choices have to bee made is too coarse, i.e. the pieces for which the symbolic engines do produce better translations than the SMT engine are accompanied by too many bad choices so that the net effect is negative. Statistical Selection The other score we used was based on probabilities as computed by the trigram language model for English provided by the organizers of the task, in a representation compatible with the SRI LM toolkit (Stolcke, 2002) . However, a correct implementation for obtaining these estimates was not available in time, so the selections generated from the statistical language model could not be used for official submissions, but were generated and evaluated after the closing date. The results, also displayed in Table 1 , show that this approach can lead to slight improvements of the BLEU score, which however turn out not to be statistically sigificant in then sense of (Zhang et al., 2004) . Next Steps When we started the experiments reported here, the hope was to find relatively simple methods to select the best among a small set of candidate translations and to achieve significant improvements of a hybrid architecture over a purely statistical approach. Although we could indeed measure certain improvements, these are not yet big enough for a conclusive \"proof of concept\". We have started a refinement of our approach that can not only pick the best among translations of complete sentences, but also judge the quality of the building blocks from which the translations are composed. First informal results look very promising. Once we can replace single phrases that appear in one translation by better alternatives taken from a competing candidate, chances are good that a significant increase of the overall translation quality can be achieved. Acknowledgements This work has been funded by the Deutsche Forschungsgemeinschaft. We want to thank two anonymous reviewers for numerous pointers to relevant literature, Bogdan Sacaleanu for his help with the collection of translations from on-line MT engines, as well as the organizers of the shared task for making these interesting experiments possible.",
         "9766288",
         "cb55a2758d26d96dadcf301a11dadee9f703f3c0",
         "18",
         "https://aclanthology.org/W05-0828",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Eisele, Andreas",
         "First Steps towards Multi-Engine Machine Translation",
         "155--158",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "eisele-2005-first",
         null,
         null
        ],
        [
         "32",
         "2013.mtsummit-papers.1",
         "This paper proposes a way of augmenting bilingual terminologies by using a \"generate and validate\" method. Using existing bilingual terminologies, the method generates \"potential\" bilingual multi-word term pairs and validates their status by searching web documents to check whether such terms actually exist in each language. Unlike most existing bilingual term extraction methods, which use parallel or comparable corpora, the proposed method can take advantage of a wider variety of textual corpora. Experiments using Japanese-English terminologies of five domains show that the method is highly promising.",
         "This paper proposes a way of augmenting bilingual terminologies by using a \"generate and validate\" method. Using existing bilingual terminologies, the method generates \"potential\" bilingual multi-word term pairs and validates their status by searching web documents to check whether such terms actually exist in each language. Unlike most existing bilingual term extraction methods, which use parallel or comparable corpora, the proposed method can take advantage of a wider variety of textual corpora. Experiments using Japanese-English terminologies of five domains show that the method is highly promising. Introduction In this paper we propose a way of detecting new bilingual term pairs for augmenting bilingual terminologies by using a \"generate and validate\" method. Augmenting bilingual terminologies is sine qua non for terminology managers, translators and document managers (Sager, 1990) , and its importance is growing in accordance with the rapid growth of terminologies in many domains. In general, new terms they tend to be created in a systematic way by compounding (Sager, 1990; Ananiadou, 1994; Justeson and Katz, 1995; Cerbah, 2000; Kageura, 2012) , resulting in an abundance of multi-word terms (MWTs). This fact results in a tendency for the correspondences between constituent elements to be retained across languages to a substantial extent. This provides us with a chance to take advantage of the information contained in existing terminologies to augment and enrich terminologies with new terms, based on a simple idea: If a terminological lexicon contains \"linear programming,\" \"linear optimization,\" \"linear function,\" \"convex programming\" and \"convex function,\" we can reasonably assume that the term \"convex optimization,\" which is not listed in the terminology, may, or will come to, exist (Figure 1 ). By generating \"potential\" term candidates and validating their existence by using web data, it should be possible to identify a range of new terms which are not covered in existing terminologies. Assuming bilingual correspondence at the level of constituent units of terms, it is possible to extend this idea to obtain new bilingual term pairs. Based on this idea, we developed a fully operating system for detecting new bilingual term pairs in order to augment bilingual terminologies. The paper is organised as follows. Section 2 briefly looks at related work. Section 3 explains the system arrangement and the methods and algorithms adopted in the modules of the system. In particular, we detail the graph-based generation of term candidate pairs. Experimental results are introduced and discussed in section 4. Section 5 discusses remaining issues. Except for section 2, Japanese-English language pairs are assumed, Related work Since the 1990s, bilingual term extraction from parallel or comparable corpora has been actively pursued (Dagan and Church, 1997; Fung and McKeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Tonoike et al., 2005; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Sima'an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2-6, 2013), p. 3-10. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Laroche and Langlais, 2010; Li and Gaussier, 2010; Morin et al., 2010) . Although extracting bilingual term pairs from parallel corpora generally attains higher precision than extracting them from comparable corpora, the problem of limited availability of parallel corpora has led to a great deal of research into bilingual term extraction using comparable corpora. In addition to work that has resulted in the steady improvement of algorithms, there are studies that address improvement of corpus comparability (Morin et al., 2010; Li and Gaussier, 2010) . In the EU, research into corpus-based term extraction culminated in an EU project (TTC, 2012) . While the essential information explored in these methods is the correspondence between two languages (most typically aligned segments in the case of parallel corpora and degree of correspondence between context vectors in the case of comparable corpora), some have taken advantage of the abundance of MWTs and used the translational relationships between constituent units of MWTs (Tonoike et al., 2005; Daille and Morin, 2008) . They partially take a \"generate and validate\" approach, for detecting target language expressions, although the essential framework is still oriented to \"extraction.\" These corpus-based approaches have shown steady technical advancement and improvement, but the results are essentially restricted by available corpora and not anchored to existing terminologies. From the point of view of augmenting terminologies for terminological management, more \"terminology-driven\" methods, i.e. those that make use of existing terminologies, are required. Our method takes this approach; it is complementary to existing work. System and methods Overall framework of the system The system consists of three main modules: (a) the module that generates potential term candidate pairs; (b) the module that collects a set of web documents against which the existence of term candidate pairs is validated; (c) the module that validates and ranks term candidate pairs. Figure 2 : Main modules of the system Figure 2 shows the main modules of the system. Among these, module (a) constitutes the core part of our approach. Validating (and ranking) candidate pairs generated in module (a) constitutes an essential part for our method. Currently we use the web as a source against which the existence of candidate term pairs is validated because it contains many new terms, but other sources could also be used for validation. The current system configuration is such that relevant documents are crawled from the web in advance, but it would also be possible to dynamically throw generated term candidate pairs into the web search engine for validation. We did not take this approach for reasons related to search engine api and in order to controlling evaluation and diagnosis. Generating term candidate pairs The following steps are carried out to generate term candidate pairs: 1. Decompose MWTs into components (CUs); 2. Establish correspondences between source language terms (SLT; Japanese in the present context) CUs and target language terms (TLT; English) CUs; 3. Generate head-modifier pairs for SLT CUs; 4. Generate a bipartite graph based on SLT head-modifier pairs; 5. Partition the bipartite graph; 6. For each connected component of the bipartite graph, take the direct product of the head and modifier vertices to generate extended head-modifier pairs; Figure 3 : Extracting head-modifier pairs from an MWT \"data file compression\" 7. For each newly created SLT head-modifier pair, take the corresponding TLT CUs and generate corresponding TLT head-modifier pairs, then generate paired MWTs. For step 1, MeCab 1 and Stanford POS Tagger (Toutanova et al., 2003) 2 are used for decomposing terms and POS-tagging CUs for Japanese and English, respectively. We retained content units, and functional units directly attached to them. For step 2, we start from aligned CU pairs taken from simple term pairs, and extend aligned pairs by iteratively removing aligned pairs from MWT pairs that have the same number of SLT CUs and TLT CUs. In the third step, head-modifier pairs are generated for SLT CUs, using the fact that Japanese MWTs are head final. We extract all possible headmodifier pairs from each MWT, as shown in N ←|SeedT erm| 7: for i ←{1, ..  In step 4, the bipartite graph (as shown in Figure 4 ) is constructed from a set of head-modifier pairs obtained in step 3. Algorithm 1 shows the procedure in pseudo-code. The bipartite graph is generated by using only those SLT CUs which have corresponding TLT CUs (given in step 2). Taking the direct product of the head and modifier vertices for this \"raw\" bipartite graph would generate a great number of head-modifier pairs which are not likely to be possible terms, due to the existence of unmotivated bridges. Assuming that there are reasonable coherent sub-graphs that contain potential term pairs, we thus partition the bipartite graph to create components of a reasonable size in step 5. This is done by: (a) first removing bridges from the graph, and (b) then partitioning large components by using the Kernighan-Lin algorithm (Kernighan and Lin, 1970) . The Kernighan-Lin algorithm is a heuristic algorithm for partitioning connected components of a graph into two connected components of similar size. Figure 5 illustrates the process of partitioning the graph according to this procedure. A wider range of methods could potentially be applied to this step. In step 6, the direct product of the head and modifier vertices is taken for each component, generating potential SLT candidates. Finally, in step 7 term candidate pairs are generated by taking and concatenating corresponding TLT CUs. Collecting web documents Both SL and TL web documents are collected, by using SLTs and TLTs listed in terminology as a query to the search engine (Figure 6 ). To avoid collecting irrelevant documents, therefore, we combined domain keywords (the name of the domain itself, such as \"computer science,\" for example) with each query term. The top 20 documents are collected for each query. As a search engine, we currently use the Yahoo! Japan api 3 . Parallel downloading is carried out to improve speed. The obtained documents are stored using Groonga 4 , which provides efficient full text search functions. Validating term candidate pairs The generated potential term candidate pairs are validated against the web documents, and the pairs for which both the SLT candidate and the TLT candidate occur at least once in the documents are retained. Currently, the result can be ranked in accordance with the number of occurrences of either SLT or TLT candidates, their average, or according to Jaccard similarity coefficient between SLT and TLT, which is defined as: Jaccard(SLT, TLT)= H(SLT ∧ TLT) H(SLT ∨ TLT) where H is the number of hits of the term in the document set. Note that ranking by the number of SLT or TLT candidate hits provides information related to whether or not the candidate is likely to be a valid term, while the ranking by Jaccard similarity coefficient measures how likely it is that the SLT and TLT candidates are actually a corresponding pair. In the proposed framework, using the Jaccard coefficient can be regarded as redundant, as the correspondence between SLT and TLT candidates is kept by CU level correspondences. We still find it important to use the Jaccard coefficient as evaluating experimental results by using Jaccard coefficient enables us to see to what extent we can rely on separate and independent validation for SLT and TLT candidates, which provides an important clue as to the extent to which monolingual domain corpora can be used in the present framework. Experiments and evaluations Experimental setup Terminological dictionaries For evaluation, we used five terminological dictionaries of computer science (henceforth COM) 5 , economics (ECN) 6 ,law(LA W) 7 , physics (PHY) 8 and psychology (PSY) 9 . These terminological dictionaries contain Japanese-English term pairs. The number of terms listed in Table 1 . Table 2 lists the number of terms by length (by the number of constituent units). The number of Japanese-English term pairs of which the number of constituent units is the same for Japanese and English is also listed (CP). There are a few terms with no constituent units; they were produced because the POS-taggers mistakenly judged the constituent units to be functional elements rather than content words. Those listed in the rows \"CP\" with more than two constituent units are the sources of the head-modifier bipartite graph. Collecting web documents The web documents for these domains were collected from November to December 2012. In collecting the web documents, the following domain keywords were used: -‰;J ¶ (keisanki kagaku) and \"computer science\" for COM, &A ¶ (keizaigaku) and \"economics\" for ECN, O ¶ (hougaku) and \"law\" for LAW, úg ¶ (butsurigaku) and \"physics\" for PHY, and úg ¶ (shinrigaku) and \"psychology\" for PSY. Table 3 shows the number of pages obtained from the web search. The \"Japanese\" and \"English\" columns show the number of pages obtained by using Japanese and English terms, respectively. Note that the number of Japanese web pages collected for COM is much smaller than its English counterpart, while in the other five domains they are more balanced. We randomly selected 200 web pages for each domain, without distinguishing between English and Japanese pages, and checked the relevance of the pages to the domain. Table 4 shows the number of pages clearly relevant to the domain in ques- Generating term candidate pairs Table 5 shows the basic statistics of the initial head-modifier bipartite graphs (created from steps 1-4 in section 3.2), in which \"mods\" stands for modifiers, \"# comp\" shows the number of connected components, \"maxcmp\" shows the number of vertices in the maximum component, \"2nd cmp\" shows the number of vertices of the second largest component (other headers should be obvious). As in many real-world networks, these initial graphs consist of one giant component and a number of small components (Newman, 2003; Newman, 2010) . Using these initial graphs for generating potential MWT candidates would be unrealistic; a terminology of a domain cannot reasonably contain terms in the order of millions. Table 6 shows the statistics of head-modifier graphs generated by removing bridges and applying the Kernighan-Lin algorithm. Essentially, the largest components in the initial graphs were partitioned into smaller components with similar sizes, while many previously connected vertices became isolated vertices. As a result, the number of po- tential head-modifier candidates was reduced to the order of tens of thousands. Given the size of the original terminological dictionaries as well as many existing terminological dictionaries, this size seems reasonable. Quantitative evaluations Table 7 shows the number of candidate term pairs after validation (those pairs of which both Japanese and English candidates were validated at least once against the collected web documents were identified as candidate pairs). We manually evaluated (i) 100 top candidate pairs according to the Jaccard coefficient value, (ii) 100 top candidate pairs as calculated by the sum of Japanese and English hits, and (iii) 100 randomly chosen candidate pairs whose Jaccard coefficient was zero. They were evaluated from two points of view: according to (a) whether the Japanese and English matched, and (b) whether the Japanese candidate could be regarded as a term in the domain in question. For (b), we took into account cases in which the candidate was not in itself a term but could be a part of a longer term. Evaluation was carried out by two people; the results of the first evaluator were cross-checked by the other 10 . The results are listed in Table 8 . The Jaccard coefficient gave the highest performance both in terms of bilingual correspondence (pairing) and in terms of validity to the domain. This indicates that the co-occurrence of SLT and TLT in the same document provides strong evidence for a pair being both a valid pair as well as valid terms. The low performance of law was due to the fact that the terminology of law we used contained many verbal expressions, which led to CU level mismatches (see section 4.4). Unfortunately, the number of candidate pairs with a non-zero Jaccard coefficient was limited, as indicated in Table 7 . However, it can be observed that the number of hits is also useful as evidence. In the present experiment we only used the sum of Japanese and English hits; we may be able to obtain more efficient information by taking into account the balance between the hits in the two languages. Lastly, there are still relevant terms among the 100 randomly selected candidates, though the ratio of correct term pairs is much lower. To take full advantage of the proposed method, further filtering of the relevant terms from this range of candidates will be necessary. Diagnosis Changes in algorithms and parameter settings of the method, such as the bipartite graph partition algorithm or the selection of domain keywords for collecting web documents (note the substantially smaller number of Japanese computer science documents), will affect the behaviour of the system. In addition, it may be useful to make further use of the information which can be derived from the graph, such as the degree of vertices. These need to be examined systematically to optimise the performance vis-à-vis the nature of terminologies, which will be our future task in methodological front. In addition, some general error patterns were observed upon closer qualitative observation: • Especially for terms in the domain of law, errors arising from the mistreatment of postpositions and delimiting symbols in Japanese and prepositions in English were observed (e.g. the output \"«…waeˆ\" (\"behaviour of consumption\") is not a valid MWT, but \"« …aeˆ\" (\"consumption behaviour\") without the postposision \"w\" (\"of\") could be. This problem can be solved by introducing MWT patterns or rules to restrict valid MWT forms to filter out candidates with invalid patterns. • \"Partial\" terms were often generated and validated which in themselves are not valid MWTs but constitute a part of certain longer MWTs. This problem arises from the limitation of our method in which we only generate term candidates with two constituent elements. This shortcoming can be overcome by detecting maximum MWT patterns in the web documents. Rich accumulation of research in pattern-based MWT extraction can be directly relevant for this purpose (Ananiadou, 1994; Daille et al., 1994; Justeson and Katz, 1995; Nakagawa, 2000; Takeuchi et al., 2004) . • Some candidates which were judged as nonterms consist of two CUs which both represent generic concepts. To avoide this type of error, it will be useful to make use of the weight of vertices in the ogirinal graph, as well as using ontological information or introducing the idea of \"stop words.\" • Some errors arising from incorrect CU level pairing were observed as well. These can be avoided, at least partially, by introducing dictionary-based pairing of source language and target language CUs. Conclusions and outlook We proposed a way of augmenting bilingual terminologies by using a \"generate and validate\" method, taking advantage of the characteristics of terms and terminologies. The results of our experiments indicate that the method will be useful for collecting term candidate pairs to be included in existing terminological dictionaries. The system which carries out this task is fully operational, although there is an uncertainty as to the free availability of the search engine api in future. For our system to be used in the real world in a Japanese-English setting, it should be complemented by methods which can detect and collect Japanese borrowed terms written in katakana,a s they tend to be used for introducing new singular terms (Kageura, 2012) . For this task, the \"collect and validate\" framework proposed by (Sato, 2010) would be useful, even though it was developed for collecting proper names. In the next stage, we will evaluate the usefulness of the system in vivo rather than in vitro,i n cooperation with dictionary companies, academic societies managing terminologies, and document management divisions of companies. A Japanese dictionary company has already expressed interest in trying our system in the process of revising some of its dictionaries.",
         "43469028",
         "3a3068d34762834a49222bf15b666fe772e2a705",
         "5",
         "https://aclanthology.org/2013.mtsummit-papers.1",
         null,
         "Nice, France",
         "2013",
         "September 2-6",
         "Proceedings of Machine Translation Summit XIV: Papers",
         "Sato, Koichi  and\nTakeuchi, Koichi  and\nKageura, Kyo",
         "Terminology-driven Augmentation of Bilingual Terminologies",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "sato-etal-2013-terminology",
         null,
         null
        ],
        [
         "33",
         "R13-1056",
         "We report on our efforts aimed at building an Open Domain Question Answering system for Polish. Our contribution is twofold: we gathered a set of question-answer pairs from various Polish sources and we performed an empirical evaluation of two re-ranking methods. The gathered collection contains factoid, list, non-factoid and yes-no questions, which makes a challenging material for experiments. We show that using two re-ranking methods based on term proximity allows to obtain significant improvement on simple information retrieval baseline. The improvement is observed as finding more answer-bearing documents among the top n search results.",
         "We report on our efforts aimed at building an Open Domain Question Answering system for Polish. Our contribution is twofold: we gathered a set of question-answer pairs from various Polish sources and we performed an empirical evaluation of two re-ranking methods. The gathered collection contains factoid, list, non-factoid and yes-no questions, which makes a challenging material for experiments. We show that using two re-ranking methods based on term proximity allows to obtain significant improvement on simple information retrieval baseline. The improvement is observed as finding more answer-bearing documents among the top n search results. Background Question Answering (QA) is an information retrieval task in which the user information need is expressed in terms of a natural language question. As this way of expressing information needs is very flexible, QA systems are mostly constructed as Open Domain QA systems (ODQA), not limited to any particular text collection or narrow domain (Pas ¸ca, 2003 ). An ODQA system can deliver an answer, as a text of a data record, but mostly it is required that it returns passages extracted from a collection of documents that are supposed to include an answer to the user's question. The goal of ODQA is to answer questions not restricted to any pre-defined domain (Pas ¸ca, 2003) . Most ODQA systems process user questions in four steps, cf (Pas ¸ca, 2003; Monz, 2003; Ferrucci, 2012) : question analysis, document retrieval, document analysis and answer selection. In addition to this general scheme, we can distinguish several typical substeps or tasks: question classification (Lally et al., 2012) , query selection and ex-pansion (Pas ¸ca, 2003) , passage retrieval and ranking (Pas ¸ca, 2003 ), candidate answer identification (Chu-Carroll et al., 2012) , answer extraction and ranking (Gondek et al., 2012) , etc., but the core is shared among systems. There are only a few known works on ODQA (working on text collections) for Polish, e.g. Walas and Jassem (2011) , Walas (2012) , and two systems publicly available: Hipisek.pl and KtoCo. pl. The latter is a commercial system and little is known about its structure. Hipisek implements the ODQA blueprint described above (Walas and Jassem, 2011) , but was focused on processing yes-no questions about time and location. The system depends on a dedicated rulebased parser (Walas and Jassem, 2011) , and was extended with a knowledge base for spatial relations (Walas, 2012) . Our long-term goal is large-scale, broadapplication ODQA with respect to different types of questions and documents indexed. We utilize the following architecture of QA system: • query analysis -query processing by language tools and rules, and generation of the search query, • search engine -fetches the N most relevant documents from a large collection of documents, • module for document ranking -the set of documents returned by the search engine is re-ranked using medium time-consuming techniques. The M top documents are selected, where M N , • module for extracting candidate answers -the most time-consuming operations are performed on the reduced set of documents, • module for answer ranking -the list of candidate answers with their context are ranked and best items are presented to user. In this paper we focus on the first three elements of QA system with the special focus on search engine and document re-ranking. QA dataset for Polish Most works performed for English rely upon the TREC datasets (Voorhees, 2001) . No such dataset was available for Polish, so we started construction of a set of question-answer pairs for Polish. We surveyed several possible ways of collecting questions and answers for Polish from the available resources. Our first idea was to crawl Internet QA communities such as zapytaj.onet.pl, pytki.pl, pytano.pl (Polish counterparts of ask.com) and grab both questions and answers. We anticipated the need for substantial manual work needed to select and curate the data, but the actual scale of the problem was quite overwhelming: while it is already not easy to find suitable questions there, finding a number of suitable answers in reasonable time was practically infeasible. The main problem was that if the answers would serve as a testing material for a system based on document retrieval, the answers should mimic normal documents. The answers posted by users of such sites are usually very short and devoid of the necessary context to understand them -they make sense only when paired with the corresponding questions (those that make sense at all). The same problem turned out to apply for FAQ sites, even official ones. This way we faced the necessity to divide the process into two separate phases: gathering questions and then finding documents that provide answers to them. Gathering questions We developed simple guidelines that help to recognise acceptable questions. A question must have syntactic structure of a question (rather than a string of query terms), be simple (one question per a sentence), not requiring any additional context for its interpretation. We did not accept questions referring to the person asked ('What bands do you like?'). Questions about opinions were discouraged unless could be conceived as addressed to a domain expert (e.g. 'Which wines go well with fish?'). We did not exclude questions which were vulgar or asked just for fun as long as they satisfied all other requirements. We considered four sources of candidate questions which we hope to reflect the actual information need of the Internet users. First, thanks to the courtesy of Marcin Walas we were given access to user query logs of Hipisek.pl. The second source was 'manual crawling' around the QA communities. Similarly, we considered FAQ sites of several Inland Revenue offices. Lastly, we decided to abuse the auto-complete feature of Google and Bing search engines to gain insight into the questions that have actually been posed (it turns out that a number of users indeed ask natural language questions as search engine queries). The task was to enter word/words that typical questions start with and copy the suggestions. This could have led to some bias concerning the selection of the question-initial words. On the other hand, the mechanism seems to work surprisingly well and it is sufficient to give two words to obtain a lot of sensible questions. All of the questions that were decided as appropriate were subjected to orthographical and grammatical correction. We considered manual translation of the TREC questions, as was done, e.g., in (Lombarović et al., 2011) . We decided against this solution, since the TREC questions seem too much oriented on the American culture and geography for our purposes. Also the TREC datasets cotains mainly factoid questions while we wanted to create a balanced dataset containing both factoid and nonfactoid questions. Finding answers We required from the answer documents to have included at least one passage (a couple of consecutive sentences) that contained the answer, i.e. that there was no necessity to construct an answer from information scattered across the document. This is because we assume the final version of the system will present such a passage to the user. We also required the answer-bearing passage to be comprehensive even when not paired with the question, e.g. if the question was about Linux, this name or its equivalent should appear in the passage rather than only general terms such as 'the system'. The set of candidate questions was given to linguists, which were asked to devote a couple of minutes per each question and try to find a satisfactory answer using search engines. They were asked to avoid typing the whole question as a query to prevent from favouring those documents that contain questions. For each candidate at most one answer document was found. Each answer document (a website) was downloaded as HTML files. We used the Web As Corpus Toolkit (Ziai and Ott, 2005) to clean up the files and remove boilerplate elements. The final output contained no mark-up and its structure was limited to plain text divided into paragraphs. Note that only those questions that the linguists were able to find an answer for have made their way to the final dataset. Final collection Ultimately, the set of questions paired with answers contains 598 entries. The statistics regarding source distribution is given as Table 1 . The collection contains the following types of questions (according to the expected answer type): Source 1. Factoid and list questions (Voorhees, 2004; Voorhees and Dang, 2005) (Mizuno et al., 2007; Fukumoto, 2007) : • definition -What is X?, What does X mean?, Who is X?, • description -What powers does the president have?, • mannerhow questions; How to start a business?, How to make a frappe coffe?, • reasonwhy questions; Why do cats purr?, How do I catch a cold?. 3. Yes-no questions (Walas, 2012; Kanayama et al., 2012) ; Is Lisbon in Europe?. To perform a reliable evaluation of the system, we had to index a lot more data than just the answers to our 598 test questions. We acquired also several collections to serve as 'distractors' and a source of possible answers, namely: • Polish Wikipedia (using dump from 22 January 2013) -956 000 documents. • A collection of press articles from Rzeczpospolita (Weiss, 2008) -180 000 documents. • Three smaller corpora: KPWr (Broda et al., 2012) , CSEN and CSER (Marcińczuk and Piasecki, 2011) -3 000 documents. Evaluation metrics The evaluation was based on the following metrics: • answers at n-th cutoff (a@n) (Monz, 2003) relevant documents recall; a fraction of questions for which the relevant document was present in the first n documents returned by the search engine; • mean reciprocal rank (MRR) -an average of the query reciprocal ranks 1 MRR is used to compare re-ranking algorithms. The higher MRR is, the higher in the ranking the relevant documents are. Baseline information retrieval As a basis for search engine we selected an open source search platform called Solr (The Apache Software Foundation, 2013a). Solr indexes large collection of documents and provides: full-text search, rich query syntax, document ranking, custom document fields and terms weighting. It was also shown that Lucene (the retrieval system underlying Solr) performs no worse for QA than other modern Information Retrieval systems (Tellex et al., 2003) . In the baseline approach we used an existing tool called Web as Corpus ToolKit (Adam Kilgarriff and Ramon Ziai and Niels Ott, 2013) to extracted plain text from the collection of HTML documents. Then, the text was tagged using WCRFT tagger (Radziszewski, 2013) and their base forms were indexed in the Solr. To fetch a ranked list of documents for a query we used a default search ranking algorithm implemented in the Lucene that is a combination of Boolean Model (BM) with refined Vector Space Model (VSM). BM is used to fetch all documents matching the boolean query. Then, VSM is applied to rank the answer documents. The detailed formula used to compute the ranking score is presented in (The Apache Software Foundation, 2013b). The formula includes following factors: • fraction of query terms present in the document -documents containing more query terms are scored higher than those with fewer, • query normalizing factor -to make the score comparable between queries, • document term frequency -documents containing more occurrences of query terms receive higher scores, • inverse document frequency -common terms (present in many documents) have lower impact on the score, • term boosting factor -weight specified in the query can be used to increase importance of selected terms (not used in our approach), • field boosting factor -some fields might be more important than others (not used by us), • field length normalization factor -shorter fields obtain higher scores. Figure 1 presents all steps of question analysis. First, a question is tagged with WCRFT tagger. All punctuation marks and words from a stoplist (including 145 prepositions) are discarded. We assumed that in most cases the answer have a form of a statement and does not mimic question structure. The remaining words are used in a query, formed as a boolean disjunction of the base forms. The a@n and MRR values for the baseline configuration are presented in Table 3 . We measured the a@n for several distinct values of n between 1 and 200 (this is an estimated maximum number of documents which can be effectively processed during re-ranking). The a@n ranges from 26% for n = 1 to 87% for n = 200. This means than only for 26% questions the relevant document was on the first position in the ranking. In the reported tests all non-stop words from the question were used to form a query. We tested also several modification of the heuristic for query term selection proposed in (Pas ¸ca, 2003) , but the results were lower. Proximity-based re-ranking Lucene default ranking algorithm does not take into consideration proximity of query terms in the documents. This leads to favouring longer documents as they are more likely to contain more query terms. However such documents can describe several different topics not related to the question. Ranking of longer documents cannot be decreased by default, as they might contain an answer. A possible solution is to analyse query term 1. Input: Co można odliczyć od podatku? (\"What can be deducted from tax?\") 2. Tagging: co można odliczyć od podatek ? (base forms) 3. Filtering: można odliczyć od podatek (\"can\", \"deduct\", \"tax\") 4. Query: base:mo_ zna OR base:odliczyć OR case:od OR base:podatek a@n n baseline 1 26.09% 5 52.17% 10 62.04% 20 70.57% 50 76.76% 100 82.61% 200 87.29% MRR 0.3860 Table 3 : a@n and MRR for baseline configuration of information retrieval. proximity inside the documents. We have evaluated two approaches to utilising term proximity in re-ranking. Maximum Cosine Similarity Weighting Maximum Cosine Similarity Weighting (MCSW) is based on the idea of using the same ranking scheme as in the retrieval component, but applied to short passages, not whole documents. Every document is divided into continuous blocks of k sentences. For every block we compute the cosine similarity between a vector representing the block and a vector representing a query. Standard tf-idf weighting (Manning et al., 2008) and cosine measure are used. A document is assigned the maximum per-block cosine similarity that was encountered. Several block sizes (k from 1 to 5) were tested producing very similar results, thus we report results only for k = 1. The final document score is computed as follows: where: • D, ordered list od documents returned from search engine for a query, • score(d), score for document d returned by Solr, • mcs(d), maximum cosine similarity for document d. Minimal Span Weighting Monz ( 2003 ) presented a simple method for weighting based on a minimal text span containing all terms from a query that occur in the document. The re-ranking score combines the original score with MSW score and is computed as follows: score (d) = score(d) * λ + (1 − λ) • |q ∩ d| |s| α • |q ∩ d| |q| β (2) where: • q, set of query terms, • s, the shortest text fragment containing all query terms occurring in the document, • λ, α, β, significance weights for the respective factors (we used default values (Monz, 2003) , i.e. λ = 0.4, α = 0.125, β = 1) Evaluation The a@n and MRR values for MCSW and MSW are presented in Table 4 . For both methods we noticed a small improvement. The increase of MRR values for both methods indicates that the average position of the relevant documents in the ranking was improved. The a@n was improved by up to 12 percentage points for MCSW and n = 1. The lower improvement for MSW might be caused by the assumption that the minimal span must contain all query terms occurring in the document. In addition, the proximity-based ranking algorithms can be used to extract the most relevant document fragments as answers instead of presenting the whole document. According to (Lin et al., 2003) , users prefer paragraph-level chunks of text with appropriate answer highlighting. Despite the observed improvements, the results are still below our expectations. If we assume that user reads up to 10 answers for a question (a typical number of results displayed on a single page in many web search engines), the top a@n will be about 70%. This means that we will not provide any relevant answer for 3 out of 10 questions. According to (Monz, 2003) , results for English reported for TREC sets are between 73% and 86% for a@10. Thus, further improvement in reranking is necessary. Conclusion We presented a preliminary results for a baseline information retrieval system and the simple proximity-based re-ranking methods in the context of a Open Domain Question Answering task for Polish. The evaluation was performed on a corpus of 598 questions and answers, collected from a wide range of questions asked by Internet users (i.e. search engines, Hipisek.pl, QA communities and Revenue FAQ). The collection covers major types of questions including: factoid, list, non-factoid and yes-no questions. The a@n of the baseline IR system (Solr) configuration ranges from 26% for n = 1 to 87% for n = 200 top documents considered. Our queries consisted of base forms of all question words except words from a stoplist. Several heuristics for query term selection inspired by the one proposed in (Monz, 2003) produced lower results. This can be explained by the properties of the ranking algorithm used in Solr -the number of terms covered and their total frequency in a document are important factors. For n = 10 (a typical single page in a Web search) we obtained 62% a@n. Two re-ranking methods based on query term proximity were applied. For both methods we obtained a noticeable improvement up to 12 percentage points of a@n for n = 1 and 9 percentage points for n = 10. Nevertheless, the results are still slightly lower than in the case of systems built for English, e.g., (Monz, 2003) . However, results reported by Monz were obtained on the TREC datasets, which contain mostly factoid and list questions. Our datasets includes also nonfactoid and yes-no questions which are more difficult to deal with. The comparison with Hipisek is difficult as no results concerning ranking precision were not reported. Moreover, Hipisek was focused on selected subclasses of questions. We plan to extend the information retrieval model on the level of document fetching and reranking. We want to utilize plWordNet 2.0 (the Polish wordnet) 2 (Maziarz et al., 2012) , tools for proper names (Marcińczuk et al., 2013) and semantic relations recognition (Marcińczuk and Ptak, 2012) , dependency 3 and shallow syntactic parsers. More advanced but also more timeconsuming tools will be used to select relevant passages in the documents fetched by the presented information retrieval module. Acknowledgements The work was funded by the European Union Innovative Economy Programme project NEKST, No POIG.01.01.02-14-013/09. We would like to thank Marcin Walas for sharing the collection of historic questions that have been posed to the Hipisek system.",
         "7009053",
         "b0069132b52e2e07b000a3959671aebd3256296f",
         "3",
         "https://aclanthology.org/R13-1056",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Marci{\\'n}czuk, Micha{\\l}  and\nRadziszewski, Adam  and\nPiasecki, Maciej  and\nPiasecki, Dominik  and\nPtak, Marcin",
         "Evaluation of baseline information retrieval for {P}olish open-domain Question Answering system",
         "428--435",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "marcinczuk-etal-2013-evaluation",
         null,
         null
        ],
        [
         "34",
         "W05-0829",
         "This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach.",
         "This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993) . Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training. In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. Translation Likelihood as a Statistical Test Given a bilingual corpus of language pair F (Foreign, source language) and E (English, target language), if we know the word alignment for each sentence pair we can calculate the co-occurrence frequency for each source/target word pair type C(f, e) and the marginal frequency C(f ) = e C(f, e) and C(e) = f C(f, e). We can apply various statistical tests (Manning and Schütze, 1999) to measure how likely is the association between f and e, in other words how likely they are mutual translations. In the following sections, we will use χ 2 statistics to measure the the mutual translation likelihood (Church and Hanks, 1990) . The Core of the Integrated Phrase Segmentation and Alignment The competitive linking algorithm (CLA) (Melamed, 1997 ) is a greedy word alignment algorithm. It was designed to overcome the problem of indirect associations using a simple heuristic: whenever several word tokens f i in one half of the bilingual corpus co-occur with a particular word token e in the other half of the corpus, the word that is most likely to be e's translation is the one for which the likelihood L(f, e) of translational equivalence is highest. The simplicity of this algorithm depends on a one-to-one alignment assumption. Each word translates to at most one other word. Thus when one pair {f, e} is \"linked\", neither f nor e can be aligned with any other words. This assumption renders CLA unusable in phrase level alignment. We propose an extension, the competitive grouping, as the core component in the ISA model. Competitive Grouping Algorithm (CGA) The key modification to the competitive linking algorithm is to make it less greedy. When a word pair is found to be the winner of the competition, we allow it to invite its neighbors to join the \"winner's club\" and group them together as an aligned phrase pair. The one-to-one assumption is thus discarded in CGA. In addition, we introduce the locality assumption for phrase alignment. Locality states that a source phrase of adjacent words can only be aligned to a target phrase composed of adjacent words. This is not true of most language pairs in cases such as the relative clause, passive tense, and prepositional clause, etc.; however this assumption renders the problem tractable. Here is a description of CGA: For a sentence pair {f , e}, represent the word pair statistics for each word pair {f, e} in a two dimensional matrix L I×J , where L(i, j) = χ 2 (f i , e j ) in our implementation. 1 1. Find i * and j * such that L(i * , j * ) is the highest. Create a seed phrase pair [i * , i * , j * , j * ] which is simply the word pair {f i * , e j * } itself. 2. Expand the current phrase pair [i start , i end , j start , j end ] to the neighboring territory to include adjacent source and target words in the phrase alignment group. There are 8 ways to group new words into the phrase pair. For example, one can expand to the north by including an additional source word f istart−1 to be aligned with all the target words in the current group; or one can expand to the northeast by including f istart−1 and e j end +1 (Figure 1 ). Two criteria have to be satisfied for each expansion: (a) If a new source word f i is to be grouped, max jstart≤j≤j end L(i , j) should be no smaller than max 1≤j≤J L(i , j). Since CGA is a greedy algorithm as described below, this is to guarantee that f i will not \"regret\" the decision of joining the phrase pair because it does not have other \"better\" target words to be aligned with. Similar constraint is applied if a new target word e j is to be grouped. (b) The highest value in the newly-expanded area needs to be \"similar\" to the seed value L(i * , j * ). Expand the current phrase pair to the largest extend possible as long as both criteria are satisfied. 3. The locality assumption means that the aligned phrase cannot be aligned again. Therefore, all the source and target words in the phrase pair are marked as \"invalid\" and will be skipped in the following steps. 4. If there is another valid pair {f i , e j }, then repeat from Step 1. Figure 2 and Figure 3 show a simple example of applying CGA on the sentence pair {je déclare reprise la session/i declare resumed the session}. Exploring all possible groupings The similarity criterion 2-(b) described previously is used to control the granularity of phrase pairs. In cases where the pairs {f 1 f 2 , e 1 e 2 }, {f 1 , e 1 } and {f 2 , e 2 } are all valid translations pairs, similarity is used to control whether we want to align {f 1 f 2 , e 1 e 2 } as one phrase pair or two shorter ones. The granularity of the phrase pairs is hard to optimize especially when the test data is unknown. On the one hand, we prefer long phrases since interaction among the words in the phrase, for example word sense, morphology and local reordering could be encapsulated. On the other hand, long phrase pairs are less likely to occur in the test data than the shorter ones and may lead to low coverage. To have both long and short phrases in the alignment, we apply a range of similarity thresholds for each of the expansion operations. By applying a low similarity threshold, the expanded phrase pairs tend to be large, while a higher similarity threshold results in shorter phrase pairs. As described above, CGA is a greedy algorithm and the expansion of the seed pair restricts the possible alignments for the rest of the sentence. Figure 4 shows an example as we explore all the possible grouping choices in a depth-first search. In the end, all unique phrase pairs along the path traveled are output as phrase translation candidates for the current sentence pair. Phrase translation probabilities Each aligned phrase pair { f , ẽ} is assigned a likelihood score L( f , ẽ), defined as: i max j log L(f i , e j ) + j max i log L(f i , e j ) | f | + |ẽ| where i ranges over all words in f and similarly j in ẽ. Given the collected phrase pairs and their likelihood, we estimate the phrase translation probability by their weighted frequency: P ( f |ẽ) = count( f , ẽ) • L( f , ẽ) f count( f , ẽ) • L( f , ẽ) No smoothing is applied to the probabilities. Learning co-occurrence information In most cases, word alignment information is not given and is treated as a hidden parameter in the training process. We initialize a word pair cooccurrence frequency by assuming uniform alignment for each sentence pair, i.e. for sentence pair (f , e) where f has I words and e has J words, each word pair {f, e} is considered to be aligned with frequency 1 I×J . These co-occurrence frequencies will be accumulated over the whole corpus to calculate the initial L(f, e). Then we iterate the ISA model: 1. Apply the competitive grouping algorithm to each sentence pair to find all possible phrase pairs. (French-English, Finnish-English, German-English and Spanish-English). Conclusion In this paper, we introduced the competitive grouping algorithm which is at the core of the ISA phrase alignment model. As an extension to the competitive linking algorithm which is used for word-to-word alignment, CGA overcomes the assumption of oneto-one mapping and makes it possible to align phrase 3 http://www.isi.edu/licensed-sw/pharaoh/ pairs. Despite its simplicity, the ISA model has achieved competitive translation results. We plan to release ISA toolkit 4 to the community in the near future.",
         "2455968",
         "633bf26c3315fcf80a6d51e1d7f8fa9ab2e72412",
         "11",
         "https://aclanthology.org/W05-0829",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Zhang, Ying  and\nVogel, Stephan",
         "Competitive Grouping in Integrated Phrase Segmentation and Alignment Model",
         "159--162",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "zhang-vogel-2005-competitive",
         null,
         null
        ],
        [
         "35",
         "W10-3201",
         "In this paper we propose a framework of verb semantic description in order to organize different granularity of similarity between verbs. Since verb meanings highly depend on their arguments we propose a verb thesaurus on the basis of possible shared meanings with predicate-argument structure. Motivations of this work are to (1) construct a practical lexicon for dealing with alternations, paraphrases and entailment relations between predicates, and (2) provide a basic database for statistical learning system as well as a theoretical lexicon study such as Generative Lexicon and Lexical Conceptual Structure. One of the characteristics of our description is that we assume several granularities of semantic classes to characterize verb meanings. The thesaurus form allows us to provide several granularities of shared meanings; thus, this gives us a further revision for applying more detailed analyses of verb meanings.",
         "In this paper we propose a framework of verb semantic description in order to organize different granularity of similarity between verbs. Since verb meanings highly depend on their arguments we propose a verb thesaurus on the basis of possible shared meanings with predicate-argument structure. Motivations of this work are to (1) construct a practical lexicon for dealing with alternations, paraphrases and entailment relations between predicates, and (2) provide a basic database for statistical learning system as well as a theoretical lexicon study such as Generative Lexicon and Lexical Conceptual Structure. One of the characteristics of our description is that we assume several granularities of semantic classes to characterize verb meanings. The thesaurus form allows us to provide several granularities of shared meanings; thus, this gives us a further revision for applying more detailed analyses of verb meanings. Introduction In natural language processing, to deal with similarities/differences between verbs is essential not only for paraphrase but also textual entailment and QA system which are expected to extract more valuable facts from massively large texts such as the Web. For example, in the QA system, assuming that the body text says \"He lent her a bicycle\", the answer of the question \"He gave her a bicycle?\" should be \"No\", however the answer of \"She rented the bicycle?\" should be \"Yes\". Thus constructing database of verb similarities/differences en-ables us to deal with detailed paraphrase/nonparaphrase relations in NLP. From the view of the current language resource, how the shared/different meanings of \"He lent her a bicycle\" and \"He gave her a bicycle\" can be described? The shared meaning of lend and give in the above sentences is that they are categorized to Giving Verbs, as in Levin's English Verb Classes and Alternations (EVCA) (Levin, 1993) , while the different meaning will be that lend does not imply ownership of the theme, i.e., a bicycle. One of the problematic issues with describing shared meaning among verbs is that semantic classes such as Giving Verbs should be dependent on the granularity of meanings we assumed. For example, the meaning of lend and give in the above sentences is not categorized into the same Frame in FrameNet (Baker et al., 1998) . The reason for this different categorization can be considered to be that the granularity of the semantic class of Giving Verbs is larger than that of the Giving Frame in FrameNet 1 . From the view of natural language processing, especially dealing the with propositional meaning of verbs, all of the above classes, i.e., the wider class of Giving Verbs containing lend and give as well as the narrower class of Giving Frame containing give and donate, are needed. Therefore, in this work, in order to describe verb meanings with several granularities of semantic classes, a thesaurus form is adopted for our verb dictionary. Based on the background, this paper presents a thesaurus of predicate-argument structure for verbs on the basis of a lexical decompositional framework such as Lexical Conceptual Structure (Jackendoff, 1990) ; thus our proposed thesaurus can deal with argument structure level alternations such as causative, transitive/intransitive, stative. Besides, taking a thesaurus form enables us to deal with shared/differenciate meaning of verbs with consistency, e.g., a verb class node of \"lend\" and \"rent\" can be described in the detailed layer of the node \"give\". We constructed this thesaurus on Japanese verbs and the current status of the verb thesaurus is this: we have analyzed 7,473 verb meanings (4,425 verbs) and organized the semantic classes in a Àve-layer thesaurus with 71 semantic roles types. Below, we describe background issues, basic design issues, what kind of problems remain, limitations and perspectives of applications. Existing Lexical Resources and Drawbacks Lexical Resources in English From the view of previous lexical databases In English, several well-considered lexical databases are available, e.g., EVCA, Dorr's LCS (Dorr, 1997 ), FrameNet, WordNet (Fellbaum, 1998 ), VerbNet (Kipper-Schuler, 2005) and PropBank (Palmer et al., 2005) . Besides there is the research project (Pustejovsky and Meyers, 2005) to Ànd general descriptional framework of predicate argument structure by merging several lexical databases such as Prop-Bank, NomBank, TimeBank and PennDiscouse Treebank. Our approach corresponds partly to each lexical database, (i.e., FrameNet's Frame and FrameElements correspond to our verb class and semantic role labels, and the way to organize verb similarity classes with thesaurus corresponds with WordNet's synset), but is not exactly the same; namely, there is no lexical database describing several granularities of semantic classes between verbs with arguments. Of course, since the above English lexical databases have links with each other, it is possible to produce a verb dictionary with several granularities of semantic classes with arguments. However, the basic categories of classify-ing verbs would be little different due to the different background theory of each English lexical database; it must be not easy to add another level of semantic granularity with keeping consistency for all the lexical databases; thus, thesaurus form is needed to be a core form for describing verb meanings 2 . Lexical Resources in Japanese In previous studies, several Japanese lexicons were published: IPAL (IPA, 1986) focuses on morpho-syntactic classes but IPAL is small 3 . EDR (Jap, 1995) consists of a large-scale lexicon and corpus (See Section 3.4). EDR is a well-considered and wide coverage dictionary focusing on translation between Japanese and English, but EDR's semantic classes were not designed with linguistically-motivated lexical relations between verbs, e.g., alternations, causative, transitive, and detransitive relations between verbs. We believe these relations must be key for dealing with paraphrase in NLP. Recently Japanese FrameNet (Ohara et al., 2006) and Japanese WordNet (Bond et al., 2008) are proposed. Japanese FrameNet currently published only less than 100 verbs 4 . Besides Japanese WordNet contains 87000 words and 46000 synsets, however, there are three major difÀculty of dealing with paraphrase relations between verbs: (1) there is no argument information; (2) existing many similar synsets force us to solve Àne disambiguation between verbs when we map a verb in a sentence to WordNet; (3) the basic verbs of Japanese (i.e., highly ambiguous verbs) are wrongly assigned to unrelated synsets because they are constructed by translation from English to Japanese. Thesaurus of Predicate-Argument Structure The proposed thesaurus of predicate-argument structure can deal with several levels of verb classes on the basis of granularity of deÀned verb meaning. In the thesaurus we incorporate LCSbased semantic description for each verb class that can provide several argument structure such as construction grammar (Goldberg, 1995) . This must be high advantage to describe the different factors from the view of not only syntactic functions but also internal semantic relations. Thus this characteristics of the proposed thesaurus can be powerful framework for calculating similarity and difference between verb senses. In the following sections we explain the total design of thesaurus and the details. Design of Thesaurus The proposed thesaurus consists of hierarchy of verb classes we assumed. A verb class, which is a conceptual class, has verbs with a shared meaning. A parent verb class includes concepts of subordinate verb class; thus a subordinate verb class is a concretization of the parent verb class. A verb class has a semantic description that is a kind of semantic skeleton inspired from lexical conceptual structure (Jackendoff, 1990; Kageyama, 1996; Dorr, 1997) . Thus a semantic description in a verb class describes core semantic relations between arguments and shadow arguments of a shared meaning of the verb class. Since verb can be polysemous, each verb sense is designated with example sentences. Verb senses with a shared meaning are assigned to a verb class. Every example sentence is analyzed into their arguments and semantic role types; and then their arguments are linked to variables in semantic description of verb class. This indicates that one semantic description in a verb class can provide several argument structure on the basis of syntactic structure. This architecture is related to construction grammar. Here we explain this structure using verbs such as rent, lend, give, hire, borrow, lease. We assume that each verb sense we focus on here is designated by example sentences, e.g., \"Mother gives a book to her child\", \"Kazuko rents a bicycle from her friend\", and \"Taro lend a car to his friend\". As Figure 1 shows that all of the above verb senses are involved in the verb class Moving of One's Possession 5 . The semantic description, which expresses core meaning of the verb class Moving of One's Possession is ([Agent] CAUSE) BECOME [Theme] BE AT [Goal]. Where the brackets [] denote variables that can be Àlled with arguments in example sentences. Likewise parentheses () denote occasional factor. \"Agent\" and \"Theme\" are semantic role labels that can be annotated to all example sentences. Figure 1 describes semantic relations between \"Agent\" and \"Theme\". Since semantic role labels are annotated to all of the example sentences, the variables in the semantic description can be linked to practical arguments in example sentences via semantic role labels (See Figure 2 ). Construction of Verb Class Hierarchy To organize hierarchical semantic verb class, we take a top down and a bottom up approaches. As for a bottom up approach, we use verb senses deÀned by a dictionary as the most Àne-grained meaning; and then we group verbs that can be considered to share some meaning. As for a dictionary, we use the Lexeed database (Fujita et al., 2006) , which consists of more than 20,000 verbs with explanations of word sense and example sentences. As a top down approach, we take three semantic classes: State, Change of State, and Activity as top level semantic classes of the thesaurus according to Vendler's aspectual analysis (Vendler, 1967) (See Figure 4 ). This is because the above three classes can be useful for dealing with the propositional, especially, resultative aspect of verbs. For example \"He threw a ball\" can be an Activity and have no special result; but \"He broke the door\" can be a Change of State and then we can imagine a result, i.e., broken door. When other verb senses can express the same results, e.g., \"He destroyed the door,\" we would like to regard them as having the same meaning. We deÀne verb classes in intermediate hierarchy by grouping verb sense on the basis of aspectual category (i.e., action, state, change of state), argument type (i.e., physical, mental, information), and more detailed aspects depending on aspectual category. For example, walk the country, travel all over Europe and get up the stairs can be considered to be in the Move on Path class. Verb class is essential for dealing with verb meanings as synsets in WordNet. Even if we had given an incorrect class name, the thesaurus will work well if the whole hierarchy keeps is-a relation, namely, the hierarchy does not contain any multiple inheritance. The most Àne-grained verb class before individual verb sense is a little wider than alternations. Currently, for the Àne-grained verb class, we are organizing what kind of differentiated classes can be assumed (e.g., manner, background, presupposition, and etc.). Semantic Role Labels The aim of describing arguments of a target verb sense is (1) to link the same role arguments in a related verb sense and (2) to provide disambiguated information for mapping a surface expression to a verb sense. The Lexeed database provides a representative sentence for each word sense. The sentence is simple, without adjunctive elements such as unessential time, location or method. Thus, a sentence is broken down into subject and object, and semantic role labels are annotated to them (Figure 3 ). ex.: nihon-ga shigen-wo yunyuu-suru trans.: Japan resouces import (NOM) (ACC) AS: Agent Theme Of course, only one representative sentence would miss some essential arguments; also, we do not know how many arguments are enough. This can be solved by adding examples 6 ; however, we consider the semantic role labels of each representative sentence in a verb class as an example of assumed argument structure to a verb class. That is to say, we regard a verb class as a concept of event and suppose it to be a Àxed argument frame for each verb class. The argument frame is described as compositional relations. The principal function of the semantic role label name is to link arguments in a verb class. One exception is the Agent label. This can be a marker discriminating transitive and intransitive verbs. Since the semantic class of the thesaurus focuses on Change of State, transitive alternation cases such as \"The president expands the business\" and \" The business expands\" can be categorized into the same verb class. Then, these two examples are differentiated by the Agent label. Compositional Semantic Description As described in Section 3.1, we incorporate compositional semantic structure to each verb class to describe syntactically motivated lexical semantic relations and entailment meanings that will expand the thesaurus. The beneÀt of compositional style is to link entailed meanings by means of compositional manner. As an example of entailment, Figure 5 shows that a verb class Move to Goal entails Theme to be Goal, and this corresponds to a verb class Exist. 6 We are currently constructing an SRL annotated corpus. In this verb thesaurus, being different from previous LCS studies, we try to ensure the compositional semantic description as much as possible by means of linking each sub-event structure to both a semantic class and example sentences. Therefore, we believe that our verb thesaurus can provide a basic example data base for LCS study. Intrinsic Evaluation on Coverage We did manual evaluation that how the proposed verb thesaurus covers verb meanings in news articles. The results on Japanese new corpus show that the coverage of verbs is 84.32% (1825/2195) in 1000 sentences randomly sampled from Japanese news articles 7 . Besides we take 200 sentences and check whether the verb meanings in the sentences can correspond to verb meaning in our thesaurus. The result shows that our thesaurus meaning covers 99.5% (199 verb meanings/200 verb meanings) of 200 7 Mainichi news article in 2003. verbs 8 . Discussions Comparison with Existing Resources Table 1 and Table 2 show a comparison of statistical characteristics with existing resources. In the tables, WN and Exp denote the number of word meanings and example sentences, respectively. Also, SRL denotes the number corresponding to semantic role label. Looking at number of concepts, our Thesaurus has 709 types of concepts (verb classes) which is similar to FrameNet and more than VerbNet. This seems to be natural because FrameNet is also language resource constructed on argument structure. Thanks to our thesaurus format, if we need more Àne grained concepts, we can expand our thesaurus by adding concepts as new nodes at the lowest layer in the hierarchy. While at the number of SRL, FrameNet has much more types than our thesaurus, and in the other resources VerbNet and EDR the number of SRL is less than our thesaurus. This comes from the different design issue of semantic role labels. In FrameNet they try to differentiate argument types on the basis of the assumed concept, i.e., Frame. In contrast with FrameNet we try to merge the same type of meaning in arguments. VerbNet and EDR also deÀned abstracted SRL; The difference between their resources and our thesaurus is that our SRLs are deÀned taking into account what kind of roles in the core concept i.e., verb class; while SRLs in VerbNet and EDR are not dependent on verb's class. Table 2 shows that our thesaurus does not have large number in registered words and examples comparing to EDR and JWordNet. As we stated in Section 3.5, the coverage of our verb class to newspaper articles are high, but we try to add examples by constructing annotated Japanese corpus of SRL and verb class. Limitations of Developed Thesaurus One of the difÀculties of annotating the semantic class of word sense is that a word sense can be considered as several semantic classes. The proposed verb thesaurus can deal with multiple semantic classes for a verb sense by adding them into several nodes in the thesaurus. However, this does not seem to be the correct approach. For example, what kind of Change of State semantic class can be considered in the following sentence? a. He took on a passenger. Assuming that passenger is Theme, Move to goal could be possible when we regard the vehicle 9 as Goal. In another semantic class, Change State of Container could be possible when we regard the vehicle as a container. Currently, all of the verb senses are linked to only one semantic class that can be considered as the most related semantic class. From the user side, i.e., dealing with the propositional meaning of the sentence (a.), various meanings should be estimated. Consider the following sentence: b. Thus, we were packed. As the semantic class of the sentence (a.) Change State of Container could better explain why they are packed in the sentence (b.) The other related issue is how we describe the scope, e.g., c. He is hospitalized. If we take the meaning as a simple manner, Move to Goal can be a semantic class. This can be correct from the view of annotation, but we can guess he cannot work or he will have a tough time as following events. FrameNet seems to be able to deal with this by means of a special type of linking between Frames. Consequently, we think the above issues of semantic class should depend on the application side's demands. Since we do not know all of the requirements of NLP applications currently, then it must be sufÀcient to provide an expandable descriptional framework of linguistically motivated lexical semantics. Remaining and Conceivable Ways of Extension One of the aims of the proposed dictionary is to identify the sentences that have the same meanings among different expressions. One of the challenging paraphrase relations is that the sentences expressed from the different view points. Given the buying and selling in Figure 6 , a human can understand that both sentences denote almost the same event from different points of view. This indicates that the sentences made by humans usually contain the point of view of the speaker. This is similar to a camera, and we need to normalize the expressions as to their original meaning. We consider that NLP application researchers need to relate these expressions. Logically, if we know \"buy\" and \"sell\" have shared meanings of giving and taking things, we can describe their relations with \"or\" in logical form. Therefore, Ànding and describing these verb relations will be essential for dealing with propositional meanings of a sentence. For further view of application, event matching to Ànd a similar situation in Web documents is supposed to be a practical and useful application. Assuming that a user is confronted with the fact that wireless LAN in the user's PC does not work, and the user wants to search for documents that provide a solution, the problem is that expressions of situations must be different from the views of individual writers, e.g., \"wireless LAN did not work\" or \"wireless LAN was disconnected\". How can we Ànd the same meaning in these expressions, and how can we extract the answers by Ànding the same situation from FAQ documents? To solve this, a lexical database describing verb relations between \"go wrong\" and \"disconnect\" must be the base for estimating how the expressions can be similar. Therefore, constructing a lexicon can be worthwhile for developing NLP applications. Conclusion In this paper, we presented a framework of a verb dictionary in order to describe shared meaning as well as to differentiate meaning between verbs from the viewpoint of relating eventual expressions of NLP. One of the characteristics is that we describe verb relations on the basis of several semantic granularities using a thesaurus form with argument structure. Semantic granularity is the basis for how we categorize (or recognize which semantic class relates to a verb meaning). Also, we ensure functions and limitations of semantic classes and argument structure from the viewpoint of dealing with paraphrases. That is, required semantic classes will be highly dependent on applications; thus, the framework of the verb-sense dictionary should have expandability. The proposed verb thesaurus can take several semantic granularities; therefore, we hope the verb thesaurus will be applicable to NLP's task 10 . In future work, we will continue to organize differentiated semantic classes between verbs and develop a system to identify the same event descriptions. Acknowledgments We would like to thank NTT Communication Research Laboratory for allowing us to use the Lexeed database, and Professor Koyama for many useful comments.",
         "10903030",
         "4fa1c9b0445f47612572f785ee803214b4e7826d",
         "10",
         "https://aclanthology.org/W10-3201",
         "Coling 2010 Organizing Committee",
         "Beijing, China",
         "2010",
         "August",
         "Proceedings of the Eighth Workshop on {A}sian Language Resouces",
         "Takeuchi, Koichi  and\nInui, Kentaro  and\nTakeuchi, Nao  and\nFujita, Atsushi",
         "A Thesaurus of Predicate-Argument Structure for {J}apanese Verbs to Deal with Granularity of Verb Meanings",
         "1--8",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "takeuchi-etal-2010-thesaurus",
         null,
         null
        ],
        [
         "36",
         "R13-1055",
         "In this paper we undertake a large crossdomain investigation of sentiment domain adaptation, challenging the practical necessity of sentiment domain adaptation algorithms. We first show that across a wide set of domains, a simple \"all-in-one\" classifier that utilizes all available training data from all but the target domain tends to outperform published domain adaptation methods. A very simple ensemble classifier also performs well in these scenarios. Combined with the fact that labeled data nowadays is inexpensive to come by, the \"kitchen sink\" approach, while technically nonglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that \"flip\" polarity across domains is not borne out empirically.",
         "In this paper we undertake a large crossdomain investigation of sentiment domain adaptation, challenging the practical necessity of sentiment domain adaptation algorithms. We first show that across a wide set of domains, a simple \"all-in-one\" classifier that utilizes all available training data from all but the target domain tends to outperform published domain adaptation methods. A very simple ensemble classifier also performs well in these scenarios. Combined with the fact that labeled data nowadays is inexpensive to come by, the \"kitchen sink\" approach, while technically nonglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that \"flip\" polarity across domains is not borne out empirically. Introduction Automatic detection and analysis of sentiment around products, brands, political issues etc. has triggered a large amount of research in the past 15 -20 years (for a recent overview see Pang & Lee 2008 and Liu 2012) . Early work focused on algorithms for mining sentiment dictionaries (Hatzivassiloglou and McKeown 1997, Turney 2002) ; this was followed by the exploration of supervised techniques (Pang et al. 2002) and, somewhat more recently, by investigations of domain adaptation techniques. Also more recently, the focus has broadened from the detection of polarity (negative/positive sentiment) to more nuanced approaches that try to identify targets and holders of sentiment, sentiment strength, or finer-grained mood distinctions (e.g. Wilson et al. 2006, Kim and Hovy 2006) . Within the polarity detection paradigm, a number of common assumptions have been shared in the community and are frequently repeated in the literature. Two of these fundamental assumptions are: 1. Obtaining sufficient labeled data for supervised training is expensive 2. Sentiment models trained on one domain tend to perform poorly on new, unseen domains A conclusion that is often drawn from these assumptions is that domain adaptation of sentiment models from a domain with sufficient labeled data to a new domain with little labeled data is an important problem and requires new and sophisticated algorithms. In this paper, we empirically re-examine the assumptions above. Based on a wide range of experiments on 27 different domains, we challenge the conclusion that domain adaptation for polarity detection necessarily requires novel and sophisticated machinery. It is important to keep in mind, however, that our claims are strictly limited to the problem under investigation, namely polarity detection. We do not make any claims whatsoever about domain adaptation for other sentiment-related problems or general problems in machine learning. Based on readily available data from 27 domains, we show that a \"kitchen sink\" approach where all source domain data are combined to train a single classifier sets a surprisingly high baseline for polarity identification accuracy across domains. We also show on a previously released data set of four domains that the result is competitive with a state-of-theart domain adaptation approach using Structural Correspondence Learning. We then show that a straightforward ensemble learner can, for some domains, improve results further, without any need for specialized learning algorithms. Since most work in domain-adaptation only provides published results on pairwise adaptation between domains and not on multi-domain adaptation, we hope to establish a new baseline for future adaptation techniques to compare against. Related Work Of direct importance to the discussion in this paper are results from domain adaptation in polarity detection. One of the earlier successful approaches (Blitzer et al. 2006 (Blitzer et al. , 2007) ) involved Structural Correspondence Learning (SCL). SCL identifies \"pivot\" features that are both highly discriminative in the labeled source domain data and also frequent in the unlabeled target domain data. In a subsequent step, linear predictors for the pivot terms are learned from the unlabeled target data and from the source data. Daumé (2007) approached domain adaptation from a fully labeled source domain to a partially labeled target domain by augmenting the feature space. Instead of using a single, general, feature set for source and target, three distinct feature sets are created: the general set of features, a source-domain specific version of the feature set, and a target-specific version of the feature set. Li and Zong (NLP-KE 2008) explore a classifier combination technique they call \"Multiple-Label Consensus Training\" which results in better accuracy than non-adapted models on the data sets used in Blitzer et al. (2007) . They also addressed the multi-domain sentiment analysis problem using feature -level fusion and classifier-level fusion approaches in Li and Zong (ACL 2008) . Dredze and Crammer (2008) have proposed a multi-domain online learning framework based on parameter combination from multiple Confidence Weighted (CW) classifiers. Their Multi-Domain Regularization (MDR) framework seeks to learn domain specific parameters guided by the shared parameter across domains. Samdani and Yih (2011) propose an ensemble learner that consists of classifiers trained on different feature groups. The feature groups are Chen et al. (2011) use a specific co-training algorithm for domain adaptation on the Blitzer et al. (2007) data set. In averaged pair-wise comparisons they establish gains over a source-plustarget logistic regression baseline. Glorot et al. (2011) investigate a deep learning approach to domain adaptation and report increased accuracy across domains both on the Blitzer et al. (2007) 4-domain data set and the larger Amazon review data set (25 domains) also made available in that release. They also introduce a new metric for transfer learning: Transfer Ratio. Datasets & Experimental Setup This section illustrates the datasets, the methods and the setup of our experiments. Datasets The datasets we used in our experiments have been obtained from three sources: 1. Amazon reviews 1 : this dataset contains more than 5.8 million reviews. It has been used in previous work on sentiment analysis (see Glorot et al. (2011) ). The Amazon reviews include 25 domains as shown in 2010 to Oct. 31, 2011 . The dataset has been originally annotated for affects. We mapped the positive affects \"joviality\" and \"serenity\" to positive sentiment and the negative affects \"fatigue\", \"hostility\", and \"sadness\" to negative sentiment. We selected a balanced dataset of 2,000 tweets from the various months of the collected tweets. The average review length for the Amazon and hotel reviews is 437 characters and 97 words. In total, we used 27 domains namely the identified based on how stable the feature distribution is across domains, which can either be estimated from the data directly or can be hypothesized based on domain knowledge. 25 Amazon domains, the hotel domain and the Twitter domain. We considered Twitter as a domain though the content of tweets spans multiple domains since it has different characteristics from the product reviews. Tweets are constrained log-likelihood ratio (LLR). Further, we used the accuracy metric to indicate the performance of each of the above four domain adaptation techniques. We also employed the Transfer Ratio metric proposed by Glorot et al. (2011) The Amazon reviews and the hotel reviews are rated between 1 and 5 on a 5 point scale where 1 is the most negative and 5 is the most positive. We have extracted only the reviews that are rated 5 and 1 to represent the positive and negative reviews respectively. Further, we ensured that the datasets we extracted and used for training are balanced between positive and negative reviews. Table 1 summarizes the 27 domains and their dataset sizes including the balanced datasets we used for training. Experimental Setup In our experiments, we employed the datasets of the 27 domains mentioned in section 3.1. In each experiment, we have employed one domain for testing while the other 26 domains have been used for training. We compared four domain adaptation techniques: 1. One classifier trained in all source domains. 2. An ensemble of classifiers, each trained on a source domain, combined into an ensemble. 3. The domain adaptation approach proposed in Daumé (2007) . 4. We also compared the results of approaches 1 and 2 to published results on Structural Correspondence Learning (SCL) by using the same datasets as in Blitzer et al. (2007) . In all our experiments, we employed Maximum Entropy-based classification with vanilla parameter settings and feature reduction using ure the performance of the all-in-one and ensemble classifiers. The rest of the subsection illustrates the experimental setup for each of the above four approaches. In-domain Classifiers To establish a \"ceiling\" performance we built an in-domain classifier for each of the 27 domains. The in-domain classifier is trained with a dataset of that one domain and tested on the same domain (using cross-validation). This standard indomain supervised setup establishes an upper bound for classification performance (although in some cases we will see that other techniques can outperform this upper bound). Features consist of binary unigram and bigram features. On average, the total number of features in each domain is 52,039. Feature reduction was performed using LLR, retaining only the top 20,000 most predictive features as established on the training set. We compare the results obtained from testing each domain with the three approaches to its indomain classifier results. All-in-one Classifier The all-in-one classifier is a maximum entropy classifier trained with the source domain datasets merged together. In this setting, the classifier is trained with data from multiple domains, which exposes it to multiple sentiment vocabularies at training time, creating a somewhat domain-independent and general model. The all-in-one classifier is trained with 26 domains datasets while being tested on the held-out 27 th domain. Ensemble Classifiers One approach to address the problem of domain adaptation is to construct an ensemble of classifiers, all of which contribute partially to the final result (see Dietterich (1997) for an overview). We constructed an ensemble of in-domain sentiment classifiers, one for each source domain. http://svmlight.joachims.org/ Daumé (2007) addresses domain adaptation where a large, annotated corpus of data from the source domain is available with only a small, annotated corpus of the target domain. Daumé's work leverages both annotated datasets to obtain a model that performs well on the target domain. For K source domains, the augmented feature space consists of K+1 copies of the original feature space. However, creating three versions of each feature in both the source and the target domains grows the feature space exponentially, which is prohibitive in a many-domain adaptation scenario such as ours which consists of a total of 27 domains. We addressed this challenge by considering the 26 source domains as a single source domain being adapted to the target domain. This setup along with feature reduction enabled us to apply Daumé's approach without too much of an inflation of the feature space. However, we also recognize that this likely compromises the power of the feature augmentation approach. Blitzer et al. (2007) employ the Structural Correspondence Learning (SCL) algorithm for sentiment domain adaptation. Blitzer et al. evaluate the SCL domain adaptation on four publicly released datasets from Amazon product reviews: books, DVDs, electronics and kitchen appliances. In these four datasets, reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative, and the rest discarded because their polarity was ambiguous. 1000 positive and 1000 negative labeled examples were used for each domain. Some unlabeled data were additionally used including 3685 (DVDs) and 5945 (kitchen). Each labeled dataset was split into 1600 instances for training and 400 instances for testing. The baseline in Blitzer et al. (2007) is a linear classifier trained without adaptation, while their ceiling reference is the same as ours, which is the in-domain classifier trained and tested on the same domain. Blitzer's Structural Correspondence Learning We conducted a set of experiments employing the four datasets used for SCL domain adaptation. In these experiments, we compare the results of our all-in-one classifier and the ensemble classifier trained and tested on the four datasets to the results of SCL and its variation SCL-MI domain adaptation as reported by Blitzer et al. (2007) baseline and ceiling in-domain classifiers for the four domains. Results & Discussion This section summarizes the results of the experiments described in section 3.2 while further scrutinizing the comparison between the four domain adaptation sentiment analysis techniques. We also report the Transfer Ratio results of the all-in-one and ensemble classifiers. Generally, the all-in-one classifier is closely comparable to the in-domain classifier of each domain Results In this section, we summarize the various results obtained from the set of experiments described in section 3.2. In the summary of each experiment results, we also plot the in-domain classifier results of each domain as the ceiling of comparison. All-in-one Classifier Experiments In the all-in-one classifier experiments, the sentiment classifier is trained with 26 domain datasets while testing it with the 27 th domain. Table 3 summarizes the results. The results of the allin-one classifier are very close to the in-domain classifiers in most domains except for the apparel, beauty, magazines, outdoor living, office products and software. Ensemble Classifier Experiments We produced the results of the ensemble of classifiers using the three settings: majority votes, sum of weights, and meta-classification using both logistic regression and SVM. Table 3 summarizes the results of the three settings used in the ensemble. Table 3 shows that the ensembles with sum of weights and meta-training (SVM sigmoid kernel) are the most comparable to the in-domain classifier of each domain. We also experimented with variations of logistic regression and SVM for meta-training. The non-linear (RBF kernel) SVM meta-classifier outperforms the linear logistic regression model. We have employed two variations of SVM, namely, a radial basis function with gamma 0.01 and sigmoid kernel. In most domains, the SVM model trained with 50 positive and 50 negative feedback examples is not far off the one trained with 5 positive and 5 negative feedback examples. This shows that even with little labeled data in the target domain, the en-semble could effectively combine the weights of the classifier votes. We expect the ensemble to achieve steady but slow performance gains over time while collecting more feedback examples. Hal Daumé's Domain-Adaptation Approach We compared the performance of the all-in-one and ensemble classifiers to Daumé's feature augmentation algorithm. Table 3 shows that the all-in-one classifier exceeds Daumé's approach in all 27 domains given our current implementation of Daumé's approach. The ensemble exceeds Daumé's approach on all domains except office, kitchen & housewares, magazines, office products, and tweets. Structural Correspondence Learning (SCL) We employed the four domains datasets used in Blitzer et al. (2007) to train and test the all-in one and the ensemble classifiers. We also replicated the in-domain results of these four datasets using our maximum entropy classifier. We compare the results of the all-in-one and the ensemble classifier to the SCL and its variation SCL-MI adaptation techniques using the four datasets used to evaluate SCL and SCL-MI in Blitzer et al. (2007) . Note that the results published in Blitzer's work represent pairwise domain-adaptation, while our ensemble and all-in-one results are based on training on three of Blitzer's domains and testing on the held-out fourth domain. This makes it impossible to draw a direct comparison, but we can still observe that in general, it is best to simply combine as many domains as possible in an all-in-one or ensemble approach as compared to carefully adapting a single domain. Table 2 Where is the transfer error defined as the test error obtained by a method trained on the source domain S and tested on the target domain T. is the test error obtained by the baseline method. The transfer ratio Q also characterizes the transfer but is defined by replacing the difference by a quotient in t:  Where n is the number of couples (S, T) with S≠T. The all-in-one classifier had a 1.12 transfer ratio across domains, which is very close to the best result of ~1.07 in Glorot et al. The ensemble with Sigmoid kernel of SVM trained on 50 positive and 50 negative feedback examples from the target domain had 1.81 transfer ratio. The ensemble with radial basis function (gamma=0.01) trained on 5 positive and 5 negative feedback examples from the target domain had 1.85 transfer ratio. Note that the transfer ratio of the indomain classifier, which is used a base-line for calculating the transfer ratio is 1. The transfer ratio of the all-in-one classifier is better than the transfer ratio of the ensemble with its two variations. Discussion The results in the previous section indicate that both the all-in-one and the ensemble approaches exceed both Daumé's domain adaptation technique on the 27 datasets (given our current implementation of Daumé's approach) and SCL on the four datasets in Blitzer et al. (2007) and that the all-in-one approach achieves comparable results in terms of transfer ratio to Glorot et al. (2011) . The ensemble approach exceeds the all-in-one in some domains like apparel and automotive. They both are very close in some domains like When comparing the all-in-one and the ensemble approaches on the four datasets in Blitzer et al. (2007) , the all-in-one exceeds the ensemble only in the DVD domain. The ensemble exceeds the all-in-one in electronics and kitchen & housewares. They both perform at the same accuracy level on the books domain. We have also employed NcNemar significance test between pairs of the all-in-one, the ensemble and Daumé's approaches on the 27 domains. Table 4 shows the significance difference between the approaches' combinations. Finally, we would like to do some initial exploration of the role of features across domains. The commonly held belief is that sentiment indicators such as \"hot\" can change their polarity from domain to domain (e.g. it is positive in the food domain while it is negative in the negative domain), contributing to the need for domain adaptation. On the other hand, the success of the all-in-one classifier indicates that a greater number of observed sentiment features and more solid statistics on those features are more important than capturing domain-specific polarity changes. In order to gather evidence for or against these hypotheses, we first calculated the number of overlapping features between each pair of domains within the 27 domains. The average percentage of features that overlap between pairs of domain is only 12.48%. Furthermore, only a very small set of the highly sentiment-correlated features overlap. 16 features overlap among the 27 domains which accounts for only 0.08% of the features. Examples of positive overlapping feature are \"highly\", \"excellent\", and \"great\". Negative overlapping features are \"waste\", \"terrible\", and \"worst\". This low feature overlap of sentiment-bearing features lends some support to the hypothesis that in order to capture a general, large-scale sentiment vocabulary nothing beats diverse and plentiful training data. The low feature overlap also justifies why the all-in-one classifier exceeds the ensemble though the latter has access to some labeled data in the target Second, we examined the question of polaritychanging sentiment features. Among the top 1000 features in each domain ranked by LLR, we counted the common features among multiple domains. The number of common features among 15 domains is 42 features. Only 13 features are common among 20 domains while there are no common features from the highest 1000 likelihood ratio features among the 27 domains. Most features do not flip polarity across domains. For example the word \"waste\" is common among 20 domains and maintains a negative polarity across the domains. Very few features flip polarity across domains. The word \"highly\" is shared across 23 domains. It maintains a positive polarity in all domains while it flips in Tools & Hardware. The word \"refund\" is shared in 20 domains. It maintains a negative polarity in almost all domains except Gourmet Food. Conclusion In this paper, we empirically re-examine the assumption that adapting one or multiple domains with plenty of labeled sentiment polarity data to one domain with little labeled data requires new and sophisticated algorithms. We evaluate four domain adaptation techniques on a wide variety of domains in two major groups of state-of-theart datasets. Our experiments show that overall, simple domain adaptation techniques like the allin-one classifier do comparably well if not better than more sophisticated domain adaptation techniques. Combined with the fact that labeled sentiment data tends to be cheap to come by through either the collection of product reviews from the web or inexpensive crowd-sourced labeling, this indicates that in practice, domain-adaptation for sentiment detection might be of less importance than previously claimed. We also show that the often anecdotally observed \"polarity-flip\" of sentiment terms from one domain to another in practice is a rather rare occurrence and might not be as detrimental to sentiment domain adaptation as assumed in much of the literature.",
         "17625127",
         "d0f6b69813dd7eaef42fd07f3c23948cd1749bdb",
         "12",
         "https://aclanthology.org/R13-1055",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Mansour, Riham  and\nRefaei, Nesma  and\nGamon, Michael  and\nAbdul-Hamid, Ahmed  and\nSami, Khaled",
         "Revisiting the Old Kitchen Sink: Do we Need Sentiment Domain Adaptation?",
         "420--427",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "mansour-etal-2013-revisiting",
         null,
         null
        ],
        [
         "37",
         "W05-0831",
         "This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly improves translation quality.",
         "This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly improves translation quality. Introduction Reordering is of crucial importance for machine translation. Already (Knight et al., 1998) use full unweighted permutations on the level of source words in their early weighted finite-state transducer approach which implemented single-word based translation using conditional probabilities. In a refinement with additional phrase-based models, (Kumar et al., 2003) define a probability distribution over all possible permutations of source sentence phrases and prune the resulting automaton to reduce complexity. A second category of finite-state translation approaches uses joint instead of conditional probabilities. Many joint probability approaches originate in speech-to-speech translation as they are the natural choice in combination with speech recognition models. The automated transducer inference techniques OMEGA (Vilar, 2000) and GIATI (Casacuberta et al., 2004 ) work on phrase level, but ignore the reordering problem from the view of the model. Without reordering both in training and during search, sentences can only be translated properly into a language with similar word order. In (Bangalore et al., 2000) weighted reordering has been applied to target sentences since defining a permutation model on the source side is impractical in combination with speech recognition. In order to reduce the computational complexity, this approach considers only a set of plausible reorderings seen on training data. Most other phrase-based statistical approaches like the Alignment Template system of Bender et al. (2004) rely on (local) reorderings which are implicitly memorized with each pair of source and target phrases in training. Additional reorderings on phrase level are fully integrated into the decoding process, which increases the complexity of the system and makes it hard to modify. Zens et al. (2003) reviewed two types of reordering constraints for this type of translation systems. In our work we follow a phrase-based translation approach, applying source sentence reordering on word level. We compute a reordering graph ondemand and take it as input for monotonic translation. This approach is modular and allows easy introduction of different reordering constraints and probabilistic dependencies. We will show that it performs at least as well as the best statistical machine translation system at the IWSLT Evaluation. In the next section we briefly review the basic theory of our translation system based on weighted finite-state transducers (WFST). In Sec. 3 we introduce new methods for reordering and alignment monotonization in training. To compare different reordering constraints used in the translation search process we develop an on-demand computable framework for permutation models in Sec. 4. In the same section we also define and analyze unrestricted and restricted permutations with some of them being first published in this paper. We conclude the paper by presenting and discussing a rich set of experimental results. Machine Translation using WFSTs Let f J 1 and e I i be two sentences from a source and target language. Assume that we have word level alignments A of all sentence pairs from a bilingual training corpus. We denote with ẽJ 1 the segmentation of a target sentence e I 1 into J phrases such that f J 1 and ẽJ 1 can be aligned to form bilingual tuples (f j , ẽj ). If alignments are only functions of target words A : {1, . . . , I} → {1, . . . , J}, the bilingual tuples (f j , ẽj ) can be inferred with e. g. the GIATI method of (Casacuberta et al., 2004) , or with our novel monotonization technique (see Sec. 3). Each source word will be mapped to a target phrase of one or more words or an \"empty\" phrase ε. In particular, the source words which will remain non-aligned due to the alignment functionality restriction are paired with the empty phrase. We can then formulate the problem of finding the best translation êI 1 of a source sentence f J 1 : êI 1 = argmax e I 1 P r(f J 1 , e I 1 ) = argmax ẽJ 1 A∈A P r(f J 1 , ẽJ 1 , A) ∼ = argmax ẽJ 1 max A∈A P r(A) • P r(f J 1 , ẽJ 1 |A) ∼ = argmax ẽJ 1 max A∈A f j :j=1...J P r(f j , ẽj |f j−1 1 , ẽj−1 1 , A) = argmax ẽJ 1 max A∈A f j :j=1...J p(f j , ẽj |f j−1 j−m , ẽj−1 j−m , A) In other words: if we assume a uniform distribution for P r(A), the translation problem can be mapped to the problem of estimating an m-gram language model over a learned set of bilingual tuples (f j , ẽj ). Mapping the bilingual language model to a WFST T is canonical and it has been shown in (Kanthak et al., 2004 ) that the search problem can then be rewritten using finite-state terminology: êI 1 = project-output(best(f J 1 • T )) . This implementation of the problem as WFSTs may be used to efficiently solve the search problem in machine translation. Reordering in Training When the alignment function A is not monotonic, target language phrases ẽ can become very long. For example in a completely non-monotonic alignment all target words are paired with the last aligned source word, whereas all other source words form tuples with the empty phrase. Therefore, for language pairs with big differences in word order, probability estimates may be poor. This problem can be solved by reordering either source or target training sentences such that alignments become monotonic for all sentences. We suggest the following consistent source sentence reordering and alignment monotonization approach in which we compute optimal, minimum-cost alignments. First, we estimate a cost matrix C for each sentence pair (f J 1 , e I 1 ). The elements of this matrix c ij are the local costs of aligning a source word f j to a target word e i . Following (Matusov et al., 2004) , we compute these local costs by interpolating state occupation probabilities from the source-to-target and target-to-source training of the HMM and IBM-4 models as trained by the GIZA++ toolkit (Och et al., 2003) . For a given alignment A ⊆ I × J, we define the costs of this alignment c(A) as the sum of the local costs of all aligned word pairs: c(A) = (i,j)∈A c ij (1) The goal is to find an alignment with the minimum costs which fulfills certain constraints. Source Sentence Reordering To reorder a source sentence, we require the alignment to be a function of source words A 1 : {1, . . . , J} → {1, . . . , I}, easily computed from the cost matrix C as: A 1 (j) = argmin i c ij (2) We do not allow for non-aligned source words. A 1 naturally defines a new order of the source words f J 1 which we denote by f J 1 . By computing this permutation for each pair of sentences in training and applying it to each source sentence, we create a corpus of reordered sentences. Alignment Monotonization In order to create a \"sentence\" of bilingual tuples ( f J 1 , ẽJ 1 ) we required alignments between reordered source and target words to be a function of target words A 2 : {1, . . . , I} → {1, . . . , J}. This alignment can be computed in analogy to Eq. 2 as: A 2 (i) = argmin j cij (3) where cij are the elements of the new cost matrix C which corresponds to the reordered source sentence. We can optionally re-estimate this matrix by repeating EM training of state occupation probabilities with GIZA++ using the reordered source corpus and the original target corpus. Alternatively, we can get the cost matrix C by reordering the columns of the cost matrix C according to the permutation given by alignment A 1 . In alignment A 2 some target words that were previously unaligned in A 1 (like \"the\" in Fig. 1 ) may now still violate the alignment monotonicity. The monotonicity of this alignment can not be guaranteed for all words if re-estimation of the cost matrices had been performed using GIZA++. The general GIATI technique (Casacuberta et al., 2004) is applicable and can be used to monotonize the alignment A 2 . However, in our experiments the following method performs better. We make use of the cost matrix representation and compute a monotonic minimum-cost alignment with a dynamic programming algorithm similar to the Levenshtein string edit distance algorithm. As costs of each \"edit\" operation we consider the local alignment costs. The resulting alignment A 3 represents a minimum-cost monotonic \"path\" through the cost matrix. To make A 3 a function of target words we do not consider the source words non-aligned in A 2 and also forbid \"deletions\" (\"many-to-one\" source word alignments) in the DP search. An example of such consistent reordering and monotonization is given in Fig. 1 . Here, we reorder the German source sentence based on the initial alignment A 1 , then compute the function of target words A 2 , and monotonize this alignment to A 3 Reordering in Search When searching the best translation ẽJ 1 for a given source sentence f J 1 , we permute the source sentence as described in (Knight et al., 1998) : êI 1 = project-output(best(permute(f J 1 ) • T )) Permuting an input sequence of J symbols results in J! possible permutations and representing the permutations as a finite-state automaton requires at least 2 J states. Therefore, we opt for computing the permutation automaton on-demand while applying beam pruning in the search. Lazy Permutation Automata For on-demand computation of an automaton in the flavor described in (Kanthak et al., 2004) it is sufficient to specify a state description and an algorithm that calculates all outgoing arcs of a state from the state description. In our case, each state represents a permutation of a subset of the source words f J 1 , which are already translated. This can be described by a bit vector b J 1 (Zens et al., 2002) . Each bit of the state bit vector corresponds to an arc of the linear input automaton and is set to one if the arc has been used on any path from the initial to the current state. The bit vectors of two states connected by an arc differ only in a single bit. Note that bit vectors elegantly solve the problem of recombining paths in the automaton as states with the same bit vectors can be merged. As a result, a fully minimized permutation automaton has only a single initial and final state. Even with on-demand computation, complexity using full permutations is unmanagable for long sentences. We further reduce complexity by additionally constraining permutations. Refer to Figure 2 for visualizations of the permutation constraints which we describe in the following. IBM Constraints The IBM reordering constraints are well-known in the field of machine translation and were first described in (Berger et al., 1996) . The idea behind these constraints is to deviate from monotonic translation by postponing translations of a limited number of words. More specifically, at each state we can translate any of the first l yet uncovered word positions. The implementation using a bit vector is straightforward. For consistency, we associate window size with the parameter l for all constraints presented here. Inverse IBM Constraints The original IBM constraints are useful for a large number of language pairs where the ability to skip some words reflects the differences in word order between the two languages. For some other pairs, it is beneficial to translate some words at the end of the sentence first and to translate the rest of the sentence nearly monotonically. Following this idea we can define the inverse IBM constraints. Let j be the first uncovered position. We can choose any position for translation, unless l − 1 words on positions j > j have been translated. If this is the case we must translate the word in position j. The inverse IBM constraints can also be expressed by invIBM(x) = transpose(IBM(transpose(x))) . As the transpose operation can not be computed on-demand, our specialized implementation uses bit vectors b J 1 similar to the IBM constraints. Local Constraints For some language pairs, e.g. Italian -English, words are moved only a few words to the left or right. The IBM constraints provide too many alternative permutations to chose from as each word can be moved to the end of the sentence. A solution that allows only for local permutations and therefore has very low complexity is given by the following permutation rule: the next word for translation comes from the window of l positions 1 counting from the first yet uncovered position. Note, that the local constraints define a true subset of the permutations defined by the IBM constraints. ITG Constraints Another type of reordering can be obtained using Inversion Transduction Grammars (ITG) (Wu, 1997) . These constraints are inspired by bilingual bracketing. They proved to be quite useful for machine translation, e.g. see (Bender et al., 2004) . Here, we interpret the input sentence as a sequence of segments. In the beginning, each word is a segment of its own. Longer segments are constructed by recursively combining two adjacent segments. combination step, we either keep the two segments in monotonic order or invert the order. This process continues until only one segment for the whole sentence remains. The on-demand computation is implemented in spirit of Earley parsing. We can modify the original ITG constraints to further limit the number of reorderings by forbidding segment inversions which violate IBM constraints with a certain window size. Thus, the resulting reordering graph contains the intersection of the reorderings with IBM and the original ITG constraints. Weighted Permutations So far, we have discussed how to generate the permutation graphs under different constraints, but permutations were equally probable. Especially for the case of nearly monotonic translation it is make sense to restrict the degree of non-monotonicity that we allow when translating a sentence. We propose a simple approach which gives a higher probability to the monotone transitions and penalizes the nonmonotonic ones. A state description b J 1 , for which the following condition holds: M on(j) : b j = δ(j ≤ j) ∀ 1 ≤ j ≤ J represents the monotonic path up to the word f j . At each state we assign the probability α to that outgoing arc where the target state description fullfills M on(j +1) and distribute the remaining probability mass 1 − α uniformly among the remaining arcs. In case there is no such arc, all outgoing arcs get the same uniform probability. This weighting scheme clearly depends on the state description and the outgoing arcs only and can be computed on-demand. Experimental Results Corpus Statistics The translation experiments were carried out on the Basic Travel Expression Corpus (BTEC), a multilingual speech corpus which contains tourism-related sentences usually found in travel phrase books. We tested our system on the so called Chinese-to-English (CE) and Japanese-to-English (JE) Supplied Tasks, the corpora which were provided during the International Workshop on Spoken Language Translation (IWSLT 2004) (Akiba et al., 2004) . In addition, we performed experiments on the Italian-to-English (IE) task, for which a larger corpus was kindly provided to us by ITC/IRST. The corpus statistics for the three BTEC corpora are given in Tab. 1. The development corpus for the Italian-to-English translation had only one reference translation of each Italian sentence. A set of 506 source sentences and 16 reference translations is used as a development corpus for Chinese-to-English and Japanese-to-English and as a test corpus for Italianto-English tasks. The 500 sentence Chinese and Japanese test sets of the IWSLT 2004 evaluation campaign were translated and automatically scored against 16 reference translations after the end of the campaign using the IWSLT evaluation server. Evaluation Criteria For the automatic evaluation, we used the criteria from the IWSLT evaluation campaign (Akiba et al., 2004) , namely word error rate (WER), positionindependent word error rate (PER), and the BLEU and NIST scores (Papineni et al., 2002; Doddington, 2002) . The two scores measure accuracy, i. e. larger scores are better. The error rates and scores were computed with respect to multiple reference transla- tions, when they were available. To indicate this, we will label the error rate acronyms with an m. Both training and evaluation were performed using corpora and references in lowercase and without punctuation marks. Experiments We used reordering and alignment monotonization in training as described in Sec. 3. To estimate the matrices of local alignment costs for the sentence pairs in the training corpus we used the state occupation probabilities of GIZA++ IBM-4 model training and interpolated the probabilities of source-to-target and target-to-source training directions. After that we estimated a smoothed 4-gram language model on the level of bilingual tuples f j , ẽj and represented it as a finite-state transducer. When translating, we applied moderate beam pruning to the search automaton only when using reordering constraints with window sizes larger than 3. For very large window sizes we also varied the pruning thresholds depending on the length of the input sentence. Pruning allowed for fast translations and reasonable memory consumption without a significant negative impact on performance. In our first experiments, we tested the four reordering constraints with various window sizes. We aimed at improving the translation results on the development corpora and compared the results with two baselines: reordering only the source training sentences and translation of the unreordered test sentences; and the GIATI technique for creating bilingual tuples (f j , ẽj ) without reordering of the source sentences, neither in training nor during translation. Highly Non-Monotonic Translation (JE) Fig. 3 (left) shows word error rate on the Japanese-to-English task as a function of the window size for different reordering constraints. For each of the constraints, good results are achieved using a window size of 9 and larger. This can be attributed to the Japanese word order which is very different from English and often follows a subjectobject-verb structure. For small window sizes, ITG or IBM constraints are better suited for this task, for larger window sizes, inverse IBM constraints perform best. The local constraints perform worst and require very large window sizes to capture the main word order differences between Japanese and English. However, their computational complexity is low; for instance, a system with local constraints and window size of 9 is as fast (25 words per second) as the same system with IBM constraints and window size of 5. Using window sizes larger than 10 is computationally expensive and does not significantly improve the translation quality under any of the constraints. Tab. 2 presents the overall improvements in translation quality when using the best setting: inverse IBM constraints, window size 9. The baseline without reordering in training and testing failed completely for this task, producing empty translations for 37 % of the sentences 2 . Most of the original alignments in training were non-monotonic which resulted in mapping of almost all Japanese words to ε when using only the GIATI monotonization technique. Thus, the proposed reordering methods are of crucial importance for this task. Further improvements were obtained with a rescoring procedure. For rescoring, we produced a k-best list of translation hypotheses and used the word penalty and deletion model features, the IBM Model 1 lexicon score, and target language n-gram models of the order up to 9. The scaling factors for all features were optimized on the development corpus for the NIST score, as described in (Bender et al., 2004) . Moderately Non-Mon. Translation (CE) Word order in Chinese and English is usually similar. However, a few word reorderings over quite large distances may be necessary. This is especially true in case of questions, in which question words like \"where\" and \"when\" are placed at the end of a sentence in Chinese. The BTEC corpora contain many sentences with questions. The inverse IBM constraints are designed to perform this type of reordering (see Sec. 4.3). As shown in Fig. 3 , the system performs well under these con- straints already with relatively small window sizes. Increasing the window size beyond 4 for these constraints only marginally improves the translation error measures for both short (under 8 words) and long sentences. Thus, a suitable language-pair-specific choice of reordering constraints can avoid the huge computational complexity required for permutations of long sentences. Tab. 2 includes error measures for the best setup with inverse IBM constraints with window size of 7, as well as additional improvements obtained by a kbest list rescoring. The best settings for reordering constraints and model scaling factors on the development corpora were then used to produce translations of the IWSLT Japanese and Chinese test corpora. These translations were evaluated against multiple references which were unknown to the authors. Our system (denoted with WFST, see Tab. 3) produced results competitive with the results of the best system at the evaluation campaign (denoted with AT (Bender et al., 2004) ) and, according to some of the error measures, even outperformed this system. Almost Monotonic Translation (IE) The word order in the Italian language does not differ much from the English. Therefore, the absolute translation error rates are quite low and translating without reordering in training and search already results in a relatively good performance. This is reflected in Tab. 4. However, even for this language pair it is possible to improve translation quality by performing reordering both in training and during translation. The best performance on the development corpus is obtained when we constrain the reodering with relatively small window sizes of 3 to 4 and use either IBM or local reordering constraints. On the test corpus, as shown in Tab. 4, all error measures can be improved with these settings. Especially for languages with similar word order it is important to use weighted reorderings (Sec. 4.6) in order to prefer the original word order. Introduction of reordering weights for this task results in notable improvement of most error measures using either the IBM or local constraints. The optimal probability α for the unreordered path was determined on the development corpus as 0.5 for both of these constraints. The results on the test corpus using this setting are also given in Tab. 4. Conclusion In this paper, we described a reordering framework which performs source sentence reordering on word level. We suggested to use optimal alignment functions for monotonization and improvement of translation model training. This allowed us to translate monotonically taking a reordering graph as input. We then described known and novel reordering constraints and their efficient finite-state implementations in which the reordering graph is computed ondemand. We also utilized weighted permutations. We showed that our monotonic phrase-based translation approach effectively makes use of the reordering framework to produce quality translations even from languages with significantly different word order. On the Japanese-to-English and Chinese-to-English IWSLT tasks, our system performed at least as well as the best machine translation system. Acknowledgement This work was partially funded by the Deutsche Forschungsgemeinschaft (DFG) under the project \"Statistische Textübersetzung\" (Ne572/5) and by the European Union under the integrated project TC-STAR -Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).",
         "1854610",
         "0cfe82145332522ed6665bd039f4fd089ae24141",
         "93",
         "https://aclanthology.org/W05-0831",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Kanthak, Stephan  and\nVilar, David  and\nMatusov, Evgeny  and\nZens, Richard  and\nNey, Hermann",
         "Novel Reordering Approaches in Phrase-Based Statistical Machine Translation",
         "167--174",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "kanthak-etal-2005-novel",
         null,
         null
        ],
        [
         "38",
         "C10-1025",
         "Text mining for global health surveillance is an emerging technology that is gaining increased attention from public health organisations and governments. The lack of multilingual resources such as Word-Nets specifically targeted at this task have so far been a major bottleneck. This paper reports on a major upgrade to the BioCaster Web monitoring system and its freely available multilingual ontology; improving its original design and extending its coverage of diseases from 70 to 336 in 12 languages.",
         "Text mining for global health surveillance is an emerging technology that is gaining increased attention from public health organisations and governments. The lack of multilingual resources such as Word-Nets specifically targeted at this task have so far been a major bottleneck. This paper reports on a major upgrade to the BioCaster Web monitoring system and its freely available multilingual ontology; improving its original design and extending its coverage of diseases from 70 to 336 in 12 languages. Introduction The number of countries who can sustain teams of experts for global monitoring of human/animal health is limited by scarce national budgets. Whilst some countries have advanced sensor networks, the world remains at risk from the health impacts of infectious diseases and environmental accidents. As seen by the recent A(H5N1), A(H1N1) and SARS outbreaks, a problem in one part of the world can be rapidly exported, leading to global hardship. The World Health Organization (WHO) estimates that in the future, between 2 to 7.4 million people could be at risk worldwide from a highly contageous avian flu virus that spreads rapidly through the international air travel network (WHO, 2005) . Pandemics of novel pathogens have the capacity to overwhelm healthcare systems, leading to widespread morbidity, mortality and socio-economic disruption (Cox et al., 2003) . Furthermore, outbreaks of livestock diseases, such as foot-and-mouth disease or equine influenza can have a devastating impact on industry, commerce and human health (Blake et al., 2003) . The challenge is to enhance vigilance and control the emergence of outbreaks. Whilst human analysis remains essential to spot complex relationships, automated analysis has a key role to play in filtering the vast volume of data in real time and highlighting unusual trends using reliable predictor indicators. BioCaster (http://born.nii.ac.jp) (Collier et al., 2008) is a Web 2.0 monitoring station for the early detection of infectious disease events. The system exploits a high-throughput semantic processing pipeline, converting unstructured news texts to structured records, alerting events based on time-series analysis and then sharing this information with users via geolocating maps (Fig. 1(a) ), graphs (Fig. 1(b )) and alerts. Underlying the system is a publicly available multilingual application ontology. Launched in 2006 (Collier et al., 2006) the BioCaster Ontology (BCO) has been downloaded by over 70 academic and industrial groups worldwide. This paper reports on a major upgrade to the system and the ontology -expanding the number of languages from 6 to 12, redefining key relations and extending coverage in the number of diseases from 70 to 336, including many veterinary diseases. Background As the world becomes more interconnected and urbanized and animal production becomes increasingly intensive, the speed with which epidemics spread becomes faster, adding to pressure on biomedical experts and governments to make quick decisions. Traditional validation methods such as field investigations or laboratory analysis are the mainstay of public health but can require days or weeks to issue reports. The World Wide Web with its economical and real time delivery of information represents a new modality in health surveillance (Wagner and Johnson, 2006) and has been shown to be an effective source by the World Health Organization (WHO) when Public Health Canada's GPHIN system detected the SARS outbreak in southern China from news reports during November 2002. The recent A(H1N1) 'swine flu' pandemic highlighted the trend towards agencies using unvalidated sources. The technological basis for such systems can be found in statistical classification approaches and light weight ontological reasoning. For example, Google Flu Trends (Ginsberg et al., 2009) is a system that depends almost entirely on automatic statistical classification of user queries; MedISys-PULS (Yangarber et al., 2008) , HealthMap (Freifeld et al., 2008) and BioCaster use a mixture of statistical and ontological classification; and GPHIN (Mawudeku and Blench, 2006) and Argus (Wilson, 2007) rely on a mixture of ontological classification and manual analysis. Compared to other similar systems BioCaster is characterized by its richly featured and publicly downloadable ontology and emphasizes critical evaluation of its text mining modules. Empirical results have included: topic classification, named entity recognition, formal concept analysis and event recognition. In the absence of a community gold standard, task performance was assessed on the best available 'silver' standard -the ProMED-mail network (Madoff and Woodall, 2005) , achieving F-score of 0.63 on 14 disease-country pairs over a 365-day period (Collier, 2010) . Despite initial skepticism within the public health community, health surveillance systems based on NLP-supported human analysis of media reports are becoming firmly established in Europe, North America and Japan as sources of health information available to governments and the public (Hartley et al., 2010) . Whilst there is no substitute for trained human analysts, automated filtering has helped experts save time by allowing them to sift quickly through massive volumes of media data. It has also enabled them to supplement traditional sources with a broader base of information. In comparison with other areas of biomedical NLP such as the clinical and genetics' domains, a relative lack of building block resources may have hindered the wider participation of NLP groups in public health applications. It is hoped that the provision of common resources like the BCO can help encourage further development and benchmarking. Method BioCaster performs analysis of over 9000 news articles per day using the NPACI Rocks cluster middleware (http://www.rockcsclusters.org) on a platform of 48 3.0GHz Xeon cores. Data is ingested 24/7 into a semantic processing pipeline in a short 1 hour cycle from over 1700 public domain RSS feeds such as Google news, the European Media Monitor and ProMED-mail. Since 2009, news has also being gathered under contract from a commercial news aggregation company, providing access to over 80,000 sources across the world's languages. The new 2010 version of BioCaster uses machine translation into English (eleven languages) to source news stories related to currently occurring infectious and environmental disease outbreaks in humans, animals and plants. Access to the site is freely available but login registration applies to some functions such as email alerts. Processing is totally automatic, but we have the potential within the login system to enable human moderated alerts which broadcast to Twitter and RSS. Below we describe in detail two key aspects of the system that have been significantly upgraded: the BCO and the event detection system. Ontology Aim The BioCaster Ontology aims: • To describe the terms and relations necessary to detect and risk assess public health events in the grey literature; • To bridge the gap between (multilingual) grey literature and existing standards in biomedicine; • To mediate integration of content across languages; • To be freely available. The central knowledge source for BioCaster is the multilingual ontology containing domain terms such as diseases, agents, symptoms, syndromes and species as well as domain sensitive relations such as a disease causing symptoms or an agent affecting particular host species. This allows the text mining system to have a basic understanding of the key concepts and relationships within the domain to fill in gaps not mentioned explicitly in the news reports. To the best of our knowledge the BCO is unique as an application ontology, providing freely available multilingual support to system developers interested in outbreak surveillance in the language of the open media. The BCO however has little to say outside of its application domain, e.g. in disease-gene interaction or for supporting automatic diagnosis. As discussed in Grey Cowell and Smith (2010), there are many other resources available that have the potential to support applications for infectious disease analysis including controlled vocabularies and ontologies such as the the Unified Medical Language System (UMLS) (Lindberg et al., 1993) , International Classification of Diseases (ICD-10) (WHO, 2004) , SNOMED CT (Stearns et al., 2001) , Medical Subject Headings (MeSH) (Lipscomb, 2000) and the Infectious Disease Ontology (IDO) (Grey Cowell and Smith, 2010) . In (Collier et al., 2006) we discussed how BCO compared to such ontologies so we will focus from now on the implication of the extensions. Scope The new version of the BCO now covers 12 languages including all the United Nation's official languages: Arabic (968 terms), English (4113), French (1281), Indonesian (1081), Japanese (2077), Korean (1176), Malaysian (1001), Russian (1187), Spanish (1171), Thai (1485), Vietnamese (1297) and Chinese (1142). The multilingual ontology can be used as a direct knowledge source in language-specific text mining modules, as an indexing resource for searching across concepts in various languages and as a dictionary for future translation modules. Currently news in all 12 languages is available via the Web portal but news in additional languages such as German, Italian and Dutch are being added using machine translation. Design Like EuroWordNet (Vossen, 1998) , on which it is loosely based, the BCO adopts a thesauruslike structure with synonym sets linking together terms across languages with similar meaning. Synonym sets are referred to using root terms. Root terms themselves are fully defined instances that provide bridges to external classification schemes and nomenclatures such as ICD10, MeSH, SNOMED CT and Wikipedia. The central backbone taxonomy is deliberately shallow and taken from the ISO's Suggested Upper Merged Ontology (Niles and Pease, 2001) . To maintain consistency and computability we kept a single inheritance structure throughout. 18 core domain concepts corresponding to named entities in the text mining system such as DISEASE and SYMP-TOM were the results of analysis using a formal theory (Guarino and Welty, 2000) . We have endeavoured to construct definitions for root terms along Aristotelean principles by specifying the difference to the parent. For example in the case of Eastern encephalitis virus: Eastern equine encephalitis virus is a species of virus that belongs to the genus Alphavirus of the family Togaviridae (order unassigned) of the group IV ((+)ssRNA) that possesses a positive single stranded RNA genome. It is the etiological agent of the eastern equine encephalitis. We are conscious though that terms used in the definitions still require more rigorous control to be considered useful for machine reasoning. To aid both human and machine analysis root terms are linked by a rich relational structure reflecting domain sensitive relations such as causes(virus,disease), has symptom(disease, symptom), has associated syndrome(disease, syndrome), has reservoir(virus, organism). In such a large undertaking, the order of work was critical. We proceeded by collecting a list of notifiable diseases from national health agencies and then grouped the diseases according to perceived relevance to the International Health Regulations 2005 (Lawrence and Gostin, 2004) . In this way we covered approximately 200 diseases, and then explored freely available resources and the biomedical literature to find academic and layman's terminology to describe their agents, affected hosts, vector species, symptoms, etc. We then expanded the coverage to less well known human diseases, zoonotic diseases, animal diseases and diseases caused by toxic substances such as sarin, hydrogen sulfide, sulfur dioxide and ethylene. At regular stages we checked and validated terms against those appearing in the news media. As we expanded the number of conditions to include veterinary diseases we found a major structural reorganization was needed to support animal symptoms. For example, a high temperature in humans would not be the same as one in bovids. This prompted us in the new version to group diseases and symptoms around major animal familes and related groups, e.g. high temperature (human) and high temperature (bovine). A second issue that we encountered was the need to restructure the hierarchy under Organi-cObject which was divided between MicroOrganism and Animal. The structure of the previous version meant that the former were doing double duty as infecting agents and the later were affected hosts. The MicroOrganism class contained bacterium, helminth, protozoan, fungus and virus, which then became the domain in a relation 'x causes y'. Expansion forced us to accomodate the fact that some animals such as worms and mites (e.g. scabies) also infect humans as well as animals. The result was a restructuring of the organic classes using the Linnean taxonomy as a guideline, although this is probably not free from errors (e.g. virus is typically not considered to be an organism). Event alerting system Figure 1 (c) shows a schematic of the modular design used by the BioCaster text mining system. Following on from machine translation and topic classification is named entity recognition and template recognition which we describe in more detail below. The final structured event frames include slot values normalized to ontology root terms for disease, pathogen (virus or bacterium), country and province. Additionally we also identify 15 aspects of public health events critical to risk assessment such as: spread across international borders, hospital worker infection, accidental or deliberate release, food contamination and vaccine contamination. Latitude and longitude of events down to the province level are found in two ways: using the Google API up to a limit of 15000 lookups per day, and then using lookup on the BCO taxonomy of 5000 country and province names derived from open sources such as Wikipedia. Each hour events are automatically alerted to a Web portal page by comparing daily aggregated event counts against historical norms (Collier, 2010) . Login users can also sign up to receive emails on specific topics. A topic would normally specify a disease or syndrome, a country or region and a specific risk condition. In order to extract knowledge from documents, BioCaster maintains a collection of rule patterns in a regular expression language that converts surface expressions into structured information. For example the surface phrase \"man exposes airline passengers to measles\" would be converted into the three templates \"species(human); disease(measles); international travel(true)\". Writing patterns to produce such templates can be very time consuming and so the BioCaster project has developed its own D3: :-name(disease){ list(@undiagnosed) words(,1) list(@disease) } S2: :-name(symptom) { list(@severity) list(@symptom)} CF1: contaminated food(\"true\") :-\"caused\" \"by\" list(@contaminate verbs past) list(@injested material) SP4: species(\"animal\") :-name(animal,A) words(,3) list(@cull verbs past) Table 1 : Examples of SRL rules for named entity and template recognition. Template rules contain a label, a head and a body, where the head specifies the template pattern to be output if the body expression matches. The body can contain word lists, literals, and wild cards. Various conditions can be placed on each of these such as orthographic matching. light weight rule language -called the Simple Rule Language (SRL) and a pattern building interface for maintaining the rule base (McCrae et al., 2009) . Both are freely available to the research community under an open source license. Currently BioCaster uses approximately 130 rules for entity recognition, 1000 word lists and 3200 template rules (of which half are for location recognition) to identify events of interest in English. Using SRL allows us to quickly adapt the system to newly emerging terminology such as the 11+ designations given to A(H1N1) during the first stages of the 2009 pandemic. The SRL rulebook for BioCaster can recognize a range of entities related to the task of disease surveillance such as bacteria, chemicals, diseases, countries, provinces, cities and major airports. Many of these classes are recognized using terms imported from the BCO. The rule book also contains specialised thesauri to recognize subclasses of entities such as locations of habitation, eateries and medical service centres. Verb lists are maintained for lexical classes such as detection, mutation, investigation, causation, contamination, culling, blaming, and spreading. Some examples of SRL rules for named entity recognition are shown in Table 1 and described below: Rule D3 in the rulebook tags phrases like 'mystery illness' or 'unknown killer bug' by matching on strings contained within two wordlists, @undiagnosed and @disease, separated by up to one word. Rule S2 allows severity indicators such as 'severe' or 'acute' to modify a list of known symptoms in order to identify symptom entities. Rule CF1 is an example of a template rule. If the body of the rule matches by picking out expressions such as 'was caused by tainted juice', this triggers the head to output an alert for contaminated food. Rule SP4 identifies the victim species as 'animal' in contexts like '250 geese were destroyed'. The rulebook also supports more complex inferences such as the home country of national public health organizations. Since BioCaster does not employ systematic manual checking of its reports, it uses a number of heuristic filters to increase specificity (the proportion of correctly identified negatives) for reports that appear on the public Web portal pages. For example, reports with no identified disease and country are rejected. Since these heuristics may reduce sensitivity they are not applied to news that appears on the user login portal pages. Results and Discussion Version 3 of the ontology represents a significant expansion in the coverage of diseases, symptoms and pathogens on version 2. Table 2 summarizes the number of root terms for diseases classified by animal familes. The thesaurus like structure of the BCO is compatible in many respects to the Simple Knowledge Organization System (SKOS) (Miles et al., 2005) . In order to extend exchange and re-use we have produced a SKOS version of the BCO which is available from the BCO site. We have also converted the BCO terms into 12 SRL rule books (1 for each language) for entity tagging. These too are freely available from the BCO site. As the ontology expands we will consider adopting a more detailed typing of diseases such as hasInfectingPart to indicate the organ affected Conclusion Multilingual resources specifically targeted at the task of global health surveillance have so far been very rare. We hope that the release of version 3 can be used to support a range of applications such as text classification, cross language search, machine translation, query expansion and so on. The BCO has been constructed to provide core vocabulary and knowledge support to the Bio-Caster project but it has also been influential in the construction of other public health ori-ented application ontologies such as the Syndromic Surveillance Ontology (Okhamatovskaia et al., 2009) . The BCO is freely available from http://code.google.com/p/biocaster-ontology/ under a Creative Commons license. Acknowledgements The authors greatly acknowledge the many coworkers who have provided comments and feedback on BioCaster. Funding support was provided in part by the Japan Science and Technology Agency under the PRESTO programme.",
         "14382934",
         "112c9ec3ec20b29e1faee91867fe4c0bebf3a770",
         "27",
         "https://aclanthology.org/C10-1025",
         "Coling 2010 Organizing Committee",
         "Beijing, China",
         "2010",
         "August",
         "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)",
         "Collier, Nigel  and\nMatsuda Goodwin, Reiko  and\nMcCrae, John  and\nDoan, Son  and\nKawazoe, Ai  and\nConway, Mike  and\nKawtrakul, Asanee  and\nTakeuchi, Koichi  and\nDien, Dinh",
         "An ontology-driven system for detecting global health events",
         "215--222",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "collier-etal-2010-ontology",
         null,
         null
        ],
        [
         "39",
         "R13-1058",
         "Wordnets are lexico-semantic resources essential in many NLP tasks. Princeton WordNet is the most widely known, and the most influential, among them. Wordnets for languages other than English tend to adopt unquestioningly WordNet's structure and its net of lexicalised concepts. We discuss a large wordnet constructed independently of WordNet, upon a model with a small yet significant difference. A mapping onto WordNet is under way; the large portions already linked open up a unique perspective on the comparison of similar but not fully compatible lexical resources. We also try to characterise numerically a wordnet's aptitude for NLP applications.",
         "Wordnets are lexico-semantic resources essential in many NLP tasks. Princeton WordNet is the most widely known, and the most influential, among them. Wordnets for languages other than English tend to adopt unquestioningly WordNet's structure and its net of lexicalised concepts. We discuss a large wordnet constructed independently of WordNet, upon a model with a small yet significant difference. A mapping onto WordNet is under way; the large portions already linked open up a unique perspective on the comparison of similar but not fully compatible lexical resources. We also try to characterise numerically a wordnet's aptitude for NLP applications. Introduction It is hard to imagine NLP without lexico-semantic resources. The Princeton WordNet (PWN) is a powerful case in point: we have come to rely on it even in \"hard-core\" statistical methods of processing English texts. Wordnets for other languages, which soon followed PWN, 1 have usually been built by the transfer-and-merge method: the structure of PWN is copied over to the target language, the lexical material is translated, and the inevitable differences in language typology and cultural background are a matter of post-processing. 2  The transfer-and-merge construction allows high compatibility between PWN and the target wordnet, so also between any wordnets built in the same way. Multi-lingual NLP work benefits from dependable interlingual relations -ensured if one uses a wordnet with PWN's structure. PWN's semantic relations are undoubtedly of general utility, but they do exhibit certain \"English bias\", and that -combined with the anglocentric network of concept underlying PWN's synsets -is a downside of the translation method of building a new wordnet. 3  The result need not be an accurate reflection of the lexico-semantic system of the target language. The translation method has another advantage: it is rather affordable, because PWN is now very complete and quite stable. To start the construction of a wordnet without looking to PWN may seem a little foolhardy, but it offers certain intriguing benefits. This paper looks at one of such independent projects, a wordnet for Polish. The plWordNet project aims to construct a large lexical resource (comparable in size to the largest existing wordnets, including PWN), based on few but precise principles and definitions. The goal is to achieve a faithful description of Polish while enabling compatibility with PWN (and by corollary with many wordnets), and yet avoid any semantic influences due to the transfer of the net of lexicalised concepts from PWN. 4 The work is semiautomatic and corpus-based. Linguists make final decisions, but supporting tools supply most of the raw material for those decisions, and naturally keep track of all aspects of the growing network. No appropriately large machine-tractable thesaurus of Polish was available to jump-start the project. The construction has been based predominantly on the exploration of large corpora, with some help from traditional dictionaries. This required precise guidelines for linguists to facilitate the consistency of decisions and definitions -focused on linguistic data analysis and well anchored in the linguistic tradition. All information was fed into a steadily growing wordnet. Today, plWordNet is large and mature enough to allow a wide-ranging observations. We analyse the consequences of the underlying wordnet model, the principles adopted, and the construction process. We take a varied perspective, including a multi-faceted comparison with PWN. 2 The structure of plWordNet 2.1 Constitutive relations \"A wordnet is a collection of synsets linked by semantic relations.\" This must be the most common quick take on wordnets in the literature. A synset is a set of synonyms which represent the same lexicalised concept, while synonyms are members of the same synset: this introduces a troubling circularity. An elaborate theory of synonymy could be a way of breaking the circle, but no such theory is operational enough in the sense of allowing precise guidelines for wordnet editors. This problem was discussed in (Derwojedowa et al., 2008; Piasecki et al., 2009; Maziarz et al., 2013) . Relations between synsets are often assumed to link concepts, and are fittingly described as conceptual relations. Their names, however, come up mainly in lexical semantics, where one considers hypernymy, meronymy etc. not between concepts but rather between words or lexical units (LUs). 5 Substitution tests usually proposed for synset relations refer to pairs of LUs (Vossen, 2002) . Relations between LUs are relatively rare in PWN and in wordnets based on it, but antonymy, for example, never holds between synsets. Neither concepts nor synsets occur directly in texts. LUs and their contexts of use do -and thus can be recognised, analysed and compared in corpora. This observation had led to a model of plWordNet different from that adopted by PWN: the basic building block is the LU, and semantic relations hold between LUs. A definition of a lexico-semantic relation includes a substitution test obligatorily applied by wordnet editors whenever a relation instance is added. 6  The synset is a secondary notion. Synsets certainly appear in plWordNet, but they are defined via LUs. The cornerstone of this definitional machinery is a set of lexico-semantic constitutive relations, which contains in particular hypernymy, hyponymy, holonymy and meronymy. A relation is considered constitutive if its instances are frequent enough and frequently shared by groups of LUs. 7 It is also important that constitutive relations be established in linguistics (so wordnet builders feel comfortable around them) and accepted in the wordnet tradition (so compatibility among wordnets is easy to accomplish). A synset is a group of LUs which share all constitutive relations; plWordNet software determines such groupings automatically. Thus, if relation R is noted as linking synsets S 1 and S 2 , it links every pair of LUs s 1 ∈ S 1 and s 2 ∈ S 2 . An instance of a synset relation is naturally interpreted as an abbreviation for a set of LU relation instances. It seems harder to recognise synonymy than LU pairs linked by constitutive relations. Relation instances are identified primarily via language data analysis (section 2.2). Avoiding the often troublesome synonymy is one of the facets of the minimal commitment principle which underlies the construction of plWordNet: make as few assumptions as possible. If no theory of meaning needs to be constantly invoked, and few intuitions about meaning variations are necessary, the construction process becomes \"agnostic\" about schools of linguistic thought. That is perhaps an opportunity: more applications are possible if fewer theoretical restrictions are imposed on a wordnet. The relation set in plWordNet (Maziarz et al., 2011a; Maziarz et al., 2011b; Maziarz et al., 2012) elaborates on relations in PWN, EuroWord-Net (Vossen, 2002) and GermaNet. 8 In addition to the expected (hyponymy, meronymy, antonymy, cause, instance for proper names, entailment -all adjusted to the reality of Polish), some relations account for the rich inflection and highly productive derivation of Polish. Assorted examples: 6 An instantiated test is automatically presented by the editor-supporting software. As a tiny example, the test «if X is a Y, then \"X\" is a hyponym of \"Y\"» can be used to determine that in PWN tiger 2 is a hyponym of big cat 1. 7 As an example, antonymy is seldom shared, while it is common for several LUs to share a hypernym. The construction process Wordnet construction is rather like writing a dictionary (Fellbaum, 1998; Broda et al., 2012b) . Lexicography distinguishes four phases: data collection, selection, analysis and presentation (Svensén, 2009) . In the plWordNet project, language technologies support all four phases. Professional linguists under the supervision of senior coordinators work with WordnetLoom, a Web application. This graph-based wordnet editor allows visual browsing and concurrent editing. Many semiautomatic tools are integrated into WordnetLoom. In the data collection phase, a large corpus is essential (Wynne, 2005) . A multi-source corpus with 1.8 billion tokens, the foundation of plWord-Net's systematic growth, supports the other phases of plWordNet's construction. The collected texts have been tagged by the morphological analyser Morfeusz (Woliński, 2006) and the TaKIPI tagger (Piasecki, 2007) . 10  In the data selection phase, the most frequent lemmas are chosen (plWordNet, 2012) and presented to the editors by WordnetLoom. The editors can also browse the plWordNet corpus using the Poliqarp interface (Janus and Przepiórkowski, 2005) . To avoid time-consuming queries on the corpus, the process employs a word-sense disambiguation algorithm (Broda et al., 2010) ; it selects up to 10 examples of word usage, representing different meanings. 11 Finally, editing is sup-9 MODIFIER is a syntagmatic relation. Its inclusion in plWordNet (rather like in Mel'čuk's Sense-Text Model) can add a lot of links, but we apply it in moderation. 10 The corpus consists of 250 million tokens in the ICS PAS Corpus (Przepiórkowski, 2004) ; 113m tokens of news items (Weiss, 2008) ; ≈80m tokens in a corpus made of Polish Wikipedia (Wikipedia, 2010) ; an annotated corpus KPWr with ≈0.5m tokens (Broda et al., 2012a) ; ≈60m tokens of shorthand notes from the Polish parliament; and ≈1.2 billion tokens collected from the Internet. 11 Usage examples, welcome by the editors, help them distinguish senses (Broda et al., 2012b) . ported by WordnetWeaver (Piasecki et al., 2009) , a system which suggests several places where best to link a given lemma in the lexico-semantic net. Its hints usually yield new distinguished senses. The corpus browser, usage examples and Word-netWeaver enable increasingly complex language processing: from simple queries in the plWord-Net corpus, through the presentation of a small list of disambiguated usage examples, to highly sophisticated lemma-placement suggestions. In the data analysis phase, the editors answer a few central questions: • whether a given lemma is correct in Polish (e.g., tagger mistakes are weeded out); Apart from primary sources and automated tools, the editors are encouraged to look up words and their descriptions in the available Polish dictionaries, thesauri, encyclopaedias, lexicons, and on the Web. At the end, the new lemma and all its LUs, or senses, are integrated with plWordNet and displayed in WordnetLoom. Intuition matters despite even the strictest definitions and tests, so one cannot expect two linguists to come up with the same wordnet structure. In corpus-building it is feasible to have two people edit the same portion and adjudicate the effect, but wordnet development is a more complicated matter. That is why we have a three-step procedure: (i) wordnet editing by a linguist, (ii) wordnet verification by a coordinator (a senior linguist), and (iii) wordnet revision, again by a linguist. Full verification would be too costly, so it is done on (large) samples of the editors' work. A coordinator corrects errors, adjust the wordnet editor's guidelines, 12 and initiates revision during which systematic errors are corrected and the wordnet undergoes synset-specific modification. 13  There also is a unique opportunity to verify the content of plWordNet meticulously: a mapping of its synsets onto PWN. That process sees every LU in plWordNet re-examined by a separate team of linguists. Section 4 explains in detail. The effects A wordnet ought to be large to be really useful. Its coverage matters a lot to potential applications. Intuitively, the higher the coverage, the more information can be acquired from the resource. The size of plWordNet approaches that of PWN, a first for a resource not built by the transfer method. A comparison may not be foolproof given the different language typologies and plWordNet's choice of the lexical unit as a basic element, but it is quite instructive nonetheless. Size in numbers Tables 1-2 present the statistics of the three largest manually constructed wordnets: Princeton Word-Net 3.1, plWordNet 2.0 and GermaNet. PWN outstrips plWordNet when it comes to the number of lemmas and lexical units (word-sense pairs). Table 2 gives the precise counts of nouns, verbs and adjectives in PWN and plWordNet. The latter has more verbs, but fewer nouns and adjectives. Lexical coverage The size of a wordnet can be contrasted with a frequency list from a large corpus. Such a measure of coverage sheds a light on the usability of a resource. A count was made of how many PWN lemmas appear in the text of English Polysemy Table 4 shows the statistics of polysemy. Average polysemy is calculated by dividing the count of LUs by the count of lemmas. The column 'poly.' lists average polysemy for polysemous lemmas, the column '+mono.' gives the polysemy statistics for polysemous and monosemous lemmas together, the last column presents the ratio of polysemous lemmas to all lemmas. Nouns and adjectives are more polysemous in plWordNet than in PWN, verbs -conversely. This ought to be considered in the light of the part-of-speech statistics in Table 2 and the measure of corpus coverage in Table 3 . There are more nouns and adjectives in PWN, and since both wordnets tend to absorb high-frequency lemmas first, the polysemy in PWN must be lower. The paradox can be explained thus: the larger a wordnet, the higher the number of monosemous lemmas it contains, because more frequent lemmas are more polysemous. On the other hand, there are more monose- Relation density Relation density comparison for PWN 3.1 and plWordNet 2.0 in Table 5 shows the average number of relations per synset. 16 The density is higher in plWordNet for nouns and verbs (+0.5 and +0.8 relation, respectively), lower for adjectives (-0.9). The total density is higher in plWordNet: on average, every other Polish synset has one synset relation instance more than PWN. The net is denser, a fact which can be explained like this: plWord-Net has a stricter definition of synonymy, so there are more smaller synsets and thus the system needs more differentiating relations (and having more relations creates a feedback loop with a magnifying effect). POS The mapping of plWordNet onto PWN, described in detail in section 4, makes it possible to collate synsets from both wordnets linked by interlingual synonymy. It is interesting to see how relation density looks for corresponding synsets. Calculations have been run for all domains selected for mapping and described in Section 4 -see Table 6 . For every plWordNet synset with interlingual synonymy, the count includes all relation instances to and from that synset, except obligatory inverse relations. The only outliers are the domains body and location: PWN has a higher density, even though Polish noun synsets have on average more relations than English noun synsets. Now, locations and body parts are special vocabulary with many instances of meronymy. In plWordNet, meronymy suffices to link a new LUs to the net. In PWN, the most welcome relation for nouns is hyponymy. For example, {dłoń 1, ręka 3} 'hand' is a meronym of {ręka 1} 'arm', while its English I-synonym {hand 1, manus 1, mitt 1, paw 2} 'the (prehensile) extremity of the superior limb' is not only a meronym of {arm 1} 'a human limb', but also a hyponym of {extremity 5} 'that part of a limb that is farthest from the torso'. Hyponymy is absent from plWordNet for synsets defined more naturally by part/whole semantics. 17 Hypernymy depth A comparison of the average hypernymy depth in plWordNet and in PWN concerned noun synsets linked via inter-lingual synonymy and presumably located at the same or a very close level in the taxonomy. Next, the number of their intra-lingual relations up and down has been checked. The average hypernymy depth up is longer in PWN (7.76 relation) than in plWordNet (5.71). This is expected in view of the fact that PWN has a complex hyponymy structure above unique beginners and many of top synsets map straight to SUMO categories. plWordNet is mainly linguistically oriented, so there are very few SUMO categories in the hyponymy hierarchy (see Table 7 ). The average hypernymy depth down is comparable: PWN 0.57, plWordNet 0.60. This is explained by the fact that the inter-lingual mapping was constructed bottom-up, thus at least half of the I-synonyms in both wordnets are leaves -the lowest nodes in the hierarchy. Linking differently structured wordnets A partial mapping of plWordNet onto PWN is ready (Rudnicka et al., 2012) . A hierarchically arranged set of inter-lingual relations (I-relations) and a unique mapping procedure have been defined. The set was inspired by equivalence relations in EuroWordNet (Vossen, 2002) and by intra-lingual relations in plWordNet (Maziarz et al., 2011a) . I-relations, complete with effective substitution tests, are considered in a strict order: I-synonymy, I-inter-register synonymy, 18 I-near-synonymy, I-hyponymy, I-hypernymy, Imeronymy, I-holonymy. The mapping procedure, working at the level of synsets, is based on a correspondence in meaning and position in the two wordnets' structures. There are three stages: recognize the sense of a source-language synset, find a target-language synset, and link the two synsets with one of the I-relations. Editors are supported by WordnetLoom (section 2.2) and by an automatic prompt system. They can also consult mono-and bilingual dictionaries. The mapping is systematically verified. For the majority of the inter-lingual links entered thus far, a coordinator examines the source and target synsets' LUs and the type of the I-relation. The coordinator reviews any questionable link in Word-netLoom and either repairs it immediately or consults the editor in order to reach a consensus. Besides the obvious advantage of building a bilingual wordnet, the mapping process enabled additional verification for plWordNet itself. The semantic domains selected for mapping were shared in such a way that one linguist constructed a particular plWordNet hypernymy branch and another linguist performed its mapping. This allowed re-editing the structure and content of plWordNet in case of mistakes. Linguists who did the mapping were encouraged to review critically the plWordNet side and introduce changes when they felt them necessary. The whole process was, naturally, regularly monitored by coordinators. Table 8 shows the number of instances of I-relations in plWordNet 2.0 and in Ger-maNet 8.0, another partially manually constructed and mapped wordnet. 19 I-synonymy, a primary relation in both wordnets has a comparable number of instances. It is the most frequent relation in GermaNet, while in plWordNet it has been overtaken by I-hyponymy. The latter statistic can be explained by profound differences in the struc- ture and content of plWordNet and PWN, discovered during mapping and discussed below. In Ger-maNet, I-hyponymy has quite few instances. On the other hand, the second largest relation in Ger-maNet is I-near synonymy. There are lexico-semantic and lexicogrammatical differences between English and Polish: lexical and cultural gaps as well as different structuring of information, differences in the degree of gender lexicalisation and the frequency of marked forms such as diminutive or augmentative. Another type of contrasts is to do with the concept of synonymy and synsets, due mainly to the existence of \"mixed\" PWN synsets made up of neutral and marked, feminine and masculine, singular and plural, mass and count, and even hypernym and hyponym forms in the same synset. Additionally, hypernymy in plWordNet is strictly conjunctive (the meaning of a hyponym must comprise the meaning components of all its hypernyms), while PWN also allows disjunctive hypernymy (easily found in the glosses describing the meaning contribution of a given synset). 20  There are also differences in the use of more than one intra-lingual relation to code the same conceptual dependencies, various granularity of meaning description, and dictionary content mismatches. Most, but not all, of these contrasts were accounted for by I-hyponymy: there were usually more lexically marked forms on the plWord-Net side, while the larger, more general synsets were usually on the PWN side. It is another factor contributing to high hyponymy count in the over- 20 Glosses for all synsets are a relatively late addition to PWN. We have only recently begun to introduce them into plWordNet. all statistics of relations. Semantic domains selected for the first stage of mapping included person, artefact, location, time, food and communication. On average, the coverage of PWN domains amounts to approximately 50% of the respective plWordNet domain coverage, except for location where it is about 25%. That is mainly because the mapping went from plWordNet to PWN, but also because of the percentages of proper-name synsets. Proper-name synsets are rare in plWordNet -it was a deliberate decision -while they have a considerable share in PWN domains such as person and location. The distribution of specific inter-lingual relations within the selected domains is as follows. For the most mapped domains -person and location -it mirrors the general distribution of I-relations (I-hyponymy slightly overtakes Isynonymy). For artefact and communication they are similar, while for food and time I-synonymy decidedly overtakes I-hyponymy. The high percentage of I-hyponymy in the person domain can be explained by the existence of many lexical and cultural gaps such as, for example, names of aristocratic titles or administrative functions, specific or even limited to one language community. All in all, the set of inter-lingual relations and the mapping procedure developed for the purpose of mapping plWordNet, and the strategies of handling different types of mapping dilemmas, appear perfectly usable in linking other wordnets. The Ihyponymy links are now a clear sign of gaps which can be repaired in the further stages of the development of the networks. Mapping plWordNet to PWN also opens up the possibility of establishing links to other wordnets already linked to PWN. Applications Freely available for any purpose on a licence identical to the PWN licence, plWordNet has already proven its value in at least 16 research applications and in many publication which cite it. The verb portion of plWordNet was used in semantic annotation in a corpus of referential gestures (Lis, 2012) and in a lexicon of semantic valency frames (Hajnicz, 2011; Hajnicz, 2012) . In the latter, plWordNet domains were also used in algorithms of verb classification. In (Maciołek, 2010; Maciołek and Dobrowolski, 2013) plWord-Net is used to extend a set of features for text mining from Web pages. In (Wróblewska et al., 2013) plWordNet was the basis for building a mapping between a lexicon and an ontology. Miłkowski (2010) included plWordNet in a set of dictionaries in his proofreading tool. There are applications of plWordNet in word-to-word similarity measures utilised in research on ontologies (Lula and Paliwoda-Pękosz, 2009) or in calculating text similarity (Siemiński, 2012) . As a semantic lexicon, plWordNet has been useful in text classification (Maciołek, 2010) , terminology extraction and clustering (Mykowiecka and Marciniak, 2012) , automated extraction of opinion attribute lexicons from product descriptions (Wawer and Gołuchowski, 2012) , named entity recognition, word-sense disambiguation, extraction of semantic relations (Gołuchowski and Przepiórkowski, 2012) , temporal information (Jarzębowski and Przepiórkowski, 2012) and anaphora resolution. Open Multilingual Wordnet (Bond, 2013) now includes plWordNet. It is referred to in other work on wordnets and semantic lexicons (Pedersen et al., 2009; Lindén and Carlson, 2010; Borin and Forsberg, 2010; Mititelu, 2012; Zafar et al., 2012; Šojat et al., 2012) . The resource has attracted about 450 registered individual and institutional users (registration upon download is not mandatory). The plWordNet Web page and Web service have had tens of thousands of visitors (hundreds of thousands of searches). The intended use includes 70 commercial applications, and 50 scientific and educational applications (at all levels: university, high school and primary school). The declared topics of scientific applications include semantic word similarity calculation, multilingual wordsense disambiguation, text classification, knowledge base for recommender systems and information retrieval (e.g., wordnet-based query expansion, user modelling, personalisation and user profile), Question Answering, Information Extraction systems (including automated event extraction), Text Mining, Opinion Mining, parsing disambiguation, ontology-based systems (ontology construction, integration and mapping to a lexicon), comparative research on languages and wordnets, chatbot systems (as a lexicon), text similarity in processing legal texts, anti-plagiarism, contrastive/comparative studies (e.g., \"Comparison of Polish, English and Swedish terms of motion and emotion, including analysis of metaphorical expressions.\" or \"Conducting a cross-linguistic study on phonesthemes.\"), Affect Analysis (multilingual systems), humour analysis, development of Polish Link Grammar, and plWordNet as an object of analysis of complex networks. Companies downloaded plWordNet for knowledge base management systems (e.g., automated conversion of text documents into a knowledge base), Business Intelligence, document similarity calculation, Polish website mapping and keyword tracking, online multilingual dictionary, search engine component development, translation inference support, analysis of public discourse, use as an additional bilingual dictionary in translation practice, Question Answering, text verification during editing, meta-data for publications, Polish dictionary and a basis for the development of bilingual dictionaries. In education, plWordNet was named in many student projects in NLP, lectures on NLP, a course on Text mining for sociologists. It has been also utilised in teaching linguistics and even as an illustration of linguistic notions in education in primary and secondary schools. Conclusions The paper has discussed the construction of plWordNet, a national wordnet not adapted from Princeton WordNet by the transfer-and-merge method. The present contents of plWordNet are comparable in size to \"The Mother of All Word-Nets\", as well as in lexical coverage, hypernymy depth and relation density. The treatment of synonymy and synsets is an alternative to the usual model adopted in PWN and numerous other wordnets: synset membership depends only on constitutive relations between lexical units. In its current mature stage of development, plWordNet is being mapped onto PWN. A unique mapping strategy aims at linking synsets based on the correspondence of meaning and position in the wordnet structure. The mapping process has revealed a number of contrasts between the two networks. They can be explained by lexicogrammatical differences between English and Polish, and the subtly different methodologies behind the construction of the two networks. Acknowledgment Work financed by the European Union, Project POIG.01.01.02-14-013/09 in the European Innovative Economy Programme.",
         "11533290",
         "5aade62511ba3756e91d2b1570bb41b0d9f0df00",
         "25",
         "https://aclanthology.org/R13-1058",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Maziarz, Marek  and\nPiasecki, Maciej  and\nRudnicka, Ewa  and\nSzpakowicz, Stan",
         "Beyond the Transfer-and-Merge {W}ordnet Construction: pl{W}ord{N}et and a Comparison with {W}ord{N}et",
         "443--452",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "maziarz-etal-2013-beyond",
         null,
         null
        ],
        [
         "40",
         "W05-0830",
         "Part-of-Speech patterns extracted from parallel corpora have been used to enhance a translation resource for statistical phrase-based machine translation.",
         "Part-of-Speech patterns extracted from parallel corpora have been used to enhance a translation resource for statistical phrase-based machine translation. Introduction The use of structural and syntactic information in language processing implementations in recent years has been producing contradictory results. Whereas language generation has benefited from syntax [Wu, 1997; Alshawi et al., 2000] , the performance of statistical phrase-based machine translation when relying solely on syntactic phrases has been reported to be poor [Koehn et al., 2003] . We carry out a set of experiments to explore whether heuristic learning of part-of-speech patterns from a parallel corpus can be used to enhance phrase-based translation resources. System The resources used for our experiments are as follows. The statistical machine translation GIZA++ toolkit was used to generate a bilingual translation table from the French-English parallel and sentence-aligned Europarl corpus. Additionally, a phrase table generated from the Europarl French-English corpus, and a training test set of 2000 French and English sentences that were made available on the webpage of the ACL 2005 work-shop 1 were also used. Syntactic tagging was realized by the TreeTagger, which is a probabilistic part-of-speech tagger and lemmatizer. The decoder used to produce machine translations was Pharaoh, version 1.2.3. We used GIZA++ to generate a translation table from the parallel corpus. The table produced consisted of individual words and phrases, followed by their corresponding translation and a unique probability value. Specifically, every line of the said table consisted of a French entry (in the form of one or more tokens), followed by an English entry (in the form of one or more tokens), followed by P(f|e), which is the probability P of translation to the French entry f given the English entry e. We added the GIZA++-generated table to the phrasebased translation table downloaded from the workshop webpage. During this merging of translation tables, no word or phrase was omitted, replaced or altered. We chose to combine the two aforementioned translation tables in order to achieve better coverage. We called the resulting merged translation table lexical phrase table. In order to utilize the syntactic information stemming from our resources, we used the Tree-Tagger to tag both the parallel corpus and the lexical phrase table. The probability values included in the lexical phrase table were not tagged. The TreeTagger uses a slightly modified version of the Penn Treebank tagset, different for each language. In order to achieve tag-uniformity, we performed the following dual tag-smoothing operation. Firstly, we changed the French tags into their English equivalents, i.e. NOM (noun -French) became NN (noun -English). Secondly, we simplified the tags, so that they reflected nothing more than general part-of-speech information. For example, tags denoting predicate-argument structures, whmovement, passive voice, inflectional variation, and so on, were simplified. For example, NNS (noun -plural) became NN (noun). Once our resources were uniformly tagged, we used them to extract part-of-speech correspondences between the two languages. Specifically, we extracted a sentence-aligned parallel corpus of French and English part-of-speech patterns from the tagged Europarl parallel corpus. We called this corpus of parallel and corresponding part-ofspeech patterns pos-corpus. The format of the poscorpus remained identical to the format of the original parallel corpus, with the sole difference that individual words were replaced by their corresponding part-of-speech tag. Similarly, we extracted a translation table of part-of-speech patterns from the tagged lexical phrase table. We called this part-of-speech translation table pos-table. The pos-table had exactly the same format as the lexical phrase table, with the unique difference that individual words were replaced by their corresponding part-of-speech tag. The translation probability values included in the lexical phrase table were copied onto the pos-table intact. Each of the part-of-speech patterns contained in the pos-corpus was matched against the part-ofspeech patterns contained in the pos-table. Matching was realized similarly to conventional left-toright string matching operations. Matching was considered to be successful not simply when a part-of-speech pattern was found to be contained in, or part of a longer pattern, but when patterns were found to be absolutely identical. When a perfect match was found, the translation probability value of the specific pattern in the pos-table was increased to the maximum value of 1. If the score were already 1, it remained unchanged. When there were no matches, values remained unchanged. We chose to match identical part-ofspeech patterns, and not to accept partial pattern matches, because the latter would require a revision of our probability recomputation method. This point is discussed in section 3 of this paper. Once all matching was complete, the newly enhanced pos-table, which now contained translation probability scores reflecting the syntactic features of the relevant languages, was used to update the original lexical phrase table. This update consisted in matching each and every part-of-speech pattern with its original lexical phrase, and replacing the initial translation probability score with the values contained in the pos-table. The identification of the original lexical phrases that generated each and every part-of-speech pattern was facilitated by the use of pattern-identifiers (pos-ids) and phraseidentifiers (phrase-ids), which were introduced at a very early stage in the process for that purpose. The resulting translation phrase table contained exactly the same entries as the lexical phrase table, but had different probability scores assigned to some of these entries, in line with the parallel partof-speech co-occurrences and correspondences found in the Europarl corpus. We called this table enhanced phrase table. Table 1 illustrates the process described above with the example of a phrase, the part-of-speech analysis of which has been used to increase its original translation probability value from 0.333333 to 1. 1 : Extracting and matching a part-ofspeech pattern to increase translation probability. We used the Pharaoh decoder firstly with our lexical phrase table, and secondly with our enhanced phrase table in order to generate statistical machine translations of source and target language variations of the French and English training test set. We measured performance using the BLEU score [Papineri et al., 2001] , which estimates the accuracy of translation output with respect to a reference translation. For both source-target language combinations, the use of the lexical phrase table received a slightly lower score than the score achieved when using the enhanced phrase table. The difference between these two approaches is not significant (p-value > 0.05). The results of our experiments are displayed in Table 2 Discussion The motivation behind this investigation has been to test whether syntactic or structural language aspects can be reflected or represented in the resources used in statistical phrase-based machine translation. We adopted a line of investigation that concentrates on the correspondence of part-of-speech patterns between French and English. We measured the usability of syntactic structures for statistical phrase-based machine translation by comparing translation performance when a standard phrase table was used, and when a syntactically enhanced phrase table was used. Both approaches scored very similarly. This similarity in the performance is justified by the following three factors. Firstly, the difference between the two translation resources, namely the lexical phrase table and the enhanced phrase table, does not relate to their entries, and thus their coverage, but to a simple alteration of the translation probability values of some of their entries. The coverage of these resources is exactly identical. Secondly, a closer examination of the translation probability value alterations that took place in order to reflect part-of-speech correspondences reveals that the proportion of the entries of the phrase table that were matched syntactically to phrases from the parallel corpus, and thus underwent a modification in their translation probability score, was very low (less than 1%). The reason behind this is the fact that the part-of-speech patterns produced by the parallel corpus were long strings in their vast majority, while the part-ofspeech patterns found in the phrase table were significantly shorter strings. The inclusion of phrases longer than three words in translation resources has been avoided, as it has been shown not to have a strong impact on translation performance [Koehn et al., 2003] . Thirdly, the above described translation probability value modifications were not parameterized, but consisted in a straightforward increase of the translation probability to its maximum value. It remains to be seen how these probability value alterations can be expanded to a type of probability value 'reweighing', in line with specific parameters, such as the size of the resources involved, the frequency of part-of-speech patterns in the resources, the length of part-of-speech patterns, as well as the syntactic classification of the members of part-of-speech patterns. If one is to compare the impact that such parameters have had upon the performance of automatic information summarisation [Mani, 2001] and retrieval technology [Belew, 2000] , it may be worth experimenting with such parameter tuning when refining machine translation resources. A note should be made to the choice of tagger for our experiments. A possible risk when attempting any syntactic examination of a large set of data may stem from the overriding role that syntax often assumes over semantics. Statistical phrasebased machine translation has been faced with instances of this phenomenon, often disguised as linguistic idiosyncrasies. This phenomenon accounts for such instances as when nouns appear in pronominal positions, or as adverbial modifiers. On these occasions, and in order for the syntactic examination to be precise, words would have to be defined on the basis of their syntactic distribution rather than their semantic function. The TreeTagger abides by this convention, which is one of the main reasons why we chose it over a plethora of other freely available taggers, the remaining reasons being its high speed and low error rate. In addition, it should be clarified that there is no statistical, linguistic, or other reason why we chose to adopt the English version of the Penn TreeBank tagset over the French, as they are both equally conclusive and transparent. The overall driving force behind our investigation has been to test whether part-of-speech structures can be of assistance to the enhancement of translation resources for statistical phrase-based machine translation. We view our use of part-ofspeech patterns as a natural extension to the introduction of structural elements to statistical machine translation by Wang [1998] and Och et al. [1999] . Our empirical results suggest that the use of partof-speech pattern correspondences to enhance existing translation resources does not damage machine translation performance. What remains to be investigated is how this approach can be optimized, and how it would respond to known statistical machine translation issues, such as mapping nested structures, or the handling of 'unorthodox' language pairs, i.e. agglutinative-fusion languages. Conclusion Syntactic and structural language information contained in a bilingual parallel corpus has been extracted and used to refine the translation probability values of a translation phrase table, using simple heuristics. The usability of the said translation table in statistical phrase-based machine translation has been tested in the shared task of the second track of the ACL 2005 Workshop on Building and Using Parallel Corpora. Findings suggest that using part-of-speech information to alter translation probabilities has had no significant effect upon translation performance. Further investigation is required to reveal how our approach can be optimized in order to produce significant performance improvement.",
         "14118934",
         "f0025f479e9feba22c88d17cad8c06fd47ba9a2b",
         "7",
         "https://aclanthology.org/W05-0830",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Lioma, Christina  and\nOunis, Iadh",
         "Deploying Part-of-Speech Patterns to Enhance Statistical Phrase-Based Machine Translation Resources",
         "163--166",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "lioma-ounis-2005-deploying",
         null,
         null
        ],
        [
         "41",
         "2009.mtsummit-posters.21",
         "The contribution discusses variants of architectures of hybrid MT systems. The three main types of architectures are: coupling of systems (serial or parallel), architecture adaptations (integrating novel components into SMT or RMT architectures, either by pre/post-editing, or by system core modifications), and genuine hybrid systems, combining components of different paradigms. The interest is to investigate which resources are required for which types of systems, and to which extent the proposals contribute to an overall increase in MT quality.",
         "The contribution discusses variants of architectures of hybrid MT systems. The three main types of architectures are: coupling of systems (serial or parallel), architecture adaptations (integrating novel components into SMT or RMT architectures, either by pre/post-editing, or by system core modifications), and genuine hybrid systems, combining components of different paradigms. The interest is to investigate which resources are required for which types of systems, and to which extent the proposals contribute to an overall increase in MT quality. Introduction In recent years, significant research in Machine Translation has been carried out, mainly in the area of data-driven MT (example-based and statistical (SMT)), as opposed to knowledge-driven approaches (rule-based (RMT), knowledge-based). Recent evaluations (Callison-Burch et al. 2009 ) show that  both types of systems reach comparable translation quality, but:  the level of output acceptance (in terms of understandability) in the best language directions is at about 50%. So the state of the art in MT is far from acceptability by human readers, which limits the success of the MT technology significantly. Error analysis (Chen et al., 2007 , Thurmair 2005) shows that the errors made by the different system types are complementary.  RMT systems have weaknesses in lexical selection in transfer, and lack robustness in case of analysis failures sentences. However they translate more accurately by trying to represent every piece of the input.  SMT systems are more robust and always produce output. They read more fluent, due to the use of Language Models, and are better in lexical selection. However, they have difficulties to cope with phenomena which require linguistic knowledge, like morphology, syntactic functions, and word order. Also, they lose adequacy due to missing or spurious translations (Vilar et al. 2006 ). Systems which try to profit from the respective other approach, and avoid mistakes for which solutions already exist (albeit in another MT paradigm) must therefore be hybrid solutions, combining knowledge-driven and data-driven elements. The purpose of this paper is to discuss different architectures of hybrid systems which have been proposed recently. The interest is to discuss the way how the hybrid systems overcome the restrictions of their respective paradigms, how they contribute to improve MT quality (in terms of fluency and adequacy), which requirements they have for language resources and performance, and how they can be adapted to new domains. Chapter 2 will discuss systems which couple different systems (RMT and/or) SMT, either in a serial (PSE) or in a parallel way; the systems themselves are not modified. Chapter 3 considers systems which use either the SMT or the RMT paradigm as basic architecture, and extend it by either knowledge-driven or data-driven components. Chapter 4 presents approaches which have hybrid architectures, and combine components of RMT and SMT systems into novel architectures. Coupling Coupling means that two or more existing systems are used to produce improved MT output. Coupling can either be done in a serial way, the most researched approach being statistical post-editing (SPE) of a rule-based system. Or it can be done in a parallel way, whereby the best translation is selected/produced from the output of several systems. First systems using small domains (Simard et al. 2007 , data from Canadian Job Bank) showed that even with relatively small training data of several thousand sentences, significant improvements of the MT output can be achieved, and a RMT+SPE component outperforms pure SMT systems in cases where only limited data are available. Serial Coupling Experiments have continued since then, on a broader data base, resulting in the following picture: a. Combinations of RMT+SPE systems are highly competitive in MT quality (cf. Schwenk et al., 2009) . The output tends to be grammatical, and the main effect of the combination is an increase in lexical selection quality (Dugast et al. 2007) , one of the weak points of pure RMT systems. b. However, care must be taken to avoid the introduction of errors by the SMT postprocessor. Such errors are: the syntactic structure of the output can be confused by the PSE component (Ehara 2007) ; accuracy drops as some parts of the translation are omitted, and special care needs to be taken to keep e.g. Named Entities in the output (Dugast et al. 2009) . c. To avoid such deteriorations, (Federmann et al. 2009 ) use a RMT system's syntactic structure, and only try local alternatives (using POS information). This helps for the lexical selection problem of the RMT systems, but less for the parse-failure problem. PSE type systems require bilingual training data, to be able to align RMT output to good output. Parallel Coupling This coupling employs several MT systems in parallel, and uses some mechanism to select or produce the best output from the result set. Two main paradigms are followed in these approaches: The first approach identifies the best translations from a list of n-best translations (Hildebrand/Vogel 2008) . They search for the best n-grams in all output hypotheses available, and then select the best hypothesis from the candidate list. They report an improvement of 2-3 BLEU compared to the best single system, as the resulting text can integrate sentences from different MT system outputs. The second approach does not work on whole sentences but on smaller segments (phrases, words). It uses confusion networks, and generates an output sentence on the basis of the available MT outputs. In its first variant, a skeleton is selected as a basis, and for each position of the skeleton the best translation alternative is identified and composed to the overall output sentence. Skeletons can be selected on sentence level (cf. Rosti et al. 2007) but also on phrase level (Heafield et al. 2009) ; the choice of the best skeleton is critical as it determines the structure / word order of the target sentence. Although most MT output stems from SMT systems, RMT output seems to add interesting hypotheses (Leusch et al. 2009) , and is sometimes used itself as the skeleton (Chen et al. 2009) . To overcome the risk of skeleton selection, techniques have been applied to build confusion network such as to let every hypothesis be the skeleton, and calculate the overall best solution; this has been done in the context of consensus translation (Matusov et al., 2006) . The results of parallel coupling seem to improve BLEU by 2-3 points; however, the 2009 MT workshop results seem to indicate that system combinations can perform as well as the best individual systems but not significantly better (Callison-Burch et al. 2009) . On top, a parallel system approach seems to be difficult to be used in practical applications; mainly for reasons of computational resources, and availability of MT systems. In praxi, at most two systems would be able to run in parallel; and the reduced number of output candidates would lead to a loss in efficiency for the decision process. Architecture Extensions While coupling means that the architecture of the participating systems is not changed, by extension we mean that the system architecture basically follows the RMT or SMT paradigm but is modified by including resources of the respective other approach. Modifications can occur as pre-editing (i.e. the system data are pre-processed), or core modification (e.g. phrase tables are extended, dictionaries are enlarged etc. by the respective other approach). RMT Extensions Approaches to improve rule-based systems with data-driven procedures focus on two problems:  Pre-editing is tried, both on the dictionary side, by running Term-Extraction tools, and on the grammar side, by automatically extracting grammar rules from corpora.  Modification of the system core is attempted, both by adding probability information to the analysis / parsing process, and by manipulating the transfer selection process. Pre-Editing Pre-Editing refers to the preparation of the language resources for RMT. Main language resources, dictionaries and grammar rules, can be set up using data-driven technology. Learning of dictionary entries Pre-Editing in rule-based systems means to apply data-driven techniques for terminology extraction from corpora, either on a monolingual basis (to find missing entries in the system's dictionaries), or from bilingual corpora, to find translation candidates, and to load them into the system dictionary. Such approaches are already in use in RMT systems. The challenges are:  recognition of multiword terms: Most of the semantically meaningful words are multiword terms (like ‚nuclear power plant'), having an internal linguistic structure.  linguistic annotation of the recognised terms. Terms must be brought into correct citation form (i.e. lemmatised), and annotated with (POS etc.) Approaches are described in (Dugast et al. 2009 , Eisele et al., 2008) . Results reported show that the MT quality improves moderately, depending on the amount of reductions of the out-of-vocabulary words, which in turn depends on the size and coverage of the already existing dictionary. The approach helps to fill dictionary gaps, and to adapt to new domains. However, in MT systems with already large dictionaries 1 the problem of lexical selection aggravates, as the amount of translations between which to select increases. This problem turns out to be much more difficult to solve than the problem of dictionary gaps. Learning of rules in RMT Research on learning grammar rules by data-driven techniques does not seem to have improved MT output quality significantly. The challenge for learning grammar rules seems to be that very many rule candidates are identified, even for small corpora, and that it is difficult to select the lowfrequent ‚good' rules from noise produced by the extraction technique. Existing RMT already use large grammars covering a lot of specific linguistic phenomena. As with dictionaries, the main problem is less that some structures are not covered but much more that the grammar rules interact and lead to problems of combinatorics and unexpected side-effects which require massive pruning and often led to parse failures. RMT core system modifications Modifications of the system core of RMT systems have been tried in several respects. The option to use probabilistic information in parsing has already been implemented in several existing RMT systems. Transfer Current hybrid approaches focus more on translation selection in the transfer phase, which is one of the weaknesses of RMT systems, esp. if dictionaries grow. Traditional approaches to RMT transfer selection rely on two techniques:  Assignment of subject area codes to translations; if a text belongs to a given subject area (which can be automatically detected, cf. Thurmair 2006), the respective translation is activated. However, even in specific domains, general readings of the terms in question are also found, so that this method is not reliable.  Tests and actions on certain contextual / structural properties (like: presence of direct object, certain prepositions, passive voice etc.), which trigger a specific translation. However, often such conditions cannot be reliably stated for lexical selection, esp. if the number of alternative translation grows; in addition, such tests rely on correct parses of the input sentence which cannot be guaranteed. Therefore, additional and robust means for lexical selection need to be developed. An obvious means is to use the more frequently used translation of a given term as default. But this technique is not sensitive to the specific context in which a term must be translated, and mostly returns the default. A second option is to use contextual disambiguation in the lexical selection process. Relevant clusters of (source language) contextual terms for a given candidate translation are built at training time from a corpus; at runtime these contexts are matched against the context of the text to be translated, and the best translation is selected. This technique, (cf. Thurmair 2006) , requires broadening the analysis scope of the system (from sentence-based to paragraph-based contexts); it achieves very good disambiguation results for the terms it was built for. Improvements of accuracy are also reported by (Kim et al., 2002) ; they use a smaller contextual window and follow a (Probabilistic) Latent Semantic Analysis approach. As a result, core modifications in RMT can improve the transfer selection process significantly; however they are less successful in case of robustness / parse failures. SMT Extensions Like RMT systems, SMT systems have also been extended to improve translation quality. Again,  Pre-editing is tried to prepare the data; the most important steps are morphology, POSinformation / syntactic information, and word reordering.  System core modifications are tried as well, by adding RMT information to the phrase tables, and by using factored translation. Pre-editing Morphology: Morphology has been researched rather extensively, mainly in languages with rich morphological schemas. Lemmatisation and POS tagging was used both on the source side (e.g. de Gispert et al., 2006) and on the target side ( Vandeghinste et al. 2006 ); the aim is to reduce data sparseness using lemma-based language models instead of textform-based ones. It seems to improve results for smaller corpora. Also, it seems that both textform and lemma based analysis should be done, as surface information has also shown to be beneficial (Koehn/Hoang 2007) . Factored translation (cf. below) is able to work on both levels simultaneously. Another research area in morphology is compounding (of English) / decompounding (of German words), to parallelise alignment (Stymne et al. 2008 , Popović et al. 2006) . Moreover, in languages with agglutinative behaviour, like Turkish (Hakkani-Tür et al. 2004), Hungarian or Arabic (Habash 2007) , preprocessing is required to split complex word strings (including pronouns, case markers etc.) into meaningful parts to be able to align them. Syntax: Syntactic preprocessing id tried e.g. in (Hannemann et al. 2009 ); the idea is to parse source and target side of a corpus, and only let syntactically well-formed phrases enter the phrase table. Both corpora are parsed, matching subtrees (mainly on NP level) are identified and aligned in the phrase table. The parsed phrases are still a minority on the phrase table but can help improving the MT output, in particular for local reorderings. Reordering: Reordering is a major challenge for SMT systems, not just because languages have different word and constituent order (SVO vs. SOV etc.) but also because the constituent order is meaning-bearing (e.g. case marking in English). While standard phrase-based models can handle local reorderings (e.g. noun-adjective position) to some extent, longer distance reordering requires different means. Proposals have been made to extend the input word sequence into a lattice containing different reorderings of the input words (based e.g. on POS information). Distortion rules can be set up manually or automatically, for contiguous and discontinuous POS sequences (Niehues/Kolss 2009), by matching them on source and target side of the training corpus. The input lattice contains the re-spective distorted strings, with weights on the probability of the distortion. Al alternative approach is proposed e.g. in (Bangalore et al. 2007) ; they do not use position at all, and try a global alignment in a kind of sentencebased bag-of-words strategy. In decoding, they create all possible permutations allowed by the Language Model (in a given window). However, apart from practical problems (window size), as all source language information is missing, results are not too promising; in addition, multiple occurrences of words in the target (‚the') need to be handled. (Birch et al. 2009 ) even claim that reordering problems determine the selection of the translation models: Long term reordering is better handled by hierarchical models (Hiero) while for short and medium reorderings, phrase-based models show better results. This remains to be researched. SMT core system modifications Three approaches can be found to incorporate RMT resources into an SMT architecture: Extension of the Phrase Table, rule-based control of the Language-Model-based generation, and factored translation. Importing RMT resources into the phrase table It was proposed (e.g. by Eisele et al. 2008) to run RMT systems in addition to SMT systems, and enrich the SMT phrase tables by terms and phrases produced by RMT systems. This approach makes use of the knowledge coded in the bilingual dictionaries of the RMT systems. Results show that the coverage of the system can indeed be increased, esp. in cases of texts from different domains; however, as the SMT decoder runs last, the effect is that the output can be less grammatical than the one of the original RMT. The proposal reacts on the data sparseness problem of the SMT training; it does not react on the output grammaticality problem. Improving decoding using target grammars First rather dramatic improvements had been reported by (Charniak et al. 2003) where the number of grammatical translations was increased in tests by 45%. Other results were less encouraging (e.g. Och et al., 2003) but this may have been due to the selection of an problematic evaluation metric. In recent times, using syntax in decoding is a major topic of research. Several proposals exist how to learn grammar and transfer rules from bilingual corpora. (Lavoie et al. 2002 , Hannemann et al. 2008 ) identify structural contexts for translation selection from bitexts. Melamed 2004 adapts parsing to allow for multiple input strings (multitrees). Hierarchical translation (Chiang 2007 ) uses synchronous context-free grammars in decoding: Different grammar and parsing alternatives are given e.g. in (Zollmann /Venugopal 2006 , Galley et al. 2004 ). An opensource toolset for target langauge parsing, Joshua, has recently been presented (Li et al., 2009) . Including syntax into the decoding process, esp. in the context of hierarchical translation, is a promising approach to boost the grammaticality of the MT output. Factored Translation While using structural information for decoding attracts increasing interest, Factored Translation (cf. Koehn/Hoang 2007) aims at enriching systems 'bottom-up', by providing more information at word level. It treats words not just as simple textforms but as vectors of features, such features being the lemma, the POS, morphology, and others. The approach decomposes phrase translations into a sequence of mapping steps, with translation steps operating on phrase level, and generation steps on word level. Models are combined in a log-linear fashion. Several papers (e.g. Stymne et al. 2008) show that phenomena like NP-agreement and compounding can be handled efficiently within a factored translation framework. As a result, treating words as feature bundles in factored translation, and using structural information for both source-to-target mapping as well as target decoding, allows significant quality improvements for systems combining these factors. They would use both knowledge-driven (dictionaries and grammars) and data-driven (phrase tables, language models) information. However, they rely on the availability of (possibly even linguistically pre-processed) bilingual corpora. This fact may reduce their applicability. Genuine hybrid architectures do not just use addons to their system architecture but combine whole system components of the respective approaches into novel systems. They use three basic components: identification of source language ‚chunks' (words, phrases or equivalents thereof), transformation of such chunks into the target language by means of a bilingual resource, and generation of a target language sentence. Several proposals have been made how such systems could look like. Rule-based analysis, bilingual dictionary, target language model Such an approach has been investigated in the METIS projects (Vandeghinste et al. 2006) . Analysis is done using available NL tools (lemmatisers, taggers, chunkers); transfer is based on existing dictionaries (consisting basically of lemma and POS in source and target language, including single and multiword terms), and generation uses a language model (based on a tokenized and tagged English corpus (BCN)). To ensure that the LM based generation produces grammatical sentences, several approaches have been investigated for different languages; e.g.  in Greek-to-English (Tambouratsis et al., 2005 , Markantonatou et al., 2006) , a pattern matcher is applied to search for the best matching patterns containing the respective lexical head, and number + POS of modifiers; the selected pattern is then analysed recursively down the structure (sentence levelchunk levelunit level) for the best matching sub-patterns. The best patterns undergoes language.model-based target search.  in German-to-English, a 'structural transfer' type of mapping component is implemented to prepare good LM-based search Other languages explored in METIS implement other solutions, like bag-of-words. Evaluation of the technology show that results are similar to basic SMT systems but worse than a complete (rule-based) system like SYSTRAN in all language combinations (Vandeghinste et al. 2008 ). However this is not surprising comparing the effort invested in the two systems. However, it needs to be seen if the proposed architecture has the poten-tial to produce superior MT quality once the effort is increased. Data driven analysis and generation, bilingual dictionary Instead of rule-based analysis, an alternative data-driven approach has been proposed by (Carbonell et al. 2006) . The required resources are: a (full-form) bilingual dictionary, and a n-gram indexed target language corpus. In analysis, an ngram window is moved over the sentence, and all words in the window are translated using the bilingual dictionary; based on these translations, the target language corpus is searched for the closest n-gram (ideally containing all words of the source, and no additional ones). The result is a lattice of ngram translations. Of these, the segments with the strongest left and right overlaps, and the highest density of terms, are selected by the decoder. While this approach also circumvents the problem of the availability of bilingual resources and uses a dictionary as main translation resource, it does not attempt any 'phrase' analysis of the input (while METIS uses phrases produced by linguistic chunkers), and any knowledge-based analysis or generation resource. It needs to be seen how grammaticality of the output can be ensured (e.g. proper morphology, word order problems), and how accuracy can be produced, as the technique seems to ignore out-of-dictionary words (like proper names) and to insert spurious translations in the target language n-grams. Domain Adaptation A special issue to be considered is domain adaptation. All kinds of MT systems must cope with the fact that they will be used not only in the domain for which they had been developed but also for other domains. While RMT systems support adaptation by dictionary import and coding, which in turn can be based on domain corpus collections, the situation is less obvious for SMT-based systems, and a significant drop of quality (up to 10 BLEU) had been observed. The most promising approach for SMT systems seems to be to use large out-of-domain training data (e.g. Europarl), and with a small in-domain training set, build different resources for both kinds of data. While the phrase tables of the out-ofdomain data moderately improve the in-domain ones (by closing gaps in the translations), the most efficient approach seems to be to run a target Language Model trained only with in-domain data (Koehn/Schroeder 2007) . Experiments have also been made for integrating customer terminology (a bilingual list of terms) into an SMT system (Itagaki/Aikawa 2008). Artificial contexts are created to identify how the phrase tables would translate a given source language term; in a post-processing phase, the phrase table translations of the terms are replaced by the target expressions of the term list. While this seems to be a significant and error-prone effort, options to manipulate the phrase tables directly (a shown above) could be more promising. Conclusion The selection of the ‚best' architecture for a practical MT system depends on three basic factors:  the intended use case, e.g. the translation domain(s). A single-domain application with enough bilingual training data is the exception rather than the rule. SMT approaches to lowresource languages are presented e.g. in (Nießen/Ney 2004)  the translation quality which can be achieved, both for the domain in which the system was trained, and other domains in which the systems are supposed to be used  the availability of resources and data, both on monolingual and bilingual level. For the determination of the MT quality, most of the presented systems claim to outperform some baseline system; however, the results are difficult to compare, also due to the fact that the used metrics often are not adequate as some of them do not treat different system types equally (Dugast et al. 2009 ). In the present context, where several types of systems need to be compared, this is a drawback. Recently, several approaches for sentencebased metrics have been proposed (an overview is given in Callison-Burch et al. 2009) ; however there is no consolidated picture, and different metrics seem to perform best for different language directions. Much more relevant, from a practical point of view, is the availability of resources. For many language directions and many domains, sufficient amounts of bilingual data still do not exist, or cannot be accessed. In this case, architectures which rely on monolingual data and use bilingual dictionaries would have to be preferred. So the selection of the best alternative would depend on quality criteria, and on the availability of (training) data. However, whichever approach is taken, there is still a long way to go before machine translation systems reach acceptable quality.",
         "17162943",
         "156a816f1e0df790ca21f19df108ae7e8f1cc73e",
         "41",
         "https://aclanthology.org/2009.mtsummit-posters.21",
         null,
         "Ottawa, Canada",
         "2009",
         "August 26-30",
         "Proceedings of Machine Translation Summit XII: Posters",
         "Thurmair, Gregor",
         "Comparing different architectures of hybrid Machine Translation systems",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "thurmair-2009-comparing",
         null,
         null
        ],
        [
         "42",
         "P07-1084",
         "Current research in text mining favours the quantity of texts over their quality. But for bilingual terminology mining, and for many language pairs, large comparable corpora are not available. More importantly, as terms are defined vis-à-vis a specific domain with a restricted register, it is expected that the quality rather than the quantity of the corpus matters more in terminology mining. Our hypothesis, therefore, is that the quality of the corpus is more important than the quantity and ensures the quality of the acquired terminological resources. We show how important the type of discourse is as a characteristic of the comparable corpus.",
         "Current research in text mining favours the quantity of texts over their quality. But for bilingual terminology mining, and for many language pairs, large comparable corpora are not available. More importantly, as terms are defined vis-à-vis a specific domain with a restricted register, it is expected that the quality rather than the quantity of the corpus matters more in terminology mining. Our hypothesis, therefore, is that the quality of the corpus is more important than the quantity and ensures the quality of the acquired terminological resources. We show how important the type of discourse is as a characteristic of the comparable corpus. Introduction Two main approaches exist for compiling corpora: \"Big is beautiful\" or \"Insecurity in large collections\". Text mining research commonly adopts the first approach and favors data quantity over quality. This is normally justified on the one hand by the need for large amounts of data in order to make use of statistic or stochastic methods (Manning and Schütze, 1999) , and on the other by the lack of operational methods to automatize the building of a corpus answering to selected criteria, such as domain, register, media, style or discourse. For lexical alignment from comparable corpora, good results on single words can be obtained from large corpora -several millions words -the accuracy of proposed translation is about 80% for the top 10-20 candidates (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002) . (Cao and Li, 2002) have achieved 91% accuracy for the top three candidates using the Web as a comparable corpus. But for specific domains, and many pairs of languages, such huge corpora are not available. More importantly, as terms are defined vis-à-vis a specific domain with a restricted register, it is expected that the quality rather than the quantity of the corpus matters more in terminology mining. For terminology mining, therefore, our hypothesis is that the quality of the corpora is more important than the quantity and that this ensures the quality of the acquired terminological resources. Comparable corpora are \"sets of texts in different languages, that are not translations of each other\" (Bowker and Pearson, 2002, p. 93) . The term comparable is used to indicate that these texts share some characteristics or features: topic, period, media, author, register (Biber, 1994) , discourse... This corpus comparability is discussed by lexical alignment researchers but never demonstrated: it is often reduced to a specific domain, such as the medical (Chiao and Zweigenbaum, 2002) or financial domains (Fung, 1998) , or to a register, such as newspaper articles (Fung, 1998) . For terminology mining, the comparability of the corpus should be based on the domain or the sub-domaine, but also on the type of discourse. Indeed, discourse acts semantically upon the lexical units. For a defined topic, some terms are specific to one discourse or another. For example, for French, within the subdomain of obesity in the domain of medicine, we find the term excès de poids (overweight) only inside texts sharing a popular science discourse, and the synonym excès pondéral (overweight) only in scientific discourse. In order to evaluate how important the discourse criterion is for building bilingual terminological lists, we carried out experiments on French-Japanese comparable corpora in the domain of medicine, more precisely on the topic of diabetes and nutrition, using texts collected from the Web and manually selected and classified into two discourse categories: one contains only scientific documents and the other contains both scientific and popular science documents. We used a state-of-the-art multilingual terminology mining chain composed of two term extraction programs, one in each language, and an alignment program. The term extraction programs are publicly available and both extract multi-word terms that are more precise and specific to a particular scientific domain than single word terms. The alignment program makes use of the direct context-vector approach (Fung, 1998; Peters and Picchi, 1998; Rapp, 1999) slightly modified to handle both singleand multi-word terms. We evaluated the candidate translations of multi-word terms using a reference list compiled from publicly available resources. We found that taking discourse type into account resulted in candidate translations of a better quality even when the corpus size is reduced by half. Thus, even using a state-of-the-art alignment method wellknown as data greedy, we reached the conclusion that the quantity of data is not sufficient to obtain a terminological list of high quality and that a real comparability of corpora is required. Multilingual terminology mining chain Taking as input a comparable corpora, the multilingual terminology chain outputs a list of single-and multi-word candidate terms along with their candidate translations. Its architecture is summarized in Figure 1 and comprises term extraction and alignment programs. Term extraction programs The terminology extraction programs are available for both French 1 (Daille, 2003) and Japanese 2 (Takeuchi et al., 2004) . The terminological units that are extracted are multi-word terms whose syntactic patterns correspond either to a canonical or a variation structure. The patterns are expressed using part-of-speech tags: for French, Brill's POS tagger 3 and the FLEM lemmatiser 4 are utilised, and for Japanese, CHASEN 5 . For French, the main patterns are N N, N Prep N et N Adj and for Japanese, N N, N Suff, Adj N and Pref N. The variants handled are morphological for both languages, syntactical only for French, and compounding only for Japanese. We consider as a morphological variant a morphological modification of one of the components of the base form, as a syntactical variant the insertion of another word into the components of the base form, and as a compounding variant the agglutination of another word to one of the components of the base form. At present, the Japanese term extraction program does not cluster terms. Term alignment The lexical alignment program adapts the direct context-vector approach proposed by (Fung, 1998) for single-word terms (SWTs) to multi-word terms (MWTs). It aligns source MWTs with target single words, SWTs or MWTs. From now on, we will refer to lexical units as words, SWTs or MWTs. Implementation of the direct context-vector method Our implementation of the direct context-vector method consists of the following 4 steps: 1. We collect all the lexical units in the context of each lexical unit $ and count their occurrence frequency in a window of % words around $ . For each lexical unit $ of the source and the target language, we obtain a context vector & (' which gathers the set of co-occurrence units ) associated with the number of times that ) and $ occur together 0 21 31 ' 4 . We normalise context vectors using an association score such as Mutual Information or Log-likelihood. In order to reduce the arity of context vectors, we keep only the co-occurrences with the highest association scores. 3. For a word to be translated, we compute the similarity between the translated context vector and all target vectors through vector distance measures such as Cosine (Salton and Lesk, 1968) or Jaccard (Tanimoto, 1958) . 4. The candidate translations of a lexical unit are the target lexical units closest to the translated context vector according to vector distance. Translation of lexical units The translation of the lexical units of the context vectors, which depends on the coverage of the bilingual dictionary vis-à-vis the corpus, is an important step of the direct approach: more elements of the context vector are translated more the context vector will be discrimating for selecting translations in the target language. If the bilingual dictionary provides several translations for a lexical unit, we consider all of them but weight the different translations by their frequency in the target language. If an MWT cannot be directly translated, we generate possible translations by using a compositional method (Grefenstette, 1999). For each element of the MWT found in the bilingual dictionary, we generate all the translated combinations identified by the term extraction program. For example, in the case of the MWT fatigue chronique (chronic fatigue), we have the following four translations for fatigue: ¢¡ , ¤£ , ¥ §¦ , © and the following two translations for chronique: , . Next, we generate all combinations of translated elements (See Table 1 7 ) and select those which refer to an existing MWT in the target language. Here, only one term has been identified by the Japanese terminology extraction program: ! . \"£ . In this approach, when it is not possible to translate all parts of an MWT, or when the translated combinations are not identified by the term extraction program, the MWT is not taken into account in the translation process. This approach differs from that used by (Robitaille et al., 2006) for French/Japanese translation. They first decompose the French MWT into combinations of shorter multi-word units (MWU) elements. This approach makes the direct translation of a subpart of the MWT possible if it is present in the 7 the French word order is inverted to take into account the different constraints between French and Japanese. chronique fatigue # $ % ¡ $ ¡ # $ % #£ $ #£ # $ % ¥ ¦ $ ¥ ¦ # $ % &© $ &© Table 1 : Illustration of the compositional method. The underlined Japanese MWT actually exists. bilingual dictionary. For an MWT of length % , (Robitaille et al., 2006) produce all the combinations of MWU elements of a length less than or equal to % . For example, the French term syndrome de fatigue chronique (chronic fatigue disease) yields the following four combinations: i) ' syndrome de fatigue chronique( , ii) ' syndrome de fatigue( )' chronique( , iii) ' syndrome( 0' fatigue chronique( and iv) ' syndrome( ' fatigue( 1' chronique( . We limit ourselves to the combination of type iv) above since 90% of the candidate terms provided by the term extraction process, after clustering, are only composed of two content words. Linguistic resources In this section we outline the different textual resources used for our experiments: the comparable corpora, bilingual dictionary and reference lexicon. Comparable corpora The French and Japanese documents were harvested from the Web by native speakers of each language who are not domain specialists. The texts are from the medical domain, within the sub-domain of diabetes and nutrition. Document harvesting was carried out by a domain-based search, then by manual selection. The search for documents sharing the same domain can be achieved using keywords reflecting the specialized domain: for French, diabète and obésité (diabetes and obesity); for Japanese, 2 3 $4 and 5 6 . Then the documents were classified according to the type of discourse: scientific or popularized science. At present, the selection and classification phases are carried out manually although research into how to automatize these two steps is ongoing. Table 2 shows the main features of the harvested comparable corpora: the number of documents, and the number of words for each language and each type of discourse. ' mixed corpora( , composed of both popular and scientific documents. French Bilingual dictionary The French-Japanese bilingual dictionary required for the translation phase is composed of four dictionaries freely available from the Web 8 , and of the French-Japanese Scientific Dictionary (1989) . It contains about 173,156 entries (114,461 single words and 58,695 multi words) with an average of 2.1 translations per entry. Terminology reference lists To evaluate the quality of the terminology mining chain, we built two bilingual terminology reference lists which include either SWTs or SMTs and MWTs: ' lexicon 1( 100 French SWTs of which the translation are Japanese SWTs. ' lexicon 2( 60 French SWTs and MWTs of which the translation could be Japanese SWTs or MWTs. These lexicons contains terms that occur at least twice in the scientific corpus, have been identified monolingually by both the French and the Japanese term extraction programs, and are found in either the UMLS 9 thesaurus or in the French part of the Grand dictionnaire terminologique 10 in the domain of medicine. These constraints prevented us from obtaining 100 French SWTs and MWTs for lexicon 2. The main reasons for this are the small number of UMLS terms dealing with the sub-domain of diabetes and the great difference between the linguistic structures of French and Japanese terms: French pattern definitions tend to cover more phrasal units while Japanese pattern definitions focus more narrowly on compounds. So, even if monolingually the same percentage of terms are detected in both languages, this does not guarantee a good result in bilingual terminology extraction. For example, the French term diabète de type 1 (Diabetes mellitus type I) extracted by the French term extraction program and found in UMLS was not extracted by the Japanese term extraction program although it appears frequently in the Japanese corpus ( ¡ 2 3 4 ). In bilingual terminology mining from specialized comparable corpora, the terminology reference lists are often composed of a hundred words (180 SWTs in (Déjean and Gaussier, 2002) and 97 SWTs in (Chiao and Zweigenbaum, 2002) ). Experiments In order to evaluate the influence of discourse type on the quality of bilingual terminology extraction, two experiments were carried out. Since the main studies relating to bilingual lexicon extraction from comparable corpora concentrate on finding translation candidates for SWTs, we first perform an experiment using ' lexicon 1( , which is composed of SWTs. In order to evaluate the hypothesis of this study, we then conducted a second experiment using ' lexicon 2( , which is composed of MWTs. The results of this experiment (see Table 3 ) show that the terms belonging to ' lexicon 1( were more easily identified in the corpus of scientific and popular documents (51% and 60% respectively for # 21 43 65 and # 21 3 ¨5 ) than in the corpus of scientific documents (49% and 52%). Since ' lexicon 1( is composed of SWTs, these terms are not more characteristic of popular discourse than scientific discourse. Alignment results for ' lexicon 1( The frequency of the terms to be translated is an important factor in the vectorial approach. In fact, the higher the frequency of the term to be translated, the more the associated context vector will be discriminant. Table 5 confirms this hypothesis since the most frequent terms, such as insuline (#occ. 364 -insulin: ¡ £ ¥ ¡ ), obésité (#occ. 333 -obesity: 5 6 ), and prévention (#occ. 120 -prevention: @ BA ), were the best translated. [ As a baseline, (Déjean et al., 2002) obtain 43% and 51% for the first 10 and 20 candidates respectively in a 100,000-word medical corpus, and 79% and 84% in a multi-domain 8 million-word corpus. For single-item French-English words applied on a medical corpus of 0.66 million words, (Chiao and Zweigenbaum, 2002) obtained 61% and 94% precision on the top-10 and top-20 candidates. In our case, we obtained 51% and 60% precision for the top 10 and 20 candidates in a 1.5 million-word French/Japanese corpus. Alignment results for ' lexicon 2( The analysis results in table 4 indicate only a small number of the terms in ' lexicon 2( were found. Since we work with small-size corpora, this result is not surprising. Because multi-word terms are more specific than single-word terms, they tend to occur less frequently in a corpus and are more difficult to translate. Here, the terms belonging ' lexicon 2( were more accurately identified from the corpus which consists of scientific documents than the corpus which consists of scientific and popular documents. In this instance, we obtained 30% and 42% precision for the top 10 and top 20 candidates in a 0.84 million-word scientific corpus. Moreover, if we count the number of terms which are correctly translated between ' scientific corpora( and ' mixed corpora( , we find the majority of the translated terms with ' mixed corpora( in those obtained with ' scientific corpora( 11 By combining parameters g a Q ¨g Q ¨a e g e Ra g a Q ig Q ¨ae 7g e a ¡ ¡ ¡ ¡ ¡ ¡ ¡ × × × × × × × nbr. win. g a Q ¨g Q ¨a e g e Ra g a Q ig Q ¨ae 7g e a ¢ ¢ ¢ ¢ ¢ ¢ ¢ × × × × × × × nbr. win. (a) parameter : Log-likelihood & cosinus (b) parameter : Log-likelihood & jaccard g a Q ¨g Q ¨a e g e Ra g a Q ig Q ¨ae 7g e a £ £ £ £ £ £ £ × × × × × × × nbr. win. g a Q ¨g Q ¨a e g e Ra g a Q ig Q ¨ae 7g e a ¤ ¤ ¤ ¥¤ ¤ ¤ ¤ × × × × × × × nbr. win. such as the window size of the context vector, association score, and vector distance measure, the terms were often identified with more precision from the corpus consisting of scientific documents than the corpus consisting of scientific and popular documents (see Figure 2 ). (c) parameter : MI & cosinus (d) parameter : MI & jaccard Here again, the most frequent terms (see Table 6 ), such as diabète (#occ. 899 -diabetes: 2 3 . 4 ), facteur de risque (#occ. 267 -risk factor: ¦ ¨ § .© ), hyperglycémie (#occ. 127 -hyperglycaemia: . 2 ), tissu adipeux (#occ. 62 -adipose tissue: . ) were the best translated. On the other hand, some terms with low frequency, such as édulcorant (#occ. 13 -sweetener: ¨ . ) and choix alimentaire (#occ. 11 -feeding preferences: ! \" .# $ ), or very low frequency, such as obésité massive (#occ. 6 -massive obesity: ¨% .5 !6 ), were also identified with this approach. [ Conclusion This article describes a first attempt at compiling French-Japanese terminology from comparable corpora taking into account both single-and multi-word terms. Our claim was that a real comparability of the corpora is required to obtain relevant terms of the domain. This comparability should be based not only on the domain and the sub-domain but also on the type of discourse, which acts semantically upon the lexical units. The discourse categorization of documents allows lexical acquisition to increase pre-cision despite the data sparsity problem that is often encountered for terminology mining and for language pairs not involving the English language, such as French-Japanese. We carried out experiments using two corpora of the specialised domain concerning diabetes and nutrition: one gathering documents from both scientific and popular science discourses, the other limited to scientific discourse. Our alignment results are close to previous works involving the English language, and are of better quality for the scientific corpus despite a corpus size that was reduced by half. The results demonstrate that the more frequent a term and its translation, the better the quality of the alignment will be, but also that the data sparsity problem could be partially solved by using comparable corpora of high quality.",
         "16875251",
         "2ead4c6bc94868b45c862f91bac48efa4165a1ab",
         "126",
         "https://aclanthology.org/P07-1084",
         "Association for Computational Linguistics",
         "Prague, Czech Republic",
         "2007",
         "June",
         "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
         "Morin, Emmanuel  and\nDaille, B{\\'e}atrice  and\nTakeuchi, Koichi  and\nKageura, Kyo",
         "Bilingual Terminology Mining - Using Brain, not brawn comparable corpora",
         "664--671",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "morin-etal-2007-bilingual",
         null,
         null
        ],
        [
         "43",
         "R13-1059",
         "Grammar induction is a basic step in natural language processing. Based on the volume of information that is used by different methods, we can distinguish three types of grammar induction method: supervised, unsupervised, and semi-supervised. Supervised and semisupervised methods require large tree banks, which may not currently exist for many languages. Accordingly, many researchers have focused on unsupervised methods. Unsupervised Data Oriented Parsing (UDOP) is currently the state of the art in unsupervised grammar induction. In this paper, we show that the performance of UDOP in free word order languages such as Persian is inferior to that of fixed order languages such as English. We also introduce a novel approach called History-based unsupervised data oriented Parsing, and show that the performance of UDOP can be significantly improved by using some history information, especially in dealing with free word order languages.",
         "Grammar induction is a basic step in natural language processing. Based on the volume of information that is used by different methods, we can distinguish three types of grammar induction method: supervised, unsupervised, and semi-supervised. Supervised and semisupervised methods require large tree banks, which may not currently exist for many languages. Accordingly, many researchers have focused on unsupervised methods. Unsupervised Data Oriented Parsing (UDOP) is currently the state of the art in unsupervised grammar induction. In this paper, we show that the performance of UDOP in free word order languages such as Persian is inferior to that of fixed order languages such as English. We also introduce a novel approach called History-based unsupervised data oriented Parsing, and show that the performance of UDOP can be significantly improved by using some history information, especially in dealing with free word order languages. Introduction Statistical methods of natural language processing have shown to be very successful in corpus based linguistics. One reason is that electronic based texts are now available more than ever (Charniak, 1997; Church, 1998) . The success of statistical Part Of Speech (POS) tagger systems has caused the trend of research in lexical analysis, language modeling, and machine translation to be changed towards using various statistical methods (Feili and Ghassem-Sani, 2004; Charniak, 1996) . Grammar is an essential tool in many applications of natural language processing (Feili and Ghassem-Sani, 2004) . Writing a natural language grammar by hand is not only a time-consuming and difficult task, but also it needs a large amount of skilled efforts. Availability of large parsed corpus such as Penn Treebank (Marcus et al., 1993) has facilitated the development of automatic methods of grammar induction. Based on the level of supervision information that is used by the different grammar induction methods, they are divided in to three major groups (i.e., supervised, semi-supervised, and unsupervised). Supervised and semi-supervised methods require large treebanks, which may not exist for many languages. Therefore, many researchers have focused on unsupervised methods. Unsupervised Data Oriented Parsing (UDOP) is currently the state of the art in unsupervised grammar induction. But in the case of free word order languages such as Persian, its performance is inferior to that of fixed order languages like English. In this paper, we present a novel unsupervised algorithm, named History-Based Unsupervised Data Oriented Parsing (HUDOP), and show, how to improve the performance of UDOP by using history information. In section 2, we discuss about different methods of grammar induction. In section 3, UDOP is explained. In section 4, the details of HUDOP are introduced. Section 5 presents our experimental results on English and Persian. Finally, we conclude the paper in section 6. Grammar induction methods As it was mentioned above, based on the level of information, there are three types of grammar inductions: supervised, semi-supervised and unsupervised. Supervised methods need fully-parsed and tagged corpora such as Penn Treebank (Marcus et al., 1993; Charniak, 1997; Collins, 1997; Charniak, 2000; Magerman, 1995; BoonkWan and Steedman, 2011) . There are also some semisupervised methods (Pereira and Schabes, 1992; Schabes et al., 1993; Koo et al., 2008) , which use less information than their supervised counterparts. Also, semi-supervised methods need a rich corpus that for some natural language (e.g., Persian) does not currently exist. Thus, we have focused our attention on unsupervised methods. Unsupervised methods do not need to pars tree of sentences in training corpus. Inside-Outside (IO) was introduced by Baker (1979) as an unsupervised algorithm. IO uses Expectation-Maximization (EM) to construct a grammar based on an un-bracketed corpus. The algorithm re-estimates rule probabilities toward some maximization on the training corpus. The algorithm may converge to local optima in different runs. This method is regarded as one of the basic algorithms of unsupervised grammar induction (Pereira and Schabes, 1992; Amaya et al., 1999; Casacuberta, 1996) . Alignment based Learning (ABL) is a learning method based on a linguistic principle: two constituents that belong to one family can be used instead of each other (Van Zaanen, 2000; Van Zaanen, 2002; Van Zaanen and Adriaans, 2001) . EMILE, another grammar induction system based on this principle, initially used some levels of supervision, but later was modified to be a completely unsupervised system (Adriaans, 2001) . Another important category of unsupervised induction method is based on the distribution of words in sentences. It usually uses some distributional evidence to identify the constituents' structures (Klein and Manning, 2001) . The main idea is that \"the same constituents appear in the same contexts\" (Clark, 2001; Klein and Manning, 2005) . The so-called Context-Constituent Model (CCM) is based on this idea and works on the basis of a weakened version of the classic linguistic constituency test (Radford, 1988) : constituents occur in their contexts. The independence of the input sentence and its surrounding context are usually assumed in parsing. For instance in a Probabilistic Context Free Grammar (PCFG) model, each constituent is as-sumed to be independent of its surrounding constituents (Charniak, 1997) . Such assumptions are not in fact valid in many cases. For instance, in English a noun phrase is more likely to be a pronoun when it is a subject of the sentence than when the noun phrase is in an object position (Allen, 1995) . Similar condition exists in Persian, too. For instance, in Persian a pronoun subject can be dropped whereas pronouns in object positions cannot be dropped (Bijankhan, 2003; Bateni, 1995) . We can reduce the impact of this invalid independence assumption by using some form of history in parsing. For instance, the information about parent non-terminals can be utilized as a history of parsing. More specifically, P(NPPronoun| Parent=SUBJ) is higher than P(NPPronoun | Parent = VP). Therefore, some of the parsing dependencies between constituents can be modeled by history based parsing. History based models were initially developed at IBM (Black et al., 1992; Jelinek et al., 1992; Jelinek et al., 1994) . Increasing the dependencies on the context is the main feature of history based models. For instance, Johnson (1998) used the parent information of each non-terminal as the history information in the condition part of each rule. He showed that, instead of P(AB|A), which is used in ordinary PCFG based parsing, using P(AB|A, parent(A)), where parent(A) is the nonterminal immediately dominating A, has a major positive impact on the accuracy of the parsing. Based on the idea proposed by Johnson (1998) , the so-called History based IO (HIO), improved the performance of IO especially in Persian (Feili and Ghassem-Sani, 2004) . Parent based CCM (PCCM) is another history based method, which improved CCM (Mirroshandel and Ghassem-Sani, 2008) . PCCM employs the parent's information of each context and constituent to prevent from divergence in the likelihood space. There are also other techniques for improving the quality of an unsupervised grammar induction algorithm by considering some limitations, or additional information. For instance, Carroll and Charniak (1992) limit the set of non-terminals of the right hand side of rules with a given left-hand side. Unsupervised Data Oriented Parsing Unsupervised Data Oriented Parsing (UDOP) was introduced in (Bod, 2006a; Bod 2006b; Bod, 2007) . In the first step, it generates all possible binary trees for each sentence of the corpus. This is followed by extracting all possible binary subtrees for parsing new sentences. In some methods, they convert each subtree to parsing rules. Number of rules will be increased exponentially. So these methods use Goodman reduction algorithm but we use subtree originally due to we want use Hidden Markov Model (HMM) for finding best parse tree for input sentence (Goodman, 2003) . UDOP uses a combination operator between the sub-trees for parsing a new sentence. We use \" \" as the symbol of the combination operator. Two sub-trees can be combined if the root of the right operand is equal to the leftmost nonterminal of the left operand. For example, let t1 and t2 be two sub-trees. Figure 1 shows t1 and t2 and the tree resulted from combining t1 and t2. X@1 K X@2 X@2 F D X@1 K X@2 X@2 F D Figure 1 . An example of the combination operator. Let T be a parse tree for an input sentence resulted from combining sub-trees t1, t2, … , tn (i.e., t1 t2 … tn), then t1 t2 … tn is said to be a derivation of T (Rankin, 2007) . UDOP takes the shortest derivation as the best derivation. However, there may exist several shortest derivations. In such cases, in order to select the best derivation, UDOP uses probability. The probability of any construction C is calculated by dividing the number of times C appears in the corpus by the number of times that any tree t with the same root appears in the corpus. (1) The probability of a derivation is calculated by the product of probabilities of all the constructions in the derivation: (2) Note that, there is an implicit assumption that, given root node root(ti), each ti is independent of every other tj where j<>i. The probability of a parse tree T is calculated by the sum the probabilities of all the possible derivations of T. (3)    ) ( ) ( ) ( T D d d P T P D(T) is the set of all possible derivations of T. Let Tj be a member in the set of all possible parse trees of a given sentence s. Then the preferred parse tree of s is the one that maximizes P(Ti|s) in: (4) History-based UDOP For computing all possible derivations of a new sentence, we can use the HMM, where each state corresponds to a sub-tree. The probability of each state is equal to the frequency of the subtree of that state. It means, the probability of the state that contains the sub-tree ti is calculated similar to UDOP as follows: (5) where statei corresponds to sub-tree ti. States in each step of HMM produce states in the next step, using the combination operator. Note that not all states can be combined. This is due to the definition of the combination operator. The transition probability between those states that cannot be combined will be set to zero. It means that if ti and tj cannot be combined, then P(titij) and P(tjtij), where titij to presents the transition between statei and stateij, are set to zero. On the other hand, let tx be a sub-tree with root X. Assume ty is any other sub-tree that can be combined with tx at node X. Also suppose that in tree ty, there is a node P(x,y) that immediately dominates X (i.e., P(x,y) is parent of node X in tree ty). In this case, there is a transition between tx and txy (i.e., txtxy) . The probability of txtxy is calculated as follows:    ) ( ) ( : | | | | ) ( t root t root t i i i t t state P    root(t) root(C) : t | | | | ) ( t C C P      j j t P ) ( ) t ... t P(t n 2 1   j j i i T P T P s T P ) ( ) ( ) | ( (6) We used top-down generative process to generate the HMM. By using parent information, the transition probabilities of HMM is calculated more accurately than in the case of UDOP. In HUDOP, the calculation of other probabilities, such as that of derivations and parse trees, is the same as UDOP. Finally, in HUDOP, similar to UDOP, in order to find the most probable parse tree, we have used the Viterbi 100-best method, which uses 100 most probable states (sub-trees) in each step of HMM (Bod, 2006b) . Experimental results Two kinds of experiments are presented in this section. At first, the result of applying HUDOP to two different English data sets are demonstrated and compared with that of related work. Then, we show the results of applying HUDOP to Persian, as a free-word order language. Experimental result in English HUDOP was tested on both ATIS (Hemphill et al., 1990) and WSJ-10 (Schabes et al., 1993) . We used PARSEVAL to evaluate the quality of the output grammars. Part of speech tag sequences were used as the only lexical information of the training sets. We executed two different experiments on the English sentences. At first, ATIS was divided in two distinct sets: the training set with almost 90% of the data and the test set including the rest. Although, HUDOP is an unsupervised approach and does not require any bracketing data set, we need the tree style syntactic information of the test data set for the evaluation purpose. We evaluated HUDOP using the ten-fold cross validation method. Similar to the original UDOP, we selected sentences with the length shorter than ten. In the first experiment, we selected the spokenlanguage transcription of the Texas Instruments subset of ATIS (Hemphill et al., 1990) The results of comparing HUDOP with other unsupervised methods, including EMILE (Adriaans and Haas, 1999), ABL (Van Zaanen, 2000) , and CCM (Klein and Manning, 2005) , on ATIS are shown in table 1 . LEFT and RIGHT are the left and the right-branching baselines applied to ATIS. The results of left and right baselines have been taken from Klein and Manning (2005) . As table 1 shows, the performance of HUDOP is superior to all the mentioned work. We also tested HUDOP on WSJ-10 and compared its results with a number of related works including the state of the art (i.e., UDOP). The results are shown in Figure 2 . Experimental results in Persian We have also applied HUDOP to Persian, which is linguistically very different from English. Although many sentences in Persian have the form of SOV, it is generally considered to be a free- word-order language, especially in proposition adjunction and complements. It means that an adverb can be used at the beginning, in the middle, or at the end of sentences. This does not often change the meaning of the sentences. In order to test HUDOP in Persian, we manually produced two different training corpora. All sentences of these corpuses contain less than 11 words, and have been extracted from a Persian corpus named Peykareh (Bijankhan, 2003; Megerdoomian, 2000) . Peykareh has more than 32,255 sentences and uses a tag set similar to the tag set used in Amtrup et al. (2003) . The first corpus included 3,000 sentences, which were manually changed in such a way that the structure of \"S PP O V\" was held. In other words, the common property of the sentences in this corpus was that the order of words were artificially fixed (i.e., they were not free in order). Table 2 shows main properties of the first corpus. Property Value Number The second corpus comprised 2,500 sentences with a high degree of free word orderness. Table 3 shows main properties of the second corpus. Property Value Number In Persian, we first ran both UDOP and HUDOP on each of the above corpora, separately. We also joined these corpuses to create a third mixed corpus, and repeated the experiments on this corpus, too. The results are shown in figure 3 . Figure 3 shows the impact of the free word orderness property on the performance of both UDOP and HUDOP. The reduction in the performance of UDOP on the first corpus, in comparison to that of the second corpus, has been 13 percent in F1 score. The results of applying both UDOP and HUDOP to the combined corpus demonstrate little improvement. This shows that the free word orderness property of the input language has a negative effect on these methods. The reason for this weakness is that these methods work based on the repetition of subtrees. Since in free word order languages, some words can freely appear in different places of sentences, the mentioned repetition decreases substantially, and as a result, the performance of the parsing is decreased. The experiments also show that HUDOP outperforms UDOP in both languages. Conclusion Unsupervised Data Oriented Parsing (UDOP) is currently the state of the art in unsupervised grammar induction. UDOP works based on the repetition of possible sub-trees of parse trees of the input sentences. However, in free word order languages such as Persian, words can grammatically appear in different places of sentences. Thus, occurrence frequency of such sub-trees substantially decreases. In this paper, we proposed a novel approach, called History-based Unsupervised Data Oriented Parsing (HUDOP). We showed how by using parent nodes as a history notion of sub-trees, HUDOP outperforms UDOP. Parent information prevents from probability divergence and parsing will be more informative. To evaluate HUDOP, it was applied to both English and Persian (as a free word order ). The results of applying the new method to several corpuses with different degree of free word orderness showed that using parent information notably improves the performance of UDOP. One possible future work to improve the performance of HUDOP can be usage of other possible forms of history information. We are working on the idea implementing a semisupervised HUDOP.",
         "14430680",
         "3f655c491d2402f91648c03f9b01e2eabd1c2adc",
         "1",
         "https://aclanthology.org/R13-1059",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Mesgar, Mohsen  and\nGhasem-Sani, Gholamreza",
         "History Based Unsupervised Data Oriented Parsing",
         "453--459",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "mesgar-ghasem-sani-2013-history",
         null,
         null
        ],
        [
         "44",
         "W05-0832",
         "Translation memories provide assistance to human translators in production settings, and are sometimes used as first-pass machine translation in assimilation settings because they produce highly fluent output very rapidly. In this paper, we describe and evaluate a simple whole-segment translation memory, placing it as a new lower bound in the well-populated space of machine translation systems. The result is a new way to gauge how far machine translation has progressed compared to an easily understood baseline system. The evaluation also sheds light on the evaluation metric and gives evidence showing that gaming translation with perfect fluency does not fool bleu the way it fools people.",
         "Translation memories provide assistance to human translators in production settings, and are sometimes used as first-pass machine translation in assimilation settings because they produce highly fluent output very rapidly. In this paper, we describe and evaluate a simple whole-segment translation memory, placing it as a new lower bound in the well-populated space of machine translation systems. The result is a new way to gauge how far machine translation has progressed compared to an easily understood baseline system. The evaluation also sheds light on the evaluation metric and gives evidence showing that gaming translation with perfect fluency does not fool bleu the way it fools people. Introduction and background Translation Memory (TM) systems provide roughly concordanced results from an archive of previously translated materials. They are typically used by translators who want computer assistance for searching large archives for tricky translations, and also to help ensure a group of translators rapidly arrive at similar terminology (Macklovitch et al., 2000) . Several companies provide commercial TMs and systems for using and sharing them. TMs can add value to computer assisted translation services (Drugan, 2004) . Machine Translation (MT) developers make use of similar historical archives (parallel texts, bitexts), to produce systems that perform a task very similar to TMs. But while TM systems and MT systems can appear strikingly similar, (Marcu, 2001 ) key differences exist in how they are used. TMs often need to be fast because they are typically used interactively. They aim to produce highly readable, fluent output, usable in document production settings. In this setting, errors of omission are more easily forgiven than errors of commission so, just like MT, TM output must look good to users who have no access to the information in source texts. MT, on the other hand, is often used in assimilation settings, where a batch job can often be run on multiple processors. This permits variable rate output and allows slower systems that produce better translations to play a part. Batch MT serving a single user only needs to run at roughly the same rate the reader can consume its output. Simple TMs operate on an entire translation segment, roughly the size of a sentence or two, while more sophisticated TMs operate on units of varying size: a word, a phrase, or an entire segment (Callison-Burch et al., 2004) . Modern approaches to MT, especially statistical MT, typically operate on more fine-grained units, words and phrases (Och and Ney, 2004) . The relationship between whole segment TM and MT can be viewed as a continuum of translation granularity: This classification motivates our work here. MT systems have well-studied and popular evaluation techniques such as bleu (Papineni et al., 2001) . In this paper we lay out a methodology for evaluating TMs along the lines of MT evaluation. This allows us to measure the raw relative value of TM and MT as translation tools, and to develop expectations for how TM performance increases as the size of the memory increases. There are many ways to perform TM segmentation and phrase extraction. In this study, we use the most obvious and simple condition-a full segment TM. This gives a lower bound on real TM performance, but a lower bound which is not trivial. Section 2 details the architecture of our simple TM. Section 3 describes experiments involving different strategies for IR, oracle upper bounds on TM performance as the memory grows, and techniques for rescoring the retrievals. Section 4 discusses the results of the experiments. A Simple Chinese-English Translation Memory For our experiments below, we constructed a simple translation memory from a sentencealigned parallel corpus. The system consists of three stages. A source-language input string is rewritten to form an information retrieval (IR) query. The IR engine is called to return a list of candidate translation pairs. Finally a single target-language translation as output is chosen. Query rewriting To retrieve a list of translation candidates from the IR engine, we first create a query which is a concatenation of all possible ngrams of the source sentence, for all ngram sizes from 1 to a fixed n. We rely on the fact that the Chinese data in the translation memory is tokenized and indexed at the unigram level. Each Chinese character in the source sentence is tokenized individually, and we make use of the IR engine's phrase query feature, which matches documents in which all terms in the phrase appear in consecutive order, to create the ngrams. For example, to produce a trigram + bigram + unigram query for a Chinese sentence of 10 characters, we would create a query consisting of eight threecharacter phrases, nine two-character phrases, and 10 single-character \"phrases\". All phrases are weighted equally in the query. This approach allows us to perform lookups for arbitrary ngram sizes. Depending on the specifics of how idf is calculated, this may yield different results from indexing ngrams directly, but it is advantageous in terms of space consumed and scalability to different ngram sizes without reindexing. This is a slight generalization of the successful approach to Chinese information retrieval using bigrams (Kwok, 1997) . Unlike that work, we perform no second stage IR after query expansion. Using a segmentation-independent engineering approach to Chinese IR allows us to sidestep the lack of a strong segmentation standard for our heterogeneous parallel corpus and prepares us to rapidly move to other languages with segmentation or lemmatization challenges. The IR engine Simply for performance reasons, an IR engine, or some other sort of index, is needed to implement a TM (Brown, 2004) . We use the opensource Lucene v 1. 4.3, (Apa, 2004) as our IR engine. Lucene scores candidate segments from the parallel text using a modified tf-idf formula that includes normalizations for the input segment length and the candidate segment length. We did not modify any Lucene defaults for these experiments. To form our translation memory, we indexed all sentence pairs in the translation memory corpora, each pair as a separate document. We Source TM output However , everything depended on the missions to be decided by the Security Council . The presentations focused on the main lessons learned from their activities in the field . It is wrong to commit suicide or to use ones own body as a weapon of destruction . There was practically full employment in all sectors . One reference translation (of four) Doug Collins said, \"He may appear any time. It really depends on how he feels.\" At present, his training is defense oriented but he also practices shots. He is elevating the intensity to test whether his body can adapt to it. So far as his knee is concerned, he thinks it heals a hundred percent after the surgery.\" indexed in such a way that IR searches can be restricted to just the source language side or just the target language side. Rescoring The IR engine returns a list of candidate translation pairs based on the query string, and the final stage of the TM process is the selection of a single target-language output sentence from that set. We consider a variety of selection metrics in the experiments below. For each metric, the source-language side of each pair in the candidate list is evaluated against the original source language input string. The target language segment of the pair with the highest score is then output as the translation. In the case of automated MT evaluation metrics, which are not necessarily symmetric, the source-language input string is treated as the reference and the source-language side of each pair returned by the IR engine as the hypothesis. All tie-breaking is done via tf-idf , i.e. if multiple entries share the same score, the one ranked higher by the search engine will be output. Table 1 gives a typical example of how the TM performs. Four contiguous source segments are presented, followed by TM output and finally one of the reference translations for those source segments. The only indicator of the translation quality available to monolingual English speakers is the awkwardness of the segments as a group. By design, the TM performs with perfect fluency at the segment level. Experiments We performed several experiments in the course of optimizing this TM, all using the same set of parallel texts for the TM database and multiple-reference translation corpus for evalutation. The parallel texts for the TM come from several Chinese-English parallel corpora, all available from the Linguistic Data Consortium (LDC). These corpora are described in Table 2. We discarded any sentence pairs that seemed trivially incomplete, corrupt, or otherwise invalid. In the case of LDC2002E18, in which sentences were aligned automatically and confidence scores produced for each alignment, we dropped all pairs with scores above 9, indicating poor alignment. No duplication checks were performed. Our final corpus contained approximately 7 million sentence pairs and contained 3.2 GB of UTF-8 data. Our evaluation corpus and reference corpus come from the data used in the NIST 2002 MT competition. (NIST, 2002) . The evaluation corpus is 878 segments of Chinese source text. The reference corpus consists of four independent human-generated reference English translations of the evaluation corpus. All performance measurements were made using a fast reimplementation of NIST's bleu. bleu exhibits a high correlation with human judgments of translation quality when measuring on large sections of text (Papineni et al., 2001) . Furthermore, using bleu allowed us to compare our performance to that of other systems that have been tested with the same evaluation data. An upper bound on whole-segment translation memory Our first experiment was to determine an upper bound for the entire translation memory corpus. In other words, given an oracle that picks the best possible translation from the translation memory corpus for each segment in the evaluation corpus, what is the bleu score for the resulting document? This score is unlikely to approach the maximum, bleu =100 because this oracle is constrained to selecting a translation from the target language side of the parallel corpus. All of the calculations for this experiment are performed on the target language side of the parallel text. We were able to take advantage of a trait particular to bleu for this experiment, avoiding many of bleu score calculations required to assess all of the 878 × 7.5 million combinations. bleu produces a score of 0 for any hypothesis string that doesn't share at least one 4-gram with one reference string. Thus, for each set of four references, we created a Lucene query that returned all translation pairs which matched at least one 4-gram with one of the references. We picked the top segment by calculating bleu scores against the references, and created a hypothesis document from these segments. Note that, for document scores, bleu's brevity penalty (BP) is applied globally to an entire document and not to individual segments. Thus, the document score does not necessarily increase monotonically with increases in scores of individual segments. As more than 99% of the segment pairs we evaluated yielded scores of zero, we felt this would not have a significant effect on our experiments. Also, the TM does not have much liberty to alter the length of the returned segments. Individual segments were chosen to optimize bleu score, and the resulting documents exhibited appropriately increasing scores. While there is no efficient strategy for whole-document bleu maximization, an iterative rescoring of the entire document while optimizing the choice of only one candidate segment at a time could potentially yield higher scores than those we report here. TM performance with varied Ngram length The second experiment was to determine the effect that different ngram sizes in the Chinese IR query have on the IR engine's ability to retrieve good English translations. We considered cumulative ngram sizes from 1 to 7, i.e. unigram, unigram + bigram, unigram + bigram + trigram, and so on. For each set of ngram sizes, we created a Lucene query for every segment of the (Chinese) evaluation corpus. We then produced a hypothesis document by combining the English sides of the top results returned by Lucene for each query. The hypothesis document was evaluated against the reference corpora by calculating a bleu score. While it was observed that IR performance is maximized by performing bigram queries (Kwok, 1997) , we had reason to believe the TM would not be similar. TMs must attempt to match short sequences of stop words that indicate grammar as well as more traditional content words. Note that our system performed neither stemming nor stop word (or ngram) removal on the input Chinese strings. An upper bound on TM N -best list rescoring The next experiment was to determine an upper bound on the performance of tf-idf for different result set sizes, i.e. for different (maximum) numbers of translation pairs returned by the IR engine. This experiment describes the trade-off between more time spent in the IR engine creating a longer list of returns and the potential increase in translation score. To determine how much IR was \"enough\" IR, we performed an oracle experiment on different IR query sizes. For each segment of the evaluation corpus, we performed a cumulative 4-gram query as described in Section 4.2. We produced the n-best list oracle's hypothesis document by selecting the English translation from this result set with the highest bleu score when evaluated against the corresponding segment from the reference corpus. We then evaluated the hypothesis documents against the reference corpus by computing bleu scores. N -best list rescoring with several MT evaluation metrics The fourth experiment was to determine whether we could improve upon tf-idf by applying automated MT metrics to pick the best sentence from the top n translation pairs returned by the IR engine. We compared a variety of metrics from MT evaluation literatures. All of these were run on the tokens in the source language side of the IR result, comparing against the single pseudo-reference, the original source language segment. While many of these metrics aren't designed to perform well with one reference, they stand in as good approximate string matching algorithms. The score that the IR engine associates with each segment is retained and marked as tf-idf in this experiment. Naturally, bleu (Papineni et al., 2001) was the first choice metric, as it was well-matched to the target language evaluation function. rouge was a reimplementation of ROUGE-L from (Lin and Och, 2004) . It computes an F-measure from precision and recall that are both based on the longest common subsequence of the hypothesis and reference strings. wer-g is a variation on traditional word error rate that was found to correlate very well with human judgments (Foster et al., 2003) , and per is the traditional position-independent error rate that was also shown to correlate well with human judgments (Leusch et al., 2003) . Finally, a random metric was added to show the bleu value one could achieve by selecting from the top n strictly by chance. After the individual metrics are calculated for these segments, a uniform-weight log-linear combination of the metrics is calculated and used to produce a new rank ordering under the belief that the different metrics will make predictions that are constructive in aggregate. Results An upper bound for whole-sentence TM Figure 1 shows the maximum possible bleu score that can an oracle can achieve by selecting the best English-side segment from the parallel text. The upper bound achieved here is a bleu score of 17.7, and this number is higher than the best performing system in the corresponding NIST evaluation. Note the log-linear growth in the resulting   bleu score of the TM with increasing database size. As the database is increased by a factor of ten, the TM gains approximately 5 points of bleu. While this trend has a natural limit at 20 orders of magnitude, it is unlikely that this amount of text, let alone parallel text, will be a indexed in the foreseeable future. This rate is more useful in interpolation, giving an idea of how much could be gained from adding to corpora that are smaller than 7.5 million segments. 4.2 The effect of ngram size on Chinese tf-idf retrieval Figure 2 shows that our best performance is realized when IR queries are composed of cumulative 4-grams (i.e. unigrams + bigrams + trigrams + 4-grams). As hypothesized, while longer sequences are not important in document retrieval in Chinese IR, they convey information that is useful in segment retrieval in the translation memory. For the remainder of the experiments, we restrict ourselves to cumulative 4-gram queries. Note that the 4-gram result here (bleu of 5.87) provides the baseline system performance measure as well as the value when the segments are reranked according to tf-idf . Upper bounds for tf-idf Figure 3 gives the n-best list rescoring bounds. The upper bound continues to increase up to the top 1000 results. The plateau achieved after 1000 IR results suggests that is little to be gained from further IR engine retrieval. Note the log-linear growth in the bleu score the oracle achieves as the n-best list extends on the left side of the figure. As the list length is increased by a factor of ten, the oracle upper bound on performance increases by roughly 3 points of bleu. Of course, for a system to perform as well as the oracle does becomes progressively harder as the n-best list size increases. Comparing this result with the experiment in section 4.1 indicates that making the oracle choose among Chinese source language IR results and limiting its view to the 1000 results given by the IR engine incurs only a minor reduction of the oracle's bleu score, from 17.7 to 16.3. This is one way to measure the impact of crossing this particular language barrier and using IR rather than exhaustive search. Surprisingly, tf-idf was outperformed only by bleu and the combination metric. While we hoped to gain much more from n-best list rescoring on this task, reaching toward the limits discovered in section 4.3, the combination metric was less than 0.5 bleu points below the lower range of systems that were entered in the NIST 2002 evals. The bleu scores of research systems in that competition roughly ranged between 7 and 15. Of course, each of the segments produced by the TM exhibit perfect fluency. Discussion The maximum bleu score attained by a TM we describe (6.56) would place it in last place in the NIST 2002 evals, but by less than 0.5 bleu. Successive NIST competitions have exhibited impressive system progress, but each year there have been newcomers who score near (or in some cases lower than) our simple TM baseline. We have presented several experiments that quantitatively describe how well a simple TM performs when measured with a standard MT evaluation measure, bleu. We showed that the translation performance of a TM grows as a loglinear function of corpus size below 7.5 million segments. We showed, somewhat surprisingly, only 1000 IR returns need be evaluated by a rescorer to get within 1 bleu point of the maximum possible score attainable by the TM. In future work, we expect to validate these results with other language pairs. One question is: how well does this simple IR query expansion address segmented languages and languages that allow more liberal word order? Supervised training of n-best reranking schemes would also determine how far the oracle bound can be pushed. The computationally more expensive reranking procedure that attempts to optimize bleu on the entire document should be investigated to determine how much can be gained by better global management of the brevity penalty. Finally, we believe it's worth noting the degree to which high fluency of the TM output could potentially mislead target-language-only readers in their estimation of the system's performance. Table 1 is representative of system output, and is a good example of why translations should not be judged solely on the fluency of a few segments of target language output.",
         "317123",
         "6158add37ce7e19122e06433a548d833a5e9c4ab",
         "1",
         "https://aclanthology.org/W05-0832",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Henderson, John  and\nMorgan, William",
         "Gaming Fluency: Evaluating the Bounds and Expectations of Segment-based Translation Memory",
         "175--182",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "henderson-morgan-2005-gaming",
         null,
         null
        ],
        [
         "45",
         "2007.mtsummit-papers.60",
         "Although many high-quality dictionaries contain a sufficient number of idioms for their intended users, the methods available for looking up entries in both paper and electronic dictionaries as well as in machine translation systems are not satisfactory. Providing an adequate automatic look-up function is complicated by the existence of idiom variants, which sometimes can be very creative. The problem is further complicated by the fact that the possible range of idiom variations has not been described in a computationally tractable way. Against this backdrop, we analysed the variation patterns of idioms using manually-created idiom variation data, and, on the basis of that, developed an idiom look-up system that automatically matches idiom variants in English texts with the canonical forms of idiom entries in dictionaries. The experimental results showed our system performs sufficiently well to be used in real-world settings, including as an aid for translators, which is our overall aim.",
         "Although many high-quality dictionaries contain a sufficient number of idioms for their intended users, the methods available for looking up entries in both paper and electronic dictionaries as well as in machine translation systems are not satisfactory. Providing an adequate automatic look-up function is complicated by the existence of idiom variants, which sometimes can be very creative. The problem is further complicated by the fact that the possible range of idiom variations has not been described in a computationally tractable way. Against this backdrop, we analysed the variation patterns of idioms using manually-created idiom variation data, and, on the basis of that, developed an idiom look-up system that automatically matches idiom variants in English texts with the canonical forms of idiom entries in dictionaries. The experimental results showed our system performs sufficiently well to be used in real-world settings, including as an aid for translators, which is our overall aim. Introduction We are currently developing a system that aids Englishto-Japanese volunteer translators who translate online documents and publish translated documents online. Among the many reference functions and aspects of reference content that require enhancement, translators have identified improvement in idiom look-up functions as a key issue they would like to see addressed by translation aid systems (Kageura et. al., 2006) . Although idioms provided by many high-quality dictionaries (e.g. Sanseido, 2004; McCaleb and Iwasaki, 2003) are basically satisfactory for translators, the methods available for looking up idiom entries are far from satisfactory, not only in paper dictionaries but also in electronic dictionaries. This is partly because the user has to guess the core constituent words of an idiom in order to consult a dictionary. Automatic look-up methods embodied in machine translation systems are not satisfactory, either. Take, for instance, the following examples: (1) He said that with his tongue in his cheek. (2) He said that with his big fat tongue in his big fat cheek. Although many available machine translation systems successfully detect the idiomatic expression \"with one's tongue in one's cheek\" in (1), none among those we checked (e.g. Excite, 2005; Fujitsu, 2005; LogoVista, 2005; Sharp, 2004; Toshiba, 2005) 1 could properly translate (2). Most existing methods for looking up idioms cannot deal with the rich variations in idioms, that are abundant in ordinary texts 2 . In the field of natural language processing (NLP), much research has been carried out into the automatic extraction of collocations and idioms (e.g. Piao, 2006; Smadja, 1993; Widdows and Dorow, 2005) , but not much work has been devoted to the automatic matching of idiom entries to their occurrences in running texts. As translators are basically satisfied with the idioms provided in existing high-quality dictionaries but frustrated with the poor look-up functions, 1 Sharp (2004) can detect some gapped idiom occurrences, but it fails to detect complex idiom variations. 2 For instance, our rough survey of online documents revealed that the idiom \"hang on\" in its basic form, \"hang on\" with insertion, and \"hang on\" with passivisation (with or without insertion) occur in roughly the same frequency. The same is true for \"dumb down\". to develop enhanced look-up functions for idioms is of utmost importance from the point of view of aiding translators. Although a few important related studies exist (Carl and Rascu, 2006; Jacquemin, 2001; Yoshihashi et. al., 2005) , and many translation memory systems realise flexible approximate matching of similar sentence or phrasal constructions (Similis, 2006; Trados, 2006) , the task of flexible automatic look-up of idioms is yet to be fully explored. Against this backdrop, we are developing a mechanism that automatically matches English idiom occurrences in texts and their possible variations with idiom entries in dictionaries, as part of an overall project that aims at developing a system to aid English-to-Japanese online volunteer translators. In the following, we will first clarify translators' basic requirements. Then we will provide the basic patterns of idiom variations based on manually-constructed idiom variation data, and explain the automatic idiom look-up system that takes into account major syntagmatic idiom variations. Finally, we will provide an evaluation of the method and outline areas for further improvement. Translators' basic requirements In order to clarify requirements for translation-aid system, we consulted eight translators working online. We also sent a questionnaire to other online translators and obtained 12 replies. In relation to idiom look-up functions, two important features became clear. Firstly, translators do not want the system to provide a single idiom entry that matches textual occurrences. This is more to do with the fact that checking multiple possibilities is an inherent and essential part of proper translation than with the fact that it is difficult to develop a satisfactorily high-performance automatic idiom look-up system. In other words, from the point of view of translators, for the system to provide multiple possibilities of matching idioms is not a defect but a necessity if the system is to be useful for them. After all, translators check many candidates that they do not eventually use in their translations. What is important is reducing translators' burden as well as the quality of candidates the system proposes. Secondly, translators -as language practitioners -want the system to be able to deal with variations in a more flexible way than described by linguists. For instance, for language practitioners \"shoot the breeze\" could be passivised (or they could imagine a situation in which they face the passive form of this idiom or they passivise the idiom by themselves in the process of writing), while according to Numberg et. al. (1994) , it is not possible. The idiom \"go halves\" can be used in the form \"go exact halves\", while Nicolas (1995) claimed that this is not possible. In a sense, even what descriptive (i.e. non-prescriptive) linguistics provides is too prescriptive for the reality of texts that translators face in their daily activities. This is especially the case for online documents (Aitchison and Lewis, 2003) , which are dealt with by the translators our system targets. From the point of view of system specifications, these two requirements mean that the system can -and shouldprovide overmatching results, with recall as close as 100%. They also mean that, in the evaluation of system performance, the concept of \"precision\" should be defined not in terms of the \"correct\" choice of candidate but in terms of its usefulness for translators. We will come back to this point later when we evaluate the performance of our system. Idiom variation patterns There are studies and reference books that describe (English) idiom variations at a variety of levels (Benson, 1985; Biber, 1999; Čermák, 1970; Fraser, 1970; Moon, 1998; Nicolas, 1995; Numberg et. al., 1994; Quirk et. al., 1985) . On the basis of these, and taking into account the comments we obtained from translators, we first classified the idiom variation patterns as follows: (1) Type variants or families: Types of idiom variations that are (or theoretically should be) registered in dictionary entries. An example is \"run around [round] like a bluearsed fly\". From the practical point of view, this type of variant can be dealt with by the variation indications in idiom entries in dictionaries. (2) Variations created by external factors: Passivisation (\"the breeze was shot\") and topicalisation (\"It is these strings that he pulled\") are typical examples of this type of variation. These variations are generally created by applying syntactic operations defined outside the idioms themselves, and could be dealt with uniformly by a few basic rules. (3) Variations applied to parts or within the construction of idioms: many syntagmatic insertions (\"go halves\" → \"go exact halves\") and paradigmatic replacements (\"head screwed on right\" → \"head screwed on wrong/left\") fall under this category. This type of variation is expected to be neither straightforwardly clear nor completely unmanageable. (4) Highly creative variations: \"point of view\" → \"ballpoint pen of view\". This class of variation is expected to be unmanageable for the time being. We focus on the third category (3) of idiom variations in this work, for reasons mentioned above. In order to develop a look-up function for idiom entries, existing linguistic studies have three limitations: (i) as mentioned, they tend to be too restrictive from the point of view of the reality of texts that translators deal with; (ii) the description of variations is not given in a computationally tractable way; and (iii) the number of variations given in these studies is small 3 . Given the dearth of basic idiom variation data, we started by constructing idiom variation data manually. We took idiom entries from a widely-used English-Japanese idiom dictionary (McCaleb and Iwasaki, 2003) , and asked three native English speakers (two of whom were professional editors) to create idiom variations with examples. We asked the informants to imagine they were writing or editing articles in the culture section of newspapers and be as creative as possible within that restriction. Table 1 shows the basic quantities of the idiom variation data. The quantity of data is rather small, and we are intending to augment it further in the same manner, asking informants to construct variations. Note, however, that using large corpora to collect the data is not our priority, for a few reasons: (i) it is difficult to extract idiom variations from large corpora, except for easily predictable regular types which could be covered by rules; (ii) it is difficult to identify the threshold of possible variations, which tend to occur in low frequencies (translators do not choose a text by the representativeness of the language in the text or by the overall frequencies in the corpora of idiom variations used in the text); (iii) translators are dealing with individual texts and not representative language expressions, so frequencies in large corpora do not automatically mean importance for translators; and (iv) the frequency of idiom variations detected in large corpora correlates with the frequency of individual idioms, and frequently occurring idioms tend to be the ones that translators are least interested in and therefore the variations of which are less important from the translators' point of view. Informants (a) idioms (b) variants (b)/( Our intention in referring to the data is not to observe common usage or dominant patterns but to define the tractable range of variation patterns, which would hopefully correspond to the range of practically possible variations; we have no interest in covering only frequently occurring patterns at the expense of less frequent but computationally tractable patterns. As such, it is expected that the use of large corpora would not cover up the shortcomings of the size of the manually constructed data. For instance, one of the informants provided the variation \"take the wild plunge\" of the idiom \"take the plunge,\" which only brings up four hits in a Google search. From the point of view of translators, this and other rare variations, if technically possible, should be covered when they occur 4 . This manually-constructed data was then analysed and variation patterns were identified (Kageura and Toyoshima, 2006) . Table 2 shows the basic variation patterns. In the table, such types as \"dependent multiple replacement\" etc. indicate that more than one type of mutually dependent variations was observed. As can be seen from Table 2 , the major patterns are syntagmatic augmentations and paradigmatic replacement. Here we focus on variations by syntagmatic augmentation, and formalise the descriptions of syntagmatic augmentation patterns. In doing so, we assume the use of POS-taggers and morphological analysers. Though high-performance parsers exist, we did not use them for two main reasons: (i) idiomatic expressions often cross over the border of constituents given by parsers (in which case we would need to flatten the parse tree anyway), and (ii) to achieve recall as close to 100% as possible is most important, and for that aim the loose definition of patterns provides a better starting point than the rigid description of variations using structural information. There are two different approaches for describing POS level patterns for variations of syntagmatic augmentations: (a) taking all constituents of the idiom into account, or (b) taking only binary constituents adjacent to words inserted into the idiom. For instance, assuming that the idiom expression \"take the plunge\" has as a variation \"take the wild plunge\", we can describe the POS level patterns as \"Verb Det Noun\" → \"Verb Det Adj Noun\" in approach (a), or \"Det Noun\" → \"Det Adj Noun\" in approach (b). In the current work we took approach (b) because we assume that: (i) inserted words are mostly bound by the adjacent words and their grammatical categories; (ii) to take into account the overall grammatical patterns would immediately lead us to taking into account the individual idioms with lexical substance; and (iii) the requirement of high recall is of utmost importance at the current stage. Using the POS-information of adjacent elements, we for-Ah, those who cannot read literary texts but still have the audacity to think themselves to be language specialists!\" As computational linguists we should try to bridge this gap between linguists and translators. mulated the basic patterns of idiom variations by syntagmatic augmentation as shown in Table 3 (\"Prep\", \"Adj\", \"Adv\", \"PosPro\" and \"PerPro\" denote preposition, adjective, adverb, possessive pronoun (e.g. \"my\", \"her\" etc.) and personal pronoun (e.g. \"I\", \"he\" etc.), respectively). POS Each variation rule in Table 3 indicates the POS sequence of constituents of an idiom and the POS tag of the inserted word. For instance, the POS pattern of (Det, Noun) in constituents of an idiom can take either a noun, adjective or adverb. This rule covers the variation from \"take the plunge\" to \"take the wild plunge\". Note that the described range of variation patterns, when incorporated into automatic matching algorithms, can be overgenerative. We can, however, reasonably expect that the overmatching will be within the manageable range, because, as mentioned, the computational problem is defined here as a problem of matching when both ends are given, rather than a problem of generating acceptable variations. The idiom look-up system The system consists of three processing modules: (1) the preprocessing module in which the input text is processed to facilitate automatic matching, including POS-tagging and normalisation of expressions, (2) the surface matching module in which all the possible idiom entries are detected by using AND matching of constituent elements of idioms with words occurring in texts, and (3) the filtering module in which the undesired candidates detected in the surface matching module are filtered out by using the POS-based variation restriction rules constructed based on the patterns given in Table 3 . The input of the target system is an English text and the output is idiom candidates occurring in the text with their Japanese translations provided in the dictionary. Figure 1 shows the overall flow of the system. We will elaborate each of these modules below and illustrate the system interface. Input text (in English) Pre-processing of the text Pre-processed text Idiom entries in a dictionary Idiom candidates Output (list of idioms) Surface matching Rule-based filtering Pre-processing In the pre-processing module, we first assign POS information to the input text by using Tree-tagger (TreeTagger, 2004) . After that, we apply the standardisation rules to the surface word forms that occur in texts. This is because, in many cases, the word form in the text is different from the word form in dictionary entries. For example, \"took his seat\" can be found in the text, but what is registered in the dictionary is \"take one's seat\". We adjusted the word form in the text so that it could be matched to the dictionary entries. Four types of formal standardisations are applied at this stage: (1) Inflected forms of verbs are transformed into basic forms. In addition, we added \"do\" or \"doing\" to absorb the matching of idioms whose entries are registered as something like \"cannot help doing\" in the dictionary. (2) Plural forms of nouns are transformed into singular forms. (3) Articles are paradigmatically expanded so that the occurrence \"a\", for instance, can be matched with an entry with \"the\". This may often lead to false matching as some idioms require the strict use of either definite or indefinite articles, but we found we can gain more than we lose by applying this processing, from the point of view of system requirements. (4) Personal pronouns are standardised into basic forms. Table 4 shows the basic standardisation patterns of word forms. Note that in the actual matching, we also retain the original forms. In addition to these, we applied a small amount of pre-processing such as splitting hyphenated words, etc. Input word Pattern (1) verb surface, basic form, do, doing (2) plural noun surface, singular noun (3) particle a, an, the (4) my, his, etc. surface, one's (4) myself, herself, etc. surface, oneself Table 4: Standardisation of words Surface matching In the surface matching module, we carry out an extensive retrieval in which all the possible idiom candidates can be detected. We retrieve idiom entries whose constituents all match the textual sequences of words in order. In the dictionary entries, such examples as \"make A of B\" or \"have ... in\" exist. In the surface matching module, we deal with these \"position fillers\" as wild cards. Figure 2 shows an example of surface matching. In Figure 2 , the dictionary entry \"have one's eye on\" is detected as an idiom candidate, because its constituent words all match the input words in the text. -when an idiom is detected -when words in the text do not correspond to the constituent words of the idiom input sentence idiom entry in dictionary input sentence idiom entry in dictionary ...... had his eye on ...... O have one's eye on ...... had his ears on ..... X have one's eye on Filtering with POS patterns As we emphasise exhaustive retrieval in the surface matching module, many of the idiom candidates detected in the surface matching module are expected to be non-relevant idioms. In the filtering module, we filter out many irrelevant idioms by using the rules constructed on the basis of POSpatterns given in Table 3 . Figure 3 shows an example of filtering. In this case, the surface matching module detected the three idiom candidates \"make a habit of doing\", \"wake up\", \"make after\" for the input text \"I make a habit of stretching after I wake up.\" The basic adjacent patterns given in Table 3 require that what can be inserted between a verb and a preposition is either  an adverb or an adjective. As the construction \"a habit of stretching\" is neither an adverb nor an adjective, the candidate \"make after\" is filtered out and excluded from the final output. Interface of the independent idiom look-up system Figure 4 shows the system interface. The interface consists of an input area, parameter specification area in which the user can specify a window size of a certain number of words within which idiom candidates are to be searched, and an output area. When the user inputs an English text, and clicks the \"search\" button, the system outputs the idiom candidates with their meaning (in Japanese). By moving the mouse over an output idiom, the matched parts of the input text in the input area are emphasised. Figure 4 shows that \"take the plunge\" and \"have one's eye on\" were detected for the input: \"I decided to take the wild plunge and buy the car I had my eye on.\" Integration to the translation editor environment In addition to the independent system interface, we incorporated the idiom look-up system into the integrated translation editor environment QRedit (Abekawa and Kageura, 2007) . Figure 5 shows the interface in which automatic idiom lookup functions within the integrated environment. The screen shot shows that the idiom entry \"(with) one's tongue in one's cheek\" matches the sentence \"He said that with his big fat tongue in his big fat cheek.\" Evaluation We carried out evaluation experiments in order to observe the overall performance of the system, as well as the following three aspects: (1) the effect of standardisation of words; (2) the effect of the POS-based filtering; (3) the overall performance of the system. Experimental setup We observed how many correct idiom candidates our system was able to locate with each set of data. The data set used for evaluation were: (a) 100 sentences containing idiom variations, randomly extracted from the idiom variation data mentioned in section 2, and (b) data consisting of 20 newspaper and journal articles (five articles taken from the BBC online news site, five articles from The Nation website, five articles from the The Independent website, and five articles from the New York Times website). We manually identified and tagged the idioms for these articles. The window size is set to a sentence. For both the data (a) and (b), we compared the method of surface matching only with the method of surface matching and filtering with POS-based information. Correct outputs were defined as follows: (2) Variations of (1) created by replacement of articles. For instance, when \"come to the point\" is the idiom actually used in the text, \"come to a point\" is evaluated as correct output. (3) Variations of (1) created by the singular/plural forms of nouns. When a constituent noun in the actual idiom is plural, such as \"in spirits\", \"in spirit\" is evaluated as correct output. (4) Embedded idioms. When the actual idiom is \"come to the point\", we evaluate \"to the point\" as correct output. These criteria were set on the basis of the consultation with eight online volunteer translators. Translators, when they come across expressions they cannot readily translate analytically, check several possible idiom candidates to reach the final correct translation. As mentioned, checking multiple candidates is not an optional, extraneous process that translators would like to omit if they can, but rather an essential process by which they make sure that their final decision is correct. As such, given that no automatic processing can substitute for human decision making, translators want the automatic idiom look-up system to show multiple candidates that are close to the set of candidates that translators actually check. The criteria given here are an approximation to this. Figure 6 shows the range of the correct output defined here 5 . 5 In an evaluation of a system that provides translation information for human translators, Sharoff et. al. (2006) introduces five grades: 5 = the suggestion is an appropriate translation as it is; 4 = the suggestion can be used with some minor amendment; 3 = the suggestion is useful as a hint for another, appropriate translation; 2 = the suggestion is not useful, even though it is still in the same domain; 1 = the suggestion is totally irrelevant. The grades \"5\", \"4\" and \"3\" can be interpreted as corresponding roughly to (C) in Figure 6 , which includes, but is not limited to, the correct idioms. Evaluation measures From the viewpoint of translation support, it is important for our system to detect all the idioms that appear in the text. Therefore, recall is the most important factor at this stage. Improvements in precision should be elaborated without negatively affecting recall. F-measure is irrelevant. precision = #CorrectSystemOutputs #SystemOutputs recall = #CorrectSystemOutputs #AllCorrectIdioms Result of the experiments Tables 5 and 6 show the results of the experiments. In both sets of data, the recall is as high as 0.97, although the precision varies between the data sets (a) and (b). The high recall is very promising, as the essential problem that prompted us to develop this system was the difficulty experienced by translators in looking up idioms, the improvement of which requires high performance in recall. The precision of 0.5 to 0.7 is in a practically useful range. This figure means that translators are provided with twice as many candidates as they need to check. For most texts this will not be an excessive number. Experimental use by a translator in an integrated editor environment has shown that the problem is more to do with the quality of unnecessary candidates rather than the quantity (because they reduce the translators' expectations of the system). The result of filtering with POS-based patterns for the data set (a) and (b) shows that this filtering improved precision greatly, without negatively affecting recall, which proves the usefulness of POS-based patterns for our aim. The result of experiments with filtering on data set (b) shows that the recall for data set (b) is higher than that for data set (a), but the precision for data set (b) is lower than that for data set (a). The cause of the low precision can be summarised as follows: (1) it is easy for our system to detect incorrect idioms for data set (b), because real-world English texts tend to have longer sentences; (2) there is room for improvement in the filtering rules. On the other hand, the higher recall for data set (b) can be explained by the fact that the manually constructed basic data include some highly creative examples, which are rather difficult to detect, while the real-world data contains less of these extremely creative variations. Diagnosis Upon analysing the results, certain patterns of errors and misses were identified. (1) Errors resulting from insufficiency of lemmatisation by the POS-tagger. For instance, \"She horribly damned him with faint praise\" is based on the idiom \"horribly damn with faint praise\". However, our system could not detect this idiom because \"damned\" was recognised as an adverb rather than the verb \"damn\". This could be avoided by the improvement of the POS-tagging performance. Another related pattern is errors resulting from the errors of POS-taggers and/or lack of parsing. For instance, the system wrongly output \"from high\" to the input text \"I graduated from high school\". This was because, on the one hand, as we do not give structures to input texts, information about the proper construction of \"from (high school)\" is not provided, while on the other hand the dictionary entry \"from high\" was wrongly tagged as \"from:prep high:adj\" instead of \"from:prep high:nn\". Although theoretically it is preferable to use a high-performance parser, many of these errors can practically be avoided by an improvement in the POS-tagging performance. (2) Errors resulting from the lack of restrictions on the side of such idiom entries as \"make A of B\" or \"have ... in\". The dictionary we used does not give detailed information for the slot \"A\", \"B\" and \"...\". As a result, the system output several irrelevant idioms. This problem can be solved by imposing restrictions on each idiom entry with place holders. (3) Misses resulting from input text variations in which long phrases are inserted into the idiom constructions. For instance, the dictionary entry \"take apart\" was not detected for the input text: \"She takes (her daughter-inlaw) apart with stinging criticism.\" In order to deal with this, we need to further our understanding of possible idiom variations. In summary, most of the errors can be avoided (a) if we impose further restrictions on the variation patterns and place holders of idiom entries and (b) if the POS-tagging performance is improved. On the other hand, if we systematically try to deal with the misses, further understanding of the potential range of idiom variations is needed. The experimental results show that the rules we have established cover most of the variations that can be described formally as POS-based patterns. The remaining variations may well be ones that are more context dependent, creative, and/or related to constructions larger than those that can be conveniently described by POS-based patterns. We are currently dealing with misses through experimental use of the system and modifications on the basis of user feedback. Conclusions This paper has reported a method for automatically looking up idiom entries in dictionaries vis-à-vis idiom occurrences in texts that may include variations. We started by defining translators' requirements, and then observed the range of idiom variations and formalised the variation patterns of syntagmatic augmentations as POS-based patterns. The result of the experiment showed that the system performance is very promising. The precision for real-world texts is slightly above 0.5, which is in a practically useful range, as users' satisfaction depends more on an improvement of what is currently provided than on \"ideal\" performance. As for the technical aspect, we used a morphological analyser but not a parser. The experimental evaluation and the error analyses suggested that most errors and misses can be dealt with without delving into the structural level information given by parsers, although this needs further analysis and examination. We are currently working in three mutually related directions: (1) Making the system available for experimental use by translators and obtaining feedback from them, including levels of satisfaction and detailed patterns of errors/misses. We have obtained feedback from two translators and two more translators will take part in the userbased evaluations; (2) Developing a mechanism that deals with paradigmatic replacements. A basic mechanism has already been developed, and we are currently carrying out evaluation experiments; and (3) Refining the algorithms for dealing with variations by syntagmatic augmentation. Acknowledgements This research is partly supported by grant-in-aid (A) 17200018 \"Construction of online multilingual reference tools for aiding translators\" by the Japan Society for the Promotion of Sciences (JSPS). We would like to thank Sanseido Publishing Company for allowing us to use the Grand Concise English-Japanese Dictionary for our experiments.",
         "12237644",
         "91e5acb223de864e644cd86a9d50911fabede9b1",
         "13",
         "https://aclanthology.org/2007.mtsummit-papers.60",
         null,
         "Copenhagen, Denmark",
         "2007",
         "September 10-14",
         "Proceedings of Machine Translation Summit XI: Papers",
         "Takeuchi, Koichi  and\nKanehila, Takashi  and\nHilao, Kazuki  and\nAbekawa, Takeshi  and\nKageura, Kyo",
         "Flexible automatic look-up of {E}nglish idiom entries in dictionaries",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "takeuchi-etal-2007-flexible",
         null,
         null
        ],
        [
         "46",
         "R13-1057",
         "In this paper we cover the problem of recognition of semantic relations between proper names (PNs) in running text. We focus on the manual rule creation approach and discuss to what extent the existing tools can be used for this task. As a result of our initial research we developed a rule-based toolset for recognition of relations between PNs called WCCL Relation. The toolset is built on the top of WCCL Match -a language for text annotation, which is a part of a WCCL framework (an open source, released under the GNU LGPL 3.0). The WCCL Relation toolset is language independent and can be used for almost any natural language and language tagset. We present several use cases and sample rules for recognition of semantic relations in Polish texts.",
         "In this paper we cover the problem of recognition of semantic relations between proper names (PNs) in running text. We focus on the manual rule creation approach and discuss to what extent the existing tools can be used for this task. As a result of our initial research we developed a rule-based toolset for recognition of relations between PNs called WCCL Relation. The toolset is built on the top of WCCL Match -a language for text annotation, which is a part of a WCCL framework (an open source, released under the GNU LGPL 3.0). The WCCL Relation toolset is language independent and can be used for almost any natural language and language tagset. We present several use cases and sample rules for recognition of semantic relations in Polish texts. Introduction Recognition of semantic relations between named entities is one of the information extraction major tasks. Its goal is to identify pairs of named entities (text fragments) connected other by a semantic relation on the basis of their context. In the majority of approaches the named entities are recognised beforehand and the task is limited to discovering and categorising connections between those entities. The list of possible relation categories is unbounded and it depends on the desired application, the scope of the named entities and the available resources. For example Marcińczuk and Ptak (2012) defined 8 coarsegrained categories of semantic relations (location, origin, nationality, affiliation, neighbourhood, creator, composition and alias) . In turn Linguistic Data Consortium (2008) defined a set of 8 general relations (i.e., physical, part-whole, personalsocial, organization-affiliation, agent-artifact and general-affiliation) with several subcategories. In the bioinformatic domain there are two common categories of relations between genes, proteins and associated entities -protein-component and subunit-complex (Pyysalo et al., 2011) . There are two main approaches to relation recognition -construction of human-readable rules and construction of statistical models (machine learning). According to Jiang (2012) the most common approach is the one based on the statistical models. There are also several rule-based approaches, like manual rule creation (Marciniak and Mykowiecka, 2007; Santos et al., 2010; Abacha and Zweigenbaum, 2011) and rule induction (Feldman et al., 2006; Brun and Hagège, 2009) . The low interest in developing rule-based systems might be caused by a lack of robust and accessible tools for rule construction and execution. For example, the well-known general framework GATE (Cunningham et al., 2011) does not support relation recognition within its rule formalism JAPE (Cunningham et al., 2000) . Despite the manual rule creation is less popular than the statistical approaches in the task of relation recognition, the rule-based approaches have several advantages over statistical-based approaches. The first one is the traceability and full control on decisions made by the system. The other one is the ease in manual tuning for new types of text. The last but not least it does not require annotated data. In this paper we investigate the problem of rule creation for recognition of semantic relations between proper names. We present a language independent formalism for rule creation and execution called WCCL Relation. The language is built on top of an open-source framework WCCL (Radziszewski et al., 2011) . We present several use cases of the language applications in the context of recognition of semantic relations between proper names for Polish. Related works Before we decided to create the WCCL Relation toolset we had considered several existing approaches described in the literature. Abacha and Zweigenbaum (2011) used a custom rule notion and a software to develop a set of rules for their task. However, the main emphasis was put on the task definition and discussion of its difficulties. Less effort was made to create a general solution that would result in an universal system or formalism for rule creation and execution. There is another group of works utilizing the Xerox Incremental Parser (XIP). According to Aït-Mokhtar et al. (2002) , XIP is a formalism which allows to recognise n-ary linguistic relations between words or constituents on the basis of global or local structural, topological and/or lexical conditions. Brun and Hagège (2009) used the formalism in semi-supervised rule creation (the rules were used to recognise Olympic games events). Santos et al. (2010) used XIP to create rules for recognition of family relations between people. Despite the formalism looks very promising the distribution and licensing is not clear and the XIP implementation is not freely available. There is also another system called TEG (Feldman et al., 2006) which offers a stochastic contextfree grammar (SCFG) to write rules for recognition of relations between named entities. The system offers a semi-supervised method for rule creation. Unfortunately, according to our best knowledge the system is not publicly accessible. An open-source Python platform for text processing called NLTK 1 (Bird et al., 2009) provides a simple tool to relation recognition based on regex patterns. The patterns are tested against a plain text enriched with part of speech tags. This approach can be suitable for many simple uses cases but it is troublesome to use for languages with rich morphology (each word is described by a set of morphological attributes, not only by the part of speech tag). It also does not support multilayered semantic annotations. Taking into consideration the above solutions we decided to construct a customized toolset for the rule-based relation recognition utilizing an existing open-source framework for text matching and annotation. The following section presents the current version of WCCL Relation toolset. 1 http://nltk.org WCCL Relation WCCL Relation is a toolset designed for a rulebased recognition of relations between pairs of annotations within a sentence in a morphologically tagged and semantically annotated texts. Its grammar is based on the WCCL Match 2 (Marcińczuk and Radziszewski, 2013) and extends it by a new operator for relation creation. A WCCL Relation rule consists of three sections. The first section (match) contains a set of operators used to match a sequence of tokens and annotations (named entities, chunks, etc.). The second section (cond) is optional and contains a set of additional conditions which must be satisfied by the matched elements. The last section (actions) contains a set of operators to be performed on the matched elements. Comparing to the original WCCL Match grammar, the WCCL Relation grammar contains an additional operator called link which allows to create a connection of given category between two matched elements. Below is a sample rule which matches a sequence \"PERSON born in CITY\" and creates a connection between the PERSON and the CITY names of type origin. apply( match( // match annotation of type person_nam is(\"person_nam\"), // group 1 // match word with base form 'born' equal( base[0], \"urodzić\"), // group 2 equal( base[0], \"się\"), // group 3 // match word with base form 'in' equal( base[0], \"w\"), // group 4 // match annotation of type city_nam is(\"city_nam\"), // group 5 ), actions( link(1, \"person_nam\", 5, \"city_nam\", \"origin\") )) WCCL Match offers a set of operators for matching a sequence of elements. Below is a list of operators used in the examples presented in the article 3 : • is(type) -matches an annotation of given type, • equal(base[0], value) -matches a token with a base form equal to value, • inter(base[0], values) -matches a token with a base form present in the array of values, • repeat(op) -matches a sequence of elements matching the op operator, • not(op) -matches a token not matching the op operator, • isannpart(0, type) -matches a token that is a part of an annotation of given type, • and(op1,op2,..,opn) -matches a token if all operators are valid, • oneof(variant1,...,variant2)matches a sequence of elements for the first valid variant, • annsub(token, type) -test if given token is part of an annotation of given type, • agrpp(word1, word2) -test agreement of two given words, • outside(index) -test if given token index is inside sentence boundary, can be used to test if given token is the first or the last token in the sentence, The execution of a WCCL Relation rule consists of three steps (all of them are transparent to user). In the first step, the WCCL Relation rule is transformed into a WCCL Match rule. In this step all the additional operators are transformed to operators valid for WCCL Match. In the second step, the WCCL Match rule is run on a given text. In the last step, the result of matching (set annotations) is interpreted and transformed into a set of relations. Below is the result of transformation the WCCL Relation rule to the WCCL Match rule. Here, the link operator was replaced with a set of three match operators. Case studies In this section we present several use cases already covered by the WCCL Relation toolset. We assumed that the proper names were recognised beforehand using an external tool. For Polish we used a tool called Liner2 4 (Marcińczuk et al., 2013) with a model for 56 categories of proper names. Auxiliary annotations The standard WCCL Match operator mark can be used to introduce the auxiliary annotations which can be referenced by other rules. This simplifies the final rules recognising the relations. For example, a common action is to ignore phrases in parentheses which can separate two named entities. This can be done using the following rule. The rule matches a text that is delimited by a pair of elements: \"(\" and \")\" or \"[\" and \"]\". apply( match( oneof( variant( in(\"(\", base[0]), repeat(not(inter(base[0], [\")\", \"(\"]))), in(\")\", base[0]) ), variant( in(\"[\", base[0]), repeat(not(inter(base[0], [\"]\", \"[\"]))), in(\"]\", base[0]) ) ) ), actions( mark(M, \"parentheses\") )) The following rule extends the previous rule recognising the origin relation between a person name and a city name by including an optional phrase in parentheses after the person name. apply( match( // match annotation of type person_nam is(\"person_nam\"), // group 1 // match optional phrase in parentheses optional(is(\"parentheses\")) // group 2 // match word with base form 'born' equal( base[0], \"urodzić\"), // group 3 equal( base[0], \"się\"), // group 4 // match word with base form 'in' equal( base[0], \"w\"), // group 5 // match annotation of type city_nam is(\"city_nam\"), // group 6 ), actions( link(1, \"person_nam\", 6, \"city_nam\", \"origin\") )) Possessive named entities The following rule is a naïve rule for recognition of a location relation between a person name and a city name (i.e. a person is in a city). apply( match( is(\"person_nam\"), // group 1 // match word with base form 'in' in(\"w\", base[0]), // group 2 is(\"city_nam\") // group 3 ), actions( link(1, \"person_nam\", 3, \"city_nam\", \"location\") )) However this rule is not always true. For example when the person name is an possessive argument of an other subject then the relation does not occur between the person name and the city name but between the possessive phrase and the city name. Consider the following sentence: Pomnik Wojtyły w Krakowie (eng. Wojtyła monument in Kraków). In the sentence it is stated that the monument is located in Kraków and it does not mean that Wojtyła is also in Kraków. In order to handle properly such situations we must recognise the possessive nouns. This can be done with the following rule. This rule test a person name preceded by a noun. If the person name and the noun do not agree in case then the person name is being recognised as possessive phrase. apply( match( in(subst, class[0]), is(\"person_nam\") ), cond( in(subst, class[first(:2)]), not(agrpp(first(:1), first(:2), {cas})) ), actions( mark(M, \"possessive\") )) Now we can add a condition in the cond section to ignore the person names which are part of a possessive phrase. Below is the original rule with the mentioned condition. apply( match( is(\"person_nam\"), // group 1 in(\"w\", base[0]), // group 2, eng. \"in\" is(\"city_nam\") // group 3 ), cond( not(annsub(:1, \"possessive\")) ), actions( link(1, \"person_nam\", 3, \"city_nam\", \"location\") )) Multiple relations The other common situation is recognition of multiple relations within a single matched sequence. Below is a sample rule which matches the sequence \"COUNTRY ( CITY and CITY )\" and creates two links: both city names are connected with the country name as separate relations. apply( match( is(\"country_nam\"), // group 1 inter(base[0], \"(\"), // group 2 is(\"city_nam\"), // group 3 inter(base[0], \"i\"), // group 4 is(\"city_nam\"), // group 5 inter(base[0], \")\"), // group 6 ), actions( link(1,\"country_nam\",3,\"city_nam\",\"location\"), link(1,\"country_nam\",5,\"city_nam\",\"location\") )) Detecting sentences containing only two annotations In some cases when there are only two proper names of given categories in a sentence, the proper names can be connected with a certain relation category no matter of their context. For example, in most case a road name and a city name preceded by a preposition in are connected with a location relation. Below is an auxiliary rule that matches the text fragments not annotated with a road name nor a city name. apply( match( repeat( and( not(isannpart(0,\"road_nam\")), not(isannpart(0,\"city_nam\")) ) ) ), actions( mark(M, \"not_road_city\") )) Using the above auxiliary annotation not_road_city we can construct the following rule (the cond is used to check if the matched sequence spans over a whole sentence). apply( match( is(\"not_road_city\"), // group 1 is(\"road_nam\"), // group 2 is(\"not_road_city\"), // group 3 in(\"w\", base[0]), // group 4, eng. ''in'' is(\"city_nam\"), // group 5 is(\"not_road_city\") // group 6 ), cond( outside(first(M) -1), outside(last(M) + 1) ), actions( link(2, \"road_nam\", 4, \"city_nam\", \"location\") )) Evaluation In the evaluation we used the KPWr corpus (Broda et al., 2012) 5 , which is the only available corpus annotated with semantic relations between proper names for Polish. We followed the evaluation procedure presented by Marcińczuk and Ptak (2012) , where the corpus was divided into three parts: train, tune and test. The train and tune parts were used for the rule development and the test part for the final performance comparison. As the work is still in progress we started from the most numerous relation category in KPWr that is location (about 800 relations). The current set contains 34 rules (6 of them are auxiliary rules). It took about 6 hours to develop the rules. The set covers almost 40% of location relations in the train part, 30% in the tune part and 22% in the test part with the precision between 87-90%. The recall is low but in terms of F-measure the results are comparable with the results obtained for the statistical methods presented by Marcińczuk and Ptak (2012) . On the test part the statistical model obtained 36.09% F-measure with 31.20% precision, while the manually crafted rules obtained already 34.97% F-measure with 87.18% precision. Higher precision is more useful for processing large volumes of texts where recall is not an issue. Our final goal is to construct a set of rules covering all categories of semantic relations present in the KPWr corpus. WCCL Relation is language independent Since the WCCL framework is language independent, also WCCL Relation is language independent. Note, that the rules written for one language are not directly usable for other languages. They can be adopted to another language or tagset but they have to be anywise translated. WCCL Relation can be used to process any language which tagset conforms the following requirements: • the tagset defines a non-empty set of grammatical classes and possibly empty set of attributes; • each grammatical class is assigned a set of attributes that are required for the class and a set of attributes; • each attribute is assigned a set of its possible values; • mnemonics used for grammatical classes and attribute values are unique; • and the tags are represented as a string of comma-separated mnemonics. 7 Input/output format The WCCL Relation rules can be executed in two ways: in a console to process an XML file in the CCL format or in a code using the API. Processing CCL files Below is a sample XML in the CCL format for a sentence \"Eiffel Tower is located in Paris\". The file contains morphological tags for each word and semantic annotations (facility_nam for Eiffel Tower and city_nam for Paris). <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE chunkList SYSTEM \"ccl.dtd\"> <chunkList> <chunk type=\"p\" id=\"ch1\"> <sentence id=\"s1\"> <tok> <orth>Wieża</orth> <!--Tower : facility_nam --> <lex disamb=\"1\"><base>wieża</base> <ctag>subst:sg:nom:f</ctag></lex> <ann chan=\"city_nam\">0</ann> <ann chan=\"facility_nam\">1</ann> </tok> <tok> <orth>Eiffla</orth> <!--Eiffel : facility_nam--> <lex disamb=\"1\"><base>Eiffel</base> <ctag>subst:sg:gen:m1</ctag></lex> <ann chan=\"city_nam\">0</ann> <ann chan=\"facility_nam\">1</ann> </tok> <tok> <orth>znajduje</orth> <!--is located --> <lex disamb=\"1\"><base>znajdować</base> <ctag>fin:sg:ter:imperf</ctag></lex> <lex disamb=\"1\"><base>znajdywać</base> <ctag>fin:sg:ter:imperf</ctag></lex> <ann chan=\"city_nam\">0</ann> <ann chan=\"facility_nam\">0</ann> </tok> <tok> <orth>się</orth> <lex disamb=\"1\"><base>się</base> <ctag>qub</ctag></lex> <ann chan=\"city_nam\">0</ann> <ann chan=\"facility_nam\">0</ann> </tok> <tok> <orth>w</orth> <!--in --> <lex disamb=\"1\"><base>w</base> <ctag>prep:loc:nwok</ctag></lex> <ann chan=\"city_nam\">0</ann> <ann chan=\"facility_nam\">0</ann> </tok> <tok> <orth>Paryżu</orth> <!--Paris : city_nam --> <lex disamb=\"1\"><base>Paryż</base> <ctag>subst:sg:loc:m3</ctag></lex> <ann chan=\"city_nam\">1</ann> <ann chan=\"facility_nam\">0</ann> </tok> </sentence> </chunk> </chunkList> Below is an XML output generated by the tool containing a single semantic relation of type location between Eiffel Tower and Paris. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE chunkList SYSTEM \"ccl.dtd\"> <relations> <rel name=\"location\" set=\"Syntactic relations\"> <from sent=\"s1\" chan=\"facility_nam\">1</to> <to sent=\"s1\" chan=\"city_nam\">1</from> </rel> </relations> Using API The WCCL Relation tool provides set of API functions in Python to execute the rules directly in the code. Below we present a very brief description of the API. More information and examples can be found on the following page: http:// nlp.pwr.wroc.pl/wccl-relation. The API provides the following functions: • process_file(filepath) -process a single CCL file, • process_files(filepaths) -process a set of CCL files, • process_sentence(sentence) -process a single sentence represented as an object of class corpus2.AnnotatedSentence 6 , • process_document(document)process a single document represented as on object of class corpus2.DocumentPtr 6 . All the presented functions return a set of objects of class corpus2.Relation 6 representing the recognised relations. Conclusion and future work In the paper we presented a result of ongoing work on creation a language independent rule-based toolset for recognition of relations between named entities, called WCCL Relation. The toolset is build on the top of an open-source framework called WCCL. A set of use cases for recognition of semantic relations between proper names for Polish was presented. WCCL Relation is build on the top of an opensource framework called WCCL which is implemented in C++ and its source code is released under GNU LGPL 3.0 7 . WCCL Relation has a form of a Python script that is also released under the same license 8 . The described work is still in progress. On one hand we are still working on a set of rules for recognition of 8 categories of semantic relations between PNs for Polish. On the other hand we are still extending the WCCL Relation toolset with 6 http://nlp.pwr.wroc.pl/redmine/ projects/corpus2/wiki 7 http://www.nlp.pwr.wroc.pl/wccl. 8 http://nlp.pwr.wroc.pl/wccl-relation. new features. One of the planned features is a support for names enumerations. The other are access to word dependency features, tests on distance between matched elements and support for relations between nested annotations. Acknowledgments The work was funded by the NCBiR NrU.: SP/I/1/77065/10.",
         "11921808",
         "c3304ae10d63d95ee00305190b06ac01e65ebd21",
         "0",
         "https://aclanthology.org/R13-1057",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Marci{\\'n}czuk, Micha{\\l}",
         "{WCCL} Relation {---} a Toolset for Rule-based Recognition of Semantic Relations Between Named Entities",
         "436--442",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "marcinczuk-2013-wccl",
         null,
         null
        ],
        [
         "47",
         "W05-0833",
         "Way and Gough, 2005) provide an indepth comparison of their Example-Based Machine Translation (EBMT) system with a Statistical Machine Translation (SMT) system constructed from freely available tools. According to a wide variety of automatic evaluation metrics, they demonstrated that their EBMT system outperformed the SMT system by a factor of two to one. Nevertheless, they did not test their EBMT system against a phrase-based SMT system. Obtaining their training and test data for English-French, we carry out a number of experiments using the Pharaoh SMT Decoder. While better results are seen when Pharaoh is seeded with Giza++ word-and phrase-based data compared to EBMT sub-sentential alignments, in general better results are obtained when combinations of this 'hybrid' data is used to construct the translation and probability models. While for the most part the EBMT system of (Gough & Way, 2004b) outperforms any flavour of the phrasebased SMT systems constructed in our experiments, combining the data sets automatically induced by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French-English.",
         "Way and Gough, 2005) provide an indepth comparison of their Example-Based Machine Translation (EBMT) system with a Statistical Machine Translation (SMT) system constructed from freely available tools. According to a wide variety of automatic evaluation metrics, they demonstrated that their EBMT system outperformed the SMT system by a factor of two to one. Nevertheless, they did not test their EBMT system against a phrase-based SMT system. Obtaining their training and test data for English-French, we carry out a number of experiments using the Pharaoh SMT Decoder. While better results are seen when Pharaoh is seeded with Giza++ word-and phrase-based data compared to EBMT sub-sentential alignments, in general better results are obtained when combinations of this 'hybrid' data is used to construct the translation and probability models. While for the most part the EBMT system of (Gough & Way, 2004b) outperforms any flavour of the phrasebased SMT systems constructed in our experiments, combining the data sets automatically induced by both Giza++ and their EBMT system leads to a hybrid system which improves on the EBMT system per se for French-English. 1 Introduction (Way and Gough, 2005) provide what are to our knowledge the first published results comparing Example-Based and Statistical models of Machine Translation (MT). Given that most MT research carried out today is corpus-based, it is somewhat surprising that until quite recently no qualitative research existed on the relative performance of the two approaches. This may be due to a number of factors: the relative unavailability of EBMT systems, the lack of participation of EBMT researchers in competitive evaluations or the dominance in the MT research community of the SMT approach-whenever one paradigm finds favour with the clear majority of MT practitioners, the assumption made by most of the community is that this way of doing things is clearly better than the alternatives. Like (Way and Gough, 2005) , we find this regrettable: the only basis on which such views should be allowed to permeate our field is following extensive testing and evaluation. Nonetheless, given that no EBMT systems are freely available, very few research groups are in the position of being able to carry out such work. This paper extends the work of (Way and Gough, 2005) by testing EBMT against phrase-based models of SMT, rather than the word-based models used in this previous work. In so doing, it provides a more complete evaluation of the main question at hand, namely whether an SMT system outperforms an EBMT system on reasonably large training and test sets. We obtained the same training and test data used in (Way and Gough, 2005) , and evaluated a number of SMT systems which use the Pharaoh decoder 1 against the Marker-Based EBMT system of (Gough & Way, 2004b) , for French-English and English-French. We provide results using a range of automatic evaluation metrics: BLEU (Papineni et al., 2002) , Precision and Recall (Turian et al., 2003) , and Word-and Sentence Error Rates. (Way and Gough, 2005) observe that EBMT tends to outperform a word-based SMT model, and our experiments show that a number of different phrase-based SMT systems still tend to fall short of the quality obtained via EBMT for these evaluation metrics. However, when Pharaoh is seeded with the data sets automatically induced by both Giza++ and their EBMT system, better results are seen for French-English than for the EBMT system per se. The remainder of the paper is constructed as follows. In section 2, we summarize the main ideas behind typical models of SMT and EBMT, as well as the EBMT system of (Gough & Way, 2004b) used in our experiments. In section 3, we revisit the experiments and results carried out by (Way and Gough, 2005) . In section 4, we describe our extensions to their work, and compare their findings to ours, and in section 5, present a number of hybrid SMT models. Finally, we conclude and offer some thoughts for future work in section 6, and in section 7 present some further comments on the narrowing gap between EBMT and phrase-based SMT. Example-Based and Statistical Models of Translation A sine qua non for both EBMT and SMT is a set of sentences in one language aligned with their translations in another. Although similar in that both models of translation automatically induce translation knowledge from this resource, there are significant differences regarding both the type of information learnt and how this is brought to bear in dealing with new input. EBMT Given a new input string, EBMT models use three separate processes in order to derive translations: 1 http://www.isi.edu/licensed-sw/pharaoh/ 1. Searching the source side of the bitext for 'close' matches and their translations; 2. Determining the sub-sentential translation links in those retrieved examples; 3. Recombining relevant parts of the target translation links to derive the translation. Searching for the best matches involves determining a similarity metric based on word occurrences and part-of-speech labels, generalised templates and bilingual dictionaries. The recombination process depends on the nature of the examples used in the first place, which may include aligning phrasestructure (sub-)trees (Hearne & Way, 2003) or dependency trees (Watanabe et al., 2003) , or using placeables (Brown, 1999) as indicators of chunk boundaries. Another method-and the one used in the EBMT system used in our experiments-is to use a set of closed-class words to segment aligned source and target sentences and to derive an additional set of lexical and phrasal resources. (Gough & Way, 2004b) base their work on the 'Marker Hypothesis' (Green, 1979) , a universal psycholinguistic constraint which posits that languages are 'marked' for syntactic structure at surface level by a closed set of specific lexemes and morphemes. In a preprocessing stage, (Gough & Way, 2004b) use 7 sets of marker words for English and French (e.g. determiners, quantifiers, conjunctions etc.) , which together with cognate matches and mutual information scores are used to derive three new data sources: sets of marker chunks, generalised templates and a lexicon. In order to describe this in more detail, we revisit an example from (Gough & Way, 2004a) , namely: (1) each layer has a layer number =⇒chaque couche a un nombre de la couche From the sentence pair in (1), the strings in (2) are generated, where marker words are automatically tagged with their marker categories: (2) <QUANT> each layer has <DET> a layer number =⇒<QUANT> chaque couche a <DET> un nombre <PREP> de la couche Taking into account marker tag information (label, and relative sentence position), and lexical similarity, the marker chunks in (3) are automatically generated from the marker-tagged strings in (2): (3) a. <QUANT> each layer has: <QUANT> chaque couche a b. <DET> a layer number: <DET> un nombre de la couche (3b) shows that n:m alignments are possible (the two French marker chunks un nombre and de la couche are absorbed into one following the lexical similarities between layer and couche and number and nombre, respectively) given the sub-sentential alignment algorithm of (Gough & Way, 2004b) . By generalising over the marker lexicon, a set of marker templates is produced by replacing the marker word by its relevant tag. From the examples in (3), the generalised templates in (4) are derived: (4) a. <QUANT> layer has: <QUANT> couche a b. <DET> layer number: <DET> nombre de la couche These templates increase the robustness of the system and make the matching process more flexible. Now any marker word can be inserted after the relevant tag if it appears with its translation in the lexicon, so that (say) the layer number can now be handled by the generalised template in (4b) and inserting a (or all) translation(s) for the in the system's lexicon. Word-and Phrase-Based SMT SMT systems require two large probability tables in order to generate translations of new input: 1. a translation model induced from a large amount of bilingual data; 2. a target language model induced from a(n even) large(r) quantity of separate monolingual text. Essentially, the translation model establishes the set of target language words (and more recently, phrases) which are most likely to be useful in translating the source string, while the language model tries to assemble these words (and phrases) in the most likely target word order. The language model is trained by determining all bigram and/or trigram frequency distributions occurring in the training data, while the translation model takes into account source and target word (and phrase) co-occurrence frequencies, sentence lengths and the relative sentence positions of source and target words. Until quite recently, SMT models of translation were based on the simple word alignment models of (Brown et al., 1990) . Nowadays, however, SMT practitioners also get their systems to learn phrasal as well as lexical alignments (e.g. (Koehn et al., 2003) ; (Och, 2003) ). Unsurprisingly, the quality obtained by today's phrase-based SMT systems is considerably better than that obtained by the poorer word-based models. 3 Comparing EBMT and Word-Based SMT (Way and Gough, 2005) obtained a large translation memory from Sun Microsystems containing 207,468 English-French sentence pairs, of which 3,939 sentence pairs were randomly extracted as a test set, with the remaining 203,529 sentences used as training data. The average sentence length for the English test set was 13.1 words and 15.2 words for the corresponding French test set. The EBMT system used was their Marker-based system as described in section 2.1 above. In order to create the necessary SMT language and translation models, they used: • Giza++ (Och & Ney, 2003) ; 2 • the CMU-Cambridge statistical toolkit; 3 • the ISI ReWrite Decoder. 4   Translation was performed from English-French and French-English, and the resulting translations were evaluated using a range of automatic metrics: BLEU (Papineni et al., 2002) , Precision and Recall (Turian et al., 2003) , and Word-and Sentence Error Rates. In order to see whether the amount of training data affected the (relative) performance of the EBMT and SMT systems, (Way and Gough, 2005) split the training data into three sets, of 50K (1.1M words), 100K (2.4M words) and 203K (4.8M words) sentence pairs (TS1-TS3 in what follows). English-French Results Table 1 : Comparing the EBMT system of (Gough & Way, 2004b ) with a Word-Based SMT (WB-SMT) system for English-French. The results obtained by (Gough & Way, 2004b ) for English-French for their EBMT system and word-based SMT (WB-SMT) are given in Table 1 . Essentially, all the automatic evaluation metrics bar one (Precision) suggest that EBMT can outperform SMT from English-French. Surprisingly, however, apart from SER, all evaluation scores are higher using 100K sentence pairs as training data rather than the full 203K sentences. It is generally assumed that increasing the size of the training data for corpusbased MT systems will improve the quality of the output translations. (Way and Gough, 2005) observe that while this dip in performance may be due to a degree of over-fitting, they intend to carry out some variance analysis on these results (e.g. performing bootstrap-resampling on the test set (Koehn, 2004 )), or re-test with different sample test sets in order to investigate whether the same phenomenon is observed. BLEU With respect to SER, however, for both SMT and EBMT, the figures improve as more training data is made available. However, the improvement is much more significant for EBMT (20.6%) than for SMT (0.1%). While the WER scores are much the same, indicating that both systems are identifying reasonable target vocabulary that should appear in the output translation, the vast differences in SER using TS3 indicate that a system containing essentially no information about target syntax has very little hope of arranging these target words in the right order. On the contrary, even a system containing some basic knowledge of how phrases fit together such as the Marker-based EBMT system of (Gough & Way, 2004b) will generate translations of far higher quality. French-English Results Table 2: Comparing the EBMT system of (Gough & Way, 2004b ) with a WB-SMT system for French-English. The results obtained by (Way and Gough, 2005) for French-English translations are presented in Table 2. Translating in this language direction is inherently 'easier' than for English-French as far fewer agreement errors and cases of boundary friction are likely. Accordingly, all WB-SMT results in Table 2 are better than for the reverse direction, while for EBMT, improved results are to be seen for BLEU, Recall and SER. BLEU While the majority of metrics obtained for English-French indicate that EBMT outperforms WB-SMT, the results for French-English are by no means as conclusive. Of the 15 tests, WB-SMT outperforms EBMT in nine. Comparing EBMT and Phrase-Based SMT From the results in the previous sections for French-English and for English-French, (Way and Gough, 2005) observe that EBMT outperforms WB-SMT in the majority of tests. If we are to treat each of the metrics as being equally significant, it can be said that EBMT appears to outperform WB-SMT by a factor of two to one. In fact, the only metric for which EBMT seems to consistently underperform is precision for French-English which, when we examine WER, indicates that the EBMT system's knowledge of word correspondences is incomplete and not as comprehensive as that of the WB-SMT system. However, it has been apparent for some time now that phrase-based SMT outperforms previous systems using word-based models. The results obtained by (Way and Gough, 2005) for SER also indicate that if phrase-based SMT were used, then improvements in translation quality ought to be seen. Accordingly, in this section we describe a set of experiments which extends the work of (Way and Gough, 2005) by evaluating the Marker-based EBMT system of (Gough & Way, 2004b ) against a phrase-based SMT system built using the following components: • Giza++, to extract the word-level correspondences; • The Giza++ word alignments are then refined and used to extract phrasal alignments ( (Och & Ney, 2003) ; or (Koehn et al., 2003) for a more recent implementation); • Probabilities of the extracted phrases are calculated from relative frequencies; • The resulting phrase translation table is passed to the Pharaoh phrase-based SMT decoder which along with SRI language modelling toolkit 5 performs translation. We seeded the phrase-based SMT system constructed from the publicly available resources listed above with the word-and phrase-alignments derived via both Giza++ and the Marker-Based EBMT system of (Gough & Way, 2004b) . Using the full 203K training set of (Gough & Way, 2004b) , and testing on their near 4K test set, the results are given in Table 3. It is clear to see that the Giza++ alignments obtain better scores than the EBMT sub-sentential data. Before one considers the full impact of these results, one should take into account that the size of the EBMT data set (word-and phrase-alignments) is 403,317, while there are over four times as many SMT sub-sentential alignments (1, 732, 715) . English-French Results Comparing these results with those in Table 1 , we can see that for the same training-test data, the phrase-based SMT system outperforms the WB-SMT system on most metrics, considerably so with respect to BLEU score (.3753 vs. .3223) . WER, however, is somewhat worse (.585 vs. .535), and SER remains disappointingly high. Compared to the EBMT system of (Gough & Way, 2004b) , the phrase-based SMT system still falls well short with respect to BLEU score (.4409 for EBMT vs. .3573 for SMT), and again, notably for SER (.656 EBMT, .868 SMT). Again, the phrase-based SMT system was seeded with the Giza++ and EBMT alignments, trained on the full 203K training set, and tested on the 4K test set. The results are given in Table 4 . As for English-French, the Giza++ alignments obtain better scores than when the EBMT sub-sentential data is used. French-English Results Comparing these results with those in Table 2 , we see that the phrase-based SMT system actually does worse than WB-SMT, which is an unexpected result 6 . As expected, therefore, the results for phrasebased SMT here are worse still compared to EBMT. Towards Hybridity: Merging SMT and EBMT Alignments We decided to experiment further by combining parts of the EBMT sub-sentential alignments with parts of the data induced by Giza++. In the following sections, for both English-French and French-English, we seed the Pharaoh phrase-based SMT system with: 1. the EBMT phrase-alignments with the Giza++ word-alignments; 2. all the EBMT and Giza++ sub-sentential alignments (both words and phrases). Giza++ Words and EBMT Phrases Here we seeded Pharaoh with the word-alignments induced by Giza++ and the EBMT phrasal chunks only (i.e. no Giza++ phrases and no EBMT lexical alignments). Using the full 203K training set of (Gough & Way, 2004b) , and testing on their near 4K test set, the results are given in Table 5 . Comparing these figures to those in Table 3 , we can see that all automatic evaluation metrics improve with this hybrid system configuration. Note that the data set size is 430,336, compared to 1.73M for the phrase-based SMT system seeded solely with Giza++ alignments. With respect to the EBMT system per se in Table 1 , these results remain slightly below those figures (except for precision). Running the same experimental set up for the reverse language direction gives the results in Table 6 . While recall drops slightly, all the other metrics show a slight increase compared to the performance obtained when Pharaoh is seeded with Giza++ wordand phrase-alignments (cf. Table 4 ). English-French Results French-English Results Merging All Data The following two experiments were carried out by seeding Pharaoh with all the EBMT and Giza++ sub-sentential alignments, i.e. both words and phrases. Inserting all Giza++ and EBMT data into Pharaoh's knowledge sources gives the results in Table 7. These are considerably better than the scores for the 'semi-hybrid' system described in section 5.1.1. This indicates that a phrase-based SMT system is likely to perform better when EBMT wordand phrase-alignments are used in the calculation of the translation and target language probability models. Note, however, that the size of the data set increases to over 2M items. Despite this, compared to the results for the EBMT system of (Gough & Way, 2004b) shown in Table 1 , these results for the 'fully hybrid' SMT system still fall somewhat short (except for Precision: .6727 vs. .7026). Carrying out a similar experiment for the reverse language direction gives the results in Table 8 . This time this hybrid SMT system does outperform the EBMT system of (Gough & Way, 2004b) , with respect to BLEU score (.4888 vs .4611) and Precision (.6927 vs. 6782), but the EBMT system still wins out where Recall, WER and SER are concerned. Regarding this latter, it seems that the correlation between low SER and high BLEU score is not as important as is claimed in (Way and Gough, 2005) . English-French Results French-English Results 6 Conclusions (Way and Gough, 2005) carried out a number of experiments designed to test their large-scale Marker-Based EBMT system described in (Gough & Way, 2004b) against a WB-SMT system constructed from publicly available tools. While the results were a little mixed, the EBMT system won out overall. Nonetheless, WB-SMT has long been abandoned in favour of phrase-based models. We extended the work of (Way and Gough, 2005) by performing a range of experiments using the Pharaoh phrasebased decoder. Our main observations are as follows: • Seeding Pharaoh with word-and phrasealignments induced via Giza++ generates better results than if EBMT sub-sentential data is used. • Seeding Pharaoh with a 'hybrid' dataset of Giza++ word alignments and EBMT phrases improves over the baseline phrase-based SMT system primed solely with Giza++ data. This would appear to indicate that the quality of the EBMT phrases is better than the SMT phrases, and that SMT practitioners should use EBMT phrasal data in the calculating of their language and translation models, if available. • Seeding Pharaoh with all data induced by Giza++ and the EBMT system leads to the bestperforming hybrid SMT system: for English-French, as well as EBMT phrasal data, EBMT word alignments also contribute positively, but the EBMT system per se still wins out (except for Precision); for French-English, however, our hybrid Example-Based SMT system outperforms the EBMT system of (Gough & Way, 2004b ) (cf. Table 9 ). A number of avenues of further work remain open to us. We would like to extend our investigations into hybrid example-based statistical approaches to machine translation by experiment with seeding the Marker-Based system of (Gough & Way, 2004b) with the SMT data, and combinations thereof with the EBMT sub-sentential alignments, to investigate the effect on translation quality. Given our findings here, we are optimistic that 'hybrid statistical EBMT' will outperform the baseline EBMT system, and that our findings will prompt EBMT practitioners to augment their data resources with SMT alignments, something which to our knowledge is currently not done. In addition, we intend to continue this line of research on different and larger data sets, and for other language pairs. Final Remarks Finally, as (Way and Gough, 2005) observe, it is difficult to explain why to this day SMT practitioners have not made full use of the large body of existing work on EBMT, from (Nagao, 1984) to (Carl & Way, 2003) and beyond, which has contributed greatly to the field of corpus-based MT. From its very inception EBMT has made use of a range of sub-sentential data -both phrasal and lexical -to perform translations whereas, until quite recently, SMT models of translation were based on the relatively simple word alignment models of (Brown et al., 1990) . With the advent of phrase-based SMT systems the line between EBMT and SMT has become significantly blurred, yet we are still unaware of any papers on SMT which acknowledge their debt to EBMT or which describe their approach as 'example-based'. Despite it becoming increasingly difficulty to distinguish between EBMT and (phrase-based) SMT models of translation, some differences still exist. Rather than using models of syntax in a post hoc fashion, as is the case with most SMT systems, an EBMT model of translation builds in syntax at its core. Given this, a phrase-based SMT system is more likely to 'learn' chunks that an EBMT system would not, as the system learns n-gram sequences rather than syntactically-motivated phrases per se. Furthermore, our research here has demonstrated quite clearly that if available, merging SMT and EBMT data improves the quality of the resulting hybrid SMT system, as phrases extracted by both methods that are more likely to function as syntactic units (and therefore be more beneficial during the translation process) are given a higher statistical significance. Conversely, the probabilities of those 'less useful' SMT n-grams that are not also gener-ated by the EBMT system are reduced. Essentially, the EBMT data helps the SMT system to make the best use of phrase alignments during translation. Moreover, we see the fact that it is becoming increasingly difficult to describe the differences between EBMT and SMT as a good thing, and that as here, this convergence can lead to hybrid systems capable of outperforming leading EBMT systems as well as state-of-the-art phrase-based SMT. We hope that the research presented here, together with that begun by (Way and Gough, 2005) , will lead to new areas of collaboration between both sets of researchers, to the clear benefit of the MT research community and the wider public. Acknowledgements We would like to thank Nano Gough for supplying us with our EBMT training data. Thanks also to three anonymous reviewers for their insightful comments. The work presented in this paper is partly supported by an IRCSET 7 PhD Fellowship Award.",
         "3011925",
         "cd0ec1b847811191c0cc4684fbe54ea997c938d9",
         "60",
         "https://aclanthology.org/W05-0833",
         "Association for Computational Linguistics",
         "Ann Arbor, Michigan",
         "2005",
         "June",
         "Proceedings of the {ACL} Workshop on Building and Using Parallel Texts",
         "Groves, Declan  and\nWay, Andy",
         "Hybrid Example-Based {SMT}: the Best of Both Worlds?",
         "183--190",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "groves-way-2005-hybrid",
         null,
         null
        ],
        [
         "48",
         "W04-0402",
         "Some particular classes of lexical paraphrases such as verb alteration and compound noun decomposition can be handled by a handful of general rules and lexical semantic knowledge. In this paper, we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs). We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements. We also propose a refinement of an existing LCS dictionary. Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases.",
         "Some particular classes of lexical paraphrases such as verb alteration and compound noun decomposition can be handled by a handful of general rules and lexical semantic knowledge. In this paper, we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs). We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements. We also propose a refinement of an existing LCS dictionary. Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases. Introduction Automatic paraphrase generation technology offers the potential to bridge gaps between the authors and readers of documents. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader (Carroll et al., 1999; Inui et al., 2003) . In Japanese, like other languages, there are several classes of paraphrasing that exhibit a degree of regularity that allows them to be explained by a handful of sophisticated general rules and lexical semantic knowledge. For example, paraphrases associated with voice alteration, verb/case alteration, compounds, and lexical derivations all fall into such classes. In this paper, we focus our discussion on another useful class of paraphrases, namely, the paraphrasing of light-verb constructions (LVCs), and propose a computational model for generating paraphrases of this class. Sentence (1s) is an example of an LVC 1 . An LVC is a verb phrase (\"kandou-o ataeta (made an impression)\" in (1s)) that consists of a light-verb (\"ataeta (give-PAST)\") that grammatically governs a nomi-nalized verb (\"kandou (an impression)\") (also see Figure 1 in Section 2.2). A paraphrase of (1s) is sentence (1t), in which the nominalized verb functions as the main verb with its verbal form (\"kandou-s-as e-ta (be impressed-CAU, PAST)\"). (1) s. Eiga-ga kare-ni kandou-o ataeta. film-NOM him-DAT impression-ACC give-PAST The film made an impression on him. t. Eiga-ga kare-o kandou-s-ase-ta. film-NOM him-ACC be impressed-CAUSATIVE, PAST The film impressed him. To generate this type of paraphrase, we need a computational model that is capable of the following two classes of choice (also see Section 2.2): Selection of the voice: The model needs to be able to choose the voice of the target sentence from active, passive, causative, etc. In example (1), the causative voice is chosen, which is indicated by the auxiliary verb \"ase (causative)\". Reassignment of the cases: The model needs to be able to reassign a case marker to each argument of the main verb. In (1), the grammatical case of \"kare (him),\" which was originally assigned the dative case, is changed to accusative. The task is not as simple as it may seem, because both decisions depend not only on the syntactic and semantic attributes of the light-verb, but also on those of the nominalized verb (Muraki, 1991) . In this paper, we propose a novel lexical semantics-based account of the LVC paraphrasing, which uses the theory of Lexical Conceptual Structure (LCS) of Japanese verbs (Kageyama, 1996; Takeuchi et al., 2001) . The theory of LCS offers an advantage as the basis of lexical resources for paraphrasing, because it has been developed to explain varieties of linguistic phenomena including lexical derivations, the construction of compounds, and verb alteration (Levin, 1993; Dorr et al., 1995; Kageyama, 1996; Takeuchi et al., 2001) , all of which are associated with the systematic paraphrasing we mentioned above. The paraphrasing associated with LVCs is not idiosyncratic to Japanese but also appears commonly in other languages such as English (Mel'čuk and Polguère, 1987; Iordanskaja et al., 1991; Dras, 1999, etc.) Our approach raises the interesting issue of whether the paraphrasing of LVCs can be modeled in an analogous way across languages. Our aim in this paper are: (i) exploring the regularity of the LVC paraphrasing based a lexical semantics-based account, and (ii) assessing the immature Japanese semantic typology through a practical task. The following sections describe our motivation, target, and related work on LVC paraphrasing (Section 2), the basics of LCS and the refinements we made (Section 3), our paraphrasing model (Section 4), and our experiments (Section 5). Finally, we conclude this paper with a brief of description of work to be done in the future (Section 6). Motivation, target, and related work Motivation One of the critical issues that we face in paraphrase generation is how to develop and maintain knowledge resources that covers a sufficiently wide range of paraphrasing patterns such as those indicating that \"to make an attempt\" can be paraphrased into \"to attempt,\" and that \"potential\" can be paraphrased into \"possibility.\" Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001) ; those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.) . This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: \"X is purchased by Y ⇒ Y buys X.\" (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (\"purchase ⇒ buy\") and a voice activization (\"X be VERB-PP by Y ⇒ Y VERB X\"). If we were to use an acquisition scheme that is not capable of decomposing such complex paraphrases correctly, we would have to collect a combinatorial number of paraphrases to gain the required coverage. Second, the results of automatic acquisition would likely include many inappropriate patterns, which would require manual correction. Manual correction, however, would be impractical if we were collecting a combinatorial number of patterns. Our approach to this dilemma is as follows: first, we manually develop the resources needed to cover those paraphrases that appear regularly, and then decompose and automatically refine the acquired paraphrasing patterns using those resources. The work reported in this paper is aimed at this resource development. Target structure and required operations Figure 1 shows the range which the LVC paraphrasing affects, where the solid boxes denote Japanese base-chunk so-called \"bunsetsu.\" 2 Being involved in the paraphrasing, the modifiers of the LVC need the following operations: Change of the dependence: The dependences of the elements (a) and (b) need to be changed because the original modifiee, the light-verb, is eliminated by the paraphrasing. Re-conjugation: The conjugation form of the elements (d), (e), and occasionally (c) need to be changed according to the category change of their modifiee, the nominalized verb. Reassignment of the cases: As described in the previous section, the case markers of the elements (b) and often (c) need to be reassigned. Selection of the voice: The voice of the nominalized verb needs to be chosen according to the combination of the nominalized verb, the lightverb, and the original voice. The first two operations are trivial in the field of text generation. Moreover, they can be done independently of the LVC paraphrasing. The most delicate operation is for the element (c) because it acts either as an adverb or as a case, relying on the con- The school (Theme) locates near the river (Goal). maintain [x CONTROL [y BE AT z]] He (Agent) maintains a machine (Theme) in good condition (Goal). text. In the former case, it needs the second operation. In the latter case, it needs the third operation as well as the element (b). In this paper, we take into account only the element (b), namely, the sibling cases of the nominalized verb. Related work Based on the Meaning-Text Theory (Mel'čuk and Polguère, 1987) , Iordanskaja et al. (1991) proposes a set of paraphrasing rules including one for LVC paraphrasing. Their rule heavily relies on what are called lexical functions, by which they virtually specify all the choices relevant to LVC paraphrasing for every combination of nominalized verb and light-verb individually. Our approach is to employ lexical semantics to provide a general account of those classes of choices. On the other hand, Kaji and Kurohashi (2004) proposes a paraphrasing model which bases on an ordinary dictionary. Given an input LVC, their model paraphrases it using the gloss of both the nominalized verb and the light-verb with the semantic feature of the light-verb. Their model looks robust because of the availability of an ordinary dictionary. However, their model fails to explain the difference in the voice selection between examples (5) and ( 6 ) since it selects the voice based only on the light-verb -in their approach, the lightverb \"ukeru (to receive)\" always maps to the passive voice irrespective of the nominalized verb. (5) s. Enkai-eno shoutai-o uketa. party-GEN invitation-ACC receive-PAST I received an invitation to the party. t. Enkai-ni shoutai-s-are-ta. party-DAT invite-PAS, PAST I was invited to the party. (6) s. Kare-no hanashi-ni his-GEN talk-DAT kandou-o uketa. impression-ACC receive-PAST I was given a good impression by his talk. t. Kare-no hanashi-ni kandou-shi-ta. his-GEN talk-DAT be impressed-ACT, PAST I was impressed by his talk. In (Kaji and Kurohashi, 2004) , the target expression is restricted only to the LVC itself (also see Figure 1 ). Hence, their model is unable to reassign the cases as we saw in example (1). Lexical Conceptual Structure Basic framework of LCS The theory of Lexical Conceptual Structure (LCS) associates a verb with a semantic structure as exemplified by Table 1 . An LCS consists of semantic predicates (\"CONTROL,\" \"BE AT,\" etc.) and their argument slots (x, y, z). Argument slots x, y, and z correspond to the semantic roles \"Agent,\" \"Theme,\" and \"Goal,\" respectively. Taking the LCS of the verb \"transmit\" as an example, [y MOVE TO z] denotes the state of affairs that the state of the \"Theme\" changes to the \"Goal,\" and [x CONTROL . . .] denotes that the \"Agent\" causes the state change. Refinements We make use of the TLCS dictionary, a Japanese verb LCS dictionary developed by Takeuchi et al. (2001) , because it offers the following advantages: • It is based on solid linguistic work, as in (Kageyama, 1996) . • Its scale is considerably larger than any other existing collections of verb LCS entries. • It provides a set of concrete rules for LCS assignment, which ensures the reliability of the dictionary. In spite of these advantages, our preliminary examination of the dictionary revealed that further refinements were needed. To refine the typology of TLCS, we collected the following sets of words: Nominalized verbs: We regard \"sahen-nouns\" 4  and nominal forms of verbs as nominalized verbs. We retrieved 1,210 nominalized verbs from the TLCS dictionary. Light-verbs: Since a verb takes different meanings when it is a part of LVCs with different case particles, we collected pairs c, v of case particle c and verb v in the following way: Step 1. We collected 876,101 types of triplets n, c, v of nominalized verb n, case particle c, and base form of verb v from the parsed 5 sentences of newspaper articles 6 . 4 A sahen-noun is a verbal noun in Japanese, which acts as a verb in the form of \"sahen-noun + suru\". 5 We used the statistical Japanese dependency parser CaboCha (Kudo and Matsumoto, 2002) for parsing. http://chasen.naist.jp/˜taku/software/cabocha/ 6 Excerpts from 9 years of the Mainichi Shinbun and 10 years of the Nihon Keizai Shinbun, giving a total of 25,061,504 sentences, were used. Step 2. For each of the 50 most frequent c, v tuples, we extracted the 10 most frequent n, c, v . Step 3. Each n, c, v was manually evaluated to determine whether it was an LVC. If any of 10 triplets was determined to be an LVC, c, v was merged into the list of light-verbs. As a result, we collected 40 types of c, v for light-verbs. Through investigating the above 1,210 nominalized verbs and 40 light-verbs, we extended the typology of TLCS as shown below (also see Table 2 ). Ext. 1. Treatment of \"Partner\": The dative case of \"hankou-suru (resist)\" and \"eikyo-suru (affect)\" does not indicate the \"Goal\" of the action but the \"Partner.\" Ext. 2. Verbs of obtaining (Levin, 1993 ): In contrast with \"ataeru (give),\" the nominative case of \"ukeru (receive)\" and \"eru (acquire)\" is the \"Goal\" of the \"Theme,\" while the ablative case indicates \"Source.\" Ext. 3. Require verb: \"motomeru (ask)\" and \"yokyu-suru (require)\" denote the existence of the external \"Agent\" who controls the action of the other \"Agent\" or \"Theme.\" Ext. 4. Verbs of psychological state (Levin, 1993) : \"kandou-suru (be impressed)\" and \"osoreru (fear)\" indicate the change of psychological state of the \"Agent.\" The ascriptive part of the change has to be described. Consequently, we defined a new LCS typology consisting of 16 types. Note that more than one LCS can be assigned to a verb if it has a polysemy. For convenience, we refer to the extended dictionary as the LCSdic 7 . 6 The predicate \"FILLED\" represents an implicit argument of the verb and the verb assigned this LCS cannot take this argument. Taking the LCS of the verb \"sign\" as an example, \"FILLED\" in [x CONTROL [BECOME [[FILLED] y BE AT z]]] denotes the name of \"Agent.\" 7 The latest version of the LCSdic is available from http://cl.it.okayama-u.ac.jp/rsc/lcs/ Paraphrasing model In this section, we describe how we generate paraphrases of LVCs. Figure 2 illustrates how our model paraphrases the LVC of example ( 7 ). ( 7 ) s. Ken-ga eiga-ni shigeki-o uketa. Ken-NOM film-DAT inspiration-ACC receive-PAST Ken received inspiration from the film. t. Ken-ga eiga-ni shigeki-s-are-ta. Ken-NOM film-DAT inspire-PAS, PAST Ken was inspired by the film. The idea is to exploit the LCS representation as a semantic representation and to model the LVC paraphrasing by the transformation of the LCS representation. The process consists of the following three steps: Step 1. Semantic analysis: The model first analyzes a given input sentence including an LVC to obtain its semantic structure in terms of the LCS representation. In Figure 2 , this step produces LCS V 1 . Step 2. Semantic transformation (LCS transformation): The model then transfers the obtained semantic structure to another semantic structure so that the target structure consists of the LCS of the nominalized verb of the input. In our example, this step generates LCS N 1 together with the supplement [BECOME [. . .]]. Step 3. Surface generation: Having obtained the target LCS representation, the model finally lexicalizes it to generate the output sentence. So, the key issue is how to control the second step, namely, the transformation of the LCS representation. The rest of this section elaborates on each step, using different symbols to denote arguments; x, y, and z for LCS V , and x', y', and z' for LCS N . Semantic analysis Given an input sentence, which we assume to be a simple clause with an LVC, we first look up the LCS template LCS V 0 for the given light-verb, and then apply the case assignment rule, below (Takeuchi et (1) Semantic analysis with the nominative and the dative, respectively. In the example shown in Figure 2 , the nominative \"Ken\" fills the leftmost argument z. Accordingly, the accusative \"shigeki (inspiration)\" and the dative \"eiga (film)\" fill y and x, respectively. (8) s. Ken-ga eiga-ni shigeki-o uketa. Ken-NOM film-DAT inspiration-ACC receive-PAST Ken received inspiration from the film. LCS V 0 [BECOME [z BE WITH [y MOVE FROM x TO z]]] LCS V 1 [BECOME [[Ken]z BE WITH [[inspiration]y MOVE FROM [film]x TO [Ken]z]]] LCS transformation The second step of our paraphrasing model matches the resultant LCS representation (LCS V 1 in Figure 2 ) with the LCS of the nominalized verb (LCS N 0 ) to generate the target LCS representation (LCS N 1 ). Figure 3 shows a more detailed view of this process for the example shown in Figure 2 . Predicate matching The first step is to determine the predicate in LCS V 1 that should be matched with the predicate in LCS N 0 . Assuming that only the agentivity is relevant to the selection of the voice in the paraphrasing of LVC, which is our primary concern, we classify the semantic predicates into the following two classes: Agentive predicates: \"CONTROL,\" \"ACT ON,\" \"ACT,\" \"BE AGAINST,\" and \"MOVE FROM TO.\" State of affair predicates: \"MOVE TO,\" \"BE AT,\" and \"BE WITH.\" Aspectual predicates: \"BECOME.\" We also assume that any pair of predicates of the same class is allowed to match, and that the aspectual predicates are ignored. In our example, \"MOVE FROM TO\" matches \"ACT ON,\" as shown in Figure 3 . LCS representations have right-branching (or right-embedding) structures. Since inner-embedded predicates denote the state of affairs, they take priority in the matching. In other words, the matching proceeds from the rightmost inner predicates to the outer predicates. Having matched the predicates, we then fill each argument slot in LCS N 0 with its corresponding argument in LCS V 1 . In Figure 3 , argument z is matched with y', and x with x'. As a result, \"Ken\" comes to the y' slot and \"eiga (film)\" comes to the x' slot 8 . This process is repeated until the leftmost predicate in LCS N 0 or that in LCS V 1 is matched. Treatment of non-transfered predicates If LCS V 1 has any non-transfered predicates when the predicate matching has been completed, they represent the semantic content that is not covered by LCS N 1 and which needs to be lexicalized by auxiliary linguistic devices such as voice auxiliaries. In the case of Figure 3 , [BECOME [[Ken]z BE WITH]]  in LCS V 1 remains non-transfered. In such a case, we attach the non-transfered predicates to LCS N 0 , which are then lexicalized by auxiliaries in the next step, the surface generation. Surface generation We again apply the aforementioned case assignment rule to generate a sentence from the resultant LCS representation. In this process, the model makes the final decisions on the selection of the voice and the reassignment of the cases, according to the following decision list: 1. If the attached predicate is filled with the same argument as the leftmost argument in LCS N 1 , the \"active\" voice is selected and the case structure is left as is. 2. If the argument of the attached predicate has the same value as either z' or y' in LCS N 1 , lexicalization is performed to make the argument a subject. Therefore, the \"passive\" voice is selected and case alternation (passivization) is applied. 3. If the attached predicate is \"BE WITH\" and its argument has the same value as x' in LCS N 1 , the \"causative\" voice is selected and case alternation (causativizaton) is applied. 4. If the attached predicate is an agentive predicate, and its argument is filled with a value different from those of the other arguments, then the \"causative\" voice is selected and case alternation (causativization) is applied. 5. Otherwise, no modification is applied. Since the example in Figure 2 satisfies the second condition, the model chooses \"s-are-ru (passive)\" and passivizes the sentence so that \"Ken\" fills the nominative case. (9 ) LCS N 1 [BECOME [[Ken]z BE WITH]] + [[film]x' ACT ON [Ken]y'] t. Ken-ga eiga-ni shigeki-s-are-ta. Ken-NOM film-DAT inspire-PAS, PAST Ken was inspired by the film. Experiment Paraphrase generation and evaluation To empirically evaluate our paraphrasing model and the LCSdic, and to clarify the remaining problems, we analyzed a set of automatically generated paraphrase candidates. The sentences used in the experiment were collected in the following way: Step 1. From the 876,101 types of triplet n, c, v collected in Section 3.2, 23,608 types of n, c, v were extracted, whose components, n and c, v , are listed in the LCSdic. Step 2. For each of the 245 most frequent n, c, v , the 3 most frequent simple clauses including the n, c, v were extracted from the corpus from which n, c, v s were extracted in Section 3.2. As a result, we collected 735 sentences. Step 3. We input these 735 sentences into our paraphrasing model, and then automatically generated paraphrase candidates. When more than one LCS is assigned to a verb in the LCSdic due to its polysemy or ergative verb such as \"kaifuku-suru (recover),\" our model generates all the possible paraphrase candidates. As a result, 825 paraphrase candidates, that is, at least one for each input, were generated. We manually classified the resultant 825 paraphrase candidates into 621 correct and 198 erroneous candidates. The remaining 6 candidates were not classified. The precision of the paraphrase generation was 75.8% (621 / 819). Error analysis To clarify the cause of the erroneous paraphrases, we manually classified 198 erroneous paraphrase candidates. Table 3 lists the error sources. LCS transformation algorithm The experiment came close to confirming that the right-first matching algorithm in our paraphrasing model operates correctly. Unfortunately, the matching rules produced some erroneous paraphrases in LCS transformation. Errors in predicate matching: To paraphrase (10s) below, \"CONTROL\" in LCS V 1 must be matched with \"CONTROL\" in LCS N 0 , and x to x'. However, our model first matched \"CONTROL\" in LCS V 1 with \"MOVE FROM TO\" in LCS N 0 . Thus, x was incorrectly matched with z' and x' remained empty. The desired form of LCS N 1 is shown in (11). (10) s. kacho-ga buka-ni section-chief-NOM subordinate-DAT shiji-o dasu. order-ACC issue-PRES The section chief issues orders to his subordinates. (N =\"order\", V =\"issue\") LCS V 1 [[chief ]x CONTROL [BECOME [[order]y BE AT [subordinate]z]]] LCS N 0 [x' CONTROL [y' MOVE FROM z' TO [FILLED]]] LCS N 1 * [x' CONTROL [[subordinate]y' MOVE FROM [chief ]z' TO [FILLED]]] (11) LCS N 1 [[chief ]x' CONTROL [y' MOVE FROM [subordinate] TO [FILLED]]] This error was caused by the mis-matching of \"CONTROL\" with \"MOVE FROM TO.\" Although we regard some predicates as being in the same classes as those described in Section 4.2.1, these need to be considered carefully. In particular \"MOVE FROM TO\" needs further investigation because it causes many errors whenever it has the \"FILLED\" argument. Errors in argument matching: Even if all the predicates are matched properly, there would still be a chance of errors being caused by incorrect argument matching. With the present algorithm, z can be matched with y' if and only if z' contains \"FILLED.\" In the case of ( 12 ), however, z has to be matched with y', even though z' is empty. The desired form of LCS N 1 is shown in ( 13 ). ( 12 ) s. Jikan-ni seigen-ga aru. time-DAT limitation-NOM exist-PRES There is a time limitation. (N =\"limitation\", V =\"exist\") LCS V 1 [BECOME [[limitation]y BE AT [time]z]] LCS N 0 [x' CONTROL [BECOME [y' BE AT z']]] LCS N 1 * [x' CONTROL [BECOME [y' BE AT [time]z']]] (13) LCS N 1 [x' CONTROL [BECOME [[timey]y' BE AT z']]] Ambiguous thematic role of dative In contrast to dative cases in English, in Japanese, the dative case has ambiguity. That is, it can be a complement to the verb or an adjunct 9 . However, since LCS is not capable of determining whether the case is a complement or an adjunct, z is occasionally incorrectly filled with an adjunct. For example, \"medo-ni\" in (14s) should not fill z, because it acts as an adverb, even though it consists of a noun, \"medo (prospect)\" and a case particle for the dative. We found that 78 erroneous candidates constitute this most dominant type of errors. ( 14 ) s. Kin'you-o medo-ni sagyo-o susumeru. Friday-NOM by-DAT work-ACC carry on-PRES I plan to finish the work by Friday. (N =\"work\", V =\"carry\") LCS V 0 [x CONTROL [BECOME [y BE AT z]]] LCS V 1 * [x CONTROL [BECOME [[work]y BE AT [by]z]]] The ambiguity of dative cases in Japanese has been discussed in the literature of linguistics and some natural language processing tasks (Muraki, 1991) . To date, however, a practical compliment/adjunct classifier has not been established. We plan to address this topic in our future research. Preliminary investigation revealed that only certain groups of nouns can constitute both compliments and adjuncts according to the governing verb. Therefore, generally whether a word acts as a complement is determined without combining it with the verb. Recognition of LVC In our model, we assume that a triplet n, c, v consisting of a nominalized verb n and a light-verb tuple c, v from our vocabulary lists (see Section 3.2) always act as an LVC. However, not only the triplet itself but also its context sometimes affects whether the given triplet can be paraphrased. For example, we regard \"imi-ga aru\" as an LVC, because the nominalized verb \"imi\" and the tuple \"ga\", \"aru\" appear in the vocabulary lists. However, the n, c, v in (15s) does not act as an LVC, while the same triplet in (16s) does. (15) s. Sanka-suru-koto-ni imi-ga aru. to participate-DAT meaning-NOM exist-PRES There is meaning in participating. meaning-NOM exist-PRES \"kennel\" has the meaning of doghouse. t. \"kennel\"-wa inugoya-o imi-suru. \"kennel\"-TOP doghouse-ACC mean-ACT, PRES \"kennel\" means doghouse. The above difference is caused by the polysemy of the nominalized verb \"imi\" that denotes \"worth\" in the context of (15s), but \"meaning\" in (16s). Although incorporating word sense disambiguation using contextual clues complicates our model, in fact only a limited number of nominalized verbs are polysemous. We therefore expect that we can list them up and use this as a trigger for making a decision as to whether we need to take the context into account. Namely, given a n, c, v , we would be able to classify it into (a) a main verb phrase, (b) a delicate case in terms of the dependence of its context, and (c) an LVC. We can adopt a different approach to avoiding incorrect paraphrase generation. As described in Section 5.1, our model generates all the possible paraphrase candidates when more than one LCS is assigned to a verb. Similarly, our approach can be extended to (i) over-generate paraphrase candidates by considering the polysemy of not only assigned LCS types, but also that of nominalized verbs (see (15s) and (16s)) and whether the given n, c, v is an LVC, and (ii) revise or reject the incorrect candidates by using handcrafted solid rules or statistical language models. Conclusion and future work In this paper, we presented an LCS-based paraphrasing model for LVCs and an extension of an ex-isting LCS dictionary. Our model achieved an accuracy of 75.8% in selecting the voice and reassigning the cases. To make our paraphrasing model more accurate, further analysis is needed, especially for the LCS transformation stage described in Section 4.2. Similarly, several levels of disambiguation should also be solved. The Japanese LCS typology has to be refined from the theoretical point of view. For example, since extensions are no more than human intuition, we must discuss how we can assign LCSs for given verbs based on explicit language tests, as described in (Takeuchi et al., 2001) . In future research, we will also extend our LCSbased approach to other classes of paraphrases that exhibit some regularity, such as verb alteration and compound noun decomposition as shown in ( 17 ) and ( 18 ), below. LCS has been discussed as a means of explaining the difference between transitive/intransitive verbs, and the construction of compounds. Therefore, our next goal is to show the applicability of LCS through practical tasks, namely, paraphrasing. (17) s. Jishin-ga building-o kowashita. earthquake-NOM building-DAT destroy-PAST The earthquake destroyed the building. t. Jishin-de building-ga kowareta. earthquake-LOC building-NOM be destroyed-PAST The building was destroyed in the earthquake. (18) s. Kare-wa kikai he-TOP machinesousa-ga jouzu-da. operation-NOM good-COPULA He is good at operating the machine. t. Kare-wa kikai-o jouzu-ni sousa-suru. he-TOP machine-DAT well-ADV operate-PRES He operates machines well.",
         "10435939",
         "02cadba296c23f7c968c934b77532cd6b6756b3c",
         "19",
         "https://aclanthology.org/W04-0402",
         "Association for Computational Linguistics",
         "Barcelona, Spain",
         "2004",
         "July",
         "Proceedings of the Workshop on Multiword Expressions: Integrating Processing",
         "Fujita, Atsushi  and\nFurihata, Kentaro  and\nInui, Kentaro  and\nMatsumoto, Yuji  and\nTakeuchi, Koichi",
         "Paraphrasing of {J}apanese Light-verb Constructions Based on Lexical Conceptual Structure",
         "9--16",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "fujita-etal-2004-paraphrasing",
         null,
         null
        ],
        [
         "49",
         "R13-1060",
         "Automatic recognition of CORROBO-RATE and CONTRAST relations between citations may enhance citation analysis. We describe a system that identifies these citation relations using predicate/argument and discourse structures.",
         "Automatic recognition of CORROBO-RATE and CONTRAST relations between citations may enhance citation analysis. We describe a system that identifies these citation relations using predicate/argument and discourse structures. Introduction The citation of publications has been used to measure the impact of authors, publications, publishers, fields of study, etc., as represented in graphs in which nodes represent documents and edges connect documents if one refers to the other (citation graphs) or if a third document refers to both (cocitation graphs). NLP may be used to supplement these graphs with information about why documents are cited. While previous work (Teufel et al., 2009; Athar, 2011; Athar and Teufel, 2012) record positive and negative sentiment about cited work, we record information about how cited documents are compared to each other. CONTRASTing documents may describe different approaches or different opinions. Documents which COR-ROBORATE each other may follow a single approach. A document that is cited as corroborating with many other documents may be very salient. Some example instances of these CON-TRAST/CORROBORATE relations are provided in Figure 1 , the document containing the citations (represented as we or this study) contrasts with [1] and corroborates [2] ; [3] and [4] contrast with each other; and [5] and [6] corroborate. We use square brackets and numbers (IEEE style) to represent citations that are the object of this study (Figure 1 ). We use last name plus date (APA style) for works cited as part of this research effort. Examples in this paper (modified for brevity) are from the PubMed Central corpus 1 . As 1 http://www.ncbi.nlm.nih.gov/pmc/ 1. In contrast to [1], we found clear detrimental effects of prophylaxis. 2. This study corroborates a study [2] finding no evidence of cross-hemisphere invasions.  In this paper, we explore the relations between sentence-internal predicate relations, intersentential discourse relations and relations between citations. Then we describe and evaluate a system which derives CORROBORATE and CONTRAST relations between citations. How Citation Relations are Encoded The phrase/citation connection (Abu Jbara and Radev, 2012) describes a system for identifying the referential scope of each citation, a text fragment that the citation is semantically related to-multiple citations can share the same referential scope. We assume that arguments of grammatical and discourse relations are essentially the referential scopes of the citations that we are concerned with. Like (Abu Jbara and Radev, 2012) , we cover 2 cases. In the first case, the citation is an argument of some predicate (Figure 1 , citation 1). In the second case, the citation is parenthetically linked to a constituent (Figure 1 , ci-tations 2, 3 and 4) ((Abu Jbara and Radev, 2012) refers to this case as non-syntactic). We found the second case to be more common than the first for CORROBORATE and CONTRAST. Authors use we, our, this research, and other phrases which we call self-citations to refer to their own work. These self-citations participate in the same citation relations as conventional citations. Thus, CORROBORATE and CONTRAST relations can have a self-citation arguments, e.g., Figure 1 , we and This study in ex. 1 and 2. Selfcitations occur in regular noun phrase positions (subject, object, etc.), but not parenthetically. In Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) and related approaches (Marcu, 2000) , the discourse structure of a document forms a tree, with the root representing the document; internal nodes representing sections, paragraphs and multi-clause sentences, and leaves representing single clauses. The edge labels on the set of outgoing branches from a node collectively represent relations among the children of that node. As in Figure 2 , substituting the leaves of a discourse tree with predicate argument representations (or parses) results in a rooted graph for a document with words as leaves. If the referential scopes of a pair of citations correspond to a pair of siblings at any level in this graph, the relation represented at the parent node can correspond to a citation relation. The sentence \"[1] contrasts with [2] regarding whether X is or is not true\" is analogous to the sentence \"X is often claimed [1] . In contrast, others claim not X [2]\" because the subject and object of the verb contrast correspond to the discourse arguments of in contrast. Taking this approach, we assume that discourse units and grammatical arguments are the referential scopes of citations. Furthermore, we limit our attention to citations scopes that are no more than a few sentences long (as in the Penn Discourse Treebank (PDTB) (Miltsakaki et al., 2004) ). Citations, Discourse and Grammar We assume that: (1) there is a grammatical or discourse relation corresponding to each COR-ROBORATE and CONTRAST citation relation; (2) each such grammatical/discourse relation takes 2 arguments, each argument being a sequence of sentences, a sentence, a phrase or a word; and (3) more than one citation can be associated with each argument. Given these assumptions we seek to identify: (a) the candidate grammatical/discourse relation and its arguments; and (b) the sets of citations that correspond to these arguments. We then hypothesize the corresponding citation relation for each ARG1/ARG2 pair in the Cartesian product of the set of citations in the ARG1 domain and the set of arguments in the ARG2 domain. This means that to identify CONTRAST and CORROBORATE citation relations, we need to identify syntactic and discourse signals that would imply that citations hold these relations. In the case of syntactic predicates, these turn out to be a list of words (contrast, corroborate, endorse, ...) that are idiosyncratic to this task. For discourse connectives, we can use some previous classifications: causal discourse connectives (thus, therefore) tend to be linked to CORROBORATE citation relations; and CONTRAST discourse connectives (in contrast, on the other hand, however) tend to be linked to CONTRAST citation relations. While all the cited work on discourse relations posit these same relations for consecutive sentences with no explicit connectives, we have not explored this avenue yet. 2 Multi-Sentence Units We recognize a third DISCOURSE relation, EX-PAND, which does not directly link to either of our citation relations. Rather, 2 sentences in an EXPAND relation are treated as a single unit, which can, itself be a discourse argument. D) Experimental infections of passerine [15], [16] characterized these birds as vulnerable. (Marcu, 2000) ; CONJUNCTION, INSTANTIATION and others in PDTB). We collapse these relations in order to simplify the task. 2 mechanisms for identifying EXPAND relations both of which are evident in Figure 3 : (i) using discourse connectives, e.g, the EXPAND relation between D and E is signaled by the connective Additionally; and (ii) based on the cohesion between 2 sentences -this is the case for the links connecting sentences {B,C} and {C,D}. Following (Marcu, 2000) (and others), cohesion can be determined by elements that indicate continuity between sentences such as anaphoric words (the demonstrative these) or repeated words from the previous sentence. In Figure 3 , birds, have, H5N1 and viruses are repeated in C, after occurring in B. D contains the demonstrative these, and repeats the word birds that is found in C. Our system takes these cohesive signals as evidence that an EXPAND relation holds between 2 consecutive sentences. Our Expand relations are used to approximate the larger citation context. (Athar and Teufel, 2012) uses similar methodology in their citation sentiment system. E) Additionally DocRelate Our Approach Each file in our corpus of PubMed scientific articles has the citations premarked. Preprocessing includes the marking of all sentence and paragraph boundaries. Our citation relation system, DocRelate processes each document from beginning to end, one sentence at a time. All processing is based on regular expressions and simple string matches and is therefore both faster and less accurate than a syntactically-sophisticated approach would be. Nevertheless, we expect that aspects of DocRelate that deal with relations across sentence boundaries to be essentially the same as they would be in systems using deeper processing. For each sentence, we find: (a) lexical signals; (b) sentence dividers (semi-colons, coordinate/subordinate conjunctions); and (c) citations (conventional and self-citations). For each lexical signal, we establish a clause 1 and a clause 2 . If the lexical signal follows a sentence divider, clause 1 is the portion of the current sentence preceding the sentence divider, whereas clause 2 follows the sentence divider, e.g., the sentence divider is and in The public considers frequently reported infectious diseases, to be the most severe [18] and therefore people's anxiety correlated with a negative perception of the disease [19]. Otherwise, the previous and current sentences are clause 1 and clause 2 (Figure 1 ex. 3 and 4). We maintain a dictionary of lexical signals, which includes: surface forms, local disambiguating information, part of speech (POS) and CITA-TION relation (EXPAND, CORROBORATE or CONTRAST). For lexical signals with POS of 'adverb', clause 1 is assumed to contain the ARG1 citations and clause 2 is assumed to contain the ARG2 citations: most examples discussed in this paper follow this pattern, e.g., Figure 1 ex. 3 and 4. For other POS: (preposition, verb, adjective and subordinate conjunction (SCONJ)) both arguments are inside clause 1 . For most sentenceinternal cases, the signal divides the clause into 2 parts: citations preceding the signal are candidate ARG1s and those following the signal are ARG2s, e.g., Figure 1 ex. 1 and 2. When an SCONJ occurs at the beginning of a clause, the clause must be divided at a centrally-located comma. Citations preceding that comma are ARG2 candidates, whereas citations following the comma are ARG1s. The 2 cases of SCONJ are: For Case 1, ARG1 is 20 and ARG2 is 21; for case 2, both 23 and 24 are ARG1s and 22 is ARG2. Our approach to SCONJ is essentially the same as that of (Marcu, 2000) among others. In the absence of discourse connectives, we can hypothesize an EXPAND relation between 2 sentences if the second sentence refers back to the first, as determined by: (a) the proportion of words in the second sentence also occurring in the first, ignoring a list of stop words; (b) the presence of abbreviations in the second sentence corresponding to word sequences in the first; (c) the presence of referring expressions found in the second sentence (this, these, those, another, it, they, them, itself, themselves, their, here, latter); and (d) the occurrence of self-citations in both the current sentence and the previous one. Algorithm 1 is our approach for finding citation relations in an article. After each sentence is processed, citations that are not embedded in a sentence-internal ARG2 are recorded as potential ARG1s for the next sentence (the cites function) and each EXPAND relation causes the previous set of ARG1 citations to be stored as well (in St ARG1). This makes analyses like that of Figure 4 possible: the (ARG1) citations preceding However are contrasted with the (ARG2) citations in the sentence containing However. The citations in sentence A are stored due to the Expand relation Algorithm 1: Identify Citation Relations between sentences A and B, motivated by the referring expression These and abbreviation HPAIV (highly pathogenic avian influenza virus). When the procedure evaluates sentence C, the citations in A are potential ARG1s. However, as there is not an EXPAND relation between sentences B and C, these potential ARG1s are not stored for connectives in subsequent sentences. Cross-sentence CONTRAST and CORROBO-RATE signals (St SLink in Figure 1 ) are stored in addition to previous ARG1s up to that point. As long as there is a continuous sequence of EX-PAND relations linking the subsequent sentences, citations in those sentences can fill the ARG2 slot for St SLink. Figure 3 is one such example: the citations in the the sentence preceding In contrast are ARG1s and the citations following the signal are ARG2s: both citations in the sentence and in subsequent sentences. Storage of these elements is emptied in the absence of EXPAND. We have implemented the following constraints on these procedures:  CORROBORATE citation relations). This creates separate multi-sentence units for CONTRAST and CORROBORATE relations, since all storage is emptied in the absence of cross-sentence EX-PAND relations; (2) the sets of citations for proposed ARG1 and ARG2 cannot have a member in common -this rules out relations that do not make sense (a document contrasting with itself) or that are uninformative (a document corroborating with itself). may be due to failures Lexical Entries for Signals We manually constructed a dictionary of signals EXPAND, CORROBORATE and CON-TRAST relations. The CORROBORATE and CONTRAST entries are signals which license citation relations, the entries being based on their roles in syntax and discourse structure. The EX-PAND entries are signals that license EXPAND discourse relations. We have 246 entries for EX-PAND, 48 for CONTRAST and 31 for CORROB-ORATE. This was feasible because there are a small number of these signals that cover most cases. We based our dictionary on previous work. We examined entries in COMLEX Syntax (Macleod et al., 1996) including the 7 coordinate conjunctions, 108 SCONJ, 96 adverbs marked as (META-ADV :CONJ T), and a few other adverb classes as well. We examined the set of discourse connectives marked in PDTB and classified in its manual (Prasad et al., 2007) . We also did some manual annotation (unpublished work) and examined files from our training corpus while creating the system. Sample lexical entries (Figure 5 ) include: base forms, parts of speech (POS), relation licensed, and constraints. Multi-word expres- Choice of POS for contrast determines the relative positions of ARG1 and ARG2, e.g., for the adverb, it is in the previous clause. Another constraint is the sentence-initial requirement, since some adverbs connect clauses when they occur initially, but not when they occur elsewhere in the sentence. For example, sentence-initial roughly can introduce a sentence that elaborates some aspect of the previous sentence (EXPAND), e.g., Roughly, the chance that this would happen was 8 to 1. However, the non-initial use of roughly still means something like approximately, but the connection with the previous sentence is no longer there, e.g., The odds were roughly 8 to 1. POS System Evaluation We ran DocRelate on a 20 document held-out test corpus. Figure 6 represent 216 correct answers out of 291 relations in the answer key (manual annotation by the author after the system was completed). We evaluated (CORROBORATE, CONTRAST) relations between citations, but not discourse relations between sentences (CAUSE, CONTRAST, EXPAND) or predicate argument relations. Concluding Remarks We achieve the highest accuracy for relations linking citations across adjacent sentences. Long-  (1) missing entries in our dictionary; (2) defects in our sentence-internal syntactic analysis; and (3) false positives for self-citations. Some sample errors are provided as figure 7 . We presented an analysis of how authors of technical documents depict corroborations and contrasts between documents We have presented a syntactically naive system, that accounts for most aspects of this analysis. We showed that it was possible in many cases to derive relations between citations from predicate and discourse relations among the constituents that those citations link to. Our current system achieves accuracy of 92% precision and 74% recall for CORROBO-RATE/CONTRAST relations, with some variation based on relation type and the distance between the citations in terms of sentences. for citations not in adjacent sentences. The main contribution of this paper is the working out of the details of how to identify citation relations. Towards this goal, we described a robust system using simple, manually written string-based rules. In future work, we plan to identify properties of additional discourse structures that impact the problem of identifying citation relations. It is likely that a more elaborate system would achieve better results. Such systems could include features based on parsing, semantic role labeling and other text processing, thus making more precise rules available. Systems based on Machine Learning approaches could also be created based on the features described here, as well as text-processing-based features. Acknowledgments Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20154. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government. Approved for Public Release; Distribution Unlimited.",
         "2167996",
         "6fe4fde67d6f1ef344821707ecfe151adc9bafeb",
         "12",
         "https://aclanthology.org/R13-1060",
         "INCOMA Ltd. Shoumen, BULGARIA",
         "Hissar, Bulgaria",
         "2013",
         "September",
         "Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013",
         "Meyers, Adam",
         "Contrasting and Corroborating Citations in Journal Articles",
         "460--466",
         null,
         null,
         null,
         null,
         null,
         null,
         "inproceedings",
         "meyers-2013-contrasting",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 63903
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "      <th>corpus_paper_id</th>\n",
       "      <th>pdf_hash</th>\n",
       "      <th>numcitedby</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>address</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>doi</th>\n",
       "      <th>number</th>\n",
       "      <th>volume</th>\n",
       "      <th>journal</th>\n",
       "      <th>editor</th>\n",
       "      <th>isbn</th>\n",
       "      <th>ENTRYTYPE</th>\n",
       "      <th>ID</th>\n",
       "      <th>language</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O02-2002</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>There is a need to measure word similarity whe...</td>\n",
       "      <td>18022704</td>\n",
       "      <td>0b09178ac8d17a92f16140365363d8df88c757d0</td>\n",
       "      <td>14</td>\n",
       "      <td>https://aclanthology.org/O02-2002</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2002</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>chen-you-2002-study</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R13-1042</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>Thread disentanglement is the task of separati...</td>\n",
       "      <td>16703040</td>\n",
       "      <td>3eb736b17a5acb583b9a9bd99837427753632cdb</td>\n",
       "      <td>10</td>\n",
       "      <td>https://aclanthology.org/R13-1042</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>jamison-gurevych-2013-headerless</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W05-0819</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>In this paper, we describe a word alignment al...</td>\n",
       "      <td>1215281</td>\n",
       "      <td>b20450f67116e59d1348fc472cfc09f96e348f55</td>\n",
       "      <td>15</td>\n",
       "      <td>https://aclanthology.org/W05-0819</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>aswani-gaizauskas-2005-aligning</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R13-1044</td>\n",
       "      <td>The paper 1 presents a rule-based approach to ...</td>\n",
       "      <td>The paper 1 presents a rule-based approach to ...</td>\n",
       "      <td>2491460</td>\n",
       "      <td>c0f1047fe0f95c367184d494e78bb07b11ee3608</td>\n",
       "      <td>2</td>\n",
       "      <td>https://aclanthology.org/R13-1044</td>\n",
       "      <td>INCOMA Ltd. Shoumen, BULGARIA</td>\n",
       "      <td>Hissar, Bulgaria</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>kedzia-maziarz-2013-recognizing</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W05-0818</td>\n",
       "      <td>In this paper we describe LIHLA, a lexical ali...</td>\n",
       "      <td>In this paper we describe LIHLA, a lexical ali...</td>\n",
       "      <td>15322146</td>\n",
       "      <td>ff3f05120d24e5dac2879f25402993bc6355f780</td>\n",
       "      <td>5</td>\n",
       "      <td>https://aclanthology.org/W05-0818</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Ann Arbor, Michigan</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>caseli-etal-2005-lihla</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63898</th>\n",
       "      <td>P99-1002</td>\n",
       "      <td>This paper describes recent progress and the a...</td>\n",
       "      <td>This paper describes recent progress and the a...</td>\n",
       "      <td>715160</td>\n",
       "      <td>ab17a01f142124744c6ae425f8a23011366ec3ee</td>\n",
       "      <td>11</td>\n",
       "      <td>https://aclanthology.org/P99-1002</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>College Park, Maryland, USA</td>\n",
       "      <td>1999</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3115/1034678.1034680</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>furui-1999-automatic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63899</th>\n",
       "      <td>P00-1009</td>\n",
       "      <td>We present an LFG-DOP parser which uses fragme...</td>\n",
       "      <td>We present an LFG-DOP parser which uses fragme...</td>\n",
       "      <td>1356246</td>\n",
       "      <td>ad005b3fd0c867667118482227e31d9378229751</td>\n",
       "      <td>12</td>\n",
       "      <td>https://aclanthology.org/P00-1009</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>2000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3115/1075218.1075227</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>bod-2000-improved</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63900</th>\n",
       "      <td>P99-1056</td>\n",
       "      <td>The processes through which readers evoke ment...</td>\n",
       "      <td>The processes through which readers evoke ment...</td>\n",
       "      <td>7277828</td>\n",
       "      <td>924cf7a4836ebfc20ee094c30e61b949be049fb6</td>\n",
       "      <td>14</td>\n",
       "      <td>https://aclanthology.org/P99-1056</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>College Park, Maryland, USA</td>\n",
       "      <td>1999</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3115/1034678.1034745</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>lange-content-1999-grapho</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63901</th>\n",
       "      <td>P99-1051</td>\n",
       "      <td>This paper examines the extent to which verb d...</td>\n",
       "      <td>This paper examines the extent to which verb d...</td>\n",
       "      <td>1829043</td>\n",
       "      <td>6b1f6f28ee36de69e8afac39461ee1158cd4d49a</td>\n",
       "      <td>92</td>\n",
       "      <td>https://aclanthology.org/P99-1051</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>College Park, Maryland, USA</td>\n",
       "      <td>1999</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3115/1034678.1034740</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>lapata-1999-acquiring</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63902</th>\n",
       "      <td>P00-1013</td>\n",
       "      <td>Spoken dialogue managers have benefited from u...</td>\n",
       "      <td>Spoken dialogue managers have benefited from u...</td>\n",
       "      <td>10903652</td>\n",
       "      <td>483c818c09e39d9da47103fbf2da8aaa7acacf01</td>\n",
       "      <td>330</td>\n",
       "      <td>https://aclanthology.org/P00-1013</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>2000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3115/1075218.1075231</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>roy-etal-2000-spoken</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63903 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         acl_id                                           abstract  \\\n",
       "0      O02-2002  There is a need to measure word similarity whe...   \n",
       "1      R13-1042  Thread disentanglement is the task of separati...   \n",
       "2      W05-0819  In this paper, we describe a word alignment al...   \n",
       "3      R13-1044  The paper 1 presents a rule-based approach to ...   \n",
       "4      W05-0818  In this paper we describe LIHLA, a lexical ali...   \n",
       "...         ...                                                ...   \n",
       "63898  P99-1002  This paper describes recent progress and the a...   \n",
       "63899  P00-1009  We present an LFG-DOP parser which uses fragme...   \n",
       "63900  P99-1056  The processes through which readers evoke ment...   \n",
       "63901  P99-1051  This paper examines the extent to which verb d...   \n",
       "63902  P00-1013  Spoken dialogue managers have benefited from u...   \n",
       "\n",
       "                                               full_text  corpus_paper_id  \\\n",
       "0      There is a need to measure word similarity whe...         18022704   \n",
       "1      Thread disentanglement is the task of separati...         16703040   \n",
       "2      In this paper, we describe a word alignment al...          1215281   \n",
       "3      The paper 1 presents a rule-based approach to ...          2491460   \n",
       "4      In this paper we describe LIHLA, a lexical ali...         15322146   \n",
       "...                                                  ...              ...   \n",
       "63898  This paper describes recent progress and the a...           715160   \n",
       "63899  We present an LFG-DOP parser which uses fragme...          1356246   \n",
       "63900  The processes through which readers evoke ment...          7277828   \n",
       "63901  This paper examines the extent to which verb d...          1829043   \n",
       "63902  Spoken dialogue managers have benefited from u...         10903652   \n",
       "\n",
       "                                       pdf_hash  numcitedby  \\\n",
       "0      0b09178ac8d17a92f16140365363d8df88c757d0          14   \n",
       "1      3eb736b17a5acb583b9a9bd99837427753632cdb          10   \n",
       "2      b20450f67116e59d1348fc472cfc09f96e348f55          15   \n",
       "3      c0f1047fe0f95c367184d494e78bb07b11ee3608           2   \n",
       "4      ff3f05120d24e5dac2879f25402993bc6355f780           5   \n",
       "...                                         ...         ...   \n",
       "63898  ab17a01f142124744c6ae425f8a23011366ec3ee          11   \n",
       "63899  ad005b3fd0c867667118482227e31d9378229751          12   \n",
       "63900  924cf7a4836ebfc20ee094c30e61b949be049fb6          14   \n",
       "63901  6b1f6f28ee36de69e8afac39461ee1158cd4d49a          92   \n",
       "63902  483c818c09e39d9da47103fbf2da8aaa7acacf01         330   \n",
       "\n",
       "                                     url  \\\n",
       "0      https://aclanthology.org/O02-2002   \n",
       "1      https://aclanthology.org/R13-1042   \n",
       "2      https://aclanthology.org/W05-0819   \n",
       "3      https://aclanthology.org/R13-1044   \n",
       "4      https://aclanthology.org/W05-0818   \n",
       "...                                  ...   \n",
       "63898  https://aclanthology.org/P99-1002   \n",
       "63899  https://aclanthology.org/P00-1009   \n",
       "63900  https://aclanthology.org/P99-1056   \n",
       "63901  https://aclanthology.org/P99-1051   \n",
       "63902  https://aclanthology.org/P00-1013   \n",
       "\n",
       "                                       publisher                      address  \\\n",
       "0                                           None                         None   \n",
       "1                  INCOMA Ltd. Shoumen, BULGARIA             Hissar, Bulgaria   \n",
       "2      Association for Computational Linguistics          Ann Arbor, Michigan   \n",
       "3                  INCOMA Ltd. Shoumen, BULGARIA             Hissar, Bulgaria   \n",
       "4      Association for Computational Linguistics          Ann Arbor, Michigan   \n",
       "...                                          ...                          ...   \n",
       "63898  Association for Computational Linguistics  College Park, Maryland, USA   \n",
       "63899  Association for Computational Linguistics                    Hong Kong   \n",
       "63900  Association for Computational Linguistics  College Park, Maryland, USA   \n",
       "63901  Association for Computational Linguistics  College Park, Maryland, USA   \n",
       "63902  Association for Computational Linguistics                    Hong Kong   \n",
       "\n",
       "       year  ...                      doi number volume journal editor  isbn  \\\n",
       "0      2002  ...                     None   None   None    None   None  None   \n",
       "1      2013  ...                     None   None   None    None   None  None   \n",
       "2      2005  ...                     None   None   None    None   None  None   \n",
       "3      2013  ...                     None   None   None    None   None  None   \n",
       "4      2005  ...                     None   None   None    None   None  None   \n",
       "...     ...  ...                      ...    ...    ...     ...    ...   ...   \n",
       "63898  1999  ...  10.3115/1034678.1034680   None   None    None   None  None   \n",
       "63899  2000  ...  10.3115/1075218.1075227   None   None    None   None  None   \n",
       "63900  1999  ...  10.3115/1034678.1034745   None   None    None   None  None   \n",
       "63901  1999  ...  10.3115/1034678.1034740   None   None    None   None  None   \n",
       "63902  2000  ...  10.3115/1075218.1075231   None   None    None   None  None   \n",
       "\n",
       "           ENTRYTYPE                                ID language  note  \n",
       "0      inproceedings               chen-you-2002-study     None  None  \n",
       "1      inproceedings  jamison-gurevych-2013-headerless     None  None  \n",
       "2      inproceedings   aswani-gaizauskas-2005-aligning     None  None  \n",
       "3      inproceedings   kedzia-maziarz-2013-recognizing     None  None  \n",
       "4      inproceedings            caseli-etal-2005-lihla     None  None  \n",
       "...              ...                               ...      ...   ...  \n",
       "63898  inproceedings              furui-1999-automatic     None  None  \n",
       "63899  inproceedings                 bod-2000-improved     None  None  \n",
       "63900  inproceedings         lange-content-1999-grapho     None  None  \n",
       "63901  inproceedings             lapata-1999-acquiring     None  None  \n",
       "63902  inproceedings              roy-etal-2000-spoken     None  None  \n",
       "\n",
       "[63903 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df[df['full_text'].notna() & df['abstract'].notna()].reset_index(drop=True)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd0b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_parquet('acl-publication-64k.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09e188e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define manual mapping\n",
    "top_venue_map = {\n",
    "    'Proceedings of the 12th Language Resources and Evaluation Conference': 'LREC-2020',\n",
    "    'Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing': 'EMNLP-2021',\n",
    "    'Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics': 'ACL-2020',\n",
    "    \"Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)\": 'LREC-2016',\n",
    "    \"Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)\": 'LREC-2018',\n",
    "    'Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)': 'EMNLP-2020',\n",
    "    'Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics': 'ACL-2019',\n",
    "    'Proceedings of the 28th International Conference on Computational Linguistics': 'COLING-2020',\n",
    "    'Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)': 'ACL-2022',\n",
    "    'Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)': 'EMNLP-IJCNLP-2019',\n",
    "}\n",
    "\n",
    "top_venues = df_clean['booktitle'].value_counts().head(10)\n",
    "abbreviated = [top_venue_map.get(v, v) for v in top_venues.index]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(abbreviated, top_venues.values)\n",
    "plt.title('Top 10 Publication Venues in ACL-OCL')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.xlabel('Venue')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b226cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# papers_per_year = df_clean['year'].value_counts().sort_index()\n",
    "papers_per_year = df['year'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "papers_per_year.plot(kind='bar')\n",
    "plt.title('Number of Papers Published per Year')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.xlabel('Year')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fdb68546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abstract_lengths = df_clean['abstract'].str.split().str.len()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(abstract_lengths, bins=30)\n",
    "plt.title('Distribution of Abstract Lengths')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1bb03ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(df_clean['abstract'])\n",
    "\n",
    "# LDA\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Get dominant topic per doc\n",
    "doc_topic_dists = lda.transform(X)\n",
    "df_clean['dominant_topic'] = doc_topic_dists.argmax(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9b8fc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: model neural models information propose network sentiment task based attention\n",
      "Topic 1: parsing based paper task dependency parser grammar features syntactic tree\n",
      "Topic 2: models model data training language performance learning tasks trained domain\n",
      "Topic 3: la des et les en une le nous dans pour\n",
      "Topic 4: language corpus annotation data paper languages research text present annotated\n",
      "Topic 5: semantic paper discourse language structure linguistic information lexical syntactic analysis\n",
      "Topic 6: translation machine english based mt language systems question paper quality\n",
      "Topic 7: dialogue user human language systems users speech based paper dialog\n",
      "Topic 8: task text evaluation news dataset results summarization paper information human\n",
      "Topic 9: word words semantic based method using approach data results embeddings\n"
     ]
    }
   ],
   "source": [
    "# Show top words per topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx}: \" + \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "display_topics(lda, vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88c5b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Define top keywords per topic\n",
    "def get_topic_keywords(model, feature_names, n_top_words=10):\n",
    "    topic_keywords = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topic_keywords.append(\" \".join(top_features))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = get_topic_keywords(lda, feature_names)\n",
    "\n",
    "# Add readable labels to each document\n",
    "df_clean['topic_keywords'] = df_clean['dominant_topic'].apply(lambda t: topic_keywords[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d6b6cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Transform abstracts to topic distribution\n",
    "topic_distributions = lda.transform(X)  # shape: [n_docs, n_topics]\n",
    "dominant_topics = np.argmax(topic_distributions, axis=1)\n",
    "\n",
    "# Count papers per topic\n",
    "topic_counts = pd.Series(dominant_topics).value_counts().sort_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "topic_counts.plot(kind='bar')\n",
    "plt.xlabel(\"Topic ID\")\n",
    "plt.ylabel(\"Number of Papers\")\n",
    "plt.title(\"Number of Papers per Topic (LDA)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "032635e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you still have access to df['year'] corresponding to X\n",
    "# Drop NaNs in abstracts and align years\n",
    "abstracts = df_clean['abstract']\n",
    "years = df_clean.loc[abstracts.index, 'year']\n",
    "\n",
    "# Create DataFrame with topic and year\n",
    "topic_df = pd.DataFrame({\n",
    "    'year': years,\n",
    "    'topic': dominant_topics\n",
    "})\n",
    "topic_df = topic_df.dropna()  # just in case some years are missing\n",
    "\n",
    "# Count papers per topic per year\n",
    "topic_year_counts = topic_df.groupby(['year', 'topic']).size().unstack(fill_value=0)\n",
    "\n",
    "# Normalize to percentage\n",
    "topic_year_percent = topic_year_counts.divide(topic_year_counts.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot\n",
    "topic_year_percent.plot(kind='area', figsize=(10, 6), colormap='tab10')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Proportion of Papers\")\n",
    "plt.title(\"Topic Distribution Over Time (LDA)\")\n",
    "plt.legend(title='Topic')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpcompsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
