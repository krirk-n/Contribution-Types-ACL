acl_id,title,abstract,year,A,B,C,D,E,F,G,raw_response,input_tokens,output_tokens,total_tokens
W19-6627,Post-editese: an Exacerbated Translationese,"Post-editing (PE) machine translation (MT) is widely used for dissemination because it leads to higher productivity than human translation from scratch (HT). In addition, PE translations are found to be of equal or better quality than HTs. However, most such studies measure quality solely as the number of errors. We conduct a set of computational analyses in which we compare PE against HT on three different datasets that cover five translation directions with measures that address different translation universals and laws of translation: simplification, normalisation and interference. We find out that PEs are simpler and more normalised and have a higher degree of interference from the source language than HTs.",2019,False,False,False,False,True,True,False,"E,F",251,2,253
W19-8715,Exploring Adequacy Errors in Neural Machine Translation with the Help of Cross-Language Aligned Word Embeddings,"Neural machine translation (NMT) was shown to produce more fluent output than phrase-based statistical (PBMT) and rulebased machine translation (RBMT). However, improved fluency makes it more difficult for post editors to identify and correct adequacy errors, because unlike RBMT and SMT, in NMT adequacy errors are frequently not anticipated by fluency errors. Omissions and additions of content in otherwise flawlessly fluent NMT output are the most prominent types of such adequacy errors, which can only be detected with reference to source texts. This contribution explores the degree of semantic similarity between source texts, NMT output and post edited output. In this way, computational semantic similarity scores (cosine similarity) are related to human quality judgments. The analyses are based on publicly available NMT post editing data annotated for errors in three language pairs (EN-DE, EN-LV, EN-HR) with the Multidimensional Quality Metrics (MQM). Methodologically, this contribution tests whether crosslanguage aligned word embeddings as the sole source of semantic information mirrors human error annotation.",2019,False,False,False,False,True,True,False,"E, F",333,3,336
W19-4617,Translating Between Morphologically Rich Languages: An {A}rabic-to-{T}urkish Machine Translation System,"This paper introduces the work on building a machine translation system for Arabic-to-Turkish in the news domain. Our work includes collecting parallel datasets in several ways for a new and low-resource language pair, building baseline systems with state-ofthe-art architectures and developing language specific algorithms for better translation. Parallel datasets are mainly collected three different ways; i) translating Arabic texts into Turkish by professional translators, ii) exploiting the web for open-source Arabic-Turkish parallel texts, iii) using back-translation. We performed preliminary experiments for Arabicto-Turkish machine translation with neural (Marian) machine translation tools with a novel morphologically motivated vocabulary reduction method.",2019,True,False,True,False,False,False,False,"A, C",250,3,253
P19-1312,Unsupervised Joint Training of Bilingual Word Embeddings,"State-of-the-art methods for unsupervised bilingual word embeddings (BWE) train a mapping function that maps pre-trained monolingual word embeddings into a bilingual space. Despite its remarkable results, unsupervised mapping is also well-known to be limited by the dissimilarity between the original word embedding spaces to be mapped. In this work, we propose a new approach that trains unsupervised BWE jointly on synthetic parallel data generated through unsupervised machine translation. We demonstrate that existing algorithms that jointly train BWE are very robust to noisy training data and show that unsupervised BWE jointly trained significantly outperform unsupervised mapped BWE in several cross-lingual NLP tasks.",2019,False,True,False,True,False,False,False,"B, D",256,3,259
D19-1573,Hint-Based Training for Non-Autoregressive Machine Translation,"Due to the unparallelizable nature of the autoregressive factorization, AutoRegressive Translation (ART) models have to generate tokens sequentially during decoding and thus suffer from high inference latency. Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time, but could only achieve inferior translation accuracy. In this paper, we proposed a novel approach to leveraging the hints from hidden states and word alignments to help the training of NART models. The results achieve significant improvement over previous NART models for the WMT14 En-De and De-En datasets and are even comparable to a strong LSTMbased ART baseline but one order of magnitude faster in inference.",2019,False,True,False,True,False,False,False,"B, D",254,3,257
2019.iwslt-1.15,How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on {J}apanese-{V}ietnamese Translation Systems,"While translating between East Asian languages, many works have discovered clear advantages of using characters as the translation unit. Unfortunately, traditional recurrent neural machine translation systems hinder the practical usage of those character-based systems due to their architectural limitations. They are unfavorable in handling extremely long sequences as well as highly restricted in parallelizing the computations. In this paper, we demonstrate that the new transformer architecture can perform character-based translation better than the recurrent one. We conduct experiments on a low-resource language pair: Japanese-Vietnamese. Our models considerably outperform the state-of-the-art systems which employ word-based recurrent architectures.",2019,False,True,False,False,False,False,True,"B, G",235,3,238
D19-1637,Generating Classical {C}hinese Poems from Vernacular {C}hinese,"Classical Chinese poetry is a jewel in the treasure house of Chinese culture. Previous poem generation models only allow users to employ keywords to interfere the meaning of generated poems, leaving the dominion of generation to the model. In this paper, we propose a novel task of generating classical Chinese poems from vernacular, which allows users to have more control over the semantic of generated poems. We adapt the approach of unsupervised machine translation (UMT) to our task. We use segmentation-based padding and reinforcement learning to address undertranslation and over-translation respectively. According to experiments, our approach significantly improve the perplexity and BLEU compared with typical UMT models. Furthermore, we explored guidelines on how to write the input vernacular to generate better poems. Human evaluation showed our approach can generate high-quality poems which are comparable to amateur poems.",2019,True,False,False,True,False,False,False,"A, D",285,3,288
W19-7008,Predicting Cognitive Effort in Translation Production,"In view of the ""predictive turn"" in Translation Studies (Schaeffer et al., 2019) , there has been increasing interest in investigating particular features of the text which can predict translation efficiency and the cognitive load of translating and post-editing. However, hypotheses of such kinds have often been on the basis of descriptive means in lack of rigorous statistical test on a large scale. In this regard, this paper seeks to empirically study the cognitive effort of translating and Machine Translation (MT) postediting in relation to different predictor variables including word frequency, word translation entropy, and syntactic choice entropy, making use of a large dataset from the CRITT Translation Process Research Database (CRITT TPR-DB, see Carl et al., 2016) which incorporates multiple languages and translation production modes. Cognitive effort is measured by eye-movement behavioural data, assuming that an increase in the number or duration of eye fixations on particular words or lexical items of the text indicates an extra processing cost in producing the translation of the corresponding items. These measures of cognitive effort are statistically correlated with frequency and entropy values of the Source Text words, followed by a qualitative analysis of the instances where these variables tend to cause increased processing effort in the translating and postediting process. With a particular focus on ambiguity resolution and the influence of formulaic expressions, the qualitative analysis intends to explain whether and how the resolution of the competition between different interpretations of a potentially",2019,False,False,False,False,True,True,False,"E, F",407,3,410
D19-1139,Recurrent Positional Embedding for Neural Machine Translation,"In the Transformer network architecture, positional embeddings are used to encode order dependencies into the input representation. However, this input representation only involves static order dependencies based on discrete numerical information, that is, are independent of word content. To address this issue, this work proposes a recurrent positional embedding approach based on word vector. In this approach, these recurrent positional embeddings are learned by a recurrent neural network, encoding word content-based order dependencies into the input representation. They are then integrated into the existing multi-head selfattention model as independent heads or part of each head. The experimental results revealed that the proposed approach improved translation performance over that of the stateof-the-art Transformer baseline in WMT'14 English-to-German and NIST Chinese-to-English translation tasks.",2019,False,True,True,False,False,False,False,"B, C",267,3,270
N19-1387,Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages,"Transfer learning approaches for Neural Machine Translation (NMT) trains a NMT model on an assisting language-target language pair (parent model) which is later fine-tuned for the source language-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios.",2019,False,False,False,True,False,False,True,"D, G",273,3,276
D19-5617,Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness,"We share a French-English parallel corpus of Foursquare restaurant reviews, and define a new task to encourage research on Neural Machine Translation robustness and domain adaptation, in a real-world scenario where betterquality MT would be greatly beneficial. We discuss the challenges of such user-generated content, and train good baseline models that build upon the latest techniques for MT robustness. We also perform an extensive evaluation (automatic and human) that shows significant improvements over existing online systems. Finally, we propose task-specific metrics based on sentiment analysis or translation accuracy of domain-specific polysemous words.",2019,True,False,False,True,False,False,False,"A, D",229,3,232
D19-6307,Surface Realization Shared Task 2019 ({MSR}19): The Team 6 Approach,"This study describes the approach developed by the Tilburg University team to the shallow track of the Multilingual Surface Realization Shared Task 2019 (SR'19) (Mille et al., 2019) . Based on Ferreira et al. ( 2017 ) and on our 2018 submission Ferreira et al. ( 2018 ), the approach generates texts by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a rulebased and a statistical machine translation (SMT) model. This year our submission is able to realize texts in the 11 languages proposed for the task, different from our last year submission, which covered only 6 Indo-European languages. The model is publicly available 1 .",2019,False,False,False,True,False,False,True,"D, G",268,3,271
P19-1653,Distilling Translations with Visual Awareness,"Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.",2019,False,True,False,True,False,False,False,"B, D",270,3,273
W19-8718,Optimising the Machine Translation Post-editing Workflow,"In this article, we describe how machine translation is used for post-editing at TransPerfect and the ways in which we optimise the workflow. This includes MT evaluation, MT engine customisation, leveraging MT suggestions compared to TM matches, and the lessons learnt from implementing MT at a large scale.",2019,False,False,False,True,False,False,True,"D, G",176,3,179
2019.icon-1.18,Towards Handling Verb Phrase Ellipsis in {E}nglish-{H}indi Machine Translation,"English-Hindi machine translation systems have difficulty interpreting verb phrase ellipsis (VPE) in English, and commit errors in translating sentences with VPE. We present a solution and theoretical backing for the treatment of English VPE, with the specific scope of enabling English-Hindi MT, based on an understanding of the syntactical phenomenon of verb-stranding verb phrase ellipsis in Hindi (VVPE). We implement a rule-based system to perform the following sub-tasks: 1) Verb ellipsis identification in the English source sentence, 2) Elided verb phrase head identification 3) Identification of verb segment which needs to be induced at the site of ellipsis 4) Modify input sentence; i.e. resolving VPE and inducing the required verb segment. This system is tested in two parts. It obtains 94.83 percent precision and 83.04 percent recall on subtask (1), tested on 3900 sentences from the BNC corpus (Leech, 1992). This is competitive with state-of-the-art results. We measure accuracy of subtasks ( 2 ) and (3) together, and obtain a 91 percent accuracy on 200 sentences taken from the WSJ corpus (Paul and Baker, 1992) . Finally, in order to indicate the relevance of ellipsis handling to MT, we carried out a manual analysis of the MT outputs of 100 sentences after passing it through our system. We set up a basic metric (1-5) for this evaluation, where 5 indicates drastic improvement, and obtained an average of 3.55.",2019,False,False,False,True,False,False,True,"D, G",440,3,443
W19-4425,Improving Precision of Grammatical Error Correction with a Cheat Sheet,"In this paper, we explore two approaches of generating error-focused phrases and examine whether these phrases can lead to better performance in grammatical error correction for the restricted track of BEA 2019 Shared Task on GEC. Our results show that phrases directly extracted from GEC corpora outperform phrases from a statistical machine translation phrase table by a large margin. Appending er-ror+context phrases to the original GEC corpora yields comparably higher precision. We also explore the generation of artificial syntactic error sentences using error+context phrases for the unrestricted track. The additional training data greatly facilitates syntactic error correction (e.g., verb form) and contributes to better overall performance.",2019,True,False,False,True,False,False,False,"A, D",253,3,256
W19-4417,The {CUED}{'}s Grammatical Error Correction Systems for {BEA}-2019,We describe two entries from the Cambridge University Engineering Department to the BEA 2019 Shared Task on grammatical error correction. Our submission to the lowresource track is based on prior work on using finite state transducers together with strong neural language models. Our system for the restricted track is a purely neural system consisting of neural language models and neural machine translation models trained with backtranslation and a combination of checkpoint averaging and fine-tuning -without the help of any additional tools like spell checkers. The latter system has been used inside a separate system combination entry in cooperation with the Cambridge University Computer Lab.,2019,False,False,False,True,False,False,True,"D, G",236,3,239
W19-6808,Corpus Building for Low Resource Languages in the {DARPA} {LORELEI} Program,"We describe corpora for the LORELEI (Low Resource Languages for Emergent Incidents) Program, whose goal is to build human language technologies to provide situational awareness during emergent incidents, with a particular focus on low resource languages. Incident Language packs are used for system development and testing in machine translation, entity disambiguation and linking, and the ""situation frame"" task, which requires aggregation of information about the emergent incident. Incident languages, as well as the incidents themselves, remain unknown until the evaluation begins, and no labeled training data is provided; systems developers must rapidly adapt technology for the incident language and return initial results within 24 hours. Given this surprise language evaluation scenario, Representative Language packs are designed to support research into cross-language projection and language universals rather than to provide training data. They contain large volumes of monolingual and parallel text, basic annotations, lexical resources and simple NLP tools for 23 languages selected for typological diversity and coverage. We discuss the creation of the LORELEI language packs with a special focus on resources for machine translation, as well as techniques for maintaining consistency across the language packs.",2019,True,False,False,False,True,False,False,"A, E",349,3,352
W19-7006,Investigating Correlations Between Human Translation and {MT} Output,"This study investigates whether there is a correlation between machine translation (MT) and human translation (HT) in terms of word translation entropy (i.e., the variance observed in different translations based on the same source text). Our analysis showed a significant strong correlation in all the three languages we examined: Arabic, Japanese, and Spanish. Furthermore, MT, as well as HT, was found to correlate across languages, although the associations were weaker than the MT-HT correlation in each language.",2019,False,False,False,False,True,True,False,"E,F",214,2,216
D19-5601,Findings of the Third Workshop on Neural Generation and Translation,"This document describes the findings of the Third Workshop on Neural Generation and Translation, held in concert with the annual conference of the Empirical Methods in Natural Language Processing (EMNLP 2019). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the two shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language.",2019,True,False,False,False,True,False,False,"A, E",244,3,247
W19-5321,{M}icrosoft Translator at {WMT} 2019: Towards Large-Scale Document-Level Neural Machine Translation,"This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that backtranslation seems to mainly help with translationese input. We explore fine-tuning techniques, deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with documentboundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of documentlevel source language monolingual data via the BERT-objective on the encoder and twopass decoding for combinations of sentencelevel and document-level systems. Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment.",2019,False,True,False,True,False,False,False,"B, D",346,3,349
W19-5357,Meteor++ 2.0: Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation,"This paper describes Meteor++ 2.0, our submission to the WMT19 Metric Shared Task. The well known Meteor metric improves machine translation evaluation by introducing paraphrase knowledge. However, it only focuses on the lexical level and utilizes consecutive n-grams paraphrases. In this work, we take into consideration syntactic level paraphrase knowledge, which sometimes may be skip-grams. We describe how such knowledge can be extracted from Paraphrase Database (PPDB) and integrated into Meteor-based metrics. Experiments on WMT15 and WMT17 evaluation datasets show that the newly proposed metric outperforms all previous versions of Meteor.",2019,False,False,False,True,False,True,False,"D, F",246,3,249
P19-1269,Putting Evaluation in Context: Contextual Embeddings Improve Machine Translation Evaluation,"Accurate, automatic evaluation of machine translation is critical for system tuning, and evaluating progress in the field. We proposed a simple unsupervised metric, and additional supervised metrics which rely on contextual word embeddings to encode the translation and reference sentences. We find that these models rival or surpass all existing metrics in the WMT 2017 sentence-level and systemlevel tracks, and our trained model has a substantially higher correlation with human judgements than all existing metrics on the WMT 2017 to-English sentence level dataset.",2019,False,True,False,True,False,False,False,"B, D",221,3,224
P19-1227,{XQA}: A Cross-lingual Open-domain Question Answering Dataset,"Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.",2019,True,False,False,False,True,False,False,"A, E",378,3,381
D19-1353,The Challenges of Optimizing Machine Translation for Low Resource Cross-Language Information Retrieval,"When performing cross-language information retrieval (CLIR) for lower-resourced languages, a common approach is to retrieve over the output of machine translation (MT). However, there is no established guidance on how to optimize the resulting MT-IR system. In this paper, we examine the relationship between the performance of MT systems and both neural and term frequency-based IR models to identify how CLIR performance can be best predicted from MT quality. We explore performance at varying amounts of MT training data, byte pair encoding (BPE) merge operations, and across two IR collections and retrieval models. We find that the choice of IR collection can substantially affect the predictive power of MT tuning decisions and evaluation, potentially introducing dissociations between MT-only and overall CLIR performance.",2019,False,False,False,False,True,True,False,"E, F",268,3,271
W19-6901,Unsupervised multi-word term recognition in {W}elsh,"This paper investigates an adaptation of an existing system for multi-word term recognition, originally developed for English, for Welsh. We overview the modifications required with a special focus on an important difference between the two representatives of two language families, Germanic and Celtic, which is concerned with the directionality of noun phrases. We successfully modelled these differences by means of lexico-syntactic patterns, which represent parameters of the system and, therefore, required no re-implementation of the core algorithm. The performance of the Welsh version was compared against that of the English version. For this purpose, we assembled three parallel domain-specific corpora. The results were compared in terms of precision and recall. Comparable performance was achieved across the three domains in terms of the two measures (P = 68.9%, R = 55.7%), but also in the ranking of automatically extracted terms measured by weighted kappa coefficient ( = 0.7758). These early results indicate that our approach to term recognition can provide a basis for machine translation of multi-word terms.",2019,False,False,False,True,False,False,True,"D, G",330,3,333
W19-5308,The {U}niversity of {M}aryland{'}s {K}azakh-{E}nglish Neural Machine Translation System at {WMT}19,"This paper describes the University of Maryland's submission to the WMT 2019 Kazakh to English news translation task. We study the impact of transfer learning from another lowresource but related language. We experiment with different ways of encoding lexical units to maximize lexical overlap between the two language pairs, as well as back-translation and ensembling. The submitted system improves over a Kazakh-only baseline by +5.45 BLEU on newstest2019.",2019,False,False,False,True,False,False,True,"D, G",209,3,212
W19-5427,Neural Machine Translation: {H}indi-{N}epali,"With the extensive use of Machine Translation (MT) technology, there is progressively interest in directly translating between pairs of similar languages. Because the main challenge is to overcome the limitation of available parallel data to produce a precise MT output. Current work relies on the Neural Machine Translation (NMT) with attention mechanism for the similar language translation of WMT19 shared task in the context of Hindi-Nepali pair. The NMT systems trained the Hindi-Nepali parallel corpus and tested, analyzed in Hindi ⇔ Nepali translation. The official result declared at WMT19 shared task, which shows that our NMT system obtained Bilingual Evaluation Understudy (BLEU) score 24.6 for primary configuration in Nepali to Hindi translation. Also, we have achieved BLEU score 53.7 (Hindi to Nepali) and 49.1 (Nepali to Hindi) in contrastive system type.",2019,False,False,False,True,False,False,True,"G, D",301,3,304
W19-7004,Translation Quality and Effort Prediction in Professional Machine Translation Post-Editing,"The focus of this controlled eye-tracking and key-logging study is to analyze the behaviour of translation professionals at the European Commission's Directorate-General for Translation (DGT) when detecting and correcting errors in neural machine translated texts (NMT) and their post-edited versions (NMTPE). The experiment was informed by quality analyses of an authentic DGT parallel corpus (Vardaro, Schaeffer, and Hansen-Schirra 2019), consisting of English source texts and corresponding German NMT, NMTPE and revisions (REV). To identify the most characteristic error categories in NMT and NMTPE, we used the automatic error annotation tool Hjerson (Popović 2011) and the more fine-grained manual MQM framework (Lommel 2014). Results show that quality assurance measures by post-editors and revisors at the DGT are most often necessary for lexical errors. More specifically, if post-editors correct mistranslations, terminology or stylistic errors in an NMT sentence, revisors are likely to correct the same type of error in the same sentence, suggesting a certain transitivity between the NMT system and human post-editors. In this study, carried out in Translog II (Carl 2012), participants' eye movements and typing behavior for test sentences where the error categories mistranslation, terminology, function words and stylistic",2019,False,False,False,False,True,True,False,"E, F",399,3,402
P19-1639,A Multi-Task Architecture on Relevance-based Neural Query Translation,"We describe a multi-task learning approach to train a Neural Machine Translation (NMT) model with a Relevance-based Auxiliary Task (RAT) for search query translation. The translation process for Cross-lingual Information Retrieval (CLIR) task is usually treated as a black box and it is performed as an independent step. However, an NMT model trained on sentence-level parallel data is not aware of the vocabulary distribution of the retrieval corpus. We address this problem with our multitask learning architecture that achieves 16% improvement over a strong NMT baseline on Italian-English query-document dataset. We show using both quantitative and qualitative analysis that our model generates balanced and precise translations with the regularization effect it achieves from multi-task learning paradigm.",2019,False,True,False,True,False,False,False,"B, D",265,3,268
W19-3817,Resolving Gendered Ambiguous Pronouns with {BERT},"Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a necessary component of machine translation systems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73% F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP researchers and practitioners. In this project, we describe our BERT-based approach to solving the problem of gender-balanced pronoun resolution. We are able to reach 92% F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team.",2019,False,True,False,True,False,False,False,"B, D",270,3,273
N19-1090,Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting,"Lexically-constrained sequence decoding allows for explicit positive or negative phrasebased constraints to be placed on target output strings in generation tasks such as machine translation or monolingual text rewriting. We describe vectorized dynamic beam allocation, which extends work in lexically-constrained decoding to work with batching, leading to a five-fold improvement in throughput when working with positive constraints. Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three.",2019,False,True,False,True,False,False,False,"B, D",234,3,237
W19-6620,A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation,"Most neural machine translation systems are built upon subword units extracted by methods such as Byte-Pair Encoding (BPE) or wordpiece. However, the choice of number of merge operations is generally made by following existing recipes. In this paper, we conduct a systematic exploration on different numbers of BPE merge operations to understand how it interacts with the model architecture, the strategy to build vocabularies and the language pair. Our exploration could provide guidance for selecting proper BPE configurations in the future. Most prominently: we show that for LSTM-based architectures, it is necessary to experiment with a wide range of different BPE operations as there is no typical optimal BPE configuration, whereas for Transformer architectures, smaller BPE size tends to be a typically optimal choice. We urge the community to make prudent choices with subword merge operations, as our experiments indicate that a sub-optimal BPE configuration alone could easily reduce the system performance by 3-4 BLEU points.",2019,False,False,False,False,True,True,False,"E,F",312,2,314
W19-1424,Comparing Pipelined and Integrated Approaches to Dialectal {A}rabic Neural Machine Translation,"When translating diglossic languages such as Arabic, situations may arise where we would like to translate a text but do not know which dialect it is. A traditional approach to this problem is to design dialect identification systems and dialect-specific machine translation systems. However, under the recent paradigm of neural machine translation, shared multidialectal systems have become a natural alternative. Here we explore under which conditions it is beneficial to perform dialect identification for Arabic neural machine translation versus using a general system for all dialects.",2019,False,False,False,False,True,True,False,"E, F",217,3,220
D19-5210,{UCSMNLP}: Statistical Machine Translation for {WAT} 2019,This paper represents UCSMNLP's submission to the WAT 2019 Translation Tasks focusing on the Myanmar-English translation. Phrase based statistical machine translation (PBSMT) system is built by using other resources: Name Entity Recognition (NER) corpus and bilingual dictionary which is created by Google Translate (GT). This system is also adopted with listwise reranking process in order to improve the quality of translation and tuning is done by changing initial distortion weight. The experimental results show that PBSMT using other resources with initial distortion weight (0.4) and listwise reranking function outperforms the baseline system.,2019,False,False,False,True,False,False,True,"D, G",239,3,242
D19-1318,Deep Copycat Networks for Text-to-Text Generation,"Most text-to-text generation tasks, for example text summarisation and text simplification, require copying words from the input to the output. We introduce copycat, a transformerbased pointer network for such tasks which obtains competitive results in abstractive text summarisation and generates more abstractive summaries. We propose a further extension of this architecture for automatic post-editing, where generation is conditioned over two inputs (source language and machine translation), and the model is capable of deciding where to copy information from. This approach achieves competitive performance when compared to state-of-the-art automated post-editing systems. More importantly, we show that it addresses a well-known limitation of automatic post-editing -overcorrecting translations -and that our novel mechanism for copying source language words improves the results.",2019,False,True,False,True,False,False,False,"B, D",271,3,274
N19-1285,Fluent Translations from Disfluent Speech in End-to-End Speech Translation,"Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected 'copy-edited' references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, the translation of conversational speech with joint removal of disfluencies.",2019,True,True,False,False,False,False,False,"A, B",264,3,267
W19-8707,The Punster{'}s Amanuensis: The Proper Place of Humans and Machines in the Translation of Wordplay,"The translation of wordplay is one of the most extensively researched problems in translation studies, but it has attracted little attention in the fields of natural language processing and machine translation. This is because today's language technologies treat anomalies and ambiguities in the input as things that must be resolved in favour of a single ""correct"" interpretation, rather than preserved and interpreted in their own right. But if computers cannot yet process such creative language on their own, can they at least provide specialized support to translation professionals? In this paper, I survey the state of the art relevant to computational processing of humorous wordplay and put forth a vision of how existing theories, resources, and technologies could be adapted and extended to support interactive, computer-assisted translation.",2019,False,False,False,True,False,False,True,"D, G",262,3,265
W19-5429,Panlingua-{KMI} {MT} System for Similar Language Translation Task at {WMT} 2019,"The present paper enumerates the development of Panlingua-KMI Machine Translation (MT) systems for Hindi ↔ Nepali language pair, designed as part of the Similar Language Translation Task at the WMT 2019 Shared Task. The Panlingua-KMI team conducted a series of experiments to explore both the phrase-based statistical (PBSMT) and neural methods (NMT). Among the 11 MT systems prepared under this task, 6 PBSMT systems were prepared for Nepali-Hindi, 1 PBSMT for Hindi-Nepali and 2 NMT systems were developed for Nepali↔ Hindi. The results show that PBSMT could be an effective method for developing MT systems for closely-related languages. Our Hindi-Nepali PBSMT system was ranked 2nd among the 13 systems submitted for the pair and our Nepali-Hindi PBSMT system was ranked 4th among the 12 systems submitted for the task.",2019,False,False,False,False,True,False,True,"G, E",309,3,312
N19-4006,"Train, Sort, Explain: Learning to Diagnose Translation Models","Evaluating translation models is a trade-off between effort and detail. On the one end of the spectrum there are automatic count-based methods such as BLEU, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort. To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts. Inspired by adversarial settings, we train a neural text classifier to distinguish human from machine translations. A classifier that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods. Our proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset translated by a state-of-the-art neural Transformer model, DiaMaT achieves a classification accuracy of 75% and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity.",2019,False,False,False,True,False,True,False,"D, F",303,3,306
P19-2017,Unsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation,"This work presents our ongoing research of unsupervised pretraining in neural machine translation (NMT). In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation (EWC) to avoid forgetting of the original language modeling tasks. We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives. The positive result is that using EWC with the decoder achieves BLEU scores similar to the previous work. However, the model converges 2-3 times faster and does not require the original unlabeled training data during the finetuning stage. In contrast, the regularization using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-toright language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context.",2019,False,False,False,True,False,True,False,"D, F",333,3,336
P19-1164,Evaluating Gender Bias in Machine Translation,"We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., ""The doctor asked the nurse to help her in the operation""). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word ""doctor""). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https://github.com/ gabrielStanovsky/mt_gender.",2019,True,False,False,False,True,False,False,"A, E",271,3,274
W19-6807,Sentence-Level Adaptation for Low-Resource Neural Machine Translation,"Current neural machine translation (NMT) approaches achieve state-of-the-art accuracy in high-resource contexts. However, NMT requires a great deal of parallel data to deliver acceptable results; thus, it is currently unsuited for translating in lowresource contexts (especially when compared to phrase-based approaches). We propose a method that better leverages the limited data available in such low-resource settings by adapting the model for each sentence at inference time. A general NMT model is trained on the limited training data; then, for each test sentence, its parameters are fine-tuned over a subset of similar sentences extracted from the training set. We experiment with various similarity metrics to extract these similar sentences. It is observed that the sentenceadapted models achieve slightly increased BLEU scores compared to standard neural approaches on a Xhosa-English dataset.",2019,False,False,False,True,False,False,True,"D, G",281,3,284
W19-6805,A free/open-source rule-based machine translation system for Crimean Tatar to Turkish,"In this paper a machine translation system for Crimean Tatar to Turkish is presented. To our knowledge this is the first Machine Translation system made available for public use for Crimean Tatar, and the first such system released as free and open source software. The system was built using Apertium 1 , a free and open source machine translation system, and is currently unidirectional from Crimean Tatar to Turkish. We describe our translation system, evaluate it on parallel corpora and compare its performance with a Neural Machine Translation system, trained on the limited amount of corpora available.",2019,False,False,False,True,False,False,True,"G, D",236,3,239
N19-1314,On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models,"Adversarial examples -perturbations to the input of a model that elicit large changes in the output -have been shown to be an effective way of assessing the robustness of sequenceto-sequence (seq2seq) models. However, these perturbations only indicate weaknesses in the model if they do not change the input so significantly that it legitimately results in changes in the expected output. This fact has largely been ignored in the evaluations of the growing body of related literature. Using the example of untargeted attacks on machine translation (MT), we propose a new evaluation framework for adversarial attacks on seq2seq models that takes the semantic equivalence of the pre-and post-perturbation input into account. Using this framework, we demonstrate that existing methods may not preserve meaning in general, breaking the aforementioned assumption that source side perturbations should not result in changes in the expected output. We further use this framework to demonstrate that adding additional constraints on attacks allows for adversarial perturbations that are more meaningpreserving, but nonetheless largely change the output sequence. Finally, we show that performing untargeted adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness, without hurting test performance. 1",2019,False,False,False,True,False,True,False,"D, F",368,3,371
2019.iwslt-1.11,The {LIG} system for the {E}nglish-{C}zech Text Translation Task of {IWSLT} 2019,"In this paper, we present our submission for the English to Czech Text Translation Task of IWSLT 2019. Our system aims to study how pre-trained language models, used as input embeddings, can improve a specialized machine translation system trained on few data. Therefore, we implemented a Transformer-based encoderdecoder neural system which is able to use the output of a pre-trained language model as input embeddings, and we compared its performance under three configurations: 1) without any pre-trained language model (constrained), 2) using a language model trained on the monolingual parts of the allowed English-Czech data (constrained), and 3) using a language model trained on a large quantity of external monolingual data (unconstrained). We used BERT as external pre-trained language model (configuration 3), and BERT architecture for training our own language model (configuration 2). Regarding the training data, we trained our MT system on a small quantity of parallel text: one set only consists of the provided MuST-C corpus, and the other set consists of the MuST-C corpus and the News Commentary corpus from WMT. We observed that using the external pre-trained BERT improves the scores of our system by +0.8 to +1.5 of BLEU on our development set, and +0.97 to +1.94 of BLEU on the test set. However, using our own language model trained only on the allowed parallel data seems to improve the machine translation performances only when the system is trained on the smallest dataset.",2019,False,False,False,True,False,False,True,"D, G",433,3,436
W19-5365,{NTT}{'}s Machine Translation Systems for {WMT}19 Robustness Task,"This paper describes NTT's submission to the WMT19 robustness task. This task mainly focuses on translating noisy text (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as news. Our submission combined techniques including utilization of a synthetic corpus, domain adaptation, and a placeholder mechanism, which significantly improved over the previous baseline. Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including emojis and emoticons with special placeholder tokens during translation, improves translation accuracy even with noisy texts.",2019,False,False,False,True,False,False,True,"D, G",222,3,225
D19-5536,Benefits of Data Augmentation for {NMT}-based Text Normalization of User-Generated Content,"One of the most persistent characteristics of written user-generated content (UGC) is the use of non-standard words. This characteristic contributes to an increased difficulty to automatically process and analyze UGC. Text normalization is the task of transforming lexical variants to their canonical forms and is often used as a pre-processing step for conventional NLP tasks in order to overcome the performance drop that NLP systems experience when applied to UGC. In this work, we follow a Neural Machine Translation approach to text normalization. To train such an encoderdecoder model, large parallel training corpora of sentence pairs are required. However, obtaining large data sets with UGC and their normalized version is not trivial, especially for languages other than English. In this paper, we explore how to overcome this data bottleneck for Dutch, a low-resource language. We start off with a small publicly available parallel Dutch data set comprising three UGC genres and compare two different approaches. The first is to manually normalize and add training data, a money and time-consuming task. The second approach is a set of data augmentation techniques which increase data size by converting existing resources into synthesized non-standard forms. Our results reveal that, while the different approaches yield similar results regarding the normalization issues in the test set, they also introduce a large amount of over-normalizations.",2019,True,False,False,True,False,False,False,"A, D",375,3,378
W19-6616,Selecting Informative Context Sentence by Forced Back-Translation,"As one of the contributions of this paper, this paper first explores the upper bound of context-based neural machine translation and attempt to utilize previously unused context information. We found that, if we could appropriately select the most informative context sentence for a given input source sentence, we could boost translation accuracy as much as approximately 10 BLEU points. This paper next explores a criterion to select the most informative context sentences that give the highest BLEU score. Applying the proposed criterion, context sentences that yield the highest forced back-translation probability when back-translating into the source sentence are selected. Experimental results with Japanese and English parallel sentences from the OpenSubtitles2018 corpus demonstrate that, when the context length of five preceding and five subsequent sentences are examined, the proposed approach achieved significant improvements of 0.74 (Japanese to English) and 1.14 (English to Japanese) BLEU scores compared to the baseline 2-to-2 model, where the oracle translation achieved upper bounds improvements of 5.88 (Japanese to English) and 9.10 (English to Japanese) BLEU scores.",2019,False,False,False,True,True,False,False,"D, E",339,3,342
W19-6715,Translating Terminologies: A Comparative Examination of {NMT} and {PBSMT} Systems,"Terminology translation is a critical aspect in translation quality assurance, as it requires exact forms not typically expected of conventional translation. Recent studies have examined the quality of machine translation, but little work has focused specifically on the translation of terms. We present a comparative evaluation of the success of NMT and PBSMT systems in term translation. We selected eight language pairs among English, French, German, Finnish, and Romanian, taking into account their diverse language families and resource abundance. Based on the evaluation of Exact Match (EM) and recall scores, we concluded that NMT, in general, performs better with context, but PB-SMT outperforms when translating without context, and found that significant differences often arise from language nature. c",2019,False,False,False,False,True,True,False,"E, F",264,3,267
D19-1142,{HABL}ex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation,"Bilingual lexicons are valuable resources used by professional human translators. While these resources can be easily incorporated in statistical machine translation, it is unclear how to best do so in the neural framework. In this work, we present the HABLex dataset, designed to test methods for bilingual lexicon integration into neural machine translation. Our data consists of humangenerated alignments of words and phrases in machine translation test sets in three language pairs (Russian→English, Chinese→English, and Korean→English), resulting in clean bilingual lexicons which are well matched to the reference. We also present two simple baselines-constrained decoding and continued training-and an improvement to continued training to address overfitting.",2019,True,False,False,True,False,False,False,"A, D",256,3,259
W19-5349,The En-{R}u Two-way Integrated Machine Translation System Based on Transformer,"Machine translation is one of the most popular areas in natural language processing. WMT is a conference to assess the level of machine translation capabilities of organizations around the world, which is the evaluation activity we participated in. In this review we participated in a two-way translation track from Russian to English and English to Russian. We used official training data, 38 million parallel corpora, and 10 million monolingual corpora. The overall framework we use is the Transformer(Vaswani et al., 2017)  neural machine translation model, supplemented by data filtering, post-processing, reordering and other related processing methods. The BLEU (Papineni et al., 2002) value of our final translation result from Russian to English is 38.7, ranking 5th, while from English to Russian is 27.8, ranking 10th.",2019,False,False,False,True,False,False,True,"D, G",294,3,297
D19-5608,Making Asynchronous Stochastic Gradient Descent Work for Transformers,"Asynchronous stochastic gradient descent (SGD) converges poorly for Transformer models, so synchronous SGD has become the norm for Transformer training. This is unfortunate because asynchronous SGD is faster at raw training speed since it avoids waiting for synchronization. Moreover, the Transformer model is the basis for state-of-the-art models for several tasks, including machine translation, so training speed matters. To understand why asynchronous SGD under-performs, we blur the lines between asynchronous and synchronous methods. We find that summing several asynchronous updates, rather than applying them immediately, restores convergence behavior. With this method, the Transformer attains the same BLEU score 1.36 times as fast.",2019,False,False,True,False,False,True,False,"C, F",250,3,253
W19-7301,Neural Machine Translation of Literary Texts from {E}nglish to {S}lovene,"Neural Machine Translation has shown promising performance in literary texts. Since literary machine translation has not yet been researched for the English-to-Slovene translation direction, this paper aims to fulfill this gap by presenting a comparison among bespoke NMT models, tailored to novels, and Google Neural Machine Translation. The translation models were evaluated by the BLEU and ME-TEOR metrics, assessment of fluency and adequacy, and measurement of the postediting effort. The findings show that all evaluated approaches resulted in an increase in translation productivity. The translation model tailored to a specific author outperformed the model trained on a more diverse literary corpus, based on all metrics except the scores for fluency. However, the translation model by Google still outperforms all bespoke models. The evaluation reveals a very low inter-rater agreement on fluency and adequacy, based on the kappa coefficient values, and significant discrepancies between posteditors. This suggests that these methods might not be reliable, which should be addressed in future studies. Recent years have seen the advent of Neural Machine Translation (NMT), which has shown promising performance in literary texts",2019,False,False,False,True,True,False,False,"D, E",343,3,346
D19-5222,{NLPRL} at {WAT}2019: Transformer-based {T}amil {--} {E}nglish Indic Task Neural Machine Translation System,"This paper describes the Machine Translation (MT) system submitted by the NLPRL team for the Tamil -English Indic Task at WAT 2019. We presented the Neural Machine Translation (NMT) system based on the Transformer approach. Training and performance of the model are evaluated on the En-Tam corpus (An English-Tamil Parallel Corpus) collected by researchers at UFAL (Institute of Formal and Applied Linguistics). The evaluation of the model done using Adequacy, BLEU, RIBES, and AM-FM scores, and the model improves translation in terms of Adequacy, RIBES and AM-FM as compared to the baseline.",2019,False,False,False,True,False,False,True,"G, D",252,3,255
K19-1027,Unsupervised Neural Machine Translation with Future Rewarding,"In this paper, we alleviate the local optimality of back-translation by learning a policy (takes the form of an encoder-decoder and is defined by its parameters) with future rewarding under the reinforcement learning framework, which aims to optimize the global word predictions for unsupervised neural machine translation. To this end, we design a novel reward function to characterize high-quality translations from two aspects: n-gram matching and semantic adequacy. The n-gram matching is defined as an alternative for the discrete BLEU metric, and the semantic adequacy is used to measure the adequacy of conveying the meaning of the source sentence to the target. During training, our model strives for earning higher rewards by learning to produce grammatically more accurate and semantically more adequate translations. Besides, a variational inference network (VIN) is proposed to constrain the corresponding sentences in two languages have the same or similar latent semantic code. On the widely used WMT'14 English-French, WMT'16 English-German and NIST Chineseto-English benchmarks, our models respectively obtain 27. 59/27.15, 19.65/23.42 and  22.40 BLEU points without using any labeled data, demonstrating consistent improvements over previous unsupervised NMT models.",2019,False,True,True,False,False,False,False,"B, C",377,3,380
W19-7203,A Multi-Hop Attention for {RNN} based Neural Machine Translation,"Among recent progresses of neural machine translation models, the invention of the Transformer model is one of the most important progresses. It is well-known that the key technologies of the Transformer include multi-head attention mechanism. This paper introduces the multi-head attention mechanism into the traditional RNNbased neural machine translation model. Moreover, inspired by the existing multihop architectures such as end-to-end memory networks and convolutional sequence to sequence learning model, this paper proposes an RNN based NMT model with a multi-hop attention mechanism. The proposed multi-hop attention model has two heads, where for each head, a context vector is calculated based on the states of the encoder and the decoder. Then, in the second turn of the context vector calculation, those context vectors are updated depending not only on one's own context vector but also on the context vector of the other head. Experimental results show that the proposed model significantly outperforms the baseline in BLEU score in Japanese-to-English/English-to-Japanese machine translation tasks with and without extended context.",2019,False,True,True,False,False,False,False,"B, C",322,3,325
W19-6809,Multilingual Multimodal Machine Translation for {D}ravidian Languages utilizing Phonetic Transcription,"Multimodal machine translation is the task of translating from a source text into the target language using information from other modalities. Existing multimodal datasets have been restricted to only highly resourced languages. In addition to that, these datasets were collected by manual translation of English descriptions from the Flickr30K dataset. In this work, we introduce MMDravi, a Multilingual Multimodal dataset for under-resourced Dravidian languages. It comprises of 30,000 sentences which were created utilizing several machine translation outputs. Using data from MMDravi and a phonetic transcription of the corpus, we build an Multilingual Multimodal Neural Machine Translation system (MMNMT) for closely related Dravidian languages to take advantage of multilingual corpus and other modalities. We evaluate our translations generated by the proposed approach with human-annotated evaluation dataset in terms of BLEU, ME-TEOR, and TER metrics. Relying on multilingual corpora, phonetic transcription, and image features, our approach improves the translation quality for the underresourced languages.",2019,True,True,False,False,False,False,False,"A, B",330,3,333
W19-6609,Automatic error classification with multiple error labels,"Although automatic classification of machine translation errors still cannot provide the same detailed granularity as manual error classification, it is an important task which enables estimation of translation errors and better understanding of the analyzed MT system, in a short time and on a large scale. State-of-the-art methods use hard decisions to assign single error labels to each word. This work presents first results of a new error classification method, which assigns multiple error labels to each word. We assign fractional counts for each label, which can be interpreted as a confidence for the label. Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions.",2019,False,True,False,True,False,False,False,"B, D",244,3,247
2019.iwslt-1.31,Controlling the Output Length of Neural Machine Translation,"The recent advances introduced by neural machine translation (NMT) are rapidly expanding the application fields of machine translation, as well as reshaping the quality level to be targeted. In particular, if translations have to fit some given layout, quality should not only be measured in terms of adequacy and fluency, but also length. Exemplary cases are the translation of document files, subtitles, and scripts for dubbing, where the output length should ideally be as close as possible to the length of the input text. This paper addresses for the first time, to the best of our knowledge, the problem of controlling the output length in NMT. We investigate two methods for biasing the output length with a transformer architecture: i) conditioning the output to a given target-source length-ratio class and ii) enriching the transformer positional embedding with length information. Our experiments show that both methods can induce the network to generate shorter translations, as well as acquiring interpretable linguistic skills.",2019,False,True,False,True,False,False,False,"B, D",311,3,314
D19-5215,{CVIT}{'}s submissions to {WAT}-2019,"This paper describes the Neural Machine Translation systems used by IIIT Hyderabad (CVIT-MT) for the translation tasks part of WAT-2019. We participated in tasks pertaining to Indian languages and submitted results for English-Hindi, Hindi-English, English-Tamil and Tamil-English language pairs. We employ Transformer architecture experimenting with multilingual models and methods for lowresource languages.",2019,False,False,False,True,False,False,True,"D, G",192,3,195
D19-5810,Cross-Task Knowledge Transfer for Query-Based Text Summarization,"We demonstrate the viability of knowledge transfer between two related tasks: machine reading comprehension (MRC) and querybased text summarization. Using an MRC model trained on the SQuAD1.1 dataset as a core system component, we first build an extractive query-based summarizer. For better precision, this summarizer also compresses the output of the MRC model using a novel sentence compression technique. We further leverage pre-trained machine translation systems to abstract our extracted summaries. Our models achieve state-of-the-art results on the publicly available CNN/Daily Mail and Debatepedia datasets, and can serve as simple yet powerful baselines for future systems. We also hope that these results will encourage research on transfer learning from large MRC corpora to query-based summarization.",2019,False,False,False,True,False,False,True,"D, G",272,3,275
W19-4601,Incremental Domain Adaptation for Neural Machine Translation in Low-Resource Settings,"We study the problem of incremental domain adaptation of a generic neural machine translation model with limited resources (e.g., budget and time) for human translations or model training. In this paper, we propose a novel query strategy for selecting ""unlabeled"" samples from a new domain based on sentence embeddings for Arabic. We accelerate the finetuning process of the generic model to the target domain. Specifically, our approach estimates the informativeness of instances from the target domain by comparing the distance of their sentence embeddings to embeddings from the generic domain. We perform machine translation experiments (Ar-to-En direction) for comparing a random sampling baseline with our new approach, similar to active learning, using two small update sets for simulating the work of human translators. For the prescribed setting we can save more than 50% of the annotation costs without loss in quality, demonstrating the effectiveness of our approach.",2019,False,False,False,True,False,False,True,"D, G",294,3,297
W19-0309,"Neural and rule-based {F}innish {NLP} models{---}expectations, experiments and experiences","In this article I take a critical look at some recent results in the field of neural language modeling of Finnish in terms of popular shared tasks. One novel point of view I present is comparing the neural methods' results to traditional rule-based systems for the given tasks, since most of the shared tasks have concentrated on the supervised learning concept. The shared task results I re-evaluate, are morphological regeneration by SIGMORPHON 2016, universal dependency parsing by CONLL-2018 and a machine translation application that imitates WMT 2018 for German instead of English. The Uralic language used throughout is Finnish. I use out of the box, best performing neural systems and rule-based systems and evaluate their results. Tiivistelmä Tässä artikkelissa tarkastelemme joitain hiljattaisia tuloksia niinkutsutuissa shared task -kilpailuissa suomen kielen hemroverkkomallien osalta. Yksi tämän artikkelin kontribuutioista on hermoverkkomallien tuottamien tulosten vertailu perinteisiin sääntöpohjaisiin kielimallituloksiin, sillä shared task -kisailut pääosin keskittyvät täysin tai osittain hallitsemattomien mallien oppimisen konseptiin. Shared taskit joita tässä artikkelissa tarkastelemme ovat SIGMORPHONin 2016 morfologisen uudelleengeneroinnin kisa, CONLL:n 2018 jäsennyskilpailu sekä WMT 2018:n konekäännöskilpailu uudelleensovellettuna saksan kielelle. Uralilainen kieli jota käytämme kaikissa kokeissa on suomi. Järjestelmät joita käytetään ovat avoimen lähdekoodin järjestelmiä jotka ovat olleet parhaita näissä kilpailuissa.",2019,False,False,False,False,True,True,False,"E,F",508,2,510
W19-8704,Designing a Frame-Semantic Machine Translation Evaluation Metric,"We propose a metric for machine translation evaluation based on frame semantics which does not require the use of reference translations or human corrections, but is aimed at comparing original and translated output directly. The metric is developed on the basis of an existing manual frame-semantic annotation of a parallel corpus with an English original and a Brazilian Portuguese and a German translation. We discuss implications of our metric design, including the potential of scaling it for multiple languages.",2019,True,False,False,False,True,False,False,"A, E",204,3,207
W19-5356,{WMDO}: Fluency-based Word Mover{'}s Distance for Machine Translation Evaluation,"We propose WMD O , a metric based on distance between distributions in the semantic vector space. Matching in the semantic space has been investigated for translation evaluation, but the constraints of a translation's word order have not been fully explored. Building on the Word Mover's Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics.",2019,False,False,True,True,False,False,False,"C, D",215,3,218
W19-6804,A Continuous Improvement Framework of Machine Translation for {S}hipibo-Konibo,"Shipibo-Konibo is a low-resource language from Peru with prior results in statistical machine translation; however, it is challenging to enhance them mainly due to the expensiveness of building more parallel corpora. Thus, we aim for a continuous improvement framework of the Spanish-Shipibo-Konibo language-pair by taking advantage of more advanced strategies and crowd-sourcing. Besides the introduction of a new domain for translation based on language learning flashcards, our main contributions are the extension of the machine translation experiments for Shipibo-Konibo to neural architectures with transfer and active learning; and the building of a conversational agent prototype to retrieve new translations through a social media platform.",2019,True,False,False,True,False,False,False,"A, D",250,3,253
W19-8629,Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation,"Neural Machine Translation (NMT) models tend to achieve best performance when larger sets of parallel sentences are provided for training. For this reason, augmenting the training set with artificially-generated sentence pairs can boost performance. Nonetheless, the performance can also be improved with a small number of sentences if they are in the same domain as the test set. Accordingly, we want to explore the use of artificially-generated sentences along with data-selection algorithms to improve Germanto-English NMT models trained solely with authentic data. In this work, we show how artificiallygenerated sentences can be more beneficial than authentic pairs, and demonstrate their advantages when used in combination with dataselection algorithms.",2019,False,False,False,True,True,False,False,"D, E",250,3,253
P19-3027,"{T}exar: A Modularized, Versatile, and Extensible Toolkit for Text Generation","We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks that transform any inputs into natural language, such as machine translation, summarization, dialog, content manipulation, and so forth. With the design goals of modularity, versatility, and extensibility in mind, Texar extracts common patterns underlying the diverse tasks and methodologies, creates a library of highly reusable modules and functionalities, and allows arbitrary model architectures and algorithmic paradigms. In Texar, model architecture, inference, and learning processes are properly decomposed. Modules at a high concept level can be freely assembled or plugged in/swapped out. Texar is thus particularly suitable for researchers and practitioners to do fast prototyping and experimentation. The versatile toolkit also fosters technique sharing across different text generation tasks. Texar supports both TensorFlow and PyTorch, and is released under Apache License 2.0 at https: //www.texar.io. 1",2019,False,True,False,True,False,False,False,"B, D",308,3,311
W19-6205,Multilingual Probing of Deep Pre-Trained Contextual Encoders,"Encoders that generate representations based on context have, in recent years, benefited from adaptations that allow for pre-training on large text corpora. Earlier work on evaluating fixed-length sentence representations has included the use of 'probing' tasks, that use diagnostic classifiers to attempt to quantify the extent to which these encoders capture specific linguistic phenomena. The principle of probing has also resulted in extended evaluations that include relatively newer wordlevel pre-trained encoders. We build on probing tasks established in the literature and comprehensively evaluate and analyse -from a typological perspective amongst others -multilingual variants of existing encoders on probing datasets constructed for 6 non-English languages. Specifically, we probe each layer of a multiple monolingual RNN-based ELMo models, the transformer-based BERT's cased and uncased multilingual variants, and a variant of BERT that uses a cross-lingual modelling scheme (XLM). Background 2.1 Deep pre-training A watershed moment in NLP has been the recent innovation spree in deep pre-training; it has represented a considerable step up from shallow pre-training methods, that have been used in NLP since the introduction of contextual word embedding models such as word2vec (Mikolov et al., 2013) . Whilst deep pre-training has been used in non-NLP, image-oriented tasks, where the standard paradigm is to pre-train deep convolutional networks on datasets like ImageNet (Russakovsky et al., 2014) , and then fine-tune on task-specific data, their introduction to textual domains has been considerably slower, yet has been picking up rapidly in recent years. An early paper in this theme was CoVe (Mc-Cann et al., 2017), that pre-trained contextual encoders on seq2seq machine translation models. Another earlier seminal work that addressed numerous technical issues with pre-training was Howard and Ruder's ULMFiT (2018). Not long after, the principle of deep pre-training saw widespread adoption with ELMo (Peters et al.,  2018a), that consisted of several innovations over CoVe: critically, the use of an unsupervised (albeit structured) task -language modelling -for pretraining, and the use of a linear combination of all encoder layers, instead of just the top layer. Architecturally, ELMo used two-layer bidirectional LSTMs along with character-level convolutions, to model word probabilities given the history. With deep pre-training having been established as a valid strategy in NLP, alternative models with different underlying architectures were proposed. The OpenAI GPT (Radford et al., 2018) was one such model; instead of LSTMs, it used the decoder of an attention-based transformer (Vaswani et al., 2017) as its underlying encoder -the justification being that using the transformer's encoder would lead to each token having access to succeeding tokens. The GPT also achieved (then) state-of-theart results by plugging generated fixed-length vectors into downstream classifiers. Another system that represented a significant innovation was BERT (Devlin et al., 2018). BERT introduced a language modelling variant, dubbed masked language modelling, that allowed them to use transformer encoders as their underlying encoding mechanism. Multilingual pre-training",2019,False,False,False,False,True,True,False,"E, F",782,3,785
D19-5228,Overcoming the Rare Word Problem for low-resource language pairs in Neural Machine Translation,"Among the six challenges of neural machine translation (NMT) coined by (Koehn and Knowles, 2017) , rare-word problem is considered the most severe one, especially in translation of low-resource languages. In this paper, we propose three solutions to address the rare words in neural machine translation systems. First, we enhance source context to predict the target words by connecting directly the source embeddings to the output of the attention component in NMT. Second, we propose an algorithm to learn morphology of unknown words for English in supervised way in order to minimize the adverse effect of rare-word problem. Finally, we exploit synonymous relation from the WordNet to overcome out-of-vocabulary (OOV) problem of NMT. We evaluate our approaches on two lowresource language pairs: English-Vietnamese and Japanese-Vietnamese. In our experiments, we have achieved significant improvements of up to roughly +1.0 BLEU points in both language pairs.",2019,False,True,True,False,False,False,False,"B, C",310,3,313
W19-6617,Memory-Augmented Neural Networks for Machine Translation,"Memory-augmented neural networks (MANNs) have been shown to outperform other recurrent neural network architectures on a series of artificial sequence learning tasks, yet they have had limited application to real-world tasks. We evaluate direct application of Neural Turing Machines (NTM) and Differentiable Neural Computers (DNC) to machine translation. We further propose and evaluate two models which extend the attentional encoder-decoder with capabilities inspired by memory augmented neural networks. We evaluate our proposed models on IWSLT Vietnamese→English and ACL Romanian→English datasets. Our proposed models and the memory augmented neural networks perform similarly to the attentional encoder-decoder on the Vietnamese→English translation task while have a 0.3-1.9 lower BLEU score for the Romanian→English task. Interestingly, our analysis shows that despite being equipped with additional flexibility and being randomly initialized memory augmented neural networks learn an algorithm for machine translation almost identical to the attentional encoder-decoder.",2019,False,False,False,False,False,True,True,"F, G",312,3,315
W19-7503,Revisiting the Role of Feature Engineering for Compound Type Identification in {S}anskrit,"We propose an automated approach for semantic class identification of compounds in Sanskrit. It is essential to extract semantic information hidden in compounds for improving overall downstream Natural Language Processing (NLP) applications such as information extraction, question answering, machine translation, and many more. In this work, we systematically investigate the following research question: Can recent advances in neural network outperform traditional hand engineered feature based methods on the semantic level multi-class compound classification task for Sanskrit? Contrary to the previous methods, our method does not require feature engineering. For well-organized analysis, we categorize neural systems based on Multi-Layer Perceptron (MLP), Convolution Neural Network (CNN) and Long Short Term Memory (LSTM) architecture and feed input to the system from one of the possible levels, namely, word level, sub-word level, and character level. Our best system with LSTM architecture and FastText embedding with end-to-end training has shown promising results in terms of F-score (0.73) compared to the state of the art method based on feature engineering (0.74) and outperformed in terms of accuracy (77.68%).",2019,False,True,False,False,False,True,False,"B, F",345,3,348
R19-1068,Study on Unsupervised Statistical Machine Translation for Backtranslation,"Machine Translation systems have drastically improved over the years for several language pairs. Monolingual data is often used to generate synthetic sentences to augment the training data which has shown to improve the performance of machine translation models. In our paper, we make use of an Unsupervised Statistical Machine Translation (USMT) to generate synthetic sentences. Our study compares the performance improvements in Neural Machine Translation model when using synthetic sentences from supervised and unsupervised Machine Translation models. Our approach of using USMT for backtranslation shows promise in low resource conditions and achieves an improvement of 3.2 BLEU score over the Neural Machine Translation model.",2019,False,False,False,True,False,False,True,"D, G",243,3,246
P19-1352,Shared-Private Bilingual Word Embeddings for Neural Machine Translation,"Word embedding is central to neural machine translation (NMT), which has attracted intensive research interest in recent years. In NMT, the source embedding plays the role of the entrance while the target embedding acts as the terminal. These layers occupy most of the model parameters for representation learning. Furthermore, they indirectly interface via a soft-attention mechanism, which makes them comparatively isolated. In this paper, we propose shared-private bilingual word embeddings, which give a closer relationship between the source and target embeddings, and which also reduce the number of model parameters. For similar source and target words, their embeddings tend to share a part of the features and they cooperatively learn these common representation units. Experiments on 5 language pairs belonging to 6 different language families and written in 5 different alphabets demonstrate that the proposed model provides a significant performance boost over the strong baselines with dramatically fewer model parameters.",2019,False,True,False,True,False,False,False,"B, D",296,3,299
W19-5436,Quality and Coverage: The {AFRL} Submission to the {WMT}19 Parallel Corpus Filtering for Low-Resource Conditions Task,"The WMT19 Parallel Corpus Filtering For Low-Resource Conditions Task aims to test various methods of filtering noisy parallel corpora, to make them useful for training machine translation systems. This year the noisy corpora are from the relatively low-resource language pairs of English-Nepali and English-Sinhala. This papers describes the Air Force Research Laboratory (AFRL) submissions, including preprocessing methods and scoring metrics. Numerical results indicate a benefit over baseline and the relative effects of different options.",2019,True,False,False,True,False,False,False,"A, D",214,3,217
W19-5341,{B}aidu Neural Machine Translation Systems for {WMT}19,"In this paper we introduce the systems Baidu submitted for the WMT19 shared task on Chinese↔English news translation. Our systems are based on the Transformer architecture with some effective improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our Chinese→English system achieved the highest case-sensitive BLEU score among all constrained submissions, and our English→Chinese system ranked the second in all submissions.",2019,False,False,False,True,False,False,True,"D, G",217,3,220
D19-1331,On {NMT} Search Errors and Model Errors: Cat Got Your Tongue?,"We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.",2019,False,False,True,False,False,True,False,"F, C",323,3,326
R19-1004,Enhancing Phrase-Based Statistical Machine Translation by Learning Phrase Representations Using Long Short-Term Memory Network,"Phrases play a key role in Machine Translation (MT). In this paper, we apply a Long Short-Term Memory (LSTM) model over conventional Phrase-Based Statistical MT (PBSMT). The core idea is to use an LSTM encoder-decoder to score the phrase table generated by the PBSMT decoder. Given a source sequence, the encoder and decoder are jointly trained in order to maximize the conditional probability of a target sequence. Analytically, the performance of a PBSMT system is enhanced by using the conditional probabilities of phrase pairs computed by an LSTM encoder-decoder as an additional feature in the existing log-linear model. We compare the performance of the phrase tables in the PBSMT to the performance of the proposed LSTM and observe its positive impact on translation quality. We construct a PBSMT model using the Moses decoder and enrich the Language Model (LM) utilizing an external dataset. We then rank the phrase tables using an LSTM-based encoder-decoder. This method produces a gain of up to 3.14 BLEU score on the test set.",2019,False,False,False,True,False,False,True,"D, G",331,3,334
W19-5428,{NICT}{'}s Machine Translation Systems for the {WMT}19 Similar Language Translation Task,"This paper presents the NICT's participation in the WMT19 shared Similar Language Translation Task. We participated in the Spanish-Portuguese task. For both translation directions, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems with the Transformer architecture were trained on the provided parallel data enlarged with a large quantity of back-translated monolingual data. Our primary submission to the task is the result of a simple combination of our SMT and NMT systems. According to BLEU, our systems were ranked second and third respectively for the Portuguese-to-Spanish and Spanish-to-Portuguese translation directions. For contrastive experiments, we also submitted outputs generated with an unsupervised SMT system.",2019,False,False,False,True,False,False,True,"D, G",269,3,272
W19-6906,Embedding {E}nglish to {W}elsh {MT} in a Private Company,"This paper reports on a Knowledge Transfer Partnership (KTP) project that aimed to implement machine translation technology at a Welsh Language Service Provider, Cymen Cyf 1 . The project involved leveraging the company's large supply of previous translations in order to train custom domain-specific translation engines for its various clients. BLEU scores achieved ranged from 59.06 for the largest domain-specific engine to 48.53 to the smallest. A small experiment using the TAUS DQF productivity evaluation tool (Görög, 2014) was also run on the highestscoring translation engine, which showed an average productivity gain of 30% across all translators. Domain-specific engines were ultimately successfully introduced into the workflow for two main clients, although a lack of domain specific data proved problematic for others. Various techniques such as domain-adaptation as well as improved tagging of previous translations may ameliorate this situation in the future. The translation industry in Wales has seen substantial growth over the past few decades, particularly in response to political pressures. Government legislation currently obliges all public sector bodies to produce bilingual versions",2019,False,False,False,True,False,False,True,"G, D",337,3,340
W19-6719,i{ADAATPA} Project: Pangeanic use cases,"The iADAATPA 1 project coded as N • 2016-EU-IA-0132 that ended in February 2019 is made for building of customized, domain-specific engines for public administrations from EU Member States. The consortium of the project decided to use neural machine translation at the beginning of the project. This represented a challenge for all involved, and the positive aspect is that all public administrations engaged in the iADAATPA project were able to try, test and use state-of-the-art neural technology with a high level of satisfaction.",2019,False,False,False,True,False,False,True,"G, D",229,3,232
W19-5313,{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 News Translation Task,"In this paper, we describe our supervised neural machine translation (NMT) systems that we developed for the news translation task for Kazakh↔English, Gujarati↔English, Chinese↔English, and English→Finnish translation directions. We focused on leveraging multilingual transfer learning and back-translation for the extremely lowresource language pairs: Kazakh↔English and Gujarati↔English translation. For the Chinese↔English translation, we used the provided parallel data augmented with a large quantity of back-translated monolingual data to train state-of-the-art NMT systems. We then employed techniques that have been proven to be most effective, such as back-translation, fine-tuning, and model ensembling, to generate the primary submissions of Chinese↔English. For English→Finnish, our submission from WMT18 remains a strong baseline despite the increase in parallel corpora for this year's task.",2019,False,False,False,True,False,False,True,"D, G",302,3,305
D19-5209,{E}nglish-{M}yanmar Supervised and Unsupervised {NMT}: {NICT}{'}s Machine Translation Systems at {WAT}-2019,"This paper presents the NICT's participation (team ID: NICT) in the 6th Workshop on Asian Translation (WAT-2019) shared translation task, specifically Myanmar (Burmese) -English task in both translation directions. We built neural machine translation (NMT) systems for these tasks. Our NMT systems were trained with language model pretraining. Back-translation technology is adopted to NMT. Our NMT systems rank the third in English-to-Myanmar and the second in Myanmar-to-English according to BLEU score.",2019,False,False,False,True,False,False,True,"D, G",229,3,232
W19-6732,Large-scale Machine Translation Evaluation of the i{ADAATPA} Project,"This paper reports the results of an indepth evaluation of 34 state-of-the-art domain-adapted machine translation (MT) systems that were built by four leading MT companies as part of the EU-funded iADAATPA project. These systems support a wide variety of languages for several domains. The evaluation combined automatic metrics and human methods, namely assessments of adequacy, fluency, and comparative ranking. The paper also discusses the most effective techniques to build domain-adapted MT systems for the relevant language combinations and domains.",2019,False,False,False,False,True,True,False,"E, F",223,3,226
W19-7010,Automatization of subprocesses in subtitling,"There has been noticeable growth in the use of intralingual and interlingual subtitling due to technological advances and accessibility legislation. The process of subtitling, however, has yet to be thoroughly investigated with empirical methods. Given that subtitling is a complex task, interpreting keylogging and eyetracking data in the overall process can be complicated. We therefore focus on the subprocesses involved in subtitling, i.e. transcription and translation of movie dialogue. With advancements in neural machine translation (NMT) especially with creative texts (Toral et al. 2018) , research in this special field of translation becomes even more essential to find meaningful ways of improving subtitling processes and informing subtitling training. This development is focus of CompAsS (Computer-Assisted Subtitling), a project funded by the EU and managed by ZDF Digital and University of Mainz with the aim to improve current subtitling processes. Within CompAsS an exploratory study was carried out where the transcription and translation processes of 13 professional subtitlers and 13 translation students were recorded. Participants performed eight intralingual and interlingual transcription tasks. Here we focus on the",2019,False,False,False,False,True,False,True,"E, G",359,3,362
W19-8620,On Leveraging the Visual Modality for Neural Machine Translation,"Leveraging the visual modality effectively for Neural Machine Translation (NMT) remains an open problem in computational linguistics. Recently, Caglayan et al. posit that the observed gains are limited mainly due to the very simple, short, repetitive sentences of the Multi30k dataset (the only multimodal MT dataset available at the time), which renders the source text sufficient for context. In this work, we further investigate this hypothesis on a new large scale multimodal Machine Translation (MMT) dataset, How2, which has 1.57 times longer mean sentence length than Multi30k and no repetition. We propose and evaluate three novel fusion techniques, each of which is designed to ensure the utilization of visual context at different stages of the Sequence-to-Sequence transduction pipeline, even under full linguistic context. However, we still obtain only marginal gains under full linguistic context and posit that visual embeddings extracted from deep vision models (ResNet for Multi30k, ResNext for How2) do not lend themselves to increasing the discriminativeness between the vocabulary elements at token level prediction in NMT. We demonstrate this qualitatively by analyzing attention distribution and quantitatively through Principal Component Analysis, arriving at the conclusion that it is the quality of the visual embeddings rather than the length of sentences, which need to be improved in existing MMT datasets.",2019,True,False,False,False,False,True,False,"A, F",386,3,389
W19-6705,Evaluating machine translation in a low-resource language combination: {S}panish-{G}alician.,"This paper reports the results of a study designed to assess the perception of adequacy of three different types of machine translation systems within the context of a minoritized language combination (Spanish-Galician). To perform this evaluation, a mixed design with three different metrics (BLEU, survey and error analysis) is used to extract quantitative and qualitative data about two marketing letters from the energy industry translated with a rulebased system (RBMT), a phrase-based system (PBMT) and a neural system (NMT). Results show that in the case of low-resource languages rule-based and phrase-based machine translations systems still play an important role.",2019,False,False,False,False,True,True,False,"E,F",244,2,246
D19-5205,{E}nglish to {H}indi Multi-modal Neural Machine Translation and {H}indi Image Captioning,"With the widespread use of Machine Translation (MT) techniques, attempt to minimize communication gap among people from diverse linguistic backgrounds. We have participated in Workshop on Asian Translation 2019 (WAT2019) multi-modal translation task. There are three types of submission track namely, multi-modal translation, Hindionly image captioning and text-only translation for English to Hindi translation. The main challenge is to provide a precise MT output. The multi-modal concept incorporates textual and visual features in the translation task. In this work, multi-modal translation track relies on pre-trained convolutional neural networks (CNN) with Visual Geometry Group having 19 layered (VGG19) to extract image features and attention-based Neural Machine Translation (NMT) system for translation. The merge-model of recurrent neural network (RNN) and CNN is used for the Hindi-only image captioning. The text-only translation track is based on the transformer model of the NMT system. The official results evaluated at WAT2019 translation task, which shows that our multi-modal NMT system achieved Bilingual Evaluation Understudy (BLEU) score 20.37, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.642838, Adequacy-Fluency Metrics (AMFM) score 0.668260 for challenge test data and BLEU score 40.55, RIBES 0.760080, AMFM score 0.770860 for evaluation test data in English to Hindi multimodal translation respectively.",2019,False,False,False,True,False,False,True,"D, G",424,3,427
R19-1107,Combining {PBSMT} and {NMT} Back-translated Data for Efficient {NMT},"Neural Machine Translation (NMT) models achieve their best performance when large sets of parallel data are used for training. Consequently, techniques for augmenting the training set have become popular recently. One of these methods is back-translation (Sennrich et al., 2016a) , which consists on generating synthetic sentences by translating a set of monolingual, target-language sentences using a Machine Translation (MT) model. Generally, NMT models are used for backtranslation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmented with back-translated data created by merging different MT approaches.",2019,False,False,False,True,True,False,False,"E, D",297,3,300
P19-4001,Latent Structure Models for Natural Language Processing,"Latent structure models are a powerful tool for modeling compositional data, discovering linguistic structure, and building NLP pipelines. They are appealing for two main reasons: they allow incorporating structural bias during training, leading to more accurate models; and they allow discovering hidden linguistic structure, which provides better interpretability. This tutorial will cover recent advances in discrete latent structure models. We discuss their motivation, potential, and limitations, then explore in detail three strategies for designing such models: gradient approximation, reinforcement learning, and end-to-end differentiable methods. We highlight connections among all these methods, enumerating their strengths and weaknesses. The models we present and analyze have been applied to a wide variety of NLP tasks, including sentiment analysis, natural language inference, language modeling, machine translation, and semantic parsing. Examples and evaluation will be covered throughout. After attending the tutorial, a practitioner will be better informed about which method is best suited for their problem.",2019,False,False,False,False,True,True,False,"E, F",303,3,306
W19-4424,Neural and {FST}-based approaches to grammatical error correction,"In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural machine translation systems: one using convolutional networks and multi-task learning, and another using a neural Transformer-based system. Training is performed on publicly available data, along with artificial examples generated through back-translation. The n-best lists of these two machine translation systems are then combined and scored using a finite state transducer (FST). Finally, an unsupervised reranking system is applied to the n-best output of the FST. The re-ranker uses a number of error detection features to re-rank the FST nbest list and identify the final 1-best correction hypothesis. Our system achieves 66.75% F 0.5 on error correction (ranking 4th), and 82.52% F 0.5 on token-level error detection (ranking 2nd) in the restricted track of the shared task.",2019,False,False,False,True,False,False,True,"D, G",336,3,339
W19-5355,{SAO} {WMT}19 Test Suite: Machine Translation of Audit Reports,"This paper describes a machine translation test set of documents from the auditing domain and its use as one of the ""test suites"" in the WMT19 News Translation Task for translation directions involving Czech, English and German. Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details. Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties. BLEU chrF3 nCDER nCharacTER nPER nTER nWER",2019,True,False,False,False,True,False,False,"A, E",289,3,292
P19-3012,Demonstration of a Neural Machine Translation System with Online Learning for Translators,"We introduce a demonstration of our system, which implements online learning for neural machine translation in a production environment. These techniques allow the system to continuously learn from the corrections provided by the translators. We implemented an end-to-end platform integrating our machine translation servers to one of the most common user interfaces for professional translators: SDL Trados Studio. Our objective was to save post-editing effort as the machine is continuously learning from human choices and adapting the models to a specific domain or user style.",2019,False,False,False,True,False,False,True,"D, G",214,3,217
W19-6711,Do translator trainees trust machine translation? An experiment on post-editing and revision,"Despite the importance of trust in any work environment, this concept has rarely been investigated for MT. The present contribution aims at filling this gap by presenting a post-editing experiment carried out with translator trainees. An institutional academic text was translated from Italian into English. All participants worked on the same target text. Half of them were told that the text was a human translation needing revision, while the other half was told that it was an MT output to be postedited. Temporal and technical effort were measured based on words per second and HTER. Results were complemented with a manual analysis of a subset of the observations.",2019,False,False,False,False,True,True,False,"E, F",239,3,242
P19-1290,Look Harder: A Neural Machine Translation Model with Hard Attention,"Soft-attention based Neural Machine Translation (NMT) models have achieved promising results on several translation tasks. These models attend all the words in the source sequence for each target token, which makes them ineffective for long sequence translation. In this work, we propose a hard-attention based NMT model which selects a subset of source tokens for each target token to effectively handle long sequence translation. Due to the discrete nature of the hard-attention mechanism, we design a reinforcement learning algorithm coupled with reward shaping strategy to efficiently train it. Experimental results show that the proposed model performs better on long sequences and thereby achieves significant BLEU score improvement on English-German (EN-DE) and English-French (EN-FR) translation tasks compared to the soft-attention based NMT.",2019,False,True,True,False,False,False,False,"B, C",272,3,275
P19-1292,A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning,"Automatic post-editing (APE) seeks to automatically refine the output of a black-box machine translation (MT) system through human post-edits. APE systems are usually trained by complementing human post-edited data with large, artificial data generated through backtranslations, a time-consuming process often no easier than training a MT system from scratch. In this paper, we propose an alternative where we fine-tune pre-trained BERT models on both the encoder and decoder of an APE system, exploring several parameter sharing strategies. By only training on a dataset of 23K sentences for 3 hours on a single GPU we obtain results that are competitive with systems that were trained on 5M artificial sentences. When we add this artificial data, our method obtains state-of-the-art results.",2019,False,True,False,True,False,False,False,"B, D",275,3,278
D19-1140,Machine Translation for Machines: the Sentiment Classification Use Case,"We propose a neural machine translation (NMT) approach that, instead of pursuing adequacy and fluency (""human-oriented"" quality criteria), aims to generate translations that are best suited as input to a natural language processing component designed for a specific downstream task (a ""machine-oriented"" criterion). Towards this objective, we present a reinforcement learning technique based on a new candidate sampling strategy, which exploits the results obtained on the downstream task as weak feedback. Experiments in sentiment classification of Twitter data in German and Italian show that feeding an English classifier with machine-oriented translations significantly improves its performance. Classification results outperform those obtained with translations produced by general-purpose NMT models as well as by an approach based on reinforcement learning. Moreover, our results on both languages approximate the classification accuracy computed on gold standard English tweets.",2019,False,True,False,True,False,False,False,"B, D",277,3,280
N19-1095,Recovering dropped pronouns in {C}hinese conversations via modeling their referents,"Pronouns are often dropped in Chinese sentences, and this happens more frequently in conversational genres as their referents can be easily understood from context. Recovering dropped pronouns is essential to applications such as Information Extraction where the referents of these dropped pronouns need to be resolved, or Machine Translation when Chinese is the source language. In this work, we present a novel end-to-end neural network model to recover dropped pronouns in conversational data. Our model is based on a structured attention mechanism that models the referents of dropped pronouns utilizing both sentence-level and word-level information. Results on three different conversational genres show that our approach achieves a significant improvement over the current state of the art.",2019,False,True,False,True,False,False,False,"B, D",255,3,258
W19-6733,Collecting domain specific data for {MT}: an evaluation of the {P}ara{C}rawlpipeline,"This paper investigates the effectiveness of the ParaCrawl pipeline for collecting domain-specific training data for machine translation. We follow the different steps of the pipeline (document alignment, sentence alignment, cleaning) and add a topic-filtering component. Experiments are performed on the legal domain for the English to French and English to Irish language pairs. We evaluate the pipeline at both intrinsic (alignment quality) and extrinsic (MT performance) levels. Our results show that with this pipeline we obtain highquality alignments and significant improvements in MT quality.",2019,False,False,False,True,False,False,True,"D, G",224,3,227
