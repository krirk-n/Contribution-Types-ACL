acl_id,title,abstract,year,A,B,C,D,E,F,G,raw_response,input_tokens,output_tokens,total_tokens
W18-2204,A Survey of Machine Translation Work in the {P}hilippines: From 1998 to 2018,"In this paper, we present a survey covering the last 20 years of machine translation work in the Philippines. We detail the various approaches used and innovations applied. We also discuss the various mechanisms and support that keep the MT community thriving, as well as the challenges ahead.",2018,False,False,False,False,True,True,False,"E,F",172,2,174
P18-2097,An Empirical Study of Building a Strong Baseline for Constituency Parsing,"This paper investigates the construction of a strong baseline based on general purpose sequence-to-sequence models for constituency parsing. We incorporate several techniques that were mainly developed in natural language generation tasks, e.g., machine translation and summarization, and demonstrate that the sequenceto-sequence model achieves the current top-notch parsers' performance without requiring explicit task-specific knowledge or architecture of constituent parsing.",2018,False,False,False,True,False,False,True,"D, G",194,3,197
D18-2015,"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition","Training and testing many possible parameters or model architectures of state-of-the-art machine translation or automatic speech recognition system is a cumbersome task. They usually require a long pipeline of commands reaching from pre-processing the training data to post-processing and evaluating the output. This paper introduces Sisyphus, a tool that aims at managing scientific experiments in an efficient way. After defining the workflow for a given task, Sisyphus runs all required steps and ensures that all commands finish successfully. It avoids unnecessary computations by reusing tasks that are needed for multiple parts of the workflow and saves the user time by determining the order in which the tasks are to be performed. Since the program and workflow are written in Python they can be easily extended to contain arbitrary code. This makes it possible to use the rich collection of Python tools for editing, debugging, and documentation. It only has few requirements on the underlying server or cluster, and has been successfully tested in many large scale setups and can handle thousands of tasks inside the workflow.",2018,False,True,False,True,False,False,False,"B, D",319,3,322
Y18-3010,{CVIT-MT} Systems for {WAT}-2018,This document describes the machine translation system used in the submissions of IIIT-Hyderabad (CVIT-MT) for the WAT-2018 English-Hindi translation task. Performance is evaluated on the associated corpus provided by the organizers. We experimented with convolutional sequence to sequence architectures. We also train with additional data obtained through backtranslation.,2018,False,True,False,False,False,False,True,"B, G",187,3,190
W18-6419,{NICT}{'}s Neural and Statistical Machine Translation Systems for the {WMT}18 News Translation Task,"This paper presents the NICT's participation to the WMT18 shared news translation task. We participated in the eight translation directions of four language pairs: Estonian-English, Finnish-English, Turkish-English and Chinese-English. For each translation direction, we prepared state-of-the-art statistical (SMT) and neural (NMT) machine translation systems. Our NMT systems were trained with the transformer architecture using the provided parallel data enlarged with a large quantity of back-translated monolingual data that we generated with a new incremental training framework. Our primary submissions to the task are the result of a simple combination of our SMT and NMT systems. Our systems are ranked first for the Estonian-English and Finnish-English language pairs (constraint) according to BLEU-cased.",2018,False,False,False,True,False,False,True,"D, G",271,3,274
C18-1274,Neural Machine Translation Incorporating Named Entity,"This study proposes a new neural machine translation (NMT) model based on the encoderdecoder model that incorporates named entity (NE) tags of source-language sentences. Conventional NMT models have two problems enumerated as follows: (i) they tend to have difficulty in translating words with multiple meanings because of the high ambiguity, and (ii) these models' ability to translate compound words seems challenging because the encoder receives a word, a part of the compound word, at each time step. To alleviate these problems, the encoder of the proposed model encodes the input word on the basis of its NE tag at each time step, which could reduce the ambiguity of the input word. Furthermore, the encoder introduces a chunk-level LSTM layer over a word-level LSTM layer and hierarchically encodes a source-language sentence to capture a compound NE as a chunk on the basis of the NE tags. We evaluate the proposed model on an English-to-Japanese translation task with the ASPEC, and English-to-Bulgarian and English-to-Romanian translation tasks with the Europarl corpus. The evaluation results show that the proposed model achieves up to 3.11 point improvement in BLEU.",2018,False,True,False,True,False,False,False,"B, D",358,3,361
Y18-1046,Automatic Error Correction on {J}apanese Functional Expressions Using Character-based Neural Machine Translation,"Correcting spelling and grammatical errors of Japanese functional expressions shows practical usefulness for Japanese Second Language (JSL) learners. However, the collection of these types of error data is difficult because it relies on detecting Japanese functional expressions first. In this paper, we propose a framework to correct the spelling and grammatical errors of Japanese functional expressions as well as the error data collection problem. Firstly, we apply a bidirectional Long Short-Term Memory with a Conditional Random Field (BiLSTM-CRF) model to detect Japanese functional expressions. Secondly, we extract phrases which include Japanese functional expressions as well as their neighboring words from native Japanese and learners' corpora. Then we generate a large scale of artificial error data via substitution, injection and deletion operations. Finally, we utilize the generated artificial error data to train a sequence-to-sequence neural machine translation model for correcting Japanese functional expression errors. We also compare the character-based method with the word-based method. The experimental results indicate that the character-based method outperforms the wordbased method both on artificial error data and real error data.",2018,True,True,False,False,False,False,False,"A, B",331,3,334
C18-1055,On Adversarial Examples for Character-Level Neural Machine Translation,"Evaluating on adversarial examples has become a standard procedure to measure robustness of deep learning models. Due to the difficulty of creating white-box adversarial examples for discrete text input, most analyses of the robustness of NLP models have been done through blackbox adversarial examples. We investigate adversarial examples for character-level neural machine translation (NMT), and contrast black-box adversaries with a novel white-box adversary, which employs differentiable string-edit operations to rank adversarial changes. We propose two novel types of attacks which aim to remove or change a word in a translation, rather than simply break the NMT. We demonstrate that white-box adversarial examples are significantly stronger than their black-box counterparts in different attack scenarios, which show more serious vulnerabilities than previously known. In addition, after performing adversarial training, which takes only 3 times longer than regular training, we can improve the model's robustness significantly.",2018,False,True,True,False,False,False,False,"B, C",299,3,302
P18-2036,Characterizing Departures from Linearity in Word Translation,"We investigate the behavior of maps learned by machine translation methods. The maps translate words by projecting between word embedding spaces of different languages. We locally approximate these maps using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying maps are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.",2018,False,False,False,False,True,True,False,"E, F",228,3,231
W18-6314,Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection,"Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of data quality and tries to reduce the negative impact of data noise on MT training, in particular, neural MT (NMT) training. This paper generalizes methods for measuring and selecting data for domain MT and applies them to denoising NMT training. The proposed approach uses trusted data and a denoising curriculum realized by online data selection. Intrinsic and extrinsic evaluations of the approach show its significant effectiveness for NMT to train on data with severe noise.",2018,False,False,False,True,False,False,True,"D, G",257,3,260
D18-1458,Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures,"Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation. * Work carried out during a visit to the machine translation group at the University of Edinburgh.",2018,False,False,False,False,True,True,False,"E,F",361,2,363
2018.gwc-1.10,Improving Wordnets for Under-Resourced Languages Using Machine Translation,"Wordnets are extensively used in natural language processing, but the current approaches for manually building a wordnet from scratch involves large research groups for a long period of time, which are typically not available for under-resourced languages. Even if wordnet-like resources are available for under-resourced languages, they are often not easily accessible, which can alter the results of applications using these resources. Our proposed method presents an expand approach for improving and generating wordnets with the help of machine translation. We apply our methods to improve and extend wordnets for the Dravidian languages, i.e., Tamil, Telugu, Kannada, which are severly under-resourced languages. We report evaluation results of the generated wordnet senses in term of precision for these languages. In addition to that, we carried out a manual evaluation of the translations for the Tamil language, where we demonstrate that our approach can aid in improving wordnet resources for under-resourced Dravidian languages.",2018,True,False,False,False,False,False,True,"A, G",308,3,311
W18-1810,An Evaluation of Two Vocabulary Reduction Methods for Neural Machine Translation,"Neural machine translation (NMT) models are conventionally trained with fixed-size vocabularies to control the computational complexity and the quality of the learned word representations. This, however, limits the accuracy and the generalization capability of the models, especially for morphologically-rich languages, which usually have very sparse vocabularies containing rare inflected or derivated word forms. Some studies tried to overcome this problem by segmenting words into subword level representations and modeling translation at this level. However, recent findings have shown that if these methods interrupt the word structure during segmentation, they might cause semantic or syntactic losses and lead to generating inaccurate translations. In order to investigate this phenomenon, we present an extensive evaluation of two unsupervised vocabulary reduction methods in NMT. The first is the wellknown byte-pair-encoding (BPE), a statistical subword segmentation method, whereas the second is linguistically-motivated vocabulary reduction (LMVR), a segmentation method which also considers morphological properties of subwords. We compare both approaches on ten translation directions involving English and five other languages (Arabic, Czech, German, Italian and Turkish), each representing a distinct language family and morphological typology. LMVR obtains significantly better performance in most languages, showing gains proportional to the sparseness of the vocabulary and the morphological complexity of the tested language.",2018,False,False,False,False,True,True,False,"E, F",386,3,389
W18-3921,{IIT} ({BHU}) System for {I}ndo-{A}ryan Language Identification ({ILI}) at {V}ar{D}ial 2018,"Text Language Identification is a Natural Language Processing (NLP) task of identifying and recognizing a given language out of many different languages from a piece of text. In the present scenario, this task has become the basis and beginning step of various other NLP tasks, for example, Machine Translation, improving search relevance for a multilingual query, processing code-switched data etc. The biggest limitation of many Language Identification systems is not being able to differentiate between closely related languages. This paper describes our submission to the ILI 2018 shared-task, which includes the identification of 5 closely related Indo-Aryan languages. We used a word-level LSTM (Long Short-Term Memory) model, a specific type of Recurrent Neural Network model, for this task. Given a sentence, our model embeds each word of the sentence and convert into its trainable word embedding, feeds them into our LSTM network and finally predict the language. We obtained an F1 macro score of 0.836, ranking 5th in the task.",2018,False,False,False,True,False,False,True,"D, G",323,3,326
Y18-3015,"{O}saka {U}niversity {MT} Systems for {WAT} 2018: Rewarding, Preordering, and Domain Adaptation","In this paper, we present Osaka University MT systems submitted to WAT 2018 shared translation tasks and analysis of their performances. For the ASPEC Japanese-English task, we use our rewarding model on neural machine translation (NMT) and preordering model on phrase-based statistical machine translation (PBSMT). For the Myanmar-English task, we further apply our mixed fine tuning method for domain adaptation on NMT. We report the translation results on these two tasks, where the rewarding model performs the best.",2018,False,False,False,True,False,False,True,"D, G",218,3,221
D18-1040,Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation,"Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While backtranslation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 BLEU points over back-translation using random sampling for GermanÑEnglish and EnglishÑGerman, respectively.",2018,False,False,False,True,False,True,False,"D, F",309,3,312
L18-1113,Correction of {OCR} Word Segmentation Errors in Articles from the {ACL} Collection through Neural Machine Translation Methods,"Depending on the quality of the original document, Optical Character Recognition (OCR) can produce a range of errors -from erroneous letters to additional and spurious blank spaces. We applied a sequence-to-sequence machine translation system to correct word-segmentation OCR errors in scientific texts from the ACL collection with an estimated precision and recall above 0.95 on test data. We present the correction process and results.",2018,False,False,False,True,False,False,True,"G, D",199,3,202
L18-1048,No more beating about the bush : A Step towards Idiom Handling for {I}ndian Language {NLP},"One of the major challenges in the field of Natural Language Processing (NLP) is the handling of idioms; seemingly ordinary phrases which could be further conjugated or even spread across the sentence to fit the context. Since idioms are a part of natural language, the ability to tackle them brings us closer to creating efficient NLP tools. This paper presents a multilingual parallel idiom dataset for seven Indian languages in addition to English and demonstrates its usefulness for two NLP applications -Machine Translation and Sentiment Analysis. We observe significant improvement for both the subtasks over baseline models trained without employing the idiom dataset.",2018,True,False,False,False,False,False,True,"A, G",238,3,241
P18-1142,Isomorphic Transfer of Syntactic Structures in Cross-Lingual {NLP},"The transfer or share of knowledge between languages is a popular solution to resource scarcity in NLP. However, the effectiveness of cross-lingual transfer can be challenged by variation in syntactic structures. Frameworks such as Universal Dependencies (UD) are designed to be cross-lingually consistent, but even in carefully designed resources trees representing equivalent sentences may not always overlap. In this paper, we measure cross-lingual syntactic variation, or anisomorphism, in the UD treebank collection, considering both morphological and structural properties. We show that reducing the level of anisomorphism yields consistent gains in cross-lingual transfer tasks. We introduce a source language selection procedure that facilitates effective cross-lingual parser transfer, and propose a typologically driven method for syntactic tree processing which reduces anisomorphism. Our results show the effectiveness of this method for both machine translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP.",2018,False,False,False,True,True,False,False,"E, D",317,3,320
P18-1180,A Distributional and Orthographic Aggregation Model for {E}nglish Derivational Morphology,"Modeling derivational morphology to generate words with particular semantics is useful in many text generation tasks, such as machine translation or abstractive question answering. In this work, we tackle the task of derived word generation. That is, given the word ""run,"" we attempt to generate the word ""runner"" for ""someone who runs."" We identify two key problems in generating derived words from root words and transformations: suffix ambiguity and orthographic irregularity. We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space. Our best open-vocabulary model, which can generate novel words, and our best closed-vocabulary model, show 22% and 37% relative error reductions over current state-of-the-art systems on the same dataset.",2018,False,True,True,False,False,False,False,"B, C",287,3,290
W18-6464,{UA}lacant machine translation quality estimation at {WMT} 2018: a simple approach using phrase tables and feed-forward neural networks,"We describe the Universitat d'Alacant submissions to the word-and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018. Our approach to word-level MT QE builds on previous work to mark the words in the machine-translated sentence as OK or BAD, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them.",2018,False,False,False,True,False,False,True,"D, G",251,3,254
N18-1118,Evaluating Discourse Phenomena in Neural Machine Translation,"For machine translation to tackle discourse phenomena, models must have access to extrasentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models' ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50% accuracy on our coreference test set and 53.5% for coherence/cohesion (compared to a non-contextual baseline of 50%). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multiencoding and decoding of two sentences leads to the best performance (72.5% for coreference and 57% for coherence/cohesion), highlighting the importance of target-side context.",2018,True,False,False,True,False,False,False,"A, D",351,3,354
N18-3015,From dictations to clinical reports using machine translation,"A typical workflow to document clinical encounters entails dictating a summary, running speech recognition, and post-processing the resulting text into a formatted letter. Post-processing entails a host of transformations including punctuation restoration, truecasing, marking sections and headers, converting dates and numerical expressions, parsing lists, etc. In conventional implementations, most of these tasks are accomplished by individual modules. We introduce a novel holistic approach to post-processing that relies on machine callytranslation. We show how this technique outperforms an alternative conventional system-even learning to correct speech recognition errors during post-processingwhile being much simpler to maintain.",2018,False,True,False,True,False,False,False,"B, D",237,3,240
D18-1342,Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation,"Beam search is widely used in neural machine translation, and usually improves translation quality compared to greedy search. It has been widely observed that, however, beam sizes larger than 5 hurt translation quality. We explain why this happens, and propose several methods to address this problem. Furthermore, we discuss the optimal stopping criteria for these methods. Results show that our hyperparameter-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation.",2018,False,False,False,True,False,True,False,"F, D",225,3,228
W18-6423,Tilde{'}s Machine Translation Systems for {WMT} 2018,"The paper describes the development process of the Tilde's NMT systems that were submitted for the WMT 2018 shared task on news translation. We describe the data filtering and pre-processing workflows, the NMT system training architectures, and automatic evaluation results. For the WMT 2018 shared task, we submitted seven systems (both constrained and unconstrained) for English-Estonian and Estonian-English translation directions. The submitted systems were trained using Transformer models.",2018,False,False,False,True,False,False,True,"D, G",212,3,215
L18-1144,Improving a Multi-Source Neural Machine Translation Model with Corpus Extension for Low-Resource Languages,"In machine translation, we often try to collect resources to improve performance. However, most of the language pairs, such as Korean-Arabic and Korean-Vietnamese, do not have enough resources to train machine translation systems. In this paper, we propose the use of synthetic methods for extending a low-resource corpus and apply it to a multi-source neural machine translation model. We showed the improvement of machine translation performance through corpus extension using the synthetic method. We specifically focused on how to create source sentences that can make better target sentences, including the use of synthetic methods. We found that the corpus extension could also improve the performance of multi-source neural machine translation. We showed the corpus extension and multi-source model to be efficient methods for a low-resource language pair. Furthermore, when both methods were used together, we found better machine translation performance.",2018,False,False,False,True,False,False,True,"D, G",285,3,288
W18-2707,Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation,"A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a method that enhances the encoder and attention using target monolingual corpora by generating multiple source sentences via sampling. By using multiple source sentences, diversity close to that of humans is achieved. Our experimental results show that the translation quality is improved by increasing the number of synthetic source sentences for each given target sentence, and quality close to that using a manually created parallel corpus was achieved.",2018,False,True,False,True,False,False,False,"B, D",254,3,257
W18-6465,{A}libaba Submission for {WMT}18 Quality Estimation Task,"The goal of WMT 2018 Shared Task on Translation Quality Estimation is to investigate automatic methods for estimating the quality of machine translation results without reference translations. This paper presents the QE Brain system, which proposes the neural Bilingual Expert model as a feature extractor based on conditional target language model with a bidirectional transformer and then processes the semantic representations of source and the translation output with a Bi-LSTM predictive model for automatic quality estimation. The system has been applied to the sentence-level scoring and ranking tasks as well as the wordlevel tasks for finding errors for each word in translations. An extensive set of experimental results have shown that our system outperformed the best results in WMT 2017 Quality Estimation tasks and obtained top results in WMT 2018. * * indicates equal contribution.",2018,False,True,False,True,False,False,False,"B, D",277,3,280
W18-6302,Character-level {C}hinese-{E}nglish Translation through {ASCII} Encoding,"Character-level Neural Machine Translation (NMT) models have recently achieved impressive results on many language pairs. They mainly do well for Indo-European language pairs, where the languages share the same writing system. However, for translating between Chinese and English, the gap between the two different writing systems poses a major challenge because of a lack of systematic correspondence between the individual linguistic units. In this paper, we enable character-level NMT for Chinese, by breaking down Chinese characters into linguistic units similar to that of Indo-European languages. We use the Wubi encoding scheme 1 , which preserves the original shape and semantic information of the characters, while also being reversible. We show promising results from training Wubi-based models on the characterand subword-level with recurrent as well as convolutional models.",2018,True,False,False,True,False,False,False,"A, D",273,3,276
N18-4015,Metric for Automatic Machine Translation Evaluation based on Universal Sentence Representations,"Sentence representations can capture a wide range of information that cannot be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of machine translation. Although it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only.",2018,False,False,False,True,False,False,True,"D, G",223,3,226
D18-1549,Phrase-Based {\&} Neural Unsupervised Machine Translation,"Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semisupervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available. 1",2018,False,True,False,True,False,False,False,"B, D",358,3,361
L18-1548,The {IIT} {B}ombay {E}nglish-{H}indi Parallel Corpus,"We present the IIT Bombay English-Hindi Parallel Corpus. The corpus is a compilation of parallel corpora previously available in the public domain as well as new parallel corpora we collected. The corpus contains 1.49 million parallel segments, of which 694k segments were not previously available in the public domain. The corpus has been pre-processed for machine translation, and we report baseline phrase-based SMT and NMT translation results on this corpus. This corpus has been used in two editions of shared tasks at the Workshop on Asian Language Translation (2016 and 2017) . The corpus is freely available for non-commercial research. To the best of our knowledge, this is the largest publicly available English-Hindi parallel corpus.",2018,True,False,False,False,False,False,True,"A, G",263,3,266
W18-2507,{O}pen{S}eq2{S}eq: Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models,We present OpenSeq2Seq -an opensource toolkit for training sequence-tosequence models. The main goal of our toolkit is to allow researchers to most effectively explore different sequence-tosequence architectures. The efficiency is achieved by fully supporting distributed and mixed-precision training. OpenSeq2Seq provides building blocks for training encoder-decoder models for neural machine translation and automatic speech recognition. We plan to extend it with other modalities in the future.,2018,False,True,False,False,False,False,True,"B, G",206,3,209
P18-4020,{M}arian: Fast Neural Machine Translation in {C}++,"We present Marian, an efficient and selfcontained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed.",2018,False,True,False,True,False,False,False,"B, D",175,3,178
D18-1336,End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification,"Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols independently in parallel. We present a novel nonautoregressive architecture based on connectionist temporal classification and evaluate it on the task of neural machine translation. Unlike other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English-German datasets. Our models achieve a significant speedup over the autoregressive models, keeping the translation quality comparable to other non-autoregressive models.",2018,False,True,False,True,False,False,False,"B, D",250,3,253
W18-3902,Encoder-Decoder Methods for Text Normalization,"Text normalization is the task of mapping non-canonical language, typical of speech transcription and computer-mediated communication, to a standardized writing. It is an up-stream task necessary to enable the subsequent direct employment of standard natural language processing tools and indispensable for languages such as Swiss German, with strong regional variation and no written standard. Text normalization has been addressed with a variety of methods, most successfully with character-level statistical machine translation (CSMT). In the meantime, machine translation has changed and the new methods, known as neural encoder-decoder (ED) models, resulted in remarkable improvements. Text normalization, however, has not yet followed. A number of neural methods have been tried, but CSMT remains the state-of-the-art. In this work, we normalize Swiss German WhatsApp messages using the ED framework. We exploit the flexibility of this framework, which allows us to learn from the same training data in different ways. In particular, we modify the decoding stage of a plain ED model to include target-side language models operating at different levels of granularity: characters and words. Our systematic comparison shows that our approach results in an improvement over the CSMT state-of-the-art.",2018,False,True,False,True,False,False,False,"B, D",353,3,356
W18-6451,Findings of the {WMT} 2018 Shared Task on Quality Estimation,"We report the results of the WMT18 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems at various granularity levels: word, phrase, sentence and document. This year we include four language pairs, three text domains, and translations produced by both statistical and neural machine translation systems. Participating teams from ten institutions submitted a variety of systems to different task variants and language pairs.",2018,True,False,False,False,True,False,False,"A, E",207,3,210
W18-2411,A Deep Learning Based Approach to Transliteration,"In this paper, we propose different architectures for language independent machine transliteration which is extremely important for natural language processing (NLP) applications. Though a number of statistical models for transliteration have already been proposed in the past few decades, we proposed some neural network based deep learning architectures for the transliteration of named entities. Our transliteration systems adapt two different neural machine translation (NMT) frameworks: recurrent neural network and convolutional sequence to sequence based NMT. It is shown that our method provides quite satisfactory results when it comes to multi lingual machine transliteration. Our submitted runs are an ensemble of different transliteration systems for all the language pairs. In the NEWS 2018 Shared Task on Transliteration, our method achieves top performance for the En-Pe and Pe-En language pairs and comparable results for other cases.",2018,False,True,False,True,False,False,False,"B, D",283,3,286
W18-2408,{NEWS} 2018 Whitepaper,"Transliteration is defined as the phonetic translation of names across languages. Transliteration of Named Entities (NEs) is a necessary subtask in many applications, such as machine translation, corpus alignment, cross-language IR, information extraction and automatic lexicon acquisition. All such systems call for highperformance transliteration, which is the focus of the shared task in NEWS 2018.",2018,True,False,False,False,False,False,True,"A, G",195,3,198
Y18-3012,{IITP}-{MT} at {WAT}2018: Transformer-based Multilingual Indic-{E}nglish Neural Machine Translation System,"This paper describes the systems submitted by the IITP-MT team to WAT 2018 multilingual Indic languages shared task. We submit two multilingual neural machine translation (NMT) systems (Indic-to-English and English-to-Indic) based on Transformer architecture and our approaches are similar to many-to-one and one-to-many approaches of Johnson et al. (2017) . We also train separate bilingual models as baselines for all translation directions involving English. We evaluate the models using BLEU score and find that a single multilingual NMT model performs better (up to 14.81 BLEU) than separate bilingual models when the target is English. However, when English is the source language, multilingual NMT model improves only for lowresource language pairs (up to 11.60 BLEU) and degrades for relatively high-resource language pairs over separate bilingual models.",2018,False,False,False,True,False,False,True,"D, G",292,3,295
D18-1203,Pointwise {HSIC}: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions,"In this paper, we propose a new kernel-based co-occurrence measure that can be applied to sparse linguistic expressions (e.g., sentences) with a very short learning time, as an alternative to pointwise mutual information (PMI). As well as deriving PMI from mutual information, we derive this new measure from the Hilbert-Schmidt independence criterion (HSIC); thus, we call the new measure the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of PMI that allows various similarity metrics (e.g., sentence embeddings) to be plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast (linear in the size of the data) matrix calculations regardless of whether we use linear or nonlinear kernels. Empirically, in a dialogue response selection task, PHSIC is learned thousands of times faster than an RNNbased PMI while outperforming PMI in accuracy. In addition, we also demonstrate that PH-SIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs. 1 In collocation extraction, simple counting c(x, y) ∝ P(x, y), rather than PMI, ranks undesirable function-word pairs (e.g., ""of the"") higher (Manning and Schütze, 1999) .",2018,False,False,True,True,False,False,False,"C, D",404,3,407
W18-6306,Coreference and Coherence in Neural Machine Translation: A Study Using Oracle Experiments,"Cross-sentence context can provide valuable information in Machine Translation and is critical for translation of anaphoric pronouns and for providing consistent translations. In this paper, we devise simple oracle experiments targeting coreference and coherence. Oracles are an easy way to evaluate the effect of different discourse-level phenomena in NMT using BLEU and eliminate the necessity to manually define challenge sets for this purpose. We propose two context-aware NMT models and compare them against models working on a concatenation of consecutive sentences. Concatenation models perform better, but are computationally expensive. We show that NMT models taking advantage of context oracle signals can achieve considerable gains in BLEU, of up to 7.02 BLEU for coreference and 1.89 BLEU for coherence on subtitles translation. Access to strong signals allows us to make clear comparisons between context-aware models.",2018,False,True,False,False,False,True,False,"B, F",290,3,293
L18-1387,Towards a Linked Open Data Edition of {S}umerian Corpora,"Linguistic Linked Open Data (LLOD) is a flourishing line of research in the language resource community, so far mostly adopted for selected aspects of linguistics, natural language processing and the semantic web, as well as for practical applications in localization and lexicography. Yet, computational philology seems to be somewhat decoupled from the recent progress in this area: even though LOD as a concept is gaining significant popularity in Digital Humanities, existing LLOD standards and vocabularies are not widely used in this community, and philological resources are underrepresented in the LLOD cloud diagram (http://linguistic-lod.org/llod-cloud). In this paper, we present an application of Linguistic Linked Open Data in Assyriology. We describe the LLOD edition of a linguistically annotated corpus of Sumerian, as well as its linking with lexical resources, repositories of annotation terminology, and the museum collections in which the artifacts bearing these texts are kept. The chosen corpus is the Electronic Text Corpus of Sumerian Royal Inscriptions, a well curated and linguistically annotated archive of Sumerian text, in preparation for the creating and linking of other corpora of cuneiform texts, such as the corpus of Ur III administrative and legal Sumerian texts, as part of the Machine Translation and Automated Analysis of Cuneiform Languages project (https://cdli-gh.github.io/mtaac/).",2018,True,False,False,False,False,False,True,"G, A",404,3,407
N18-3014,Pieces of Eight: 8-bit Neural Machine Translation,"Neural machine translation has achieved levels of fluency and adequacy that would have been surprising a short time ago. Output quality is extremely relevant for industry purposes, however it is equally important to produce results in the shortest time possible, mainly for latency-sensitive applications and to control cloud hosting costs. In this paper we show the effectiveness of translating with 8-bit quantization for models that have been trained using 32-bit floating point values. Results show that 8-bit translation makes a non-negligible impact in terms of speed with no degradation in accuracy and adequacy. * A piece of eight was a Spanish dollar that was divided into 8 reales, also known as Real de a Ocho.",2018,False,True,False,True,False,False,False,"B, D",258,3,261
Y18-3007,Combination of Statistical and Neural Machine Translation for {M}yanmar-{E}nglish,"This paper presents the NICT's machine translation system combining neural and statistical machine translation for the WAT2018 Myanmar-English translation task. For both translation directions, we built state-of-the-art statistical (SMT) and neural (NMT) machine translation systems and combined them to improve translation quality. Our NMT systems were trained with the Transformer architecture using the provided parallel data. Our systems combining SMT and NMT are ranked first for this task according to BLEU. This paper also describes the impact of using a small quantity of back-translated monolingual data.",2018,False,False,False,True,False,False,True,"D, G",232,3,235
L18-1533,{BULB}asaa: A Bilingual Basaa-{F}rench Speech Corpus for the Evaluation of Language Documentation Tools,"Bàsàá is one of the three Bantu languages of BULB (Breaking the Unwritten Language Barrier), a project whose aim is to provide NLP-based tools to support linguists in documenting under-resourced and unwritten languages. To develop technologies such as automatic phone transcription or machine translation, a massive amount of speech data is needed. Approximately 50 hours of Bàsàá speech were thus collected and then carefully re-spoken and orally translated into French in a controlled environment by a few bilingual speakers. For a subset of ≈10 hours of the corpus, each utterance was additionally phonetically transcribed to establish a golden standard for the output of our NLP tools. The experiments described in this paper are meant to provide an automatic phonetic transcription using a set of derived phone-like units. As every language features a specific set of idiosyncrasies, automating the process of phonetic unit discovery in its entirety is a challenging task. Within BULB, we envision a workflow where linguists are able to refine the set of automatically discovered units and the system is then able to re-iterate on the data, providing a better approximation of the actual phone set.",2018,True,False,False,False,False,False,True,"A, G",356,3,359
D18-1396,Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter,"Neural machine translation usually adopts autoregressive models and suffers from exposure bias as well as the consequent error propagation problem. Many previous works have discussed the relationship between error propagation and the accuracy drop (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting findings. (1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are confirmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.",2018,False,False,False,False,True,True,False,"E,F",327,2,329
N18-1120,Guiding Neural Machine Translation with Retrieved Translation Pieces,"One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases. In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process. Specifically, for an input sentence, we use a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect n-grams that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call ""translation pieces"". We compute pseudoprobabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces. Finally, an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrievalbased method with respect to accuracy, speed, and simplicity of implementation.",2018,False,False,False,True,False,False,True,"D, G",359,3,362
W18-6318,On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation,"This work investigates the alignment problem in state-of-the-art multi-head attention models based on the transformer architecture. We demonstrate that alignment extraction in transformer models can be improved by augmenting an additional alignment head to the multi-head source-to-target attention component. This is used to compute sharper attention weights. We describe how to use the alignment head to achieve competitive performance. To study the effect of adding the alignment head, we simulate a dictionaryguided translation task, where the user wants to guide translation using pre-defined dictionary entries. Using the proposed approach, we achieve up to 3.8% BLEU improvement when using the dictionary, in comparison to 2.4% BLEU in the baseline case. We also propose alignment pruning to speed up decoding in alignment-based neural machine translation (ANMT), which speeds up translation by a factor of 1.8 without loss in translation performance. We carry out experiments on the shared WMT 2016 English→Romanian news task and the BOLT Chinese→English discussion forum task.",2018,False,True,False,True,False,False,False,"B, D",323,3,326
D18-1324,The Importance of Generation Order in Language Modeling,"Neural language models are a critical component of state-of-the-art systems for machine translation, summarization, audio transcription, and other tasks. These language models are almost universally autoregressive in nature, generating sentences one token at a time from left to right. This paper studies the influence of token generation order on model quality via a novel two-pass language model that produces partially-filled sentence ""templates"" and then fills in missing tokens. We compare various strategies for structuring these two passes and observe a surprisingly large variation in model quality. We find the most effective strategy generates function words in the first pass followed by content words in the second. We believe these experimental results justify a more extensive investigation of generation order for neural language models. * Work done as a member of the Google AI Residency program (g.co/airesidency)",2018,False,True,False,False,False,True,False,"B, F",282,3,285
D18-1510,Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation,"Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.",2018,False,True,True,False,False,False,False,"B, C",291,3,294
Y18-3013,Multilingual {I}ndian Language Translation System at {WAT} 2018: Many-to-one Phrase-based {SMT},"This paper describes our trained models of phrase-based statistical machine translation (PBSMT) systems for Indic− →English and English− →Indic language-pairs, which has been submitted to the WAT 2018 shared task. In addition, we have introduced many-to-one statistical machine translation (SMT). This new approach produced comparable results in terms of translation accuracy with respect to the result of baseline SMT.",2018,False,False,False,True,False,False,True,"D, G",199,3,202
D18-1512,Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,"Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese-English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.",2018,False,False,False,False,True,True,False,"E,F",239,2,241
P18-1129,Distilling Knowledge for Search-based Structured Prediction,"Many natural language processing tasks can be modeled into structured prediction and solved as a search problem. In this paper, we distill an ensemble of multiple models trained with different initialization into a single model. In addition to learning to match the ensemble's probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration. Experimental results on two typical search-based structured prediction tasks -transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model's performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures.",2018,False,True,False,True,False,False,False,"B, D",270,3,273
W18-2705,Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation,"Supervised domain adaptation-where a large generic corpus and a smaller indomain corpus are both available for training-is a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second model, then continue training the second model on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the cross entropy between the indomain model's output word distribution and that of the out-of-domain model to prevent the model's output from differing too much from the original out-ofdomain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU.",2018,False,False,False,True,False,False,True,"D, G",286,3,289
W18-6404,An Empirical Study of Machine Translation for the Shared Task of {WMT}18,"This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT18 shared news translation task. We participated in the English-to-Chinese direction and get the best BLEU (43.8) scores among all the participants. The submitted system focus on data clearing and techniques to build a competitive model for this task. Unlike other participants, the submitted system are mainly relied on the data filtering to obtain the best BLEU score. We do data filtering not only for provided sentences but also for the back translated sentences. The techniques we apply for data filtering include filtering by rules, language models and translation models. We also conduct several experiments to validate the effectiveness of training techniques. According to our experiments, the Annealing Adam optimizing function and ensemble decoding are the most effective techniques for the model training.",2018,False,False,False,True,False,False,True,"D, G",279,3,282
Y18-3011,The {RGNLP} Machine Translation Systems for {WAT} 2018,"This paper presents the system description of Machine Translation (MT) system(s) for Indic Languages Multilingual Task for the 2018 edition of the WAT Shared Task. In our experiments, we (the RGNLP team) explore both statistical and neural methods across all language pairs. (We further present an extensive comparison of language-related problems for both the approaches in the context of low-resourced settings.) Our PBSMT models were highest score on all automaticevaluation metrics in the English into Telugu, Hindi, Bengali, Tamil portion of the shared task.",2018,False,False,False,True,True,False,False,"D, E",229,3,232
L18-1538,Creating Large-Scale Multilingual Cognate Tables,"Low-resource languages often suffer from a lack of high-coverage lexical resources. In this paper, we propose a method to generate cognate tables by clustering words from existing lexical resources. We then employ character-based machine translation methods in solving the task of cognate chain completion by inducing missing word translations from lower-coverage dictionaries to fill gaps in the cognate chain, finding improvements over single language pair baselines when employing simple but novel multi-language system combination on the Romance and Turkic language families. For the Romance family, we show that system combination using the results of clustering outperforms weights derived from the historical-linguistic scholarship on language phylogenies. Our approach is applicable to any language family and has not been previously performed at such scale. The cognate tables are released to the research community.",2018,True,False,False,True,False,False,False,"A, D",278,3,281
W18-2201,Using Morphemes from Agglutinative Languages like {Q}uechua and {F}innish to Aid in Low-Resource Translation,"Quechua is a low-resource language spoken by nearly 9 million persons in South America (Hintz and Hintz, 2017 ). Yet, in recent times there are few published accounts of successful adaptations of machine translation systems for low-resource languages like Quechua. In some cases, machine translations from Quechua to Spanish are inadequate due to error in alignment. We attempt to improve previous alignment techniques by aligning two languages that are similar due to agglutination: Quechua and Finnish. Our novel technique allows us to add rules that improve alignment for the prediction algorithm used in common machine translation systems.",2018,False,False,True,True,False,False,False,"C, D",242,3,245
W18-6425,The {U}niversity of {H}elsinki submissions to the {WMT}18 news task,"This paper describes the University of Helsinki's submissions to the WMT18 shared news translation task for English-Finnish and English-Estonian, in both directions. This year, our main submissions employ a novel neural architecture, the Transformer, using the open-source OpenNMT framework. Our experiments couple domain labeling and fine tuned multilingual models with shared vocabularies between the source and target language, using the provided parallel data of the shared task and additional back-translations. Finally, we compare, for the English-to-Finnish case, the effectiveness of different machine translation architectures, starting from a rule-based approach to our best neural model, analyzing the output and highlighting future research.",2018,False,True,False,False,False,True,False,"B, F",254,3,257
W18-2803,Language Production Dynamics with Recurrent Neural Networks,"We present an analysis of the internal mechanism of the recurrent neural model of sentence production presented by Calvillo et al. (2016) . The results show clear patterns of computation related to each layer in the network allowing to infer an algorithmic account, where the semantics activates the semantically related words, then each word generated at each time step activates syntactic and semantic constraints on possible continuations, while the recurrence preserves information through time. We propose that such insights could generalize to other models with similar architecture, including some used in computational linguistics for language modeling, machine translation and image caption generation.",2018,False,False,False,False,True,True,False,"F, E",239,3,242
W18-1811,A Smorgasbord of Features to Combine Phrase-Based and Neural Machine Translation,"Superiority of neural machine translation (NMT) and phrase-based statistical machine translation (PBSMT) depends on the translation task. For some translation tasks, such as those involving low-resource language pairs or close languages, NMT may underperform PBSMT. In order to have a translation system that performs consistently better regardless of the translation task, recent work proposed to combine PBSMT and NMT approaches. In this paper, we propose an empirical comparison of the most popular existing approaches that combine PBSMT and NMT. Despite its simplicity, our simple reranking system using a smorgasbord of informative features significantly and consistently outperforms other methods, even for translation tasks where PBSMT and NMT produce translations of a very different quality.",2018,False,False,False,True,False,True,False,"D, F",266,3,269
2018.iwslt-1.15,Neural Speech Translation at {A}pp{T}ek,"This work describes AppTek's speech translation pipeline that includes strong state-of-the-art automatic speech recognition (ASR) and neural machine translation (NMT) components. We show how these components can be tightly coupled by encoding ASR confusion networks, as well as ASR-like noise adaptation, vocabulary normalization, and implicit punctuation prediction during translation. In another experimental setup, we propose a direct speech translation approach that can be scaled to translation tasks with large amounts of text-only parallel training data but a limited number of hours of recorded and human-translated speech.",2018,False,True,False,True,False,False,False,"B, D",228,3,231
P18-2049,Compositional Representation of Morphologically-Rich Input for Neural Machine Translation,"Neural machine translation (NMT) models are typically trained with fixed-size input and output vocabularies, which creates an important bottleneck on their accuracy and generalization capability. As a solution, various studies proposed segmenting words into sub-word units and performing translation at the sub-lexical level. However, statistical word segmentation methods have recently shown to be prone to morphological errors, which can lead to inaccurate translations. In this paper, we propose to overcome this problem by replacing the source-language embedding layer of NMT with a bi-directional recurrent neural network that generates compositional representations of the input at any desired level of granularity. We test our approach in a low-resource setting with five languages from different morphological typologies, and under different composition assumptions. By training NMT to compose word representations from character trigrams, our approach consistently outperforms (from 1.71 to 2.48 BLEU points) NMT learning embeddings of statistically generated sub-word units.",2018,False,True,False,True,False,False,False,"B, D",315,3,318
2018.iwslt-1.7,Multi-Source Neural Machine Translation with Data Augmentation,"Multi-source translation systems translate from multiple languages to a single target language. By using information from these multiple sources, these systems achieve large gains in accuracy. To train these systems, it is necessary to have corpora with parallel text in multiple sources and the target language. However, these corpora are rarely complete in practice due to the difficulty of providing human translations in all of the relevant languages. In this paper, we propose a data augmentation approach to fill such incomplete parts using multi-source neural machine translation (NMT). In our experiments, results varied over different language combinations but significant gains were observed when using a source language similar to the target language.",2018,True,False,False,True,False,False,False,"A, D",247,3,250
J18-3006,Feature-Based Decipherment for Machine Translation,"Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction (decipherment) for closely related language pairs. The existing decipherment models, however, are not well suited for exploiting these orthographic similarities. We propose a loglinear model with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed log-linear model. To address this challenge, we perform approximate inference via Markov chain Monte Carlo sampling and contrastive divergence. Our results show that the proposed log-linear model with contrastive divergence outperforms the existing generative decipherment models by exploiting the orthographic features. The model both scales to large vocabularies and preserves accuracy in low-and no-resource contexts.",2018,False,True,True,False,False,False,False,"B, C",265,3,268
Y18-3006,{E}nglish-{M}yanmar {NMT} and {SMT} with Pre-ordering: {NICT}{'}s Machine Translation Systems at {WAT}-2018,"This paper presents the NICT's participation (team ID: NICT) in the 5th Workshop on Asian Translation (WAT-2018) shared translation task, specifically Myanmar (Burmese) -English task in both translation directions. We built state-of-the-art neural machine translation (NMT) as well as phrase-based statistical machine translation (PBSMT) systems for these tasks. Our NMT systems were trained with the Transformer architecture on the provided parallel data. Pre-ordering technology is adopted to both NMT and PBSMT. Our NMT systems rank the first in English-to-Myanmar and the second in Myanmar-to-English according to the official human evaluation.",2018,False,False,False,True,False,False,True,"D, G",254,3,257
D18-1357,Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation,"Neural text generation, including neural machine translation, image captioning, and summarization, has been quite successful recently. However, during training time, typically only one reference is considered for each example, even though there are often multiple references available, e.g., 4 references in NIST MT evaluations, and 5 references in image captioning data. We first investigate several different ways of utilizing multiple human references during training. But more importantly, we then propose an algorithm to generate exponentially many pseudo-references by first compressing existing human references into lattices and then traversing them to generate new pseudo-references. These approaches lead to substantial improvements over strong baselines in both machine translation (+1.5 BLEU) and image captioning (+3.1 BLEU / +11.7 CIDEr).",2018,False,True,True,False,False,False,False,"B, C",280,3,283
L18-1143,Exploiting Pre-Ordering for Neural Machine Translation,"Neural Machine Translation (NMT) has drawn much attention due to its promising translation performance in recent years. However, the under-translation and over-translation problem still remain a big challenge. Through error analysis, we find that under-translation is much more prevalent than over-translation and the source words that need to be reordered during translation are more likely to be ignored. To address the under-translation problem, we explore the pre-ordering approach for NMT. Specifically, we pre-order the source sentences to approximate the target language word order. We then combine the pre-ordering model with position embedding to enhance the monotone translation. Finally, we augment our model with the coverage mechanism to tackle the over-translation problem. Experimental results on Chinese-to-English translation have shown that our method can significantly improve the translation quality by up to 2.43 BLEU points. Furthermore, the detailed analysis demonstrates that our approach can substantially reduce the number of under-translation cases by 30.4% (compared to 17.4% using the coverage model).",2018,False,False,False,True,False,False,True,"D, G",332,3,335
S18-1182,{SNU}{\_}{IDS} at {S}em{E}val-2018 Task 12: Sentence Encoder with Contextualized Vectors for Argument Reasoning Comprehension,"We present a novel neural architecture for the Argument Reasoning Comprehension task of SemEval 2018. It is a simple neural network consisting of three parts, collectively judging whether the logic built on a set of given sentences (a claim, reason, and warrant) is plausible or not. The model utilizes contextualized word vectors pre-trained on large machine translation (MT) datasets as a form of transfer learning, which can help to mitigate the lack of training data. Quantitative analysis shows that simply leveraging LSTMs trained on MT datasets outperforms several baselines and non-transferred models, achieving accuracies of about 70% on the development set and about 60% on the test set.",2018,False,True,False,True,False,False,False,"B, D",260,3,263
W18-6476,{MAJE} Submission to the {WMT}2018 Shared Task on Parallel Corpus Filtering,This paper describes the participation of Webinterpret in the shared task on parallel corpus filtering at the Third Conference on Machine Translation (WMT 2018). The paper describes the main characteristics of our approach and discusses the results obtained on the data sets published for the shared task. * Marina Fomicheva worked at Webinterpret at the time of preparation of this submission. 1 http://www.statmt.org/wmt18/ parallel-corpus-filtering.html 2 https://paracrawl.eu/,2018,False,False,False,True,False,False,True,"D, G",218,3,221
N18-4016,Neural Machine Translation for Low Resource Languages using Bilingual Lexicon Induced from Comparable Corpora,"Resources for the non-English languages are scarce and this paper addresses this problem in the context of machine translation, by automatically extracting parallel sentence pairs from the multilingual articles available on the Internet. In this paper, we have used an endto-end Siamese bidirectional recurrent neural network to generate parallel sentences from comparable multilingual articles in Wikipedia. Subsequently, we have showed that using the harvested dataset improved BLEU scores on both NMT and phrase-based SMT systems for the low-resource language pairs: English-Hindi and English-Tamil, when compared to training exclusively on the limited bilingual corpora collected for these language pairs.",2018,True,False,False,False,False,False,True,"A, G",238,3,241
2018.jeptalnrecital-court.18,Traduction automatique du japonais vers le fran{\c{c}}ais Bilan et perspectives (Machine Translation from {J}apanese to {F}rench - Review and Prospects),"Nous étudions la possibilité de construire un dispositif de traduction automatique neuronale du japonais vers le français, capable d'obtenir des résultats à la hauteur de l'état de l'art, sachant que l'on ne peut disposer de grands corpus alignés bilingues. Nous proposons un état de l'art et relevons de nombreux signes d'amélioration de la qualité des traductions, en comparaison aux traductions statistiques jusque-là prédominantes. Nous testons ensuite un des baselines librement disponibles, OpenNMT, qui produit des résultats encourageants. Sur la base de cette expérience, nous proposons plusieurs pistes pour améliorer à terme la traduction et pour compenser le manque de corpus.",2018,False,False,False,True,False,False,True,"D, G",253,3,256
L18-1261,Handling Rare Word Problem using Synthetic Training Data for {S}inhala and {T}amil Neural Machine Translation,"Lack of parallel training data influences the rare word problem in Neural Machine Translation (NMT) systems, particularly for underresourced languages. Using synthetic parallel training data (data augmentation) is a promising approach to handle the rare word problem. Previously proposed methods for data augmentation do not consider language syntax when generating synthetic training data. This leads to generation of sentences that lower the overall quality of parallel training data. In this paper, we discuss the suitability of using Parts of Speech (POS) tagging and morphological analysis as syntactic features to prune the generated synthetic sentence pairs that do not adhere to language syntax. Our models show an overall 2.16 and 5.00 BLEU score gains over our benchmark Sinhala to Tamil and Tamil to Sinhala translation systems, respectively. Although we focus on Sinhala and Tamil NMT for the domain of official government documents, we believe that these synthetic data pruning techniques can be generalized to any language pair.",2018,False,False,False,True,False,False,True,"D, G",304,3,307
D18-1100,{S}witch{O}ut: an Efficient Data Augmentation Algorithm for Neural Machine Translation,"In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a) . Code to implement this method is included in the appendix.",2018,False,False,True,True,False,False,False,"C, D",281,3,284
2018.iwslt-1.9,Machine Translation Human Evaluation: an investigation of evaluation based on Post-Editing and its relation with Direct Assessment,"In this paper we present an analysis of the two most prominent methodologies used for the human evaluation of MT quality, namely evaluation based on Post-Editing (PE) and evaluation based on Direct Assessment (DA). To this purpose, we exploit a publicly available large dataset containing both types of evaluations. We first focus on PE and investigate how sensitive TER-based evaluation is to the type and number of references used. Then, we carry out a comparative analysis of PE and DA to investigate the extent to which the evaluation results obtained by methodologies addressing different human perspectives are similar. This comparison sheds light not only on PE but also on the so-called reference bias related to monolingual DA. Also, we analyze if and how the two methodologies can complement each other's weaknesses.",2018,False,False,False,False,True,True,False,"E,F",268,2,270
J18-3007,{S}urvey: Anaphora With Non-nominal Antecedents in Computational Linguistics: a {S}urvey,"This article provides an extensive overview of the literature related to the phenomenon of nonnominal-antecedent anaphora (also known as abstract anaphora or discourse deixis), a type of anaphora in which an anaphor like ""that"" refers to an antecedent (marked in boldface) that is syntactically non-nominal, such as the first sentence in ""It's way too hot here. That's why I'm moving to Alaska."" Annotating and automatically resolving these cases of anaphora is interesting in its own right because of the complexities involved in identifying non-nominal antecedents, which typically represent abstract objects such as events, facts, and propositions. There is also practical value in the resolution of non-nominal-antecedent anaphora, as this would help computational systems in machine translation, summarization, and question answering, as well as, conceivably, any other task dependent on some measure of text understanding. Most of the existing approaches to anaphora annotation and resolution focus on nominalantecedent anaphora, classifying many of the cases where the antecedents are syntactically Submission",2018,False,False,False,False,True,False,True,"E, G",352,3,355
D18-1339,Context and Copying in Neural Machine Translation,"Neural machine translation systems with subword vocabularies are capable of translating or copying unknown words. In this work, we show that they learn to copy words based on both the context in which the words appear as well as features of the words themselves. In contexts that are particularly copy-prone, they even copy words that they have already learned they should translate. We examine the influence of context and subword features on this and other types of copying behavior.",2018,False,False,False,False,True,True,False,"E,F",210,2,212
W18-1804,Combining Quality Estimation and Automatic Post-editing to Enhance Machine Translation output,"We investigate different strategies for combining quality estimation (QE) and automatic postediting (APE) to improve the output of machine translation (MT) systems. The joint contribution of the two technologies is analyzed in different settings, in which QE serves as either: i) an activator of APE corrections, or ii) a guidance to APE corrections, or iii) a selector of the final output to be returned to the user. In the first case (QE as activator), sentence-level predictions on the raw MT output quality are used to trigger its automatic correction when the estimated (TER) scores are below a certain threshold. In the second case (QE as guidance), word-level binary quality predictions (""good""/""bad"") are used to inform APE about problematic words in the MT output that should be corrected. In the last case (QE as selector), both sentence-and word-level quality predictions are used to identify the most accurate translation between the original MT output and its post-edited version. For the sake of comparison, the underlying APE technologies explored in our evaluation are both phrase-based and neural. Experiments are carried out on the English-German data used for the QE/APE shared tasks organized within the First Conference on Machine Translation (WMT 2016). Our evaluation shows positive but mixed results, with higher performance observed when word-level QE is used as a selector for neural APE applied to the output of a phrase-based MT system. Overall, our findings motivate further investigation on QE technologies. By reducing the gap between the performance of current solutions and ""oracle"" results, QE could significantly add to competitive APE technologies.",2018,False,False,False,True,False,True,False,"D, F",446,3,449
C18-1263,Multilingual Neural Machine Translation with Task-Specific Attention,"Multilingual machine translation addresses the task of translating between multiple source and target languages. We propose task-specific attention models, a simple but effective technique for improving the quality of sequence-to-sequence neural multilingual translation. Our approach seeks to retain as much of the parameter sharing generalization of NMT models as possible, while still allowing for language-specific specialization of the attention model to a particular language-pair or task. Our experiments on four languages of the Europarl corpus show that using a target-specific model of attention provides consistent gains in translation quality for all possible translation directions, compared to a model in which all parameters are shared. We observe improved translation quality even in the (extreme) low-resource zero-shot translation directions for which the model never saw explicitly paired parallel data.",2018,False,True,False,True,False,False,False,"B, D",271,3,274
L18-1602,Multimodal Lexical Translation,"Inspired by the tasks of Multimodal Machine Translation and Visual Sense Disambiguation we introduce a task called Multimodal Lexical Translation (MLT). The aim of this new task is to correctly translate an ambiguous word given its context -an image and a sentence in the source language. To facilitate the task, we introduce the MLT dataset, where each data point is a 4-tuple consisting of an ambiguous source word, its visual context (an image), its textual context (a source sentence), and its translation that conforms with the visual and textual contexts. The dataset has been created from the Multi30K corpus using word-alignment followed by human inspection for translations from English to German and English to French. We also introduce a simple heuristic to quantify the extent of the ambiguity of a word from the distribution of its translations and use it to select subsets of the MLT Dataset which are difficult to translate. These form a valuable multimodal and multilingual language resource with several potential uses including evaluation of lexical disambiguation within (Multimodal) Machine Translation systems.",2018,True,False,False,False,True,False,False,"A, E",332,3,335
L18-1149,Automatic Enrichment of Terminological Resources: the {IATE} {RDF} Example,"Terminological resources have proven necessary in many organizations and institutions to ensure communication between experts. However, the maintenance of these resources is a very time-consuming and expensive process. Therefore, the work described in this contribution aims to automate the maintenance process of such resources. As an example, we demonstrate enriching the RDF version of IATE with new terms in the languages for which no translation was available, as well as with domain-disambiguated sentences and information about usage frequency. This is achieved by relying on machine translation trained on parallel corpora that contains the terms in question and multilingual word sense disambiguation performed on the context provided by the sentences. Our results show that for most languages translating the terms within a disambiguated context significantly outperforms the approach with randomly selected sentences.",2018,False,False,False,True,False,False,True,"D, G",272,3,275
D18-1332,Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation,In order to extract the best possible performance from asynchronous stochastic gradient descent one must increase the mini-batch size and scale the learning rate accordingly. In order to achieve further speedup we introduce a technique that delays gradient updates effectively increasing the mini-batch size. Unfortunately with the increase of mini-batch size we worsen the stale gradient problem in asynchronous stochastic gradient descent (SGD) which makes the model convergence poor. We introduce local optimizers which mitigate the stale gradient problem and together with fine tuning our momentum we are able to train a shallow machine translation system 27% faster than an optimized baseline with negligible penalty in BLEU.,2018,False,True,True,False,False,False,False,"B, C",243,3,246
W18-6428,The {LMU} {M}unich Unsupervised Machine Translation Systems,"We describe LMU Munich's unsupervised machine translation systems for English↔German translation. These systems were used to participate in the WMT18 news translation shared task and more specifically, for the unsupervised learning sub-track. The systems are trained on English and German monolingual data only and exploit and combine previously proposed techniques such as using word-by-word translated data based on bilingual word embeddings, denoising and on-the-fly backtranslation.",2018,False,False,False,True,False,False,True,"D, G",210,3,213
W18-6307,A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation,"The translation of pronouns presents a special challenge to machine translation to this day, since it often requires context outside the current sentence. Recent work on models that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as BLEU. However, metrics that quantify the overall translation quality are illequipped to measure gains from additional context. We argue that a different kind of evaluation is needed to assess how well models translate inter-sentential phenomena such as pronouns. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several contextaware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of accuracy on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multiencoder architectures.",2018,True,False,False,True,False,False,False,"A, D",298,3,301
L18-1549,Extracting an {E}nglish-{P}ersian Parallel Corpus from Comparable Corpora,"Parallel data are an important part of a reliable Statistical Machine Translation (SMT) system. The more of these data are available, the better the quality of the SMT system. However, for some language pairs such as Persian-English, parallel sources of this kind are scarce. In this paper, a bidirectional method is proposed to extract parallel sentences from English and Persian document aligned Wikipedia. Two machine translation systems are employed to translate from Persian to English and the reverse after which an IR system is used to measure the similarity of the translated sentences. Adding the extracted sentences to the training data of the existing SMT systems is shown to improve the quality of the translation. Furthermore, the proposed method slightly outperforms the one-directional approach. The extracted corpus consists of about 200,000 sentences which have been sorted by their degree of similarity calculated by the IR system and is freely available for public access on the Web 1 .",2018,True,False,False,True,False,False,False,"A, D",301,3,304
P18-1195,Token-level and sequence-level loss smoothing for {RNN} language models,"Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from ""exposure bias"": during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach i.e. sequence-level smoothing that encourages the model to predict sentences close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.",2018,False,True,True,False,False,False,False,"B, C",283,3,286
D18-1037,Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing,"The addition of syntax-aware decoding in Neural Machine Translation (NMT) systems requires an effective tree-structured neural network, a syntax-aware attention model and a language generation model that is sensitive to sentence structure. We exploit a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola (2017)  to create an NMT model called Seq2DRNN that combines a sequential encoder with tree-structured decoding augmented with a syntax-aware attention model. Unlike previous approaches to syntax-based NMT which use dependency parsing models our method uses constituency parsing which we argue provides useful information for translation. In addition, we use the syntactic structure of the sentence to add new connections to the tree-structured decoder neural network (Seq2DRNN+SynC). We compare our NMT model with sequential and state of the art syntax-based NMT models and show that our model produces more fluent translations with better reordering. Since our model is capable of doing translation and constituency parsing at the same time we also compare our parsing accuracy against other neural parsing models.",2018,False,True,False,True,False,False,False,"B, D",343,3,346
D18-1104,Compact Personalized Models for Neural Machine Translation,We propose and compare methods for gradientbased domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture-combining a state-of-the-art self-attentive model with compact domain adaptation-provides high quality personalized machine translation that is both space and time efficient.,2018,False,True,False,True,False,False,False,"B, D",234,3,237
W18-3023,Multilingual Seq2seq Training with Similarity Loss for Cross-Lingual Document Classification,"In this paper we continue the line of work where neural machine translation training is used to produce joint cross-lingual fixed-dimensional sentence embeddings. In this framework we introduce a simple method of adding a loss to the learning objective which penalizes distance between representations of bilingually aligned sentences. We evaluate cross-lingual transfer using two approaches, cross-lingual similarity search on an aligned corpus (Europarl) and cross-lingual document classification on a recently published benchmark Reuters corpus, and we find the similarity loss significantly improves performance on both. Our cross-lingual transfer performance is competitive with stateof-the-art, even while there is potential to further improve by investing in a better inlanguage baseline. Our results are based on a set of 6 European languages.",2018,False,True,False,True,False,False,False,"B, D",272,3,275
W18-6403,Findings of the {WMT} 2018 Biomedical Translation Shared Task: Evaluation on {M}edline test sets,"Machine translation enables the automatic translation of textual documents between languages and can facilitate access to information only available in a given language for nonspeakers of this language, e.g. research results presented in scientific publications. In this paper, we provide an overview of the Biomedical Translation shared task in the Workshop on Machine Translation (WMT) 2018, which specifically examined the performance of machine translation systems for biomedical texts. This year, we provided test sets of scientific publications from two sources (EDP and Medline) and for six language pairs (English with each of Chinese, French, German, Portuguese, Romanian and Spanish). We describe the development of the various test sets, the submissions that we received and the evaluations that we carried out. We obtained a total of 39 runs from six teams and some of this year's BLEU scores were somewhat higher that last year's, especially for teams that made use of biomedical resources or state-of-the-art MT algorithms (e.g. Transformer). Finally, our manual evaluation scored automatic translations higher than the reference translations for German and Spanish.",2018,True,False,False,False,True,False,False,"A, E",331,3,334
W18-6432,{E}val{D} Reference-Less Discourse Evaluation for {WMT}18,"We present the results of automatic evaluation of discourse in machine translation (MT) outputs using the EVALD tool. EVALD was originally designed and trained to assess the quality of human writing, for native speakers and foreign-language learners. MT has seen a tremendous leap in translation quality at the level of sentences and it is thus interesting to see if the human-level evaluation is becoming relevant.",2018,False,False,False,False,True,False,True,"E, G",195,3,198
C18-1255,Multi-layer Representation Fusion for Neural Machine Translation,"Neural machine translation systems require a number of stacked layers for deep models. But the prediction depends on the sentence representation of the top-most layer with no access to low-level representations. This makes it more difficult to train the model and poses a risk of information loss to prediction. In this paper, we propose a multi-layer representation fusion (MLRF) approach to fusing stacked layers. In particular, we design three fusion functions to learn a better representation from the stack. Experimental results show that our approach yields improvements of 0.92 and 0.56 BLEU points over the strong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-ofthe-art in German-English translation.",2018,False,True,False,True,False,False,False,"B, D",267,3,270
D18-2012,{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,"This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.",2018,False,True,False,True,False,False,False,"B, D",274,3,277
C18-1086,Multi-Task Neural Models for Translating Between Styles Within and Across Languages,"Generating natural language requires conveying content in an appropriate style. We explore two related tasks on generating text of varying formality: monolingual formality transfer and formality-sensitive machine translation. We propose to solve these tasks jointly using multi-task learning, and show that our models achieve state-of-the-art performance for formality transfer and are able to perform formality-sensitive translation without being explicitly trained on styleannotated translation examples.",2018,True,False,False,True,False,False,False,"A, D",202,3,205
L18-1604,Evaluation of Machine Translation Performance Across Multiple Genres and Languages,"In this paper, we present evaluation corpora covering four genres for four language pairs that we harvested from the web in an automated fashion. We use these multi-genre benchmarks to evaluate the impact of genre differences on machine translation (MT). We observe that BLEU score differences between genres can be large and that, for all genres and all language pairs, translation quality improves when using four genre-optimized systems rather than a single genre-agnostic system. Finally, we train and use genre classifiers to route test documents to the most appropriate genre systems. The results of these experiments show that our multi-genre benchmarks can serve to advance research on text genre adaptation for MT.",2018,True,False,False,False,True,False,False,"A, E",251,3,254
N18-1055,Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task,"Previously, neural methods in grammatical error correction (GEC) did not reach state-ofthe-art results compared to phrase-based statistical machine translation (SMT) baselines. We demonstrate parallels between neural GEC and low-resource neural MT and successfully adapt several methods from low-resource MT to neural GEC. We further establish guidelines for trustable results in neural GEC and propose a set of model-independent methods for neural GEC that can be easily applied in most GEC settings. Proposed methods include adding source-side noise, domain-adaptation techniques, a GEC-specific training-objective, transfer learning with monolingual data, and ensembling of independently trained GEC models and language models. The combined effects of these methods result in better than state-of-the-art neural GEC models that outperform previously best neural GEC systems by more than 10% M 2 on the CoNLL-2014 benchmark and 5.9% on the JFLEG test set. Non-neural state-of-the-art systems are outperformed by more than 2% on the CoNLL-2014 benchmark and by 4% on JFLEG.",2018,False,False,False,True,False,False,True,"D, G",350,3,353
W18-6417,The {JHU} Machine Translation Systems for {WMT} 2018,"We report on the efforts of the Johns Hopkins University to develop neural machine translation systems for the shared task for news translation organized around the Conference for Machine Translation (WMT) 2018. We developed systems for German-English, English-German, and Russian-English. Our novel contributions are iterative back-translation and fine-tuning on test sets from prior years.",2018,False,False,False,True,False,False,True,"D, G",190,3,193
D18-1334,Getting Gender Right in Neural Machine Translation,"Speakers of different languages must attend to and encode strikingly different aspects of the world in order to use their language correctly (Sapir, 1921; Slobin, 1996) . One such difference is related to the way gender is expressed in a language. Saying ""I am happy"" in English, does not encode any additional knowledge of the speaker that uttered the sentence. However, many other languages do have grammatical gender systems and so such knowledge would be encoded. In order to correctly translate such a sentence into, say, French, the inherent gender information needs to be retained/recovered. The same sentence would become either ""Je suis heureux"", for a male speaker or ""Je suis heureuse"" for a female one. Apart from morphological agreement, demographic factors (gender, age, etc.) also influence our use of language in terms of word choices or even on the level of syntactic constructions (Tannen, 1991; Pennebaker et al., 2003) . We integrate gender information into NMT systems. Our contribution is twofold: (1) the compilation of large datasets with speaker information for 20 language pairs, and (2) a simple set of experiments that incorporate gender information into NMT for multiple language pairs. Our experiments show that adding a gender feature to an NMT system significantly improves the translation quality for some language pairs.",2018,True,False,False,True,False,False,False,"A, D",395,3,398
P18-3010,{S}uper{NMT}: Neural Machine Translation with Semantic Supersenses and Syntactic Supertags,"In this paper we incorporate semantic supersensetags and syntactic supertag features into EN-FR and EN-DE factored NMT systems. In experiments on various test sets, we observe that such features (and particularly when combined) help the NMT model training to converge faster and improve the model quality according to the BLEU scores.",2018,False,False,False,True,False,True,False,"D, F",186,3,189
