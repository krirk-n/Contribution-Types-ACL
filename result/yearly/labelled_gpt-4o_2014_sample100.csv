acl_id,title,abstract,year,A,B,C,D,E,F,G,raw_response,input_tokens,output_tokens,total_tokens
U14-1014,{OCR} and Automated Translation for the Navigation of non-{E}nglish Handsets: A Feasibility Study with {A}rabic,"In forensics, mobile phones or handsets store potentially valuable information such as Contact lists, SMS Messages, or possibly emails and Calendar appointments. However, navigating to this content on non-English configured handsets, when the operator is untrained in the language, becomes a difficult task. We discuss a feasibility study that explored the performance of optical character recognition (OCR) systems against Arabic menus on handset LCD screens. Further, a method of automated spell correction and translation is explored considering fully automated or user-interactive workflow options. A capability technology demonstrator for non-English handset navigation was implemented based on outcomes of these studies, providing a platform for investigating workflow and usability.",2014,False,False,False,True,False,False,True,"G, D",249,3,252
Y14-1039,Transition-based Knowledge Graph Embedding with Relational Mapping Properties,"Many knowledge repositories nowadays contain billions of triplets, i.e. (head-entity, relationship, tail-entity), as relation instances. These triplets form a directed graph with entities as nodes and relationships as edges. However, this kind of symbolic and discrete storage structure makes it difficult for us to exploit the knowledge to enhance other intelligenceacquired applications (e.g. the Question-Answering System), as many AI-related algorithms prefer conducting computation on continuous data. Therefore, a series of emerging approaches have been proposed to facilitate knowledge computing via encoding the knowledge graph into a low-dimensional embedding space. TransE is the latest and most promising approach among them, and can achieve a higher performance with fewer parameters by modeling the relationship as a transitional vector from the head entity to the tail entity. Unfortunately, it is not flexible enough to tackle well with the various mapping properties of triplets, even though its authors spot the harm on performance. In this paper, we thus propose a superior model called TransM to leverage the structure of the knowledge graph via pre-calculating the distinct weight for each training triplet according to its relational mapping property. In this way, the optimal function deals with each triplet depending on its own weight. We carry out extensive experiments to compare TransM with the state-of-the-art method TransE and other prior arts. The performance of each approach is evaluated within two different application scenarios on several benchmark datasets. Results show that the model we proposed significantly outperforms the former ones with lower parameter complexity as TransE.",2014,False,True,False,True,False,False,False,"B, D",425,3,428
W14-3414,A repository of semantic types in the {MIMIC} {II} database clinical notes,"The MIMIC II database contains 1,237,686 clinical documents of various kinds. A common task for researchers working with this database is to run MetaMap, which uses the UMLS Metathesaurus, on those documents to identify specific semantic types of entities mentioned in them. However, this task is computationally expensive and time-consuming. Research in many groups could be accelerated if there were a community-accessible set of outputs from running MetaMap on this document collection, cached and available on the MIMIC-II website. This paper describes a repository of all MetaMap output from the MIMIC II database, publicly available, assuming compliance with usage agreements required by UMLS and MIMIC-II. Additionally, software for manipulating MetaMap output, available on Source-Forge with a liberal Open Source license, is described.",2014,True,False,False,False,False,False,True,"A, G",285,3,288
W14-2504,Location and Language Use in Social Media,"We now know that social interactions are critical in many knowledge and information processes. In this talk, I plan to illustrate a model-driven approach to understanding social behavior around user location and different languages in social media.",2014,False,False,False,False,True,True,False,"E, F",159,3,162
W14-5509,A Dictionary Data Processing Environment and Its Application in Algorithmic Processing of {P}ali Dictionary Data for Future {NLP} Tasks,"This paper presents a highly flexible infrastructure for processing digitized dictionaries and that can be used to build NLP tools in the future. This infrastructure is especially suitable for low resource languages where some digitized information is available but not (yet) suitable for algorithmic use. It allows researchers to do at least some processing in an algorithmic way using the full power of the C# programming language, reducing the effort of manual editing of the data. To test this in practice, the paper describes the processing steps taken by making use of this infrastructure in order to identify word classes and cross references in the dictionary of Pali in the context of the SeNeReKo project. We also conduct an experiment to make use of this data and show the importance of the dictionary. This paper presents the experiences and results of the selected approach.",2014,True,False,False,False,False,False,True,"A, G",281,3,284
P14-5003,"Open-Source Tools for Morphology, Lemmatization, {POS} Tagging and Named Entity Recognition","We present two recently released opensource taggers: NameTag is a free software for named entity recognition (NER) which achieves state-of-the-art performance on Czech; MorphoDiTa (Morphological Dictionary and Tagger) performs morphological analysis (with lemmatization), morphological generation, tagging and tokenization with state-of-the-art results for Czech and a throughput around 10-200K words per second. The taggers can be trained for any language for which annotated data exist, but they are specifically designed to be efficient for inflective languages, Both tools are free software under LGPL license and are distributed along with trained linguistic models which are free for non-commercial use under the CC BY-NC-SA license. The releases include standalone tools, C++ libraries with Java, Python and Perl bindings and web services.",2014,True,False,False,False,False,False,True,"A, G",281,3,284
W14-4611,{DECHE} and the {W}elsh National Corpus Portal,"This paper describes the on-going project on Digitization, E-publishing and Electronic Corpus (DECHE). It also describes the building of a common infrastructure and portal for displaying and disseminating other Welsh language and bilingual Welsh/English text corpora. An overview is given of other corpora included in the on-line corpus portal, as well as corpora intended for future publication through the portal site. This is done within the context of developing resources frugally and efficiently for less-resourced languages.",2014,True,False,False,False,True,False,False,"A, E",217,3,220
D14-1222,A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents,"Bridging resolution plays an important role in establishing (local) entity coherence. This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution, where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations. The system consists of eight rules which target different relations based on linguistic insights. Our rule-based system significantly outperforms a reimplementation of a previous rule-based system (Vieira and Poesio, 2000) . Furthermore, it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system. Additionally, incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system.",2014,False,False,False,True,True,False,False,"D, E",274,3,277
S14-1013,Compositional Distributional Semantics Models in Chunk-based Smoothed Tree Kernels,"The field of compositional distributional semantics has proposed very interesting and reliable models for accounting the distributional meaning of simple phrases. These models however tend to disregard the syntactic structures when they are applied to larger sentences. In this paper we propose the chunk-based smoothed tree kernels (CSTKs) as a way to exploit the syntactic structures as well as the reliability of these compositional models for simple phrases. We experiment with the recognizing textual entailment datasets. Our experiments show that our CSTKs perform better than basic compositional distributional semantic models (CDSMs) recursively applied at the sentence level, and also better than syntactic tree kernels.",2014,False,True,False,True,False,False,False,"B, D",249,3,252
D14-1012,Revisiting Embedding Features for Simple Semi-supervised Learning,"Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism. However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain. In this study, we investigate and analyze three different approaches, including a new proposed distributional prototype approach, for utilizing the embedding features. The presented approaches can be integrated into most of the classical linear models in NLP. Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features, among which the distributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score.",2014,False,False,False,True,False,True,False,"D,F",279,2,281
W14-2502,Creating and Destroying Party Brands,"Party brands are central to theories of Congressional action. While previous work assumes that a party's brand-its long run reputation-is a direct consequence of the content of legislation, in this presentation I show how partisans from both parties use public statements to craft their own party's reputation and to undermine their opponents party. The incentive to craft and destroy brands varies across legislators, creating systematic distortions in who contributes to partisan branding efforts, what is said about a party's brand, and when partisan criticism becomes salient. To demonstrate the construction of party brands I use new collections of newsletters from Congressional offices, along with press releases, floor speeches, and media broadcasts. Across the diverse sources, I show that ideologically extreme legislators are the most likely to explain their party's work in Washington and the most likely to criticize the opposing party-particularly when their is an opposing party president. Extreme legislators also engage in more vitriolic criticism of the opposing party, particularly when opposing presidents are unpopular. The result is that parties in rhetoric appear even more combative and polarized in public debate outside Congress than inside Congress.",2014,True,False,False,False,True,False,False,"A, E",336,3,339
D14-1153,Intrinsic Plagiarism Detection using N-gram Classes,"When it is not possible to compare the suspicious document to the source document(s) plagiarism has been committed from, the evidence of plagiarism has to be looked for intrinsically in the document itself. In this paper, we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes. The proposed method was evaluated on three publicly available standard corpora. The obtained results are comparable to the ones obtained by the best state-of-the-art methods.",2014,True,True,False,False,False,False,False,"A, B",218,3,221
W14-4001,Vector Space Models for Phrase-based Machine Translation,"This paper investigates the application of vector space models (VSMs) to the standard phrase-based machine translation pipeline. VSMs are models based on continuous word representations embedded in a vector space. We exploit word vectors to augment the phrase table with new inferred phrase pairs. This helps reduce out-of-vocabulary (OOV) words. In addition, we present a simple way to learn bilingually-constrained phrase vectors. The phrase vectors are then used to provide additional scoring of phrase pairs, which fits into the standard log-linear framework of phrase-based statistical machine translation. Both methods result in significant improvements over a competitive in-domain baseline applied to the Arabic-to-English task of IWSLT 2013.",2014,False,False,False,True,False,False,True,"D, G",258,3,261
E14-1032,Subcategorisation Acquisition from Raw Text for a Free Word-Order Language,We describe a state-of-the-art automatic system that can acquire subcategorisation frames from raw text for a free word-order language. We use it to construct a subcategorisation lexicon of German verbs from a large Web page corpus. With an automatic verb classification paradigm we evaluate our subcategorisation lexicon against a previous classification of German verbs; the lexicon produced by our system performs better than the best previous results.,2014,False,False,False,False,True,False,True,"E, G",202,3,205
W14-5701,Modelling Regular Subcategorization Changes in {G}erman Particle Verbs,"German particle verbs are a type of multi word expression which is often compositional with respect to a base verb. If they are compositional they tend to express the same types of semantic arguments, but they do not necessarily express them in the same syntactic subcategorization frame: some arguments may be expressed by differing syntactic subcategorization slots and other arguments may be only implicit in either the base or the particle verb. In this paper we present a method which predicts syntactic slot correspondences between syntactic slots of base and particle verb pairs. We can show that this method can predict subcategorization slot correspondences with a fair degree of success.",2014,False,False,False,True,True,False,False,"D, E",248,3,251
S14-2071,{L}y{S}: Porting a {T}witter Sentiment Analysis Approach from {S}panish to {E}nglish,"This paper proposes an approach to solve message-and phrase-level polarity classification in Twitter, derived from an existing system designed for Spanish. As a first step, an ad-hoc preprocessing is performed. We then identify lexical, psychological and semantic features in order to capture different dimensions of the human language which are helpful to detect sentiment. These features are used to feed a supervised classifier after applying an information gain filter, to discriminate irrelevant features. The system is evaluated on the SemEval 2014 task 9: Sentiment Analysis in Twitter. Our approach worked competitively both in message-and phraselevel tasks. The results confirm the robustness of the approach, which performed well on different domains involving short informal texts.",2014,False,False,False,True,False,False,True,"D, G",257,3,260
W14-6905,Emotion Detection from text: A Survey,"This survey describes recent works in the field of Emotion Detection from text, being a part of the broader area of Affective Computing. This survey has been inspired on the well-known fact that, despite there is a lot of work on emotional detection systems, a lot of work is expected to be done yet. The increment of these systems is due to the large amount of emotional data available in Social Web. Detecting emotions from text have attracted the attention of many researchers in computational linguistics because it has a wide range of applications, such as suicide prevention or measuring well-being of a community. This paper mainly collects works based on lexical and machine learning approaches and these works are classificated in accordance with the emotional model and the approach used.",2014,False,False,False,False,True,True,False,"E, F",264,3,267
W14-5411,Cross-media Cross-genre Information Ranking based on Multi-media Information Networks,"Current web technology has brought us a scenario that information about a certain topic is widely dispersed in data from different domains and data modalities, such as texts and images from news and social media. Automatic extraction of the most informative and important multimedia summary (e.g. a ranked list of inter-connected texts and images) from massive amounts of cross-media and cross-genre data can significantly save users' time and effort that is consumed in browsing. In this paper, we propose a novel method to address this new task based on automatically constructed Multi-media Information Networks (MiNets) by incorporating cross-genre knowledge and inferring implicit similarity across texts and images. The facts from MiNets are exploited in a novel random walk-based algorithm to iteratively propagate ranking scores across multiple data modalities. Experimental results demonstrated the effectiveness of our MiNets-based approach and the power of cross-media cross-genre inference.",2014,True,False,True,False,False,False,False,"A, C",296,3,299
W14-3624,Evaluating Distant Supervision for Subjectivity and Sentiment Analysis on {A}rabic {T}witter Feeds,"Supervised machine learning methods for automatic subjectivity and sentiment analysis (SSA) are problematic when applied to social media, such as Twitter, since they do not generalise well to unseen topics. A possible remedy of this problem is to apply distant supervision (DS) approaches, which learn from large amounts of automatically annotated data. This research empirically evaluates the performance of DS approaches for SSA on Arabic Twitter feeds. Results for emoticon-and lexiconbased DS show a significant performance gain over a fully supervised baseline, especially for detecting subjectivity, where we achieve 95.19% accuracy, which is a 48.47% absolute improvement over previous fully supervised results.",2014,False,False,False,True,False,False,True,"D, G",251,3,254
W14-3620,{GWU}-{HASP}: Hybrid {A}rabic Spelling and Punctuation Corrector,"In this paper, we describe our Hybrid Arabic Spelling and Punctuation Corrector (HASP). HASP was one of the systems participating in the QALB-2014 Shared Task on Arabic Error Correction. The system uses a CRF (Conditional Random Fields) classifier for correcting punctuation errors, an open-source dictionary (or word list) for detecting errors and generating and filtering candidates, an n-gram language model for selecting the best candidates, and a set of deterministic rules for text normalization (such as removing diacritics and kashida and converting Hindi numbers into Arabic numerals). We also experiment with word alignment for spelling correction at the character level and report some preliminary results.",2014,False,False,False,True,False,False,True,"D, G",256,3,259
W14-3310,{EU-BRIDGE} {MT}: Combined Machine Translation,"This paper describes one of the collaborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two European language pairs, German→English and English→German. Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). We combined up to nine different machine translation engines via system combination. RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT news-test2013 test set compared to the best single systems.",2014,False,False,False,True,False,True,False,"D, F",307,3,310
E14-4035,Some Experiments with a Convex {IBM} Model 2,"Using a recent convex formulation of IBM Model 2, we propose a new initialization scheme which has some favorable comparisons to the standard method of initializing IBM Model 2 with IBM Model 1. Additionally, we derive the Viterbi alignment for the convex relaxation of IBM Model 2 and show that it leads to better F-Measure scores than those of IBM Model 2.",2014,False,False,True,True,False,False,False,"C, D",193,3,196
W14-1806,The pragmatics of margin comments: An empirical study,"This paper describes the design and rationale behind a classification scheme for English margin comments. The scheme's design was informed by pragmatics and pedagogy theory, and by observations made from a corpus of 24,387 margin comments from assessed university assignments. The purpose of the scheme is to computationally explore content and form relationships between margin comments and the passages to which they point. The process of designing the scheme resulted in the conclusion that margin comments require more work to understand than utterances do, and that they are more prone to being misunderstood.",2014,False,False,False,False,True,False,True,"E, G",225,3,228
W14-0135,{A}ssamese {W}ord{N}et based Quality Enhancement of Bilingual Machine Translation System,"Machine Translation is a task to translate the text from a source language to a target language in an automatic manner. Here, we describe a system that translate the English language to Assamese language text which is based on Phrase based statistical translation technique. To overcome the translation problem related with highly open word class like Proper Noun or the Out Of Vocabulary words we develop a transliteration system which is also embedded with our translation system. We enhance the translation output by replacing words with their most appropriate synonymous word for that particular context with the help of Assamese Word-Net Synset. This Machine Translation system outcomes with a reasonable translation output when analyzed by linguist for Assamese language which is a less computationally aware language among the Indian languages.",2014,False,False,False,True,False,False,True,"D, G",260,3,263
W14-1208,Segmentation of patent claims for improving their readability,"Good readability of text is important to ensure efficiency in communication and eliminate risks of misunderstanding. Patent claims are an example of text whose readability is often poor. In this paper, we aim to improve claim readability by a clearer presentation of its content. Our approach consist in segmenting the original claim content at two levels. First, an entire claim is segmented to the components of preamble, transitional phrase and body, using a rule-based approach. Second, a conditional random field is trained to segment the components into clauses. An alternative approach would have been to modify the claim content which is, however, prone to also changing the meaning of this legal text. For both segmentation levels, we report results from statistical evaluation of segmentation performance. In addition, a qualitative error analysis was performed to understand the problems underlying the clause segmentation task. Our accuracy in detecting the beginning and end of preamble text is 1.00 and 0.97, respectively. For the transitional phase, these numbers are 0.94 and 1.00 and for the body text, 1.00 and 1.00. Our precision and recall in the clause segmentation are 0.77 and 0.76, respectively. The results give evidence for the feasibility of automated claim and clause segmentation, which may help not only inventors, researchers, and other laypeople to understand patents but also patent experts to avoid future legal cost due to litigations.",2014,False,False,False,True,True,False,False,"D, E",405,3,408
2014.iwslt-papers.14,Improving in-domain data selection for small in-domain sets,"Finding sufficient in-domain text data for language modeling is a recurrent challenge. Some methods have already been proposed for selecting parts of out-of-domain text data most closely resembling the in-domain data using a small amount of the latter. Including this new ""near-domain"" data in training can potentially lead to better language model performance, while reducing training resources relative to incorporating all data. One popular, state-of-the-art selection process based on cross-entropy scores makes use of in-domain and out-ofdomain language models. In order to compensate for the limited availability of the in-domain data required for this method, we introduce enhancements to two of its steps. Firstly, we improve the procedure for drawing the outof-domain sample data used for selection. Secondly, we use word-associations in order to extend the underlying vocabulary of the sample language models used for scoring. These enhancements are applied to selecting text for language modeling of talks given in a technical subject area. Besides comparing perplexity, we judge the resulting language models by their performance in automatic speech recognition and machine translation tasks. We evaluate our method in different contexts. We show that it yields consistent improvements, up to 2% absolute reduction in word error rate and 0.3 Bleu points. We achieve these improvements even given a much smaller in-domain set.",2014,False,False,False,True,False,False,True,"D, G",376,3,379
P14-1076,Robust Domain Adaptation for Relation Extraction via Clustering Consistency,"We propose a two-phase framework to adapt existing relation extraction classifiers to extract relations for new target domains. We address two challenges: negative transfer when knowledge in source domains is used without considering the differences in relation distributions; and lack of adequate labeled samples for rarer relations in the new domain, due to a small labeled data set and imbalance relation distributions. Our framework leverages on both labeled and unlabeled data in the target domain. First, we determine the relevance of each source domain to the target domain for each relation type, using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain. To overcome the lack of labeled samples for rarer relations, these clusterings operate on both the labeled and unlabeled data in the target domain. Second, we trade-off between using relevance-weighted sourcedomain predictors and the labeled target data. Again, to overcome the imbalance distribution, the source-domain predictors operate on the unlabeled target data. Our method outperforms numerous baselines and a weakly-supervised relation extraction method on ACE 2004 and YAGO.",2014,False,False,False,True,False,False,True,"D, G",341,3,344
P14-1042,Grammatical Relations in {C}hinese: {GB}-Ground Extraction and Data-Driven Parsing,"This paper is concerned with building linguistic resources and statistical parsers for deep grammatical relation (GR) analysis of Chinese texts. A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs. The reliability of this linguistically-motivated GR extraction procedure is highlighted by manual evaluation. Based on the converted corpus, we study transition-based, datadriven models for GR parsing. We present a novel transition system which suits GR graphs better than existing systems. The key idea is to introduce a new type of transition that reorders top k elements in the memory module. Evaluation gauges how successful GR parsing for Chinese can be by applying datadriven models.",2014,True,True,False,False,False,False,False,"A, B",262,3,265
P14-1096,That{'}s sick dude!: Automatic identification of word sense change across different timescales,"In this paper, we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books. We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points. Subsequently, we compare these sense clusters of two different time points to find if (i) there is birth of a new sense or (ii) if an older sense has got split into more than one sense or (iii) if a newer sense has been formed from the joining of older senses or (iv) if a particular sense has died. We conduct a thorough evaluation of the proposed methodology both manually as well as through comparison with WordNet. Manual evaluation indicates that the algorithm could correctly identify 60.4% birth cases from a set of 48 randomly picked samples and 57% split/join cases from a set of 21 randomly picked samples. Remarkably, in 44% cases the birth of a novel sense is attested by WordNet, while in 46% cases and 43% cases split and join are respectively confirmed by WordNet. Our approach can be applied for lexicography, as well as for applications like word sense disambiguation or semantic search.",2014,True,False,False,True,False,False,False,"A, D",388,3,391
W14-4505,Word Clustering Based on Un-{LP} Algorithm,"Word clustering which generalizes specific features cluster words in the same syntactic or semantic categories into a group. It is an effective approach to reduce feature dimensionality and feature sparseness which are clearly useful for many NLP applications. This paper proposes an unsupervised label propagation algorithm (Un-LP) for word clustering which uses multi-exemplars to represent a cluster. Experiments on a synthetic 2D dataset show the strong ability of selfcorrecting of the proposed algorithm. Besides, the experimental results on 20NG demonstrate that our algorithm outperforms the conventional cluster algorithms.",2014,False,False,True,True,False,False,False,"C, D",235,3,238
F14-2001,Machine translation for litterature: a pilot study (Traduction automatis{\'e}e d{'}une oeuvre litt{\'e}raire: une {\'e}tude pilote) [in {F}rench],"Les techniques actuelles de traduction automatique (TA) permettent de produire des traductions dont la qualité ne cesse de croitre. Dans des domaines spécifiques, la post-édition (PE) de traductions automatiques permet, par ailleurs, d'obtenir des traductions de qualité relativement rapidement. Mais un tel pipeline (TA+PE) est il envisageable pour traduire une oeuvre littéraire ? Cet article propose une ébauche de réponse à cette question. Un essai de l'auteur américain Richard Powers, encore non disponible en français, est traduit automatiquement puis post-édité et révisé par des traducteurs non-professionnels. La plateforme de post-édition du LIG utilisée permet de lire et éditer l'oeuvre traduite en français continuellement, suggérant (pour le futur) une communauté de lecteurs-réviseurs qui améliorent en continu les traductions de leur auteur favori. En plus de la présentation des résultats d'évaluation expérimentale du pipeline TA+PE (système de TA utilisé, scores automatiques), nous discutons également la qualité de la traduction produite du point de vue d'un panel de lecteurs (ayant lu la traduction en français, puis répondu à une enquête). Enfin, quelques remarques du traducteur français de R. Powers, sollicité à cette occasion, sont présentées à la fin de cet article.",2014,False,False,False,True,False,False,True,"D, G",395,3,398
W14-3320,The {UA}-Prompsit hybrid machine translation system for the 2014 Workshop on Statistical Machine Translation,"This paper describes the system jointly developed by members of the Departament de Llenguatges i Sistemes Informàtics at Universitat d'Alacant and the Prompsit Language Engineering company for the shared translation task of the 2014 Workshop on Statistical Machine Translation. We present a phrase-based statistical machine translation system whose phrase table is enriched with information obtained from dictionaries and shallowtransfer rules like those used in rule-based machine translation. The novelty of our approach lies in the fact that the transfer rules used were not written by humans, but automatically inferred from a parallel corpus.",2014,False,False,False,True,False,False,True,"D, G",233,3,236
W14-5401,The Effect of Sensor Errors in Situated Human-Computer Dialogue,"Errors in perception are a problem for computer systems that use sensors to perceive the environment. If a computer system is engaged in dialogue with a human user, these problems in perception lead to problems in the dialogue. We present two experiments, one in which participants interact through dialogue with a robot with perfect perception to fulfil a simple task, and a second one in which the robot is affected by sensor errors and compare the resulting dialogues to determine whether the sensor problems have an impact on dialogue success.",2014,False,False,False,False,True,False,True,"E, G",214,3,217
D14-1069,Non-linear Mapping for Improved Identification of 1300+ Languages,"Non-linear mappings of the form P (ngram) γ and log(1+τ P (ngram)) log(1+τ ) are applied to the n-gram probabilities in five trainable open-source language identifiers. The first mapping reduces classification errors by 4.0% to 83.9% over a test set of more than one million 65-character strings in 1366 languages, and by 2.6% to 76.7% over a subset of 781 languages. The second mapping improves four of the five identifiers by 10.6% to 83.8% on the larger corpus and 14.4% to 76.7% on the smaller corpus. The subset corpus and the modified programs are made freely available for download at http://www.cs.cmu.edu/∼ralf/langid.html.",2014,True,False,False,True,False,False,False,"D, A",294,3,297
Y14-1050,Influence of Information Structure on Word Order Change and Topic Marker {WA} in {J}apanese,"The purpose of this study is to investigate the influence of given-new ordering on word order change and topic marker WA, using a self-paced reading task. The results demonstrated that OACCSNOMV is sensitive to given-new information, but SNOMOACCV, STOPOACCV, and OTOPSNOMV are not. This fact can be explained by the Markedness Principle for Discourse Rule Violation (Kuno, 1987: 212): both SNOMOACCV and STOPOACCV are not penalized even when they violate given-new ordering because they are unmarked options, OACCSNOMV is penalized when it violates given-new ordering because it is a marked option, and OTOPSNOMV is penalized even when given-new ordering is preserved because it requires more contrastive contexts (McGloin, 1990:113). Another point is that topic marker WA is not responsive to the given-new distinction. This suggests that the usage of WA does not rely on anaphoricity in general. Note that there are two usages of WA: thematic topic needs to be previously mentioned while contrastive topic does not require anaphoricity. Taken together, we can conclude that the essence of WA is not thematic topic but contrastive topic.",2014,False,False,False,False,True,True,False,"E,F",379,2,381
P14-2012,Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction,"Relation extraction suffers from a performance loss when a model is applied to out-of-domain data. This has fostered the development of domain adaptation techniques for relation extraction. This paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors.",2014,False,False,False,True,False,True,False,"D, F",206,3,209
2014.lilt-11.5,Semi-separate exponence in cumulative paradigms. Information-theoretic properties exemplified by {A}ncient {G}reek verb endings,"By using the system of Ancient Greek verb endings as a case study, this paper deals with the cross-linguistically recurrent appearance of inflectional paradigms that, though generally characterized by cumulative exponence, contain segmentable ""semi-separate"" endings in correspondence with low-frequency cells. Such an exponence system has information-theoretic properties which may be relevant from the point of view of morphological theory. In particular, both the phenomena of semi-separate exponence and the instances of syncretism that conform to the Brøndalian Principle of Compensation may be viewed as different manifestations of a same cross-linguistic tendency not to let a paradigm's exponent set be too distant from the situation of equiprobability.",2014,False,False,False,False,True,True,False,"E, F",261,3,264
P14-1064,Graph-based Semi-Supervised Learning of Translation Models from Monolingual Data,"Statistical phrase-based translation learns translation rules from bilingual corpora, and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates. In this work, we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data. The proposed technique first constructs phrase graphs using both source and target language monolingual corpora. Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets.",2014,True,False,False,True,False,False,False,"A, D",270,3,273
P14-2059,Citation Resolution: A method for evaluating context-based citation recommendation systems,"Wouldn't it be helpful if your text editor automatically suggested papers that are relevant to your research? Wouldn't it be even better if those suggestions were contextually relevant? In this paper we name a system that would accomplish this a context-based citation recommendation (CBCR) system. We specifically present Citation Resolution, a method for the evaluation of CBCR systems which exclusively uses readily-available scientific articles. Exploiting the human judgements that are already implicit in available resources, we avoid purpose-specific annotation. We apply this evaluation to three sets of methods for representing a document, based on a) the contents of the document, b) the surrounding contexts of citations to the document found in other documents, and c) a mixture of the two.",2014,True,False,False,True,False,False,False,"A, D",266,3,269
W14-4012,On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches,"Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder-Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.",2014,False,True,False,False,False,True,False,"F, B",269,3,272
W14-0142,pl{W}ord{N}et as the Cornerstone of a Toolkit of Lexico-semantic Resources,"A wordnet is many things to many people: a graph of inter-related lexicalised concepts, a taxonomy, a thesaurus, and so on. A wordnet makes good sense as the mainstay of any deep automated semantic analysis of text. We have begun the construction of a multi-component, multi-use toolkit of natural language processing tools with plWordNet, a very large Polish wordnet, at its centre. The components will include plWordNet and its mapping onto an ontology (the upper level and elements of the middle level), a lexicon of proper names and a semantic valency lexicon. Some of those elements will be aligned with plWordNet, and there will be a mapping onto Princeton WordNet. Several challenging applications will show the utility of the toolkit in practice.",2014,True,False,False,False,False,False,True,"A, G",275,3,278
W14-0313,Online Word Alignment for Online Adaptive Machine Translation,"A hot task in the Computer Assisted Translation scenario is the integration of Machine Translation (MT) systems that adapt sentence after sentence to the postedits made by the translators. A main role in the MT online adaptation process is played by the information extracted from source and post-edited sentences, which in turn depends on the quality of the word alignment between them. In fact, this step is particularly crucial when the user corrects the MT output with words for which the system has no prior information. In this paper, we first discuss the application of popular state-of-the-art word aligners to this scenario and reveal their poor performance in aligning unknown words. Then, we propose a fast procedure to refine their outputs and to get more reliable and accurate alignments for unknown words. We evaluate our enhanced word-aligner on three language pairs, namely English-Italian, English-French, and English-Spanish, showing a consistent improvement in aligning unknown words up to 10% absolute Fmeasure.",2014,False,False,False,True,False,True,False,"D, F",313,3,316
S14-2039,{DLS}@{CU}: Sentence Similarity from Word Alignment,"We present an algorithm for computing the semantic similarity between two sentences. It adopts the hypothesis that semantic similarity is a monotonically increasing function of the degree to which (1) the two sentences contain similar semantic units, and (2) such units occur in similar semantic contexts. With a simplistic operationalization of the notion of semantic units with individual words, we experimentally show that this hypothesis can lead to state-of-the-art results for sentencelevel semantic similarity. At the Sem-Eval 2014 STS task (task 10), our system demonstrated the best performance (measured by correlation with human annotations) among 38 system runs.",2014,False,False,True,True,False,False,False,"C, D",244,3,247
W14-1801,Automated Measures of Specific Vocabulary Knowledge from Constructed Responses ({`}Use These Words to Write a Sentence Based on this Picture{'}),"We describe a system for automatically scoring a vocabulary item type that asks test-takers to use two specific words in writing a sentence based on a picture. The system consists of a rule-based component and a machine learned statistical model which uses a variety of construct-relevant features. Specifically, in constructing the statistical model, we investigate if grammar, usage, and mechanics features developed for scoring essays can be applied to short answers, as in our task. We also explore new features reflecting the quality of the collocations in the response, as well as features measuring the consistency of the response to the picture. System accuracy in scoring is 15 percentage points greater than the majority class baseline and 10 percentage points less than human performance.",2014,False,False,False,True,False,False,True,"D, G",261,3,264
W14-5317,Experiments in Sentence Language Identification with Groups of Similar Languages,"Language identification is a simple problem that becomes much more difficult when its usual assumptions are broken. In this paper we consider the task of classifying short segments of text in closely-related languages for the Discriminating Similar Languages shared task, which is broken into six subtasks, (A) Bosnian, Croatian, and Serbian, (B) Indonesian and Malay, (C) Czech and Slovak, (D) Brazilian and European Portuguese, (E) Argentinian and Peninsular Spanish, and (F) American and British English. We consider a number of different methods to boost classification performance, such as feature selection and data filtering, but we ultimately find that a simple naïve Bayes classifier using character and word n-gram features is a strong baseline that is difficult to improve on, achieving an average accuracy of 0.8746 across the six tasks.",2014,False,False,False,True,True,False,False,"D, E",293,3,296
W14-3910,Language Identification in Code-Switching Scenario,This paper describes a CRF based token level language identification system entry to Language Identification in Code-Switched (CS) Data task of CodeSwitch 2014. Our system hinges on using conditional posterior probabilities for the individual codes (words) in code-switched data to solve the language identification task. We also experiment with other linguistically motivated language specific as well as generic features to train the CRF based sequence labeling algorithm achieving reasonable results.,2014,False,False,False,True,False,False,True,"G, D",206,3,209
2014.eamt-1.8,{TRANSLAIDE}. The translaide.pl system: an effective real world installation of translation memory searching and {EBMT},"PolEng Sp. z o.o. http://translaide.pl Description translaide.pl is a CAT system developed by the Polish company PolEng Sp. z o.o. that supports multiple input and output languages. The main idea of the system is to enable the sharing of resources among translators. A demo version of the system is available on the internet (http://translaide.pl), yet it is primarily intended for exclusive use in a single corporation. The system has been successfully implemented in two companies dealing with high-volume content to be translated.",2014,False,False,False,True,False,False,True,"G, D",228,3,231
W14-5136,{A}uto{P}ar{S}e: An Automatic Paradigm Selector For Nouns in {K}onkani,"In this paper, we discuss a rule based method which automatically assigns paradigms to Konkani nouns using morphophonemic rules, stem formation rules and relevance score of the paradigms. The first contribution is computation of relevance score of a paradigm, which is computed using a corpus and paradigm differentiating measure assigned to inflectional suffixes in the paradigm. Relevance score helps assign multiple paradigms to the input word wherever appropriate. The other contribution is a method for computing paradigm differentiating measure for inflectional suffixes. We have proposed a pruning technique based on derivational suffixes to further improve the precision. The experimental study has been carried out using the Konkani WordNet and the Asmitai Corpus. The proposed method successfully assigned relevant paradigms to 10,068 nouns with F-Score of 0.93.",2014,False,False,False,True,True,False,False,"D, E",284,3,287
D14-1200,Modeling Joint Entity and Relation Extraction with Table Representation,"This paper proposes a history-based structured learning approach that jointly extracts entities and relations in a sentence. We introduce a novel simple and flexible table representation of entities and relations. We investigate several feature settings, search orders, and learning methods with inexact search on the table. The experimental results demonstrate that a joint learning approach significantly outperforms a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders.",2014,False,True,False,True,False,False,False,"B, D",201,3,204
W14-1213,Classifying easy-to-read texts without parsing,"Document classification using automated linguistic analysis and machine learning (ML) has been shown to be a viable road forward for readability assessment. The best models can be trained to decide if a text is easy to read or not with very high accuracy, e.g. a model using 117 parameters from shallow, lexical, morphological and syntactic analyses achieves 98,9% accuracy. In this paper we compare models created by parameter optimization over subsets of that total model to find out to which extent different high-performing models tend to consist of the same parameters and if it is possible to find models that only use features not requiring parsing. We used a genetic algorithm to systematically optimize parameter sets of fixed sizes using accuracy of a Support Vector Machine classifier as fitness function. Our results show that it is possible to find models almost as good as the currently best models while omitting parsing based features.",2014,False,False,False,True,False,True,False,"D, F",291,3,294
U14-1005,Trading accuracy for faster named entity linking,"Named entity linking (NEL) can be applied to documents such as financial reports, web pages and news articles, but state of the art disambiguation techniques are currently too slow for web-scale applications because of a high complexity with respect to the number of candidates. In this paper, we accelerate NEL by taking two successful disambiguation features (popularity and context comparability) and use them to reduce the number of candidates before further disambiguation takes place. Popularity is measured by in-link score, and context similarity is measured by locality sensitive hashing. We present a novel approach to locality sensitive hashing which embeds the projection matrix into a smaller array and extracts columns of the projection matrix using feature hashing, resulting in a lowmemory approximation. We run the linker on a test set in 63% of the baseline time with an accuracy loss of 0.72%.",2014,False,False,True,True,False,False,False,"C, D",292,3,295
W14-3101,{M}i{T}ext{E}xplorer: Linked brushing and mutual information for exploratory text data analysis,"In this paper I describe a preliminary experimental system, MITEXTEXPLORER, for textual linked brushing, which allows an analyst to interactively explore statistical relationships between (1) terms, and (2) document metadata (covariates). An analyst can graphically select documents embedded in a temporal, spatial, or other continuous space, and the tool reports terms with strong statistical associations for the region. The user can then drill down to specific term and term groupings, viewing further associations, and see how terms are used in context. The goal is to rapidly compare language usage across interesting document covariates. I illustrate examples of using the tool on several datasets: geo-located Twitter messages, presidential State of the Union addresses, the ACL Anthology, and the King James Bible.",2014,False,False,False,True,False,False,True,"D, G",276,3,279
C14-1205,Supervised Ranking of Co-occurrence Profiles for Acquisition of Continuous Lexical Attributes,"Certain common lexical attributes such as polarity and formality are continuous, creating challenges for accurate lexicon creation. Here we present a general method for automatically placing words on these spectra, using co-occurrence profiles, counts of co-occurring words within a large corpus, as a feature vector to a supervised ranking algorithm. With regards to both polarity and formality, we show this method consistently outperforms commonly-used alternatives, both with respect to the intrinsic quality of the lexicon and also when these newly-built lexicons are used in downstream tasks.",2014,False,False,True,True,False,False,False,"C, D",226,3,229
P14-2057,Tri-Training for Authorship Attribution with Limited Training Data,"Authorship attribution (AA) aims to identify the authors of a set of documents. Traditional studies in this area often assume that there are a large set of labeled documents available for training. However, in the real life, it is often difficult or expensive to collect a large set of labeled data. For example, in the online review domain, most reviewers (authors) only write a few reviews, which are not enough to serve as the training data for accurate classification. In this paper, we present a novel three-view tritraining method to iteratively identify authors of unlabeled data to augment the training set. The key idea is to first represent each document in three distinct views, and then perform tri-training to exploit the large amount of unlabeled documents. Starting from 10 training documents per author, we systematically evaluate the effectiveness of the proposed tritraining method for AA. Experimental results show that the proposed approach outperforms the state-of-the-art semi-supervised method CNG+SVM and other baselines.",2014,False,True,False,True,False,False,False,"B, D",322,3,325
W14-3347,{VERT}a participation in the {WMT}14 Metrics Task,"In this paper we present VERTa, a linguistically-motivated metric that combines linguistic features at different levels. We provide the linguistic motivation on which the metric is based, as well as describe the different modules in VERTa and how they are combined. Finally, we describe the two versions of VERTa, VERTa-EQ and VERTa-W, sent to WMT14 and report results obtained in the experiments conducted with the WMT12 and WMT13 data into English.",2014,False,True,False,True,False,False,False,"B, D",220,3,223
P14-1059,How to make words with vectors: Phrase generation in distributional semantics,"We introduce the problem of generation in distributional semantics: Given a distributional vector representing some meaning, how can we generate the phrase that best expresses that meaning? We motivate this novel challenge on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions. We test this in a monolingual scenario (paraphrase generation) as well as in a cross-lingual setting (translation by synthesizing adjectivenoun phrase vectors in English and generating the equivalent expressions in Italian).",2014,True,False,False,True,False,False,False,"A, D",219,3,222
W14-3502,An analysis of a {F}rench as a Foreign Language Corpus for Readability Assessment,"Readability aims to assess the difficulty of texts based on various linguistic predictors (the lexicon used, the complexity of sentences, the coherence of the text, etc.). It is an active field that has applications in a large number of NLP domains, among which machine translation, text simplification, text summarisation, or CALL (Computer-Assisted Language Learning). For CALL, readability tools could be used to help the retrieval of educational materials or to make CALL platforms more adaptive. However, developing a readability formula is a costly process that requires a large amount of texts annotated in terms of difficulty. The current mainstream method to gather such a large corpus of annotated texts is to get them from educational resources such as textbooks or simplified readers. In this paper, we describe the collection process of an annotated corpus of French as a foreign language texts with the purpose of training a readability model. We follow the mainstream approach, getting the texts from textbooks, but we are concerned with the limitations of such ""annotation"" approach, in particular, as regards the homogeneity of the difficulty annotations across textbook series. Their reliability is assessed using both a qualitative and a quantitative analysis. It appears that, for some educational levels, the hypothesis of the annotation homogeneity must be rejected. Various reasons for such findings are discussed and the paper concludes with recommandations for future similar attempts.",2014,True,False,False,False,True,False,False,"A, E",384,3,387
2014.iwslt-evaluation.13,Phrase-based language modelling for statistical machine translation,"In this paper, we present our submitted MT system for the IWSLT2014 Evaluation Campaign. We participated in the English-French translation task. In this article we focus on one of the most important component of SMT: the language model. The idea is to use a phrase-based language model. For that, sequences from the source and the target language models are retrieved and used to calculate a phrase n-gram language model. These phrases are used to rewrite the parallel corpus which is then used to calculate a new translation model.",2014,False,False,False,True,False,False,True,"D, G",224,3,227
2014.amta-researchers.3,Coarse {``}split and lump{''} bilingual language models for richer source information in {SMT},"Recently, there has been interest in automatically generated word classes for improving statistical machine translation (SMT) quality: e.g, (Wuebker et al, 2013) . We create new models by replacing words with word classes in features applied during decoding; we call these ""coarse models"". We find that coarse versions of the bilingual language models (biLMs) of (Niehues et al, 2011) yield larger BLEU gains than the original biLMs. BiLMs provide phrase-based systems with rich contextual information from the source sentence; because they have a large number of types, they suffer from data sparsity. Niehues et al (2011) mitigated this problem by replacing source or target words with parts of speech (POSs). We vary their approach in two ways: by clustering words on the source or target side over a range of granularities (word clustering), and by clustering the bilingual units that make up biLMs (bitoken clustering). We find that loglinear combinations of the resulting coarse biLMs with each other and with coarse LMs (LMs based on word classes) yield even higher scores than single coarse models. When we add an appealing ""generic"" coarse configuration chosen on English > French devtest data to four language pairs (keeping the structure fixed, but providing language-pair-specific models for each pair), BLEU gains on blind test data against strong baselines averaged over 5 runs are +0.80 for English > French, +0.35 for French > English, +1.0 for Arabic > English, and +0.6 for Chinese > English.",2014,False,False,False,True,False,False,True,"D, G",452,3,455
E14-4001,Easy Web Search Results Clustering: When Baselines Can Reach State-of-the-Art Algorithms,"This work discusses the evaluation of baseline algorithms for Web search results clustering. An analysis is performed over frequently used baseline algorithms and standard datasets. Our work shows that competitive results can be obtained by either fine tuning or performing cascade clustering over well-known algorithms. In particular, the latter strategy can lead to a scalable and real-world solution, which evidences comparative results to recent text-based state-of-the-art algorithms.",2014,False,False,False,True,False,True,False,"D, F",198,3,201
D14-1208,Noisy Or-based model for Relation Extraction using Distant Supervision,"Distant supervision, a paradigm of relation extraction where training data is created by aligning facts in a database with a large unannotated corpus, is an attractive approach for training relation extractors. Various models are proposed in recent literature to align the facts in the database to their mentions in the corpus. In this paper, we discuss and critically analyse a popular alignment strategy called the ""at least one"" heuristic. We provide a simple, yet effective relaxation to this strategy. We formulate the inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the ""at least one "" heuristic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches.",2014,False,False,True,True,False,False,False,"C, D",270,3,273
P14-2046,How to Speak a Language without Knowing It,"We develop a system that lets people overcome language barriers by letting them speak a language they do not know. Our system accepts text entered by a user, translates the text, then converts the translation into a phonetic spelling in the user's own orthography. We trained the system on phonetic spellings in travel phrasebooks.",2014,False,False,False,True,False,False,True,"G, D",182,3,185
2014.eamt-1.35,"Collaborative web {UI} localization, or how to build feature-rich multilingual datasets","We present a method to generate featurerich multilingual parallel datasets for machine translation systems, including e.g. type of widget, user's locale, or geolocation. To support this argument, we have developed a bookmarklet that instruments arbitrary websites so that casual end users can modify their texts on demand. After surveying 52 users, we conclude that people is leaned toward using this method in lieu of other comparable alternatives. We validate our prototype in a controlled study with 10 users, showing that language resources can be easily generated.",2014,True,False,False,False,False,False,True,"A, G",223,3,226
W14-2703,The Enrollment Effect: A Study of {A}mazon{'}s Vine Program,"Do rewards from retailers such as free products and recognition in the form of status badges 1 influence the recipient's behavior? We present a novel application of natural language processing to detect differences in consumer behavior due to such rewards. Specifically, we investigate the ""Enrollment"" effect, i.e. whether receiving products for free affect how consumer reviews are written. Using data from Amazon's Vine program, we conduct a detailed analysis to detect stylistic differences in product reviews written by reviewers before and after enrollment in the Vine program. Our analysis suggests that the ""Enrollment"" effect exists. Further, we are able to characterize the effect on syntactic and semantic dimensions. This work has implications for researchers, firms and consumer advocates studying the influence of user-generated content as these changes in style could potentially influence consumer decisions.",2014,False,False,False,True,True,False,False,"D, E",276,3,279
W14-4602,Using {I}rish {NLP} resources in Primary School Education,"This paper looks at the use of Natural Language Processing (NLP) resources in primary school education in Ireland. It shows how two Irish NLP resources, the Irish Finite State Transducer Morphological Engine (IFSTME) (Uí Dhonnchadha, 2002) and Gramadóir (Scannell, 2005) were used as the underlying engines for two Computer Assisted Language Learning (CALL) resources for Irish. The IFSTME was used to supply verb conjugation information for a Verb Checker Component of a CALL resource, while Gramadóir was the underlying engine for a Writing Checker Component. The paper outlines the motivation behind the development of these resources which include trying to leverage some of the benefits of CALL for students studying Irish in primary school. In order to develop CALL materials that were not just an electronic form of a textbook, it was considered important to incorporate existing NLP resources into the CALL materials. This would have the benefit of not re-inventing the wheel and of using tools that had been designed and testing by a knowledgeable NLP researcher, rather than starting from scratch. The paper reports on the successful development of the CALL resources and some positive feedback from students and teachers. There are several non-technical reasons, mainly logistical, which hinder the deployment of Irish CALL resources in schools, but Irish NLP researchers should strive to disseminate their research and findings to a wider audience than usual, if they wish others to benefit from their work.",2014,False,False,False,True,False,False,True,"G, D",411,3,414
W14-1215,An evaluation of syntactic simplification rules for people with autism,"Syntactically complex sentences constitute an obstacle for some people with Autistic Spectrum Disorders. This paper evaluates a set of simplification rules specifically designed for tackling complex and compound sentences. In total, 127 different rules were developed for the rewriting of complex sentences and 56 for the rewriting of compound sentences. The evaluation assessed the accuracy of these rules individually and revealed that fully automatic conversion of these sentences into a more accessible form is not very reliable.",2014,False,False,False,True,True,False,False,"D, E",207,3,210
P14-1074,Linguistic Structured Sparsity in Text Categorization,"We introduce three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters for text categorization. These regularizers impose linguistic bias in feature weights, enabling us to incorporate prior knowledge into conventional bagof-words models. We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction problems: topic classification, sentiment analysis, and forecasting.",2014,False,False,False,True,True,False,False,"D, E",224,3,227
W14-1809,Automatic evaluation of spoken summaries: the case of language assessment,"This paper investigates whether ROUGE, a popular metric for the evaluation of automated written summaries, can be applied to the assessment of spoken summaries produced by non-native speakers of English. We demonstrate that ROUGE, with its emphasis on the recall of information, is particularly suited to the assessment of the summarization quality of non-native speakers' responses. A standard baseline implementation of ROUGE-1 computed over the output of the automated speech recognizer has a Spearman correlation of ρ = 0.55 with experts' scores of speakers' proficiency (ρ = 0.51 for a content-vector baseline). Further increases in agreement with experts' scores can be achieved by using types instead of tokens for the computation of word frequencies for both candidate and reference summaries, as well as by using multiple reference summaries instead of a single one. These modifications increase the correlation with experts' scores to a Spearman correlation of ρ = 0.65. Furthermore, we found that the choice of reference summaries does not have any impact on performance, and that the adjusted metric is also robust to errors introduced by automated speech recognition (ρ = 0.67 for human transcriptions vs. ρ = 0.65 for speech recognition output).",2014,False,False,False,False,True,False,True,"E, G",364,3,367
S14-2091,{SAP}-{RI}: {T}witter Sentiment Analysis in Two Days,"We describe the submission of the SAP Research & Innovation team to the Se-mEval 2014 Task 9: Sentiment Analysis in Twitter. We challenged ourselves to develop a competitive sentiment analysis system within a very limited time frame. Our submission was developed in less than two days and achieved an F 1 score of 77.26% for contextual polarity disambiguation and 55.47% for message polarity classification, which shows that rapid prototyping of sentiment analysis systems with reasonable accuracy is possible.",2014,False,False,False,True,False,False,True,"D, G",219,3,222
W14-0210,Navigation Dialog of Blind People: Recovery from Getting Lost,"Navigation of blind people is different from the navigation of sighted people and there is also difference when the blind person is recovering from getting lost. In this paper we focus on qualitative analysis of dialogs between lost blind person and navigator, which is done through the mobile phone. The research was done in two outdoor and one indoor location. The analysis revealed several areas where the dialog model must focus on detailed information, like evaluation of instructions provided by blind person and his/her ability to reliably locate navigation points.",2014,False,False,False,False,True,True,False,"E, F",216,3,219
D14-1183,Large-scale Reordering Model for Statistical Machine Translation using Dual Multinomial Logistic Regression,"Phrase reordering is a challenge for statistical machine translation systems. Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model. However, Training this discriminative model using large-scale parallel corpus might be computationally expensive. In this paper, we explore recent advancements in solving large-scale classification problems. Using the dual problem to multinomial logistic regression, we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy.",2014,False,True,True,False,False,False,False,"B, C",225,3,228
W14-3006,Bridging Text and Knowledge with Frames,"FrameNet is the best currently operational version of Chuck Fillmore's Frame Semantics. As FrameNet has evolved over the years, we have been building a series of increasingly ambitious prototype systems that exploit FrameNet as a semantic resource. Results from this work point to frames as a natural representation for applications that require linking textual meaning to world knowledge.",2014,False,False,False,True,False,False,True,"D, G",186,3,189
W14-3329,{DCU} Terminology Translation System for Medical Query Subtask at {WMT}14,"This paper describes the Dublin City University terminology translation system used for our participation in the query translation subtask in the medical translation task in the Workshop on Statistical Machine Translation (WMT14). We deployed six different kinds of terminology extraction methods, and participated in three different tasks: FR-EN and EN-FR query tasks, and the CLIR task. We obtained 36.2 BLEU points absolute for FR-EN and 28.8 BLEU points absolute for EN-FR tasks where we obtained the first place in both tasks. We obtained 51.8 BLEU points absolute for the CLIR task.",2014,False,False,False,True,False,False,True,"G, D",242,3,245
P14-2135,Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More,"Models that learn semantic representations from both linguistic and perceptual input outperform text-only models in many contexts and better reflect human concept acquisition. However, experiments suggest that while the inclusion of perceptual input improves representations of certain concepts, it degrades the representations of others. We propose an unsupervised method to determine whether to include perceptual input for a concept, and show that it significantly improves the ability of multi-modal models to learn and represent word meanings. The method relies solely on image data, and can be applied to a variety of other NLP tasks.",2014,False,True,False,True,False,False,False,"B, D",228,3,231
D14-1215,Brighter than Gold: Figurative Language in User Generated Comparisons,"Comparisons are common linguistic devices used to indicate the likeness of two things. Often, this likeness is not meant in the literal sense-for example, ""I slept like a log"" does not imply that logs actually sleep. In this paper we propose a computational study of figurative comparisons, or similes. Our starting point is a new large dataset of comparisons extracted from product reviews and annotated for figurativeness. We use this dataset to characterize figurative language in naturally occurring comparisons and reveal linguistic patterns indicative of this phenomenon. We operationalize these insights and apply them to a new task with high relevance to text understanding: distinguishing between figurative and literal comparisons. Finally, we apply this framework to explore the social context in which figurative language is produced, showing that similes are more likely to accompany opinions showing extreme sentiment, and that they are uncommon in reviews deemed helpful.",2014,True,False,False,False,True,False,False,"A, E",292,3,295
S14-2079,{OPI}: {S}emeval-2014 Task 3 System Description,"In this paper, we describe the OPI system participating in the Semeval-2014 task 3 Cross-Level Semantic Similarity. Our approach is knowledge-poor, there is no exploitation of any structured knowledge resources as Wikipedia, WordNet or Babel-Net. The method is also fully unsupervised, the training set is only used in order to tune the system. System measures the semantic similarity of texts using corpusbased measures of termsets similarity.",2014,False,False,False,True,False,False,True,"D, G",209,3,212
W14-5811,Extended phraseological information in a valence dictionary for {NLP} applications,"The aim of this paper is to propose a far-reaching extension of the phraseological component of a valence dictionary for Polish. The dictionary is the basis of two different parsers of Polish; its format has been designed so as to maximise the readability of the information it contains and its re-applicability. We believe that the extension proposed here follows this approach and, hence, may be an inspiration in the design of valence dictionaries for other languages.",2014,True,False,False,False,True,False,False,"A, E",208,3,211
P14-1142,A Joint Graph Model for {P}inyin-to-{C}hinese Conversion with Typo Correction,"It is very import for Chinese language processing with the aid of an efficient input method engine (IME), of which pinyinto-Chinese (PTC) conversion is the core part. Meanwhile, though typos are inevitable during user pinyin inputting, existing IMEs paid little attention to such big inconvenience. In this paper, motivated by a key equivalence of two decoding algorithms, we propose a joint graph model to globally optimize PTC and typo correction for IME. The evaluation results show that the proposed method outperforms both existing academic and commercial IMEs.",2014,False,True,False,True,False,False,False,"B, D",232,3,235
F14-2018,"Comparing two analyzers of {J}apanese corpora for helping linguists: {M}e{C}ab and Sagace (Comparaison de deux outils d{'}analyse de corpus japonais pour l{'}aide au linguiste, Sagace et Mecab) [in {F}rench]","L'objectif est de comparer deux outils d'analyse de corpus de textes bruts pour l'aide à la recherche en linguistique japonaise. Nous mesurons leur précision dans la tâche de comptage de chaînes de morphes. Les deux outils représentent chacun une approche spécifique. Le premier, un dispositif basé sur l'analyseur morphologique statistique MeCab, segmente et étiquette préalablement les phrases complètes. Le second compte les occurrences de la chaîne dans le texte en l'état. Les performances de Sagace sont globalement un peu inférieures mais la différence est moins importante qu'attendu. Du fait de leur facilité de mise en oeuvre, les outils comme Sagace sans analyse morphologique préalable sont donc des outils malgré tout intéressants pour le linguiste.",2014,False,False,False,False,True,True,False,"E, F",271,3,274
Y14-1070,On the Argument Structures of the Transitive Verb {`}annoy; be annoyed; bother to do{'}: A Study Based on Two Comparable Corpora,"This paper investigates the transitive uses of the verb fan ""annoy; be annoyed; bother to do"", which exhibit both similarities and disparities between Beijing Mandarin and Taiwan Mandarin, as far as the data from Gigaword corpus, containing data from Mainland China (XIN) and Taiwan (CNA), are concerned. In terms of similarities, the causative (and agentive) use(s) of the transitive fan is/are shared by both Beijing Mandarin and Taiwan Mandarin. The disparity mainly lies in the mental use of fan ""be annoyed"", which is not only unattested in the corpus of Taiwan Mandarin but also reported as weird by our informants. This mental use, on the other hand, is well attested in the corpus. In order to describe as well as explain the difference in uses between Beijing Mandarin and Taiwan Mandarin, we adopt the Theta System Theory (Reinhart 2002; Marelj 2004) to probe into the argument structures of the transitive verb fan and further pinpoint the fundamental syntactic difference between Beijing Mandarin and Taiwan Mandarin, that is, the absence or presence of the /+c feature in the argument structure. In particular, Taiwan Mandarin requires the obligatory presence of the /+c feature in the argument structure of fan, while Beijing Mandarin does not.",2014,False,False,False,False,True,True,False,"E,F",380,2,382
F14-2005,Automated Analysis for Stem Spaces: the case of {F}rench verbs (Analyse automatique d{'}espaces th{\'e}matiques) [in {F}rench],"Basé sur les calculs d'entropie conditionnelle de (Bonami & Boyé, à paraître), nous proposons un analyseur automatique de la flexion dans le cadre de la morphologie thématique qui produit le graphe de régularités du paradigme. Le traitement se base sur un lexique de 6440 verbes extraits du BDLex (de Calmès & Pérennou, 1998) associés à leurs fréquences dans Lexique3 (New et al., 2001). L'algorithme se compose de trois éléments : calcul de l'entropie conditionnelle entre paires de formes fléchies, distillation des paradigmes, construction du graphe de régularités. Pour l'entropie, nous utilisons deux modes de calcul différents, l'un se base sur la distribution de l'effectif des verbes entre leurs différentes options, l'autre sur la distribution des lexèmes verbaux en fonction de leurs fréquences pour contrebalancer l'influence des verbes ultra-fréquents sur les calculs.",2014,False,False,True,False,True,False,False,"C, E",337,3,340
J14-3006,Similarity-Driven Semantic Role Induction via Graph Partitioning,"As in many natural language processing tasks, data-driven models based on supervised learning have become the method of choice for semantic role labeling. These models are guaranteed to perform well when given sufficient amount of labeled training data. Producing this data is costly and time-consuming, however, thus raising the question of whether unsupervised methods offer a viable alternative. The working hypothesis of this article is that semantic roles can be induced without human supervision from a corpus of syntactically parsed sentences based on three linguistic principles: (1) arguments in the same syntactic position (within a specific linking) bear the same semantic role, (2) arguments within a clause bear a unique role, and (3) clusters representing the same semantic role should be more or less lexically and distributionally equivalent. We present a method that implements these principles and formalizes the task as a graph partitioning problem, whereby argument instances of a verb are represented as vertices in a graph whose edges express similarities between these instances. The graph consists of multiple edge layers, each one capturing a different aspect of argument-instance similarity, and we develop extensions of standard clustering algorithms for partitioning such multi-layer graphs. Experiments for English and German demonstrate that our approach is able to induce semantic role clusters that are consistently better than a strong baseline and are competitive with the state of the art.",2014,False,False,True,True,False,False,False,"C, D",385,3,388
W14-5814,"A Database of Paradigmatic Semantic Relation Pairs for {G}erman Nouns, Verbs, and Adjectives","A new collection of semantically related word pairs in German is presented, which was compiled via human judgement experiments and comprises (i) a representative selection of target lexical units balanced for semantic category, polysemy, and corpus frequency, (ii) a set of humangenerated semantically related word pairs based on the target units, and (iii) a subset of the generated word pairs rated for their relation strength, including positive and negative relation evidence. We address the three paradigmatic relations antonymy, hypernymy and synonymy, and systematically work across the three word classes of adjectives, nouns, and verbs. A series of quantitative and qualitative analyses demonstrates that (i) antonyms are more canonical than hypernyms and synonyms, (ii) relations are more or less natural with regard to the specific word classes, (iii) antonymy is clearly distinguishable from hypernymy and synonymy, but hypernymy and synonymy are often confused. We anticipate that our new collection of semantic relation pairs will not only be of considerable use in computational areas in which semantic relations play a role, but also in studies in theoretical linguistics and psycholinguistics.",2014,True,False,False,False,True,False,False,"A, E",353,3,356
P14-2062,Experiments with crowdsourced re-annotation of a {POS} tagging data set,"Crowdsourcing lets us collect multiple annotations for an item from several annotators. Typically, these are annotations for non-sequential classification tasks. While there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced. This paper shows that workers can actually annotate sequential data almost as well as experts. Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.",2014,False,False,False,False,True,False,True,"E, G",224,3,227
W14-4326,Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems,"Many goal-oriented dialog agents are expected to identify slot-value pairs in a spoken query, then perform lookup in a knowledge base to complete the task. When the agent encounters unknown slotvalues, it may ask the user to repeat or reformulate the query. But a robust agent can proactively seek new knowledge from a user, to help reduce subsequent task failures. In this paper, we propose knowledge acquisition strategies for a dialog agent and show their effectiveness. The acquired knowledge can be shown to subsequently contribute to task completion.",2014,False,False,False,True,False,False,True,"D, G",219,3,222
E14-4034,Simple and Effective Approach for Consistent Training of Hierarchical Phrase-based Translation Models,"In this paper, we present a simple approach for consistent training of hierarchical phrase-based translation models. In order to consistently train a translation model, we perform hierarchical phrasebased decoding on training data to find derivations between the source and target sentences. This is done by synchronous parsing the given sentence pairs. After extracting k-best derivations, we reestimate the translation model probabilities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German→English and English→French translation tasks. Our results show improvements of up to 1.6 points BLEU.",2014,False,False,False,True,False,False,True,"D, G",234,3,237
W14-5125,A Sandhi Splitter for {M}alayalam,"Sandhi splitting is the primary task for computational processing of text in Sanskrit and Dravidian languages. In these languages, words can join together with morpho-phonemic changes at the point of joining. This phenomenon is known as Sandhi. Sandhi splitter splits the string of conjoined words into individual words. Accurate execution of sandhi splitting is crucial for text processing tasks such as POS tagging, topic modelling and document indexing. We have tried different approaches to address the challenges of sandhi splitting in Malayalam, and finally, we have thought of exploiting the phonological changes that take place in the words while joining. This resulted in a hybrid method which statistically identifies the split points and splits using predefined character level linguistic rules. Currently, our system gives an accuracy of 91.1% .",2014,False,False,False,True,False,False,True,"D, G",276,3,279
C14-1182,Latent Domain Translation Models in Mix-of-Domains Haystack,"This paper addresses the problem of selecting adequate training sentence pairs from a mix-ofdomains parallel corpus for a translation task represented by a small in-domain parallel corpus. We propose a novel latent domain translation model which includes domain priors, domaindependent translation models and language models. The goal of learning is to estimate the probability of a sentence pair in mix-domain corpus to be in-or out-domain using in-domain corpus statistics as prior. We derive an EM training algorithm and provide solutions for estimating out-domain models (given only in-and mix-domain data). We report on experiments in data selection (intrinsic) and machine translation (extrinsic) on a large parallel corpus consisting of a mix of a rather diverse set of domains. Our results show that our latent domain invitation approach outperforms the existing baselines significantly. We also provide analysis of the merits of our approach relative to existing approaches.",2014,False,True,True,False,False,False,False,"B, C",294,3,297
P14-1132,Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world,"Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images, we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning, in which an image of a previously unseen object is mapped to a linguistic representation denoting its word. We then introduce fast mapping, a challenging and more cognitively plausible variant of the zero-shot task, in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts. By combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts.",2014,True,False,False,True,False,False,False,"A, D",289,3,292
P14-2067,Determiner-Established Deixis to Communicative Artifacts in Pedagogical Text,"Pedagogical materials frequently contain deixis to communicative artifacts such as textual structures (e.g., sections and lists), discourse entities, and illustrations. By relating such artifacts to the prose, deixis plays an essential role in structuring the flow of information in informative writing. However, existing language technologies have largely overlooked this mechanism. We examine properties of deixis to communicative artifacts using a corpus rich in determiner-established instances of the phenomenon (e.g., ""this section"", ""these equations"", ""those reasons"") from Wikibooks, a collection of learning texts. We use this corpus in combination with WordNet to determine a set of word senses that are characteristic of the phenomenon, showing its diversity and validating intuitions about its qualities. The results motivate further research to extract the connections encoded by such deixis, with the goals of enhancing tools to present pedagogical e-texts to readers and, more broadly, improving language technologies that rely on deictic phenomena.",2014,False,False,False,False,True,False,True,"E, G",313,3,316
W14-5144,{E}nglish to {P}unjabi Transliteration using Orthographic and Phonetic Information,"Machine transliteration is an emerging and a very important research area in the field of machine translation. While the translation system finds the same meaning word/sentence in another language, the transliteration helps us to pronounce them. This paper describes the process of transliteration from English to Punjabi language using a rule based approach. Both source grapheme and phonetic information of words have been considered for rule formation to achieve high performance and more accurate result. Phonetic information proved vital for correct transliteration as well as for ambiguous words. The system is tested on news domain text of more than 10,000 words and achieved accuracy of 95%.",2014,False,False,False,True,False,False,True,"D, G",245,3,248
D14-1140,Don{'}t Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation,"We introduce a reinforcement learningbased approach to simultaneous machine translation-producing a translation while receiving input wordsbetween languages with drastically different word orders: from verb-final languages (e.g., German) to verb-medial languages (English). In traditional machine translation, a translator must ""wait"" for source material to appear before translation begins. We remove this bottleneck by predicting the final verb in advance. We use reinforcement learning to learn when to trust predictions about unseen, future portions of the sentence. We also introduce an evaluation metric to measure expeditiousness and quality. We show that our new translation model outperforms batch and monotone translation strategies.",2014,False,True,True,False,False,False,False,"B, C",246,3,249
O14-1011,Towards automatic enrichment of standardized electronic dictionaries by semantic classes,"In this paper we propose an approach for the automatic enrichment of standardized electronic dictionaries by the semantic classes. This approach consists of three phases. The first phase treat the semantic classification process founded on the studies of Gaston Gross. The second phase profites from the existed subject fields in the dictionary's lexical entries in order to attribute the suitable semantic classes. The final phase realizes syntactic analyses of the textual content of meanings's lexical entries. This phase, aims to refine the subject field based enrichment and also treats the non enriched meanings in the second phase. In addition, it attributes the same semantic classes for the synonym meanings. We used an available standardized Arabic dictionary to tested the performance of the proposed approach.",2014,False,False,False,True,False,False,True,"D, G",257,3,260
P14-2065,The {V}erb{C}orner Project: Findings from Phase 1 of crowd-sourcing a semantic decomposition of verbs,"Any given verb can appear in some syntactic frames (Sally broke the vase, The vase broke) but not others (*Sally broke at the vase, *Sally broke the vase to John). There is now considerable evidence that the syntactic behaviors of some verbs can be predicted by their meanings, and many current theories posit that this is true for most if not all verbs. If true, this fact would have striking implications for theories and models of language acquisition, as well as numerous applications in natural language processing. However, empirical investigations to date have focused on a small number of verbs. We report on early results from VerbCorner, a crowd-sourced project extending this work to a large, representative sample of English verbs.",2014,True,False,False,False,True,False,False,"A, E",264,3,267
D14-1224,Building {C}hinese Discourse Corpus with Connective-driven Dependency Tree Structure,"In this paper, we propose a Connectivedriven Dependency Tree (CDT) scheme to represent the discourse rhetorical structure in Chinese language, with elementary discourse units as leaf nodes and connectives as non-leaf nodes, largely motivated by the Penn Discourse Treebank and the Rhetorical Structure Theory. In particular, connectives are employed to directly represent the hierarchy of the tree structure and the rhetorical relation of a discourse, while the nuclei of discourse units are globally determined with reference to the dependency theory. Guided by the CDT scheme, we manually annotate a Chinese Discourse Treebank (CDTB) of 500 documents. Preliminary evaluation justifies the appropriateness of the CDT scheme to Chinese discourse analysis and the usefulness of our manually annotated CDTB corpus.",2014,True,False,False,False,True,False,False,"A, E",269,3,272
Q14-1028,{T}ree{T}alk: Composition and Compression of Trees for Image Descriptions,"We present a new tree based approach to composing expressive image descriptions that makes use of naturally occuring web images with captions. We investigate two related tasks: image caption generalization and generation, where the former is an optional subtask of the latter. The high-level idea of our approach is to harvest expressive phrases (as tree fragments) from existing image descriptions, then to compose a new description by selectively combining the extracted (and optionally pruned) tree fragments. Key algorithmic components are tree composition and compression, both integrating tree structure with sequence structure. Our proposed system attains significantly better performance than previous approaches for both image caption generalization and generation. In addition, our work is the first to show the empirical benefit of automatically generalized captions for composing natural image descriptions.",2014,True,False,True,False,False,False,False,"A, C",270,3,273
P14-2003,Probabilistic Labeling for Efficient Referential Grounding based on Collaborative Discourse,"When humans and artificial agents (e.g. robots) have mismatched perceptions of the shared environment, referential communication between them becomes difficult. To mediate perceptual differences, this paper presents a new approach using probabilistic labeling for referential grounding. This approach aims to integrate different types of evidence from the collaborative referential discourse into a unified scheme. Its probabilistic labeling procedure can generate multiple grounding hypotheses to facilitate follow-up dialogue. Our empirical results have shown the probabilistic labeling approach significantly outperforms a previous graphmatching approach for referential grounding.",2014,False,False,True,True,False,False,False,"C,D",228,2,230
W14-0613,Developing a {T}agalog {L}inguistic {I}nquiry and {W}ord {C}ount ({LIWC}) {`}Disaster{'} Dictionary for Understanding Mixed Language Social Media: A Work-in-Progress Paper,"In the wake of super typhoon Yolanda (known internationally as Haiyan) in the Philippines in 2013, many individuals in the Philippines turned to social media to express their thoughts and emotions in a variety of languages. In order to understand and analyze the sentiment of populations on the ground, we used a novel approach of developing a conceptual Linguistic Inquiry and Word Count (LIWC) dictionary comprised of Tagalog words relating to disaster. This work-in-progress paper documents our process of filtering and choosing terms and offers suggestions for validating the dictionary. When results on how the dictionary was used are available, we can better assess the process for creating conceptual LIWC dictionaries.",2014,True,False,False,False,True,False,False,"A, E",251,3,254
W14-6602,"{BACANAL}: Short Length Random Walks For Lexical Analysis, Application to lexical substitution ({BACANAL} : Balades Al{\'e}atoires Courtes pour {ANA}lyses Lexicales Application {\`a} la substitution lexicale) [in {F}rench]","Nous proposons ici des méthodes de désambiguisation sémantique par substition lexicale pour la tâche 1 de l'atelier SemDis2014. Les méthodes exposées dans ce papier sont toutes bâties à partir de balades aléatoires courtes dans des graphes unipartis ou bipartis construits sur diverses ressources. Certaines de ces méthodes n'utilisent que des graphes construits automatiquement à partir de corpus (méthodes non supervisées), d'autres utilisent des graphes construits à partir de ressources produites « à la main » par des lexicographes ou par les foules (méthodes supervisées).",2014,False,False,False,True,False,False,True,"D, G",254,3,257
Q14-1042,A New Corpus and Imitation Learning Framework for Context-Dependent Semantic Parsing,"Semantic parsing is the task of translating natural language utterances into a machineinterpretable meaning representation. Most approaches to this task have been evaluated on a small number of existing corpora which assume that all utterances must be interpreted according to a database and typically ignore context. In this paper we present a new, publicly available corpus for context-dependent semantic parsing. The MRL used for the annotation was designed to support a portable, interactive tourist information system. We develop a semantic parser for this corpus by adapting the imitation learning algorithm DAGGER without requiring alignment information during training. DAGGER improves upon independently trained classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively.",2014,True,False,True,False,False,False,False,"A, C",259,3,262
