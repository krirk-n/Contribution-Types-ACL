acl_id,title,abstract,year,A,B,C,D,E,F,G,raw_response,input_tokens,output_tokens,total_tokens
Y15-2015,An Efficient Annotation for Phrasal Verbs using Dependency Information,"In this paper, we present an efficient semiautomatic method for annotating English phrasal verbs on the OntoNotes corpus. Our method first constructs a phrasal verb dictionary based on Wiktionary, then annotates each candidate example on the corpus as an either a phrasal verb usage or a literal one. For efficient annotation, we use the dependency structure of a sentence to filter out highly plausible positive and negative cases, resulting in a drastic reduction of annotation cost. We also show that a naive binary classification achieves better MWE identification performance than rule-based and sequence-labeling methods.",2015,True,False,False,True,False,False,False,"A, D",239,3,242
U15-1006,Similarity Metrics for Clustering {P}ub{M}ed Abstracts for Evidence Based Medicine,"We present a clustering approach for documents returned by a PubMed search, which enable the organisation of evidence underpinning clinical recommendations for Evidence Based Medicine. Our approach uses a combination of document similarity metrics, which are fed to an agglomerative hierarchical clusterer. These metrics quantify the similarity of published abstracts from syntactic, semantic, and statistical perspectives. Several evaluations have been performed, including: an evaluation that uses ideal documents as selected and clustered by clinical experts; a method that maps the output of PubMed to the ideal clusters annotated by the experts; and an alternative evaluation that uses the manual clustering of abstracts. The results of using our similarity metrics approach shows an improvement over K-means and hierarchical clustering methods using TF-IDF.",2015,False,False,False,True,False,False,True,"D, G",263,3,266
D15-1268,Fatal or not? Finding errors that lead to dialogue breakdowns in chat-oriented dialogue systems,"This paper aims to find errors that lead to dialogue breakdowns in chat-oriented dialogue systems. We collected chat dialogue data, annotated them with dialogue breakdown labels, and collected comments describing the error that led to the breakdown. By mining the comments, we first identified error types. Then, we calculated the correlation between an error type and the degree of dialogue breakdown it incurred, quantifying its impact on dialogue breakdown. This is the first study to quantitatively analyze error types and their effect in chat-oriented dialogue systems.",2015,False,False,False,False,True,True,False,"E, F",219,3,222
W15-0602,Feature selection for automated speech scoring,"Automated scoring systems used for the evaluation of spoken or written responses in language assessments need to balance good empirical performance with the interpretability of the scoring models. We compare several methods of feature selection for such scoring systems and show that the use of shrinkage methods such as Lasso regression makes it possible to rapidly build models that both satisfy the requirements of validity and intepretability, crucial in assessment contexts as well as achieve good empirical performance.",2015,False,False,False,True,False,True,False,"D, F",205,3,208
2015.mtsummit-wptp.1,How to teach machine translation post-editing? Experiences from a post-editing course,"Advances of machine translation technology have in recent years increased its use in various contexts. In the translation industry, work processes involving the use of machine translated texts as a raw translation to be post-edited by a translator are becoming increasingly common. The increasing use of post-editing processes also raises questions related to teaching and training of translators and post-editors, and institutions offering translator training have started to incorporate post-editing in their curricula. This paper describes a machine translation and post-editing course arranged at the University of Helsinki in Fall 2014. From the teacher's perspective, we discuss experiences of planning and teaching of the post-editing course. The development of the students' experiences and perception of the course contents, machine translation technology and post-editing are also discussed based on reflective essays written by the students after the course.",2015,False,False,False,False,True,False,True,"E, G",284,3,287
Q15-1022,Improving Topic Models with Latent Feature Word Representations,"Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature vector representations of words have been used to obtain high performance in many NLP tasks. In this paper, we extend two different Dirichlet multinomial topic models by incorporating latent feature vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus. Experimental results show that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.",2015,False,True,False,True,False,False,False,"B, D",233,3,236
W15-4917,Stripping Adjectives: Integration Techniques for Selective Stemming in {SMT} Systems,"In this paper we present an approach to reduce data sparsity problems when translating from morphologically rich languages into less inflected languages by selectively stemming certain word types. We develop and compare three different integration strategies: replacing words with their stemmed form, combined input using alternative lattice paths for the stemmed and surface forms and a novel hidden combination strategy, where we replace the stems in the stemmed phrase table by the observed surface forms in the test data. This allows us to apply advanced models trained on the surface forms of the words. We evaluate our approach by stemming German adjectives in two German→English translation scenarios: a low-resource condition as well as a large-scale state-of-the-art translation system. We are able to improve between 0.2 and 0.4 BLEU points over our baseline and reduce the number of out-of-vocabulary words by up to 16.5%.",2015,False,False,False,True,False,False,True,"D, G",296,3,299
S15-2120,{D}s{U}ni{P}i: An {SVM}-based Approach for Sentiment Analysis of Figurative Language on {T}witter,"The DsUniPi team participated in the SemEval 2015 Task#11: Sentiment Analysis of Figurative Language in Twitter. The proposed approach employs syntactical and morphological features, which indicate sentiment polarity in both figurative and non-figurative tweets. These features were combined with others that indicate presence of figurative language in order to predict a fine-grained sentiment score. The method is supervised and makes use of structured knowledge resources, such as Senti-WordNet sentiment lexicon for assigning sentiment score to words and WordNet for calculating word similarity. We have experimented with different classification algorithms (Naïve Bayes, Decision trees, and SVM), and the best results were achieved by an SVM classifier with linear kernel.",2015,False,False,False,True,False,False,True,"D, G",266,3,269
K15-1029,Transition-based Spinal Parsing,"We present a transition-based arc-eager model to parse spinal trees, a dependencybased representation that includes phrasestructure information in the form of constituent spines assigned to tokens. As a main advantage, the arc-eager model can use a rich set of features combining dependency and constituent information, while parsing in linear time. We describe a set of conditions for the arc-eager system to produce valid spinal structures. In experiments using beam search we show that the model obtains a good trade-off between speed and accuracy, and yields state of the art performance for both dependency and constituent parsing measures.",2015,False,True,True,False,False,False,False,"B, C",234,3,237
W15-3112,Learning Salient Samples and Distributed Representations for Topic-Based {C}hinese Message Polarity Classification,"We describe our participation in the Topic-Based Chinese Message Polarity Classification Task, based on the restricted and unrestricted resources respectively. In the restricted resource based classification, we focus on the selection of parameters in a multi-class classification model with highly-biased training data. In the unrestricted resource based classification, we explore the distributed representation of Chinese words through unsupervised feature learning and the annotation of salient samples through active learning, with a raw corpus of over 90 million messages extracted from Chinese Weibo Platform. For two classification subtasks, our submitted results ranked the 4th and the 2nd respectively.",2015,False,False,False,True,False,False,True,"D, G",238,3,241
N15-1162,Type-Driven Incremental Semantic Parsing with Polymorphism,"Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GEOQUERY, JOBS and ATIS domains. * We thank the reviewers for helpful suggestions. We are also grateful to Luke Zettelmoyer, Yoav Artzi, and Tom Kwiatkowski for providing data.",2015,False,True,True,False,False,False,False,"B, C",328,3,331
2015.tc-1.5,{K}amusi pre-{D}-source-side disambiguation and a sense aligned multilingual lexicon,"This paper discusses Kamusi Pre:D, a system to improve translation by disambiguating word senses in a source document with reference to a large concept-based lexicon that is aligned by sense across numerous languages. Currently under active development, the program prompts users to select the intended meaning when polysemous terms occur, and gives the user the option to select multiword expressions instead of individual words when the MWE occurs as a lexicalized dictionary entry. The disambiguated text is then automatically matched to sense-specific translation equivalents that have been aligned across languages. Pre:D is intended to integrate with existing translation tools, but greatly improve accuracy by involving human intelligence in vocabulary selection, both through manual document review of ambiguous terms and by reference to the underlying curated multilingual Kamusi dictionary data. Pre:D will aid accurate vocabulary translation among a wide range of language pairs, most currently unserved, and offer significant advantages in time, effort, and quality for multilingual translation projects by disambiguating a document one time for concepts that can be rendered appropriately across numerous languages.",2015,True,False,False,False,False,False,True,"A, G",326,3,329
W15-4809,Accounting for Allomorphy in Finite-state Transducers,"Building morphological parsers with existing finite state toolkits can result in something of a mis-match between the programming language of the toolkit and the linguistic concepts familiar to the average linguist. We illustrate this mismatch with a particular linguistic construct, suppletive allomorphy, and discuss ways to encode suppletive allomorphy in the Stuttgart Finite State tools (sfst). The complexity of the general solution motivates our work in providing an alternative formalism for morphology and phonology, one which can be translated automatically into sfst or other morphological parsing engines.",2015,False,True,True,False,False,False,False,"B, C",231,3,234
W15-5708,First Steps in Using Word Senses as Contextual Features in Maxent Models for Machine Translation,"Despite the common assumption that word sense disambiguation (WSD) should help to improve lexical choice and improve the quality of the output of machine translation systems, how to successfully integrate word senses into such systems remains an unanswered question. While significant improvements have been reported using reformulated approaches to the disambiguation task itself -most notably in predicting translations of full phrases as opposed to the senses of single words -little improvement or encouragement has been gleaned from the incorporation of traditional WSD into machine translation. In this paper, we present preliminary results that suggest that incorporating output from WSD as contextual features in a maxent-based translation model yields a slight improvement in the quality of machine translation and is potentially a step in the right direction, in contrast to other approaches to introducing word senses into a machine translation system which significantly impede its performance.",2015,False,False,False,True,False,True,False,"D, F",284,3,287
D15-1139,Do we need bigram alignment models? On the effect of alignment quality on transduction accuracy in {G}2{P},"We investigate the need for bigram alignment models and the benefit of supervised alignment techniques in graphemeto-phoneme (G2P) conversion. Moreover, we quantitatively estimate the relationship between alignment quality and overall G2P system performance. We find that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task. Moreover, we find that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so. Finally, we estimate a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature.",2015,False,False,False,False,True,True,False,"E, F",253,3,256
W15-4647,Automatic Detection of Miscommunication in Spoken Dialogue Systems,"In this paper, we present a data-driven approach for detecting instances of miscommunication in dialogue system interactions. A range of generic features that are both automatically extractable and manually annotated were used to train two models for online detection and one for offline analysis. Online detection could be used to raise the error awareness of the system, whereas offline detection could be used by a system designer to identify potential flaws in the dialogue design. In experimental evaluations on system logs from three different dialogue systems that vary in their dialogue strategy, the proposed models performed substantially better than the majority class baseline models.",2015,False,False,False,True,False,False,True,"D, G",232,3,235
2015.jeptalnrecital-court.42,Apport de l{'}information temporelle des contextes pour la repr{\'e}sentation vectorielle continue des mots,"Les représentations vectorielles continues des mots sont en plein essor et ont déjà été appliquées avec succès à de nombreuses tâches en traitement automatique de la langue (TAL). Dans cet article, nous proposons d'intégrer l'information temporelle issue du contexte des mots au sein des architectures fondées sur les sacs-de-mots continus (continuous bag-of-words ou CBOW) ou sur les Skip-Grams. Ces approches sont manipulées au travers d'un réseau de neurones, l'architecture CBOW cherchant alors à prédire un mot sachant son contexte, alors que l'architecture Skip-Gram prédit un contexte sachant un mot. Cependant, ces modèles, au travers du réseau de neurones, s'appuient sur des représentations en sac-de-mots et ne tiennent pas compte, explicitement, de l'ordre des mots. En conséquence, chaque mot a potentiellement la même influence dans le réseau de neurones. Nous proposons alors une méthode originale qui intègre l'information temporelle des contextes des mots en utilisant leur position relative. Cette méthode s'inspire des modèles contextuels continus. L'information temporelle est traitée comme coefficient de pondération, en entrée du réseau de neurones par le CBOW et dans la couche de sortie par le Skip-Gram. Les premières expériences ont été réalisées en utilisant un corpus de test mesurant la qualité de la relation sémantique-syntactique des mots. Les résultats préliminaires obtenus montrent l'apport du contexte des mots, avec des gains de 7 et 7,7 points respectivement avec l'architecture Skip-Gram et l'architecture CBOW.",2015,False,True,True,False,False,False,False,"B, C",458,3,461
P15-2036,Frame-Semantic Role Labeling with Heterogeneous Annotations,We consider the task of identifying and labeling the semantic arguments of a predicate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach augments an existing model with features derived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4% absolute increase in F 1 versus the original model.,2015,False,False,False,True,False,False,True,"D, G",200,3,203
D15-1051,Spelling Correction of User Search Queries through Statistical Machine Translation,"We use character-based statistical machine translation in order to correct user search queries in the e-commerce domain. The training data is automatically extracted from event logs where users re-issue their search queries with potentially corrected spelling within the same session. We show results on a test set which was annotated by humans and compare against online autocorrection capabilities of three additional web sites. Overall, the methods presented in this paper outperform fully productized spellchecking and autocorrection services in terms of accuracy and F1 score. We also propose novel evaluation steps based on retrieved search results of the corrected queries in terms of quantity and relevance.",2015,False,False,False,True,False,False,True,"G, D",238,3,241
U15-1009,How few is too few? Determining the minimum acceptable number of {LSA} dimensions to visualise text cohesion with Lex,"Building comprehensive language models using latent semantic analysis (LSA) requires substantial processing power. At the ideal parameters suggested in the literature (for an overview, see Bradford, 2008) it can take up to several hours, or even days, to complete. For linguistic researchers, this extensive processing time is inconvenient but toleratedbut when LSA is deployed in commercial software targeted at non-specialists, these processing times become untenable. One way to reduce processing time is to reduce the number of dimensions used to build the model. While the existing research has found that the model's reliability starts to degrade as dimensions are reduced, the point at which reliability becomes unacceptably poor varies greatly depending on the application. Therefore, in this paper, we set out to determine the lowest number of LSA dimensions that can still produce an acceptably reliable language model for our particular application: Lex, a visual cohesion analysis tool. We found that, across all three texts that we analysed, the cohesion-relevant visual motifs created by Lex start to become apparent and consistent at 50 retained dimensions.",2015,False,False,False,True,True,False,False,"D, E",333,3,336
W15-2106,Emotion and Inner State Adverbials in {R}ussian,"We study a group of adverbials that are composed of a preposition and a noun denoting an emotion or an inner state, such as v jarosti 'in a rage', s udovol'stviem 'with pleasure', ot radosti 'out of joy', s gorja 'out of grief', na udivlenie 'to the surprise of', k dosade 'to one's disappointment' etc. Being collocations, they occupy an intermediate position between free phrases and idioms. On the one hand, some of them are simple adverbial derivatives of nouns and therefore inherit some of their properties. On the other hand, they may have specific properties of their own. Two types of properties of the adverbials are studied: the actantial properties in their correlation with the properties of the source nouns, and the semantics proper. At the end a case study of the adverbials of the gratitude field is given. We show that adverbial derivatives can be shifted in the dependency structure from the subordinate clause to the main one.",2015,False,False,False,False,True,True,False,"E,F",331,2,333
W15-5301,"{U}niversal {D}ependencies for {C}roatian (that work for {S}erbian, too)","We introduce a new dependency treebank for Croatian within the Universal Dependencies framework. We construct it on top of the SETIMES.HR corpus, augmenting the resource by additional part-of-speech and dependency-syntactic annotation layers adherent to the framework guidelines. In this contribution, we outline the treebank design choices, and we use the resource to benchmark dependency parsing of Croatian and Serbian. We also experiment with cross-lingual transfer parsing into the two languages, and we make all resources freely available.",2015,True,False,False,False,False,False,True,"A, G",220,3,223
W15-2121,Dependency-based analyses for function words {--} Introducing the polygraphic approach,"This paper scrutinizes various dependency-based representations of the syntax of function words, such as prepositions. The focus is on the underlying formal object used to encode the linguistic analyses and its relation to the corresponding linguistic theory. The polygraph structure is introduced: it consists of a generalization of the concept of graph that allows edges to be vertices of other edges. Such a structure is used to encode dependency-based analyses that are founded on two kinds of morphosyntactic criteria: presence constraints and distributional constraints.",2015,False,False,True,False,True,False,False,"C, E",219,3,222
2015.jeptalnrecital-demonstration.10,Une aide {\`a} la communication par pictogrammes avec pr{\'e}diction s{\'e}mantique,"Cette démonstration présente une application mobile (pour tablette et smartphone) pour des personnes souffrant de troubles du langage et/ou de la parole permettant de générer des phrases à partir de la combinaison de pictogrammes puis de verbaliser le texte généré en Text-To-Speech (TTS). La principale critique adressée par les patients utilisant les solutions existantes est le temps de composition trop long d'une phrase. Cette limite ne permet pas ou très difficilement d'utiliser les solutions actuelles en condition dialogique. Pour pallier cela, nous avons développé un moteur de génération de texte avec prédiction sémantique ne proposant à l'utilisateur que les pictogrammes pertinents au regard de la saisie en cours (e.g. après le pictogramme [manger], l'application propose les pictogrammes [pomme] ou encore [viande] correspondant à des concepts comestibles). Nous avons ainsi multiplié de 5 à 10 la vitesse de composition d'une phrase par rapport aux solutions existantes.",2015,False,False,False,True,False,False,True,"D, G",326,3,329
W15-0901,A Method of Accounting Bigrams in Topic Models,"The paper describes the results of an empirical study of integrating bigram collocations and similarities between them and unigrams into topic models. First of all, we propose a novel algorithm PLSA-SIM that is a modification of the original algorithm PLSA. It incorporates bigrams and maintains relationships between unigrams and bigrams based on their component structure. Then we analyze a variety of word association measures in order to integrate top-ranked bigrams into topic models. All experiments were conducted on four text collections of different domains and languages. The experiments distinguish a subgroup of tested measures that produce topranked bigrams, which demonstrate significant improvement of topic models quality for all collections, when integrated into PLSA-SIM algorithm.",2015,False,False,True,False,True,False,False,"C, E",266,3,269
D15-1101,{C}3{EL}: A Joint Model for Cross-Document Co-Reference Resolution and Entity Linking,"Cross-document co-reference resolution (CCR) computes equivalence classes over textual mentions denoting the same entity in a document corpus. Named-entity linking (NEL) disambiguates mentions onto entities present in a knowledge base (KB) or maps them to null if not present in the KB. Traditionally, CCR and NEL have been addressed separately. However, such approaches miss out on the mutual synergies if CCR and NEL were performed jointly. This paper proposes C3EL, an unsupervised framework combining CCR and NEL for jointly tackling both problems. C3EL incorporates results from the CCR stage into NEL, and vice versa: additional global context obtained from CCR improves the feature space and performance of NEL, while NEL in turn provides distant KB features for already disambiguated mentions to improve CCR. The CCR and NEL steps are interleaved in an iterative algorithm that focuses on the highest-confidence still unresolved mentions in each iteration. Experimental results on two different corpora, news-centric and web-centric, demonstrate significant gains over state-of-the-art baselines for both CCR and NEL.",2015,False,True,False,True,False,False,False,"B, D",340,3,343
J15-4003,Inducing Implicit Arguments from Comparable Texts: A Framework and Its Applications,"In this article, we investigate aspects of sentential meaning that are not expressed in local predicate-argument structures. In particular, we examine instances of semantic arguments that are only inferable from discourse context. The goal of this work is to automatically acquire and process such instances, which we also refer to as implicit arguments, to improve computational models of language. As contributions towards this goal, we establish an effective framework for the difficult task of inducing implicit arguments and their antecedents in discourse and empirically demonstrate the importance of modeling this phenomenon in discourse-level tasks. Our framework builds upon a novel projection approach that allows for the accurate detection of implicit arguments by aligning and comparing predicate-argument structures across pairs of comparable texts. As part of this framework, we develop a graph-based model for predicate alignment that significantly outperforms previous approaches. Based on such alignments, we show that implicit argument instances can be automatically induced and applied to improve a current model of linking implicit arguments in discourse. We further validate that decisions on argument realization, although being a subtle phenomenon most of the time, can considerably affect the perceived coherence of a text. Our experiments reveal that previous models of coherence are not able to predict this impact. Consequently, we develop a novel coherence model, which learns to accurately predict argument realization based on automatically aligned pairs of implicit and explicit arguments.",2015,False,True,False,False,False,True,False,"B, F",384,3,387
R15-1066,Classifying Idiomatic and Literal Expressions Using Vector Space Representations,"We describe an algorithm for automatic classification of idiomatic and literal expressions. Our starting point is that idioms and literal expressions occur in different contexts. Idioms tend to violate cohesive ties in local contexts, while literals are expected to fit in. Our goal is to capture this intuition using a vector representation of words. We propose two approaches: (1) Compute inner product of context word vectors with the vector representing a target expression. Since literal vectors predict well local contexts, their inner product with contexts should be larger than idiomatic ones, thereby telling apart literals from idioms; and (2) Compute literal and idiomatic scatter (covariance) matrices from local contexts in word vector space. Since the scatter matrices represent context distributions, we can then measure the difference between the distributions using the Frobenius norm. We provide experimental results validating the proposed techniques.",2015,False,False,True,True,False,False,False,"C, D",288,3,291
P15-4020,Multi-level Translation Quality Prediction with {Q}u{E}st++,"This paper presents QUEST++ , an open source tool for quality estimation which can predict quality for texts at word, sentence and document level. It also provides pipelined processing, whereby predictions made at a lower level (e.g. for words) can be used as input to build models for predictions at a higher level (e.g. sentences). QUEST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that QUEST++ achieves state-of-the-art performance.",2015,False,True,False,False,False,False,True,"B, G",226,3,229
W15-5124,From {E}uropean {P}ortuguese to {P}ortuguese {S}ign {L}anguage,"Several efforts have been done towards the development of platforms that allow the translation of specific sign languages to the correspondent spoken language (and vice-versa). In this (demo) paper, we describe a freely available system that translates, in real time, European Portuguese (text) into Portuguese Sign Language (LGP), by using an avatar. We focus in how some linguistic elements are dealt in LGP, and in the Natural Language Processing (NLP) step implemented in our system. The system's interface will be used to demonstrate it. Although only a small set of linguistic phenomena are implemented, it can be seen how the system copes with it.",2015,False,False,False,True,False,False,True,"G, D",248,3,251
W15-3033,Extended Translation Models in Phrase-based Decoding,"We propose a novel extended translation model (ETM) to counteract some problems in phrase-based translation: The lack of translation context when using singleword phrases and uncaptured dependencies beyond phrase boundaries. The ETM operates on word-level and augments the IBM models by an additional bilingual word pair and a reordering operation. Its implementation in a phrase-based decoder introduces translation and reordering dependencies for single-word phrases and dependencies across phrase boundaries. More, the model incorporates an explicit treatment of multiple and empty alignments. Its integration outperforms competitive systems that include lexical and phrase translation models as well as hierarchical reordering models on 4 language pairs significantly by +0.7% BLEU on average. Although simpler and using fewer dependencies, the ETM proves to be on par with 7-gram operation sequence models (Durrani et al., 2013b) .",2015,False,True,True,False,False,False,False,"B, C",294,3,297
N15-3006,"An {AMR} parser for {E}nglish, {F}rench, {G}erman, {S}panish and {J}apanese and a new {AMR}-annotated corpus","In this demonstration, we will present our online parser 1 that allows users to submit any sentence and obtain an analysis following the specification of AMR (Banarescu et al., 2014) to a large extent. This AMR analysis is generated by a small set of rules that convert a native Logical Form analysis provided by a preexisting parser (see Vanderwende, 2015) into the AMR format. While we demonstrate the performance of our AMR parser on data sets annotated by the LDC, we will focus attention in the demo on the following two areas: 1) we will make available AMR annotations for the data sets that were used to develop our parser, to serve as a supplement to the LDC data sets, and 2) we will demonstrate AMR parsers for German, French, Spanish and Japanese that make use of the same small set of LF-to-AMR conversion rules.",2015,True,False,False,False,False,False,True,"A, G",306,3,309
P15-1091,A Unified Kernel Approach for Learning Typed Sentence Rewritings,"Many high level natural language processing problems can be framed as determining if two given sentences are a rewriting of each other. In this paper, we propose a class of kernel functions, referred to as type-enriched string rewriting kernels, which, used in kernel-based machine learning algorithms, allow to learn sentence rewritings. Unlike previous work, this method can be fed external lexical semantic relations to capture a wider class of rewriting rules. It also does not assume preliminary syntactic parsing but is still able to provide a unified framework to capture syntactic structure and alignments between the two sentences. We experiment on three different natural sentence rewriting tasks and obtain state-of-the-art results for all of them. Type-Enriched String Rewriting Kernel Kernel functions measure the similarity between two elements. Used in machine learning methods",2015,False,True,True,False,False,False,False,"B, C",277,3,280
W15-2705,Semantically Enriched Models for Modal Sense Classification,"Modal verbs have different interpretations depending on their context. Previous approaches to modal sense classification achieve relatively high performance using shallow lexical and syntactic features. In this work we uncover the difficulty of particular modal sense distinctions by eliminating both distributional bias and sparsity of existing small-scale annotated corpora used in prior work. We build a semantically enriched model for modal sense classification by novelly applying features that relate to lexical, proposition-level, and discourse-level semantic factors. Besides improved classification performance, especially for difficult sense distinctions, closer examination of interpretable feature sets allows us to obtain a better understanding of relevant semantic and contextual factors in modal sense classification.",2015,False,False,False,True,True,False,False,"D, E",245,3,248
R15-1035,About Emotion Identification in Visual Sentiment Analysis,"In this paper we present an approach for analysis of sentiments and emotions in image tagging using SentiWordNet as an external linguistic resource of emotional words. Our aim is to design and implement algorithms that assess the emotions and polarity given a set of image tags. The approach is not limited to object analysis only (considering informational keywords) but deals with the involvement tags and employs some techniques used for sentiment analysis in social networks. We consider the issue of tag sense disambiguation when image keywords are mapped to SentiWordNet. The Lesk algorithm helps to identify correctly the meaning of about 50% of the ambiguous single keywords of 200 images. The total number of tags we process is about 10,000. Calculating a ""sentiment score"" for each image, the system classifies images into three classes (positive, negative, neutral). These classes are compared to emotional assessments done (i) by humans and (ii) by training of a SVM classifier that provides the baseline of 69.7% precision, 29.9% recall and 41.8% Fmeasure. Our approach works with 63.53% precision, 58.7% recall and 61.02% Fmeasure. The experiments are performed using the annotations of the industrial auto-tagging platform Imagga that identifies automatically image objects with high precision.",2015,False,False,False,True,False,False,True,"D, G",391,3,394
W15-4657,Evaluation of Crowdsourced User Input Data for Spoken Dialog Systems,"Using the Internet for the collection of data is quite common these days. This process is called crowdsourcing and enables the collection of large amounts of data at reasonable costs. While being an inexpensive method, this data typically is of lower quality. Filtering data sets is therefore required. The occurring errors can be classified into different groups. There are technical issues and human errors. For speech recording, technical issues could be a noisy background. Human errors arise when the task is misunderstood. We employ several techniques for recognizing errors and eliminating faulty data sets in user input data for a Spoken Dialog System (SDS). Furthermore, we compare three different kinds of questionnaires (QNRs) for a given set of seven tasks. We analyze the characteristics of the resulting data sets and give a recommendation which type of QNR might be the most suitable one for a given purpose.",2015,False,False,False,False,True,False,True,"E, G",287,3,290
W15-3814,Using word embedding for bio-event extraction,"Bio-event extraction is an important phase towards the goal of extracting biological networks from the scientific literature. Recent advances in word embedding make computation of word distribution more efficient and possible. In this study, we investigate methods bringing distributional characteristics of words in the text into event extraction by using the latest word embedding methods. By using bag-ofwords (BOW) features as the baseline, the result has been improved by the introduction of word-embedding features, and is comparable to the state-of-the-art solution.",2015,False,False,False,True,False,False,True,"D, G",218,3,221
W15-5942,Simultaneous Feature Selection and Parameter Optimization Using Multi-objective Optimization for Sentiment Analysis,"In this paper, we propose a method of feature selection and parameter optimization for sentiment analysis in Twitter messages. Appropriate features and parameter combinations have significant effect to the performance of any classifier. As base learning algorithms we make use of Random Forest and Support Vector Machines. We perform sentiment analysis at the message level, and use the platform of SemEval-2014 shared task. We achieve substantial performance improvement with our proposed model over the systems that are developed with random feature subsets and default parameter combinations.",2015,False,False,False,True,False,False,True,"D, G",215,3,218
D15-1245,Any-language frame-semantic parsing,"We present a multilingual corpus of Wikipedia and Twitter texts annotated with FRAMENET 1.5 semantic frames in nine different languages, as well as a novel technique for weakly supervised cross-lingual frame-semantic parsing. Our approach only assumes the existence of linked, comparable source and target language corpora (e.g., Wikipedia) and a bilingual dictionary (e.g., Wiktionary or BABELNET). Our approach uses a truly interlingual representation, enabling us to use the same model across all nine languages. We present average error reductions over running a state-of-the-art parser on word-to-word translations of 46% for target identification, 37% for frame identification, and 14% for argument identification.",2015,True,True,False,False,False,False,False,"A, B",265,3,268
W15-4903,Using on-line available sources of bilingual information for word-level machine translation quality estimation,"This paper explores the use of external sources of bilingual information available on-line for word-level machine translation quality estimation (MTQE). These sources of bilingual information are used as a black box to spot sub-segment correspondences between a source-language (SL) sentence S to be translated and a given translation hypothesis T in the target-language (TL). This is done by segmenting both S and T into overlapping sub-segments of variable length and translating them into the TL and the SL, respectively, using the available bilingual sources of information on the fly. A collection of features is then obtained from the resulting sub-segment translations, which is used by a binary classifier to determine which target words in T need to be post-edited. Experiments are conducted based on the data sets published for the word-level MTQE task in the 2014 edition of the Workshop on Statistical Machine Translation (WMT 2014). The sources of bilingual information used are: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context. The results obtained confirm that, using less information and fewer features, our approach obtains results comparable to those of state-of-the-art approaches, and even outperform them in some data sets. c",2015,False,False,False,True,False,False,True,"D, G",366,3,369
W15-3810,Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion,"In the medical domain, identifying and expanding abbreviations in clinical texts is a vital task for both better human and machine understanding. It is a challenging task because many abbreviations are ambiguous especially for intensive care medicine texts, in which phrase abbreviations are frequently used. Besides the fact that there is no universal dictionary of clinical abbreviations and no universal rules for abbreviation writing, such texts are difficult to acquire, expensive to annotate and even sometimes, confusing to domain experts. This paper proposes a novel and effective approach -exploiting taskoriented resources to learn word embeddings for expanding abbreviations in clinical notes. We achieved 82.27% accuracy, close to expert human performance.",2015,False,False,False,True,False,False,True,"D, G",253,3,256
D15-1093,Learning Timeline Difference for Text Categorization,"This paper addresses text categorization problem that training data may derive from a different time period from the test data. We present a learning framework which extends a boosting technique to learn accurate model for timeline adaptation. The results showed that the method was comparable to the current state-of-theart biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data.",2015,False,False,False,True,False,False,True,"D, G",199,3,202
P15-1002,Addressing the Rare Word Problem in Neural Machine Translation,"Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence. This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT'14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT'14 contest task.",2015,False,False,False,True,False,False,True,"D, G",356,3,359
S15-1029,A Distant Supervision Approach to Semantic Role Labeling,"Semantic role labeling has become a key module for many language processing applications such as question answering, information extraction, sentiment analysis, and machine translation. To build an unrestricted semantic role labeler, the first step is to develop a comprehensive proposition bank. However, creating such a bank is a costly enterprise, which has only been achieved for a handful of languages. In this paper, we describe a technique to build proposition banks for new languages using distant supervision. Starting from PropBank in English and loosely parallel corpora such as versions of Wikipedia in different languages, we carried out a mapping of semantic propositions we extracted from English to syntactic structures in Swedish using named entities. We trained a semantic parser on the generated Swedish propositions and we report the results we obtained. Using the CoNLL 2009 evaluation script, we could reach the scores of 52.25 for labeled propositions and 62.44 for the unlabeled ones. We believe our approach can be applied to train semantic role labelers for other resource-scarce languages.",2015,True,False,False,False,False,False,True,"A, G",323,3,326
2015.jeptalnrecital-court.41,Exploration de mod{\`e}les distributionnels au moyen de graphes 1-{PPV},"Dans cet article, nous montrons qu'un graphe à 1 plus proche voisin (graphe 1-PPV) offre différents moyens d'explorer les voisinages sémantiques captés par un modèle distributionnel. Nous vérifions si les composantes connexes de ce graphe, qui représentent des ensembles de mots apparaissant dans des contextes similaires, permettent d'identifier des ensembles d'unités lexicales qui évoquent un même cadre sémantique. Nous illustrons également différentes façons d'exploiter le graphe 1-PPV afin d'explorer un modèle ou de comparer différents modèles.",2015,False,False,False,False,True,True,False,"E, F",247,3,250
P15-1043,Content Models for Survey Generation: A Factoid-Based Evaluation,"We present a new factoid-annotated dataset for evaluating content models for scientific survey article generation containing 3,425 sentences from 7 topics in natural language processing. We also introduce a novel HITS-based content model for automated survey article generation called HITSUM that exploits the lexical network structure between sentences from citing and cited papers. Using the factoid-annotated data, we conduct a pyramid evaluation and compare HITSUM with two previous state-of-the-art content models: C-Lexrank, a network based content model, and TOPICSUM, a Bayesian content model. Our experiments show that our new content model captures useful survey-worthy information and outperforms C-Lexrank by 4% and TOPICSUM by 7% in pyramid evaluation.",2015,True,True,False,False,False,False,False,"A, B",272,3,275
D15-1166,Effective Approaches to Attention-based Neural Machine Translation,"An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",2015,False,True,False,True,False,False,False,"B, D",321,3,324
W15-1609,Annotating Geographical Entities on Microblog Text,"This paper presents a discussion of the problems surrounding the task of annotating geographical entities on microblogs and reports the preliminary results of our efforts to annotate Japanese microblog texts. Unlike prior work, we not only annotate geographical location entities but also facility entities, such as stations, restaurants, shopping stores, hospitals and schools. We discuss ways in which to build a gazetteer, the types of ambiguities that need to be considered, reasons why the annotator tends to disagree, and the problems that need to be solved to automate the task of annotating the geographical entities. All the annotation data and the annotation guidelines are publicly available for research purposes from our web site.",2015,True,False,False,False,True,False,False,"A, E",251,3,254
S15-2151,{S}em{E}val-2015 Task 17: Taxonomy Extraction Evaluation ({TE}x{E}val),"This paper describes the first shared task on Taxonomy Extraction Evaluation organised as part of SemEval-2015. Participants were asked to find hypernym-hyponym relations between given terms. For each of the four selected target domains the participants were provided with two lists of domainspecific terms: a WordNet collection of terms and a well-known terminology extracted from an online publicly available taxonomy. A total of 45 taxonomies submitted by 6 participating teams were evaluated using standard structural measures, the structural similarity with a gold standard taxonomy, and through manual quality assessment of sampled novel relations.",2015,True,False,False,False,True,False,False,"A, E",234,3,237
W15-0610,The Impact of Training Data on Automated Short Answer Scoring Performance,"Automatic evaluation of written responses to content-focused assessment items (automated short answer scoring) is a challenging educational application of natural language processing. It is often addressed using supervised machine learning by estimating models to predict human scores from detailed linguistic features such as word n-grams. However, training data (i.e., human-scored responses) can be difficult to acquire. In this paper, we conduct experiments using scored responses to 44 prompts from 5 diverse datasets in order to better understand how training set size and other factors relate to system performance. We believe this will help future researchers and practitioners working on short answer scoring to answer practically important questions such as, ""How much training data do I need?"" *",2015,False,False,False,False,True,False,True,"E, G",258,3,261
Y15-2040,A Light Rule-based Approach to {E}nglish Subject-Verb Agreement Errors on the Third Person Singular Forms,"Verb errors are one of the most common grammar errors made by non-native writers of English. This work especially focus on an important type of verb usage errors, subject-verb agreement for the third person singular forms, which has a high proportion in errors made by non-native English learners. Existing work has not given a satisfied solution for this task, in which those using supervised learning method usually fail to output good enough performance, and rule-based methods depend on advanced linguistic resources such as syntactic parsers. In this paper, we propose a rule-based method to detect and correct the concerned errors. The proposed method relies on a series of rules to automatically locate subject and predicate in four types of sentences. The evaluation shows that the proposed method gives state-of-the-art performance with quite limited linguistic resources.",2015,False,False,False,True,False,False,True,"D, G",275,3,278
2015.mtsummit-papers.7,{METEOR} for multiple target languages using {DB}nary,"This paper proposes an extension of METEOR, a well-known MT evaluation metric, for multiple target languages using an in-house lexical resource called DBnary (an extraction from Wiktionary provided to the community as a Multilingual Lexical Linked Open Data). Today, the use of the synonymy module of METEOR is only exploited when English is the target language (use of WordNet). A synonymy module using DBnary would allow its use for the 21 languages (covered up to now) as target languages. The code of this new instance of METEOR, adapted to several target languages, is provided to the community. We also show that our DBnary augmented METEOR increases the correlation with human judgements on the WMT 2013 and 2014 metrics dataset for English-to-(French, Russian, German, Spanish) language pairs.",2015,False,False,False,True,True,False,False,"D, E",295,3,298
W15-2508,Part-of-Speech Driven Cross-Lingual Pronoun Prediction with Feed-Forward Neural Networks,"For some language pairs, pronoun translation is a discourse-driven task which requires information that lies beyond its local context. This motivates the task of predicting the correct pronoun given a source sentence and a target translation, where the translated pronouns have been replaced with placeholders. For crosslingual pronoun prediction, we suggest a neural network-based model using preceding nouns and determiners as features for suggesting antecedent candidates. Our model scores on par with similar models while having a simpler architecture.",2015,True,True,False,False,False,False,False,"A, B",214,3,217
P15-2005,Semi-Stacking for Semi-supervised Sentiment Classification,"In this paper, we address semi-supervised sentiment learning via semi-stacking, which integrates two or more semi-supervised learning algorithms from an ensemble learning perspective. Specifically, we apply metalearning to predict the unlabeled data given the outputs from the member algorithms and propose N-fold cross validation to guarantee a suitable size of the data for training the meta-classifier. Evaluation on four domains shows that such a semi-stacking strategy performs consistently better than its member algorithms.",2015,False,False,False,True,False,False,True,"D, G",210,3,213
S15-1035,Resolving Discourse-Deictic Pronouns: A Two-Stage Approach to Do It,"Discourse deixis is a linguistic phenomenon in which pronouns have verbal or clausal, rather than nominal, antecedents. Studies have estimated that between 5% and 10% of pronouns in non-conversational data are discourse deictic. However, current coreference resolution systems ignore this phenomenon. This paper presents an automatic system for the detection and resolution of discourse-deictic pronouns. We introduce a two-step approach that first recognizes instances of discourse-deictic pronouns, and then resolves them to their verbal antecedent. Both components rely on linguistically motivated features. We evaluate the components in isolation and in combination with two state-of-the-art coreference resolvers. Results show that our system outperforms several baselines, including the only comparable discourse deixis system, and leads to small but statistically significant improvements over the full coreference resolution systems. An error analysis lays bare the need for a less strict evaluation of this task.",2015,True,False,False,True,False,False,False,"A, D",310,3,313
W15-3220,{TECHLIMED}@{QALB}-Shared Task 2015: a hybrid {A}rabic Error Correction System,"This paper reports on the participation of Techlimed in the Second Shared Task on Automatic Arabic Error Correction organized by the Arabic Natural Language Processing Workshop. This year's competition includes two tracks, and, in addition to errors produced by native speakers (L1), also includes correction of texts written by learners of Arabic as a foreign language (L2). Techlimed participated in the L1 track. For our participation in the L1 evaluation task, we developed two systems. The first one is based on the spellchecker Hunspell with specific dictionaries. The second one is a hybrid system based on rules, morphology analysis and statistical machine translation. Our results on the test set show that the hybrid system outperforms the lexicon driven approach with a precision of 71.2%, a recall of 64.94% and an F-measure of 67.93%.",2015,False,False,False,True,False,False,True,"D, G",291,3,294
W15-4923,Target-Side Generation of Prepositions for {SMT},"We present a translation system that models the selection of prepositions in a targetside generation component. This novel approach allows the modeling of all subcategorized elements of a verb as either NPs or PPs according to target-side requirements relying on source and target side features. The BLEU scores are encouraging, but fail to surpass the baseline. We additionally evaluate the preposition accuracy for a carefully selected subset and discuss how typical problems of translating prepositions can be modeled with our method. c",2015,False,False,False,True,False,True,False,"D, F",215,3,218
W15-0516,Combining Argument Mining Techniques,"In this paper, we look at three different methods of extracting the argumentative structure from a piece of natural language text. These methods cover linguistic features, changes in the topic being discussed and a supervised machine learning approach to identify the components of argumentation schemes, patterns of human reasoning which have been detailed extensively in philosophy and psychology. For each of these approaches we achieve results comparable to those previously reported, whilst at the same time achieving a more detailed argument structure. Finally, we use the results from these individual techniques to apply them in combination, further improving the argument structure identification.",2015,False,False,False,True,False,False,True,"D, G",231,3,234
W15-5602,7x1-{PT}: um Corpus extra{\'\i}do do {T}witter para An{\'a}lise de Sentimentos em L{\'\i}ngua Portuguesa (7x1-{PT}: a Corpus extracted from {T}witter for Sentiment Analysis in {P}ortuguese Language),"This paper describes the 7x1PT corpus that contains a set of tweets, in Portuguese, posted during the match Germany vs Brazil at the FIFA World Cup 2014. We describe data collection, cleaning and organization, and also the current stage of the linguistic annotation of this corpus. Resumo. Este artigo descreve o corpus 7x1PT que contém um conjunto de tweets, em português, postados ao longo da partida da Alemanha com o Brasil durante a Copa do Mundo de 2014 da FIFA. Nós descrevemos como foi realizada a coleta, a limpeza e a organização, bem como comentamos o estágio atual de anotação linguística desse corpus.",2015,True,False,False,False,True,False,False,"A, E",253,3,256
W15-4416,Condition Random Fields-based Grammatical Error Detection for {C}hinese as Second Language,"The foreign learners are not easy to learn Chinese as a second language. Because there are many special rules different from other languages in Chinese. When the people learn Chinese as a foreign language usually make some grammatical errors, such as missing, redundant, selection and disorder. In this paper, we proposed the conditional random fields (CRFs) to detect the grammatical errors. The features based on statistical word and part-ofspeech (POS) pattern were adopted here. The relationships between words by part-of-speech are helpful for Chinese grammatical error detection. Finally, we according to CRF determined which error types in sentences. According to the observation of experimental results, the performance of the proposed model is acceptable in precision and recall rates.",2015,False,False,False,True,False,False,True,"D, G",261,3,264
W15-4808,Temporal Forces and Type Coercion in Strings,"Durative forces are introduced to Finite State Temporality (the application of Finite State Methods to Temporal Semantics). Punctual and durative forces are shown to have natural representations as fluents which place certain constraints on strings. These forces are related to previous work on stative explanations of aspectual classification. Given this extended ontology, it is shown how type coercion can be handled in this framework.",2015,False,False,True,False,True,False,False,"C, E",200,3,203
W15-3413,{LINA}: Identifying Comparable Documents from {W}ikipedia,"This paper describes the LINA system for the BUCC 2015 shared track. Following (Enright and Kondrak, 2007), our system identify comparable documents by collecting counts of hapax words. We extend this method by filtering out document pairs sharing target documents using pigeonhole reasoning and cross-lingual information.",2015,False,False,False,True,False,False,True,"D, G",183,3,186
D15-1030,Birds of a Feather Linked Together: A Discriminative Topic Model using Link-based Priors,"A wide range of applications, from social media to scientific literature analysis, involve graphs in which documents are connected by links. We introduce a topic model for link prediction based on the intuition that linked documents will tend to have similar topic distributions, integrating a max-margin learning criterion and lexical term weights in the loss function. We validate our approach on the tweets from 2,000 Sina Weibo users and evaluate our model's reconstruction of the social network.",2015,False,True,False,True,False,False,False,"B, D",207,3,210
W15-4410,A System for Generating Multiple Choice Questions: With a Novel Approach for Sentence Selection,Multiple Choice Question (MCQ) plays a major role in educational assessment as well as in active learning. In this paper we present a system that generates MCQs automatically using a sports domain text as input. All the sentences in a text are not capable of generating MCQs; the first step of the system is to select the informative sentences. We propose a novel technique to select informative sentences by using topic modeling and parse structure similarity. The parse structure similarity is computed between the parse structure of an input sentence and a set of reference parse structures. In order to compile the reference set we use a number of existing MCQs collected from the web. Keyword selection is done with the help of occurrence of domain specific word and named entity word in the sentence. Distractors are generated using a set of rules and name dictionary. Experimental results demonstrate that the proposed technique is quite accurate.,2015,True,False,False,True,False,False,False,"A, D",291,3,294
W15-0627,Using Learner Data to Improve Error Correction in Adjective{--}Noun Combinations,"This paper presents a novel approach to error correction in content words in learner writing focussing on adjective-noun (AN) combinations. We show how error patterns can be used to improve the performance of the error correction system, and demonstrate that our approach is capable of suggesting an appropriate correction within the top two alternatives in half of the cases and within top 10 alternatives in 71% of the cases, performing with an M RR of 0.5061. We then integrate our error correction system with a state-of-the-art content word error detection system and discuss the results.",2015,False,False,False,True,False,False,True,"D, G",233,3,236
W15-1803,Invited Talk: Embedding Probabilistic Logic for Machine Reading,"We want to build machines that read, and make inferences based on what was read. A long line of the work in the field has focussed on approaches where language is converted (possibly using machine learning) into a symbolic and relational representation. A reasoning algorithm (such as a theorem prover) then derives new knowledge from this representation. This allows for rich knowledge to captured, but generally suffers from two problems: acquiring sufficient symbolic background knowledge and coping with noise and uncertainty in data. Probabilistic logics (such as Markov Logic) offer a solution, but are known to often scale poorly. In recent years a third alternative emerged: latent variable models in which entities and relations are embedded in vector spaces (and represented ""distributional""). Such approaches scale well and are robust to noise, but they raise their own set of questions: What type of inferences do they support? What is a proof in embeddings? How can explicit background knowledge be injected into embeddings? In this talk I first present our work on latent variable models for machine reading, using ideas from matrix factorisation as well as both closed and open information extraction. Then I will present recent work we conducted to address the questions of injecting and extracting symbolic knowledge into/from models based on embeddings. In particular, I will show how one can rapidly build accurate relation extractors through combining logic and embeddings.",2015,False,True,False,True,False,False,False,"B, D",388,3,391
W15-4925,Content Translation: Computer assisted translation tool for {W}ikipedia articles,"The quality and quantity of articles in each Wikipedia language varies greatly. Translating from another Wikipedia is a natural way to add more content, but the translation process is not properly supported in the software used by Wikipedia. Past computer-assisted translation tools built for Wikipedia are not commonly used. We created a tool that adapts to the specific needs of an open community and to the kind of content in Wikipedia. Qualitative and quantitative data indicates that the new tool helps users translate articles easier and faster. c",2015,False,False,False,True,False,False,True,"D, G",217,3,220
W15-3018,The {RWTH} {A}achen {G}erman-{E}nglish Machine Translation System for {WMT} 2015,"This paper describes the statistical machine translation system developed at RWTH Aachen University for the German→English translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). A phrase-based machine translation system was applied and augmented with hierarchical phrase reordering and word class language models. Further, we ran discriminative maximum expected BLEU training for our system. In addition, we utilized multiple feed-forward neural network language and translation models and a recurrent neural network language model for reranking.",2015,False,False,False,True,False,False,True,"D, G",222,3,225
W15-5922,punct-An Alternative Verb Semantic Ontology Representation,"The goal is to build a verb ontology based on Indian grammatical tradition. We propose here an ontological structure to represent verbs in a language, which can be adapted across languages. This is an ongoing work and presently the method has been applied to develop ontologically informed etymon in English.",2015,True,False,False,False,False,False,True,"A, G",176,3,179
P15-1066,Learning the Semantics of Manipulation Action,"In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs λ-calculus; (2) enable a probabilistic semantic parsing schema to learn the lambda-calculus representation of manipulation action from an annotated action corpus of videos; (3) use ( 1 ) and (2) to develop a system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation.",2015,True,True,False,False,False,False,False,"A, B",303,3,306
P15-2012,{Z}oom: a corpus of natural language descriptions of map locations,"This paper describes an experiment to elicit referring expressions from human subjects for research in natural language generation and related fields, and preliminary results of a computational model for the generation of these expressions. Unlike existing resources of this kind, the resulting data set -the Zoom corpus of natural language descriptions of map locations -takes into account a domain that is significantly closer to real-world applications than what has been considered in previous work, and addresses more complex situations of reference, including contexts with different levels of detail, and instances of singular and plural reference produced by speakers of Spanish and Portuguese.",2015,True,False,False,False,True,False,False,"A, E",231,3,234
O15-2004,Automatically Detecting Syntactic Errors in Sentences Writing by Learners of {C}hinese as a Foreign Language,"This paper proposed a method that can automatically detect syntax errors in Chinese sentences. The algorithm for identifying syntax errors proposed in this study is known as KNGED, which uses a large database of rules to identify whether syntax errors exist in a sentence. The rules were generated either manually or automatically. This paper further proposed an algorithm for identifying the type of error that a sentence contained. Experimental results shown that the false positive rate and F1-measure of the proposed method for detecting syntax errors in Chinese sentences are 0.90 and 0.65.",2015,False,True,True,False,False,False,False,"B, C",228,3,231
W15-3221,{UMMU}@{QALB}-2015 Shared Task: Character and Word level {SMT} pipeline for Automatic Error Correction of {A}rabic Text,"In this paper we present the LIUM (Laboratoire d'Informatique de l'Universit du Maine) and CMU-Q (Carnegie Mellon University in Qatar) joint submission in the Arabic shared task on automatic spelling error correction. Our best system is a sequential combination of two statistical machine translation systems (SMT) trained on top of the MADAMIRA output. The first is a Character-based one, used to produce a first correction at the character level. Characters are then glued to form the input to the second system working at the Word level. This sequential combination achieves an F 1 score of (69.42) that is better than the best F 1 score reported on the 2014 test set (67.91). The UMMU best submission to the QALB-15 shared task is ranked first over 10 submission on the L2 test condition and second over 12 submission on the L1 testsset.",2015,False,False,False,True,False,False,True,"D, G",311,3,314
W15-2522,On Statistical Machine Translation and Translation Theory,"The translation process in statistical machine translation (SMT) is shaped by technical constraints and engineering considerations. SMT explicitly models translation as search for a target-language equivalent of the input text. This perspective on translation had wide currency in mid-20th century translation studies, but has since been superseded by approaches arguing for a more complex relation between source and target text. In this paper, we show how traditional assumptions of translational equivalence are embodied in SMT through the concepts of word alignment and domain and discuss some limitations arising from the word-level/corpus-level dichotomy inherent in these concepts.",2015,False,False,False,False,True,True,False,"E,F",235,2,237
W15-2606,Annotation of Clinically Important Follow-up Recommendations in Radiology Reports,"Communication of follow-up recommendations when abnormalities are identified on imaging studies is prone to error. The absence of an automated system to identify and track radiology recommendations is an important barrier to ensuring timely follow-up of patients especially with non-acute incidental findings on imaging studies. We are in the process of building a natural language processing (NLP) system to identify follow-up recommendations in free-text radiology reports. In this paper, we describe our efforts in creating a multiinstitutional radiology report corpus annotated for follow-up recommendation information. The annotated corpus will be used to train and test the NLP system.",2015,True,False,False,False,False,False,True,"A, G",237,3,240
D15-1223,Krimping texts for better summarization,"Automated text summarization is aimed at extracting essential information from original text and presenting it in a minimal, often predefined, number of words. In this paper, we introduce a new approach for unsupervised extractive summarization, based on the Minimum Description Length (MDL) principle, using the Krimp dataset compression algorithm (Vreeken et al., 2011) . Our approach represents a text as a transactional dataset, with sentences as transactions, and then describes it by itemsets that stand for frequent sequences of words. The summary is then compiled from sentences that compress (and as such, best describe) the document. The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document. We solve it by a greedy algorithm and present the evaluation results.",2015,False,False,True,True,False,False,False,"C, D",294,3,297
W15-5954,Ranking Model with a Reduced Feature Set for an Automated Question Generation System,"A metric has been proposed to automatically generate well-formed-ness rank for machine generated questions. The grammatical correctness of a question, challenge due to the negation in the source text and number of transformations required to convert an illformed (not appealing to humans) question to a well-formed (meaningful and human appealing) question seem the prominent features. We used 135 questions generated by Heilman and Smith's system (Heilman and  Smith, 2011), developed a regression model using our feature set and tested it. The Rsquared value is 93.5% which is quiet acceptable. Relevance of the model has been corroborated by means of the residual plot. (Heilman and Smith, 2011) Metric is of 187 features and that of (Liu, 2012) is of 11. We suggested 16 features for a general automatic question quality enhancer to look into. However, the present model has been built by considering only 10 out of them. (Heilman and Smith , 2011)generated questions do not possess any infirmities indicated by the remaining ones. 75 questions were used in training and 60 were tested. Recognition accuracy is 94.4%.",2015,False,False,False,True,False,True,False,"D, F",369,3,372
W15-3048,Alignment-based sense selection in {METEOR} and the {RATATOUILLE} recipe,"This paper describes Meteor-WSD and RATATOUILLE, the LIMSI submissions to the WMT15 metrics shared task. Meteor-WSD extends synonym mapping to languages other than English based on alignments and gives credit to semantically adequate translations in context. We show that context-sensitive synonym selection increases the correlation of the Meteor metric with human judgments of translation quality on the WMT14 data. RATATOUILLE combines Meteor-WSD with nine other metrics for evaluation and outperforms the best metric (BEER) involved in its computation.",2015,False,False,False,True,False,True,False,"D, F",226,3,229
W15-0807,Evaluation Algorithms for Event Nugget Detection : A Pilot Study,"Event Mention detection is the first step in textual event understanding. Proper evaluation is important for modern natural language processing tasks. In this paper, we present our evaluation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements.",2015,False,False,False,False,True,True,False,"E, F",200,3,203
P15-1125,Entity Hierarchy Embedding,"Existing distributed representations are limited in utilizing structured knowledge to improve semantic relatedness modeling. We propose a principled framework of embedding entities that integrates hierarchical information from large-scale knowledge bases. The novel embedding model associates each category node of the hierarchy with a distance metric. To capture structured semantics, the entity similarity of context prediction are measured under the aggregated metrics of relevant categories along all inter-entity paths. We show that both the entity vectors and category distance metrics encode meaningful semantics. Experiments in entity linking and entity search show superiority of the proposed method.",2015,False,True,True,False,False,False,False,"B, C",226,3,229
W15-0606,Measuring Feature Diversity in Native Language Identification,"The task of Native Language Identification (NLI) is typically solved with machine learning methods, and systems make use of a wide variety of features. Some preliminary studies have been conducted to examine the effectiveness of individual features, however, no systematic study of feature interaction has been carried out. We propose a function to measure feature independence and analyze its effectiveness on a standard NLI corpus.",2015,False,False,False,True,True,False,False,"E, D",193,3,196
Y15-1033,Large-scale Dictionary Construction via Pivot-based Statistical Machine Translation with Significance Pruning and Neural Network Features,"We present our ongoing work on large-scale Japanese-Chinese bilingual dictionary construction via pivot-based statistical machine translation. We utilize statistical significance pruning to control noisy translation pairs that are induced by pivoting. We construct a large dictionary which we manually verify to be of a high quality. We then use this dictionary and a parallel corpus to learn bilingual neural network language models to obtain features for reranking the n-best list, which leads to an absolute improvement of 5% in accuracy when compared to a setting that does not use significance pruning and reranking. 2 The highest accuracy evaluated based on the 1 best translation is 21.7% in (Tsunakawa et al., 2009) .",2015,True,False,False,True,False,False,False,"A, D",259,3,262
W15-1407,A Corpus of Rich Metaphor Annotation,"Metaphor is a central phenomenon of language, and thus a central problem for natural language understanding. Previous work on the analysis of metaphors has identified which target concepts are being thought of and described in terms of which source concepts, but this is not adequate to explain what motivates the use of particular metaphors. This work proposes the use of conceptual schemas to represent the underspecified scenarios that motivate a metaphoric mapping. To support the creation of systems that can understand metaphors in this way, we have created and are publicly releasing a corpus of manually validated metaphor annotations.",2015,True,False,False,False,True,False,False,"A, E",231,3,234
P15-1167,Accurate Linear-Time {C}hinese Word Segmentation via Embedding Matching,"This paper proposes an embedding matching approach to Chinese word segmentation, which generalizes the traditional sequence labeling framework and takes advantage of distributed representations. The training and prediction algorithms have linear-time complexity. Based on the proposed model, a greedy segmenter is developed and evaluated on benchmark corpora. Experiments show that our greedy segmenter achieves improved results over previous neural network-based word segmenters, and its performance is competitive with state-of-the-art methods, despite its simple feature set and the absence of external resources for training.",2015,False,True,False,True,False,False,False,"B, D",220,3,223
P15-2124,Harnessing Context Incongruity for Sarcasm Detection,The relationship between context incongruity and sarcasm has been studied in linguistics. We present a computational system that harnesses context incongruity as a basis for sarcasm detection. Our statistical sarcasm classifiers incorporate two kinds of incongruity features: explicit and implicit. We show the benefit of our incongruity features for two text forms -tweets and discussion forum posts. Our system also outperforms two past works (with Fscore improvement of 10-20%). We also show how our features can capture intersentential incongruity.,2015,False,False,False,True,False,False,True,"D, G",229,3,232
P15-1057,Context-aware Entity Morph Decoding,"People create morphs, a special type of fake alternative names, to achieve certain communication goals such as expressing strong sentiment or evading censors. For example, ""Black Mamba"", the name for a highly venomous snake, is a morph that Kobe Bryant created for himself due to his agility and aggressiveness in playing basketball games. This paper presents the first end-to-end context-aware entity morph decoding system that can automatically identify, disambiguate, verify morph mentions based on specific contexts, and resolve them to target entities. Our approach is based on an absolute ""cold-start"" -it does not require any candidate morph or target entity lists as input, nor any manually constructed morph-target pairs for training. We design a semi-supervised collective inference framework for morph mention extraction, and compare various deep learning based approaches for morph resolution. Our approach achieved significant improvement over the state-of-the-art method (Huang et al., 2013) , which used a large amount of training data. 1",2015,True,True,False,False,False,False,False,"A, B",319,3,322
N15-3002,Analyzing and Visualizing Coreference Resolution Errors,We present a toolkit for coreference resolution error analysis. It implements a recently proposed analysis framework and contains rich components for analyzing and visualizing recall and precision errors.,2015,False,False,False,True,True,False,False,"D, E",150,3,153
W15-4626,Memory-Based Acquisition of Argument Structures and its Application to Implicit Role Detection,"We propose a generic, memory-based approach for the detection of implicit semantic roles. While state-of-the-art methods for this task combine hand-crafted rules with specialized and costly lexical resources, our models use large corpora with automated annotations for explicit semantic roles only to capture the distribution of predicates and their associated roles. We show that memory-based learning can increase the recognition rate of implicit roles beyond the state-of-the-art.",2015,False,True,False,True,False,False,False,"B, D",200,3,203
W15-1302,Filled Pauses in User-generated Content are Words with Extra-propositional Meaning,"In this paper, we present a corpus study investigating the use of the fillers äh (uh) and ähm (uhm) in informal spoken German youth language and in written text from social media. Our study shows that filled pauses occur in both corpora as markers of hesitations, corrections, repetitions and unfinished sentences, and that the form as well as the type of the fillers are distributed similarly in both registers. We present an analysis of fillers in written microblogs, illustrating that äh and ähm are used intentionally and can add a subtext to the message that is understandable to both author and reader. We thus argue that filled pauses in user-generated content from social media are words with extrapropositional meaning.",2015,False,False,False,False,True,False,True,"E, G",261,3,264
W15-2413,Motif discovery in infant- and adult-directed speech,"Infant-directed speech (IDS) is thought to play a key role in determining infant language acquisition. It is thus important to describe how computational models of infant language acquisition behave when given an input of IDS, as compared to adult-directed speech (ADS). In this paper, we explore how an acoustic motif discovery algorithm fares when presented with speech from both registers. Results show small but significant differences in performance, with lower recall and lower cluster collocation in IDS than ADS, but a higher cluster purity in IDS. Overall, these results are inconsistent with a view suggesting that IDS is acoustically clearer than ADS in a way that systematically facilitates lexical recognition. Similarities and differences with human infants' word segmentation are discussed.",2015,False,False,False,False,True,True,False,"E,F",259,2,261
S15-2106,{SWATAC}: A Sentiment Analyzer using One-Vs-Rest Logistic Regression,"This paper describes SWATAC, a system built for SemEval-2015's Task 10 Subtask B, namely the Message Polarity Classification Task. Given a tweet, the system classifies the sentiment as either positive, negative, or neutral. Several preprocessing tasks such as negation detection, spell checking, and tokenization are performed to enhance lexical information. The features are then augmented with external sentiment lexicons. Classification is done with Logistic Regression using a one-vsrest configuration. For the test runs, the system was trained using only the provided training tweets. The classifier was successful, with an F1 score of 58.43 on the official 2015 test data, and an F1 score of 66.64 on the Twitter 2014 progress data.",2015,False,False,False,True,False,False,True,"G, D",275,3,278
W15-0813,Classification and Acquisition of Contradictory Event Pairs using Crowdsourcing,"We propose a taxonomy of contradictory event pairs and a method for building a database of such pairs. When a dialog system participates in an open-domain conversation with a human, it is important to avoid the generation of utterances that conflict with the context of the dialog. Here, we refer to a pair of events that are not able to co-occur or that are not inconsistent with each other as a contradictory event pair. In this study, we collected contradictory event pairs using crowdsourcing and constructed a taxonomy of such pairs. We also built a large-scale database of Japanese contradictory event pairs for each class using crowdsourcing. This database will be used for consistent utterance generation in dialog systems.",2015,True,False,False,False,True,False,False,"A, E",254,3,257
R15-1004,A Statistical Model for Measuring Structural Similarity between Webpages,This paper presents a statistical model for measuring structural similarity between webpages from bilingual websites. Starting from basic assumptions we derive the model and propose an algorithm to estimate its parameters in unsupervised manner. Statistical approach appears to benefit the structural similarity measure: in the task of distinguishing parallel webpages from bilingual websites our languageindependent model demonstrates an Fscore of 0.94-0.99 which is comparable to the results of language-dependent methods involving content similarity measures.,2015,False,False,True,False,False,False,True,"C, G",209,3,212
Y15-1021,Sentiment Analyzer with Rich Features for Ironic and Sarcastic Tweets,"Sentiment Analysis of tweets is a complex task, because these short messages employ unconventional language to increase the expressiveness. This task becomes even more difficult when people use figurative language (e.g. irony, sarcasm and metaphors) because it causes a mismatch between the literal meaning and the actual expressed sentiment. In this paper, we describe a sentiment analysis system designed for handling ironic and sarcastic tweets. Features grounded on several linguistic levels are proposed and used to classify the tweets in a 11-scale range, using a decision tree. The system is evaluated on the dataset released by the organizers of the SemEval 2015, task 11. The results show that our method largely outperforms the systems proposed by the participants of the task on ironic and sarcastic tweets.",2015,False,False,False,True,False,False,True,"D, G",274,3,277
D15-1218,A Coarse-Grained Model for Optimal Coupling of {ASR} and {SMT} Systems for Speech Translation,"Speech translation is conventionally carried out by cascading an automatic speech recognition (ASR) and a statistical machine translation (SMT) system. The hypotheses chosen for translation are based on the ASR system's acoustic and language model scores, and typically optimized for word error rate, ignoring the intended downstream use: automatic translation. In this paper, we present a coarseto-fine model that uses features from the ASR and SMT systems to optimize this coupling. We demonstrate that several standard features utilized by ASR and SMT systems can be used in such a model at the speech-translation interface, and we provide empirical results on the Fisher Spanish-English speech translation corpus.",2015,False,False,False,True,False,False,True,"D, G",252,3,255
D15-1128,Hierarchical Phrase-based Stream Decoding,"This paper proposes a method for hierarchical phrase-based stream decoding. A stream decoder is able to take a continuous stream of tokens as input, and segments this stream into word sequences that are translated and output as a stream of target word sequences. Phrase-based stream decoding techniques have been shown to be effective as a means of simultaneous interpretation. In this paper we transfer the essence of this idea into the framework of hierarchical machine translation. The hierarchical decoding framework organizes the decoding process into a chart; this structure is naturally suited to the process of stream decoding, leading to an efficient stream decoding algorithm that searches a restricted subspace containing only relevant hypotheses. Furthermore, the decoder allows more explicit access to the word re-ordering process that is of critical importance in decoding while interpreting. The decoder was evaluated on TED talk data for English-Spanish and English-Chinese. Our results show that like the phrase-based stream decoder, the hierarchical is capable of approaching the performance of the underlying hierarchical phrase-based machine translation decoder, at useful levels of latency. In addition the hierarchical approach appeared to be robust to the difficulties presented by the more challenging English-Chinese task.",2015,False,True,False,True,False,False,False,"B, D",342,3,345
P15-3006,Evaluation Dataset and System for {J}apanese Lexical Simplification,"We have constructed two research resources of Japanese lexical simplification. One is a simplification system that supports reading comprehension of a wide range of readers, including children and language learners. The other is a dataset for evaluation that enables open discussions with other systems. Both the system and the dataset are made available providing the first such resources for the Japanese language.",2015,True,False,False,False,False,False,True,"A, G",187,3,190
W15-1601,Scaling Semantic Frame Annotation,"Large-scale data resources needed for progress toward natural language understanding are not yet widely available and typically require considerable expense and expertise to create. This paper addresses the problem of developing scalable approaches to annotating semantic frames and explores the viability of crowdsourcing for the task of frame disambiguation. We present a novel supervised crowdsourcing paradigm that incorporates insights from human computation research designed to accommodate the relative complexity of the task, such as exemplars and real-time feedback. We show that non-experts can be trained to perform accurate frame disambiguation, and can even identify errors in gold data used as the training exemplars. Results demonstrate the efficacy of this paradigm for semantic annotation requiring an intermediate level of expertise.",2015,True,False,False,True,False,False,False,"A, D",257,3,260
S15-1015,Ideological Perspective Detection Using Semantic Features,"In this paper, we propose the use of word sense disambiguation and latent semantic features to automatically identify a person's perspective from his/her written text. We run an Amazon Mechanical Turk experiment where we ask Turkers to answer a set of constrained and open-ended political questions drawn from the American National Election Studies (ANES). We then extract the proposed features from the answers to the open-ended questions and use them to predict the answer to one of the constrained questions, namely, their preferred Presidential Candidate. In addition to this newly created dataset, we also evaluate our proposed approach on a second standard dataset of ""Ideological-Debates"". This latter dataset contains topics from four domains: Abortion, Creationism, Gun Rights and Gay-Rights. Experimental results show that using word sense disambiguation and latentsemantics, whether separately or combined, beats the majority and random baselines on the cross-validation and held-out-test sets for both the ANES and the four domains of the ""Ideological Debates"" datasets. Moreover combining both feature sets outperforms a stronger unigram-only classification system.",2015,True,False,False,True,False,False,False,"A, D",337,3,340
W15-2803,Towards Reliable Automatic Multimodal Content Analysis,"This poster presents a pilot where audio description is used to enhance automatic content analysis, for a project aiming at creating a tool for easy access to large AV archives.",2015,False,False,False,True,False,False,True,"G, D",150,3,153
