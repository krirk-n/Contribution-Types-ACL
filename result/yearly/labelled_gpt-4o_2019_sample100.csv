acl_id,title,abstract,year,A,B,C,D,E,F,G,raw_response,input_tokens,output_tokens,total_tokens
W19-5362,{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 Translation Robustness Task,"In this paper we describe our neural machine translation (NMT) systems for Japanese↔English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et al., 2017) to improve translation quality for Japanese↔English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest.",2019,False,False,False,True,False,False,True,"D, G",250,3,253
W19-8302,Are Talkative {AI} Agents More Likely to Win the Werewolf Game?,"The Werewolf game is a communication game where, usually, two teams compete against each other. As players discuss and share ideas during the game to define their strategy, being talkative or not is one of the characteristics that define them. This paper presents a data analysis over logs from the shared task of The 1st International Workshop of AI Werewolf and Dialogue System to discuss if being talkative or not can be related to winning or losing when AI agents play the Werewolf game. Overall results show that the difference in the average of utterances sent by winning and losing players is not significant. However, they also suggest further analysis and discussion.",2019,False,False,False,False,True,True,False,"E,F",247,2,249
N19-1137,Multi-Channel Convolutional Neural Network for {T}witter Emotion and Sentiment Recognition,"The advent of micro-blogging sites has paved the way for researchers to collect and analyze huge volumes of data in recent years. Twitter, being one of the leading social networking sites worldwide, provides a great opportunity to its users for expressing their states of mind via short messages which are called tweets. The urgency of identifying emotions and sentiments conveyed through tweets has led to several research works. It provides a great way to understand human psychology and impose a challenge to researchers to analyze their content easily. In this paper, we propose a novel use of a multi-channel convolutional neural architecture which can effectively use different emotion and sentiment indicators such as hashtags, emoticons and emojis that are present in the tweets and improve the performance of emotion and sentiment identification. We also investigate the incorporation of different lexical features in the neural network model and its effect on the emotion and sentiment identification task. We analyze our model on some standard datasets and compare its effectiveness with existing techniques.",2019,False,True,False,True,False,False,False,"B, D",305,3,308
R19-1115,Enhancing Unsupervised Sentence Similarity Methods with Deep Contextualised Word Representations,"Calculating Semantic Textual Similarity (STS) plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. All modern state of the art STS methods rely on word embeddings one way or another. The recently introduced contextualised word embeddings have proved more effective than standard word embeddings in many natural language processing tasks. This paper evaluates the impact of several contextualised word embeddings on unsupervised STS methods and compares it with the existing supervised/unsupervised STS methods for different datasets in different languages and different domains.",2019,False,False,False,False,True,True,False,"E,F",232,2,234
R19-1066,Multi-level analysis and recognition of the text sentiment on the example of consumer opinions,"In this article, we present a novel multidomain dataset of Polish text reviews, annotated with sentiment on different levels: sentences and the whole documents. The annotation was made by linguists in a 2+1 scheme (with inter-annotator agreement analysis). We present a preliminary approach to the classification of labelled data using logistic regression, bidirectional long short-term memory recurrent neural networks (BiLSTM) and bidirectional encoder representations from transformers (BERT).",2019,True,False,False,False,False,False,True,"A, G",210,3,213
W19-0417,Cross-Lingual Transfer of Semantic Roles: From Raw Text to Semantic Roles,"We describe a transfer method based on annotation projection to develop a dependency-based semantic role labeling system for languages for which no supervised linguistic information other than parallel data is available. Unlike previous work that presumes the availability of supervised features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features. Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features. Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank.",2019,False,True,False,True,False,False,False,"B, D",244,3,247
W19-4513,Detecting Argumentative Discourse Acts with Linguistic Alignment,We report the results of preliminary investigations into the relationship between linguistic alignment and dialogical argumentation at the level of discourse acts. We annotated a proof of concept dataset with illocutions and transitions at the comment level based on Inference Anchoring Theory. We estimated linguistic alignment across discourse acts and found significant variation. Alignment features calculated at the dyad level are found to be useful for detecting a range of argumentative discourse acts.,2019,False,False,False,False,True,False,True,"E, G",202,3,205
2019.ijclclp-1.3,探究端對端混合模型架構於華語語音辨識 (An Investigation of Hybrid {CTC}-Attention Modeling in {M}andarin Speech Recognition),"The recent emergence of end-to-end automatic speech recognition (ASR) frameworks has streamlined the complicated modeling procedures of ASR systems in contrast to the conventional deep neural network-hidden Markov (DNN-HMM) ASR systems. Among the most popular end-to-end ASR approaches are the connectionist temporal classification (CTC) and the attention-based encoder-decoder model (Attention Model). In this paper, we explore the utility of combining CTC and the attention model in an attempt to yield better ASR performance. we also analyze the impact of the combination weight and the",2019,False,False,False,True,False,True,False,"D,F",234,2,236
Q19-1002,Semantic Neural Machine Translation Using {AMR},"It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequenceto-sequence neural translation model.",2019,False,False,False,True,False,False,True,"D, G",233,3,236
N19-3015,Expectation and Locality Effects in the Prediction of Disfluent Fillers and Repairs in {E}nglish Speech,"This study examines the role of three influential theories of language processing, viz., Surprisal Theory, Uniform Information Density (UID) hypothesis and Dependency Locality Theory (DLT), in predicting disfluencies in speech production. To this end, we incorporate features based on lexical surprisal, word duration and DLT integration and storage costs into logistic regression classifiers aimed to predict disfluencies in the Switchboard corpus of English conversational speech. We find that disfluencies occur in the face of upcoming difficulties and speakers tend to handle this by lessening cognitive load before disfluencies occur. Further, we see that reparandums behave differently from disfluent fillers possibly due to the lessening of the cognitive load also happening in the word choice of the reparandum, i.e., in the disfluency itself. While the UID hypothesis does not seem to play a significant role in disfluency prediction, lexical surprisal and DLT costs do give promising results in explaining language production. Further, we also find that as a means to lessen cognitive load for upcoming difficulties speakers take more time on words preceding disfluencies, making duration a key element in understanding disfluencies.",2019,False,False,False,False,True,True,False,"E,F",353,2,355
D19-5619,On the Importance of Word Boundaries in Character-level Neural Machine Translation,"Neural Machine Translation (NMT) models generally perform translation using a fixedsize lexical vocabulary, which is an important bottleneck on their generalization capability and overall translation quality. The standard approach to overcome this limitation is to segment words into subword units, typically using some external tools with arbitrary heuristics, resulting in vocabulary units not optimized for the translation task. Recent studies have shown that the same approach can be extended to perform NMT directly at the level of characters, which can deliver translation accuracy on-par with subword-based models, on the other hand, this requires relatively deeper networks. In this paper, we propose a more computationally-efficient solution for character-level NMT which implements a hierarchical decoding architecture where translations are subsequently generated at the level of words and characters. We evaluate different methods for open-vocabulary NMT in the machine translation task from English into five languages with distinct morphological typology, and show that the hierarchical decoding model can reach higher translation accuracy than the subword-level NMT model using significantly fewer parameters, while demonstrating better capacity in learning longer-distance contextual and grammatical dependencies than the standard character-level NMT model.",2019,False,True,False,True,False,False,False,"B, D",344,3,347
S19-2089,{T}u{E}val at {S}em{E}val-2019 Task 5: {LSTM} Approach to Hate Speech Detection in {E}nglish and {S}panish,"The detection of hate speech, especially in online platforms and forums, is quickly becoming a hot topic as anti-hate speech legislation begins to be applied to public discourse online. The HatEval shared task was created with this in mind; participants were expected to develop a model capable of determining whether or not input (in this case, Twitter posts in English and Spanish) could be considered hate speech (designated as Subtask A), if they were aggressive, and whether the tweet was targeting an individual, or speaking generally (Subtask B). We approached this Subtask by creating a LSTM model with an embedding layer. We found that our model performed considerably better on English language input when compared to Spanish language input. In English, we achieved an F1-Score of 0.466 for Subtask A and 0.462 for Subtask B; In Spanish, we achieved scores of 0.617 and 0.612 on Subtask A and Subtask B, respectively.",2019,True,False,False,False,False,False,True,"A, G",316,3,319
W19-8611,A Tree-to-Sequence Model for Neural {NLG} in Task-Oriented Dialog,"Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Sequence-to-sequence models on flat meaning representations (MR) have been dominant in this task, for example in the E2E NLG Challenge. Previous work has shown that a tree-structured MR can improve the model for better discourse-level structuring and sentence-level planning. In this work, we propose a tree-to-sequence model that uses a tree-LSTM encoder to leverage the tree structures in the input MR, and further enhance the decoding by a structure-enhanced attention mechanism. In addition, we explore combining these enhancements with constrained decoding to improve semantic correctness. Our method not only shows significant improvements over standard seq2seq baselines, but also is more data-efficient and generalizes better to hard scenarios.",2019,False,True,True,False,False,False,False,"B, C",279,3,282
S19-2110,Ghmerti at {S}em{E}val-2019 Task 6: A Deep Word- and Character-based Approach to Offensive Language Identification,"This paper presents the models submitted by Ghmerti team for subtasks A and B of the Of-fensEval shared task at SemEval 2019. Offen-sEval addresses the problem of identifying and categorizing offensive language in social media in three subtasks; whether or not a content is offensive (subtask A), whether it is targeted (subtask B) towards an individual, a group, or other entities (subtask C). The proposed approach includes character-level Convolutional Neural Network, word-level Recurrent Neural Network, and some preprocessing. The performance achieved by the proposed model for subtask A is 77.93% macro-averaged F 1 -score.",2019,False,False,False,True,False,False,True,"D, G",257,3,260
N19-1059,Learning Outside the Box: Discourse-level Features Improve Metaphor Identification,"Most current approaches to metaphor identification use restricted linguistic contexts, e.g. by considering only a verb's arguments or the sentence containing a phrase. Inspired by pragmatic accounts of metaphor, we argue that broader discourse features are crucial for better metaphor identification. We train simple gradient boosting classifiers on representations of an utterance and its surrounding discourse learned with a variety of document embedding methods, obtaining near state-of-the-art results on the 2018 VU Amsterdam metaphor identification task without the complex metaphorspecific features or deep neural architectures employed by other systems. A qualitative analysis further confirms the need for broader context in metaphor processing.",2019,False,False,False,True,True,False,False,"D, E",239,3,242
W19-3206,{HITSZ}-{ICRC}: A Report for {SMM}4{H} Shared Task 2019-Automatic Classification and Extraction of Adverse Effect Mentions in Tweets,"This is the system description of the Harbin Institute of Technology Shenzhen (HITSZ) team for the first and second subtasks of the fourth Social Media Mining for Health Applications (SMM4H) shared task in 2019. The two subtasks are automatic classification and extraction of adverse effect mentions in tweets. The systems for the two subtasks are based on bidirectional encoder representations from transformers (BERT), and achieves promising results. Among the systems we developed for subtask1, the best F1-score was 0.6457, for subtask2, the best relaxed F1-score and the best strict F1-score were 0.614 and 0.407 respectively. Our system ranks first among all systems on subtask1.",2019,False,False,False,True,False,False,True,"D, G",271,3,274
W19-8711,Translation Quality Assessment Tools and Processes in Relation to {CAT} Tools,"Modern translation QA tools are the latest attempt to overcome the inevitable subjective component of human revisers. This paper analyzes the current situation in the translation industry in respect to those tools and their relationship with CAT tools. The adoption of international standards has set the basic frame that defines ""quality"". Because of the clear impossibility to develop a universal QA tool, all of the existing ones have in common a wide variety of settings for the user to choose from. A brief comparison is made between most popular standalone QA tools. In order to verify their results in practice, QA outputs from two of those tools have been compared. Polls that cover a period of 12 years have been collected. Their participants explained what practices they adopted in order to guarantee quality.",2019,False,False,False,False,True,True,False,"E, F",266,3,269
S19-2186,Tintin at {S}em{E}val-2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens,"Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of Se-mEval 2019, is exclusively based on the tokens that make up the documents and a standard supervised learning procedure. It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first. An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach.",2019,False,False,False,True,False,True,False,"D, F",218,3,221
P19-3031,{TARGER}: Neural Argument Mining at Your Fingertips,"We present TARGER, an open source neural argument mining framework for tagging arguments in free input texts and for keyword-based retrieval of arguments from an argument-tagged web-scale corpus. The currently available models are pre-trained on three recent argument mining datasets and enable the use of neural argument mining without any reproducibility effort on the user's side. The open source code ensures portability to other domains and use cases, such as an application to search engine ranking that we also describe shortly.",2019,True,False,False,False,False,False,True,"A, G",212,3,215
D19-1206,How to Build User Simulators to Train {RL}-based Dialog Systems,"User simulators are essential for training reinforcement learning (RL) based dialog models. The performance of the simulator directly impacts the RL policy. However, building a good user simulator that models real user behaviors is challenging. We propose a method of standardizing user simulator building that can be used by the community to compare dialog system quality using the same set of user simulators fairly. We present implementations of six user simulators trained with different dialog planning and generation methods. We then calculate a set of automatic metrics to evaluate the quality of these simulators both directly and indirectly. We also ask human users to assess the simulators directly and indirectly by rating the simulated dialogs and interacting with the trained systems. This paper presents a comprehensive evaluation framework for user simulator study and provides a better understanding of the pros and cons of different user simulators, as well as their impacts on the trained systems. 1 * Equal contribution. 1 The code and data are released at https://github. com/wyshi/user-simulator.",2019,True,False,False,False,True,False,False,"A, E",319,3,322
S19-2015,{M}ask{P}arse@Deskin at {S}em{E}val-2019 Task 1: Cross-lingual {UCCA} Semantic Parsing using Recursive Masked Sequence Tagging,"This paper describes our recursive system for SemEval-2019 Task 1: Cross-lingual Semantic Parsing with UCCA. Each recursive step consists of two parts. We first perform semantic parsing using a sequence tagger to estimate the probabilities of the UCCA categories in the sentence. Then, we apply a decoding policy which interprets these probabilities and builds the graph nodes. Parsing is done recursively, we perform a first inference on the sentence to extract the main scenes and links and then we recursively apply our model on the sentence using a masking feature that reflects the decisions made in previous steps. Process continues until the terminal nodes are reached. We choose a standard neural tagger and we focused on our recursive parsing strategy and on the cross lingual transfer problem to develop a robust model for the French language, using only few training samples.",2019,False,True,False,True,False,False,False,"B, D",284,3,287
W19-2601,Distantly Supervised Biomedical Knowledge Acquisition via Knowledge Graph Based Attention,"The increased demand for structured scientific knowledge has attracted considerable attention in extracting scientific relation from the ever growing scientific publications. Distant supervision is widely applied approach to automatically generate large amounts of labelled data for Relation Extraction (RE). However, distant supervision inevitably accompanies the wrong labelling problem, which will negatively affect the RE performance. To address this issue, (Han et al., 2018) proposes a novel framework for jointly training RE model and Knowledge Graph Completion (KGC) model to extract structured knowledge from nonscientific dataset. In this work, we firstly investigate the feasibility of this framework on scientific dataset, specifically on biomedical dataset. Secondly, to achieve better performance on the biomedical dataset, we extend the framework with other competitive KGC models. Moreover, we proposed a new endto-end KGC model to extend the framework. Experimental results not only show the feasibility of the framework on the biomedical dataset, but also indicate the effectiveness of our extensions, because our extended model achieves significant and consistent improvements on distantly supervised RE as compared with baselines.",2019,False,True,False,False,False,False,True,"B, G",327,3,330
W19-8007,Nested Coordination in {U}niversal {D}ependencies,"The aim of this paper is to extend the representation of coordination in Universal Dependencies in a way that makes it possible to distinguish between different embeddings in coordinate structures. 7 The numbers of different nestings -i.e. 1 (for two conjuncts), 3 (for three conjuncts), 11 (for four conjuncts, as in Table 1 ), 45 (for five conjuncts), etc. -form a sequence known in combinatorics as (little) Schröder numbers, Schröder-Hipparchus numbers or super-Catalan numbers; see e.g. Stanley, 1997 for the history of these numbers, and their other interpretations. This is an exponential sequence; for example, for ten conjuncts, there are 103,049 possible nestings (as calculated already by Hipparchus of Nicaea,. 8 Unlike the representation in chapter 38, which suggests a WG-like analysis.",2019,True,False,False,False,True,False,False,"E, A",310,3,313
D19-1119,An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction,"The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models. However, consensus is lacking on experimental configurations, namely, choosing how the pseudo data should be generated or used. In this study, these choices are investigated through extensive experiments, and state-of-the-art performance is achieved on the CoNLL-2014 test set (F 0.5 = 65.0) and the official test set of the BEA-2019 shared task (F 0.5 = 70.2) without making any modifications to the model architecture.",2019,False,False,False,True,False,True,False,"D, F",245,3,248
W19-5928,A Quantitative Analysis of Patients{'} Narratives of Heart Failure,"Patients with chronic conditions like heart failure are most likely to be re-hospitalized. One step towards avoiding re-hospitalization is to devise strategies for motivating patients to take care of their own health. In this paper, we perform a quantitative analysis of patients' narratives of their experience with heart failure and explore the different topics that patients talk about. We compare two different groups of patients-those unable to take charge of their illness, and those who make efforts to improve their health. We will use the findings from our analysis to refine and personalize the summaries of hospitalizations that our system automatically generates.",2019,False,False,False,False,True,False,True,"E, G",237,3,240
P19-1151,Soft Representation Learning for Sparse Transfer,"Transfer learning is effective for improving the performance of tasks that are related, and Multi-task learning (MTL) and Cross-lingual learning (CLL) are important instances. This paper argues that hard-parameter sharing, of hard-coding layers shared across different tasks or languages, cannot generalize well, when sharing with a loosely related task. Such case, which we call sparse transfer, might actually hurt performance, a phenomenon known as negative transfer. Our contribution is using adversarial training across tasks, to ""softcode"" shared and private spaces, to avoid the shared space gets too sparse. In CLL, our proposed architecture considers another challenge of dealing with low-quality input.",2019,False,True,False,True,False,False,False,"B, D",255,3,258
W19-3712,Tuning Multilingual Transformers for Language-Specific Named Entity Recognition,"Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and took the 1st place in 3 competition metrics out of 4 we participated in. We open-sourced NER models and BERT model pre-trained on the four Slavic languages.",2019,False,True,False,False,False,False,True,"B, G",274,3,277
S19-2208,{DBMS}-{KU} at {S}em{E}val-2019 Task 9: Exploring Machine Learning Approaches in Classifying Text as Suggestion or Non-Suggestion,"This paper describes the participation of DBMS-KU team in the SemEval 2019 Task 9, that is, suggestion mining from online reviews and forums. To deal with this task, we explore several machine learning approaches, i.e.",2019,False,False,False,True,False,False,True,"G, D",167,3,170
W19-6701,Competitiveness Analysis of the {E}uropean Machine Translation Market,"This paper presents the key results of a study on the global competitiveness of the European Machine Translation market in comparison to North America and Asia. The study focuses on seven dimensions that have been selected to characterize the machine translation market. The study concludes that while Europe still has strong positions in Research and Innovation, it lags behind North America and Asia in Industry and Investments, and is also weaker than North America in Infrastructure, Data availability, and Market visibility.",2019,False,False,False,False,True,False,True,"E, G",207,3,210
D19-3009,{EASSE}: Easier Automatic Sentence Simplification Evaluation,"We introduce EASSE, a Python package aiming to facilitate and standardise automatic evaluation and comparison of Sentence Simplification (SS) systems. EASSE provides a single access point to a broad range of evaluation resources: standard automatic metrics for assessing SS outputs (e.g. SARI), wordlevel accuracy scores for certain simplification transformations, reference-independent quality estimation features (e.g. compression ratio), and standard test data for SS evaluation (e.g. TurkCorpus). Finally, EASSE generates easy-to-visualise reports on the various metrics and features above and on how a particular SS output fares against reference simplifications. Through experiments, we show that these functionalities allow for better comparison and understanding of the performance of SS systems.",2019,True,False,False,False,False,True,False,"A, F",262,3,265
W19-9004,Controlled Semi-automatic Annotation of Classical Ethiopic,"Preservation of the cultural heritage by means of digital methods became extremely popular during last years. After intensive digitization campaigns the focus moves slowly from the genuine preservation (i.e digital archiving together with standard search mechanisms) to researchoriented usage of materials available electronically. This usage is intended to go far beyond simple reading of digitized materials; researchers should be able to gain new insigts in materials, discover new facts by means of tools relying on innovative algorithms.In this article we will describe the workflow necessary for the annotation of a dichronic corpus of classical Ethiopic, language of essential importance for the study of Early Christianity",2019,True,False,False,False,False,False,True,"A, G",242,3,245
N19-1046,{U}nderstanding and {I}mproving {H}idden {R}epresentations for {N}eural {M}achine {T}ranslation,"Multilayer architectures are currently the gold standard for large-scale neural machine translation. Existing works have explored some methods for understanding the hidden representations, however, they have not sought to improve the translation quality rationally according to their understanding. Towards understanding for performance improvement, we first artificially construct a sequence of nested relative tasks and measure the feature generalization ability of the learned hidden representation over these tasks. Based on our understanding, we then propose to regularize the layer-wise representations with all treeinduced tasks. To overcome the computational bottleneck resulting from the large number of regularization terms, we design efficient approximation methods by selecting a few coarse-to-fine tasks for regularization. Extensive experiments on two widely-used datasets demonstrate the proposed methods only lead to small extra overheads in training but no additional overheads in testing, and achieve consistent improvements (up to +1.3 BLEU) compared to the state-of-the-art translation model.",2019,False,True,True,False,False,False,False,"B, C",304,3,307
W19-3302,Thirty Musts for Meaning Banking,"Meaning banking{---}creating a semantically annotated corpus for the purpose of semantic parsing or generation{---}is a challenging task. It is quite simple to come up with a complex meaning representation, but it is hard to design a simple meaning representation that captures many nuances of meaning. This paper lists some lessons learned in nearly ten years of meaning annotation during the development of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017). The paper{'}s format is rather unconventional: there is no explicit related work, no methodology section, no results, and no discussion (and the current snippet is not an abstract but actually an introductory preface). Instead, its structure is inspired by work of Traum (2000) and Bender (2013). The list starts with a brief overview of the existing meaning banks (Section 1) and the rest of the items are roughly divided into three groups: corpus collection (Section 2 and 3, annotation methods (Section 4{--}11), and design of meaning representations (Section 12{--}30). We hope this overview will give inspiration and guidance in creating improved meaning banks in the future",2019,True,False,False,False,True,False,False,"E, A",370,3,373
D19-5505,Exploiting {BERT} for End-to-End Aspect-based Sentiment Analysis,"In this paper, we investigate the modeling power of contextualized embeddings from pretrained language models, e.g. BERT, on the E2E-ABSA task. Specifically, we build a series of simple yet insightful neural baselines to deal with E2E-ABSA. The experimental results show that even with a simple linear classification layer, our BERT-based architecture can outperform state-of-the-art works. Besides, we also standardize the comparative study by consistently utilizing a hold-out development dataset for model selection, which is largely ignored by previous works. Therefore, our work can serve as a BERT-based benchmark for E2E-ABSA. 1",2019,False,False,False,True,False,True,False,"F, D",253,3,256
W19-5301,Findings of the 2019 Conference on Machine Translation ({WMT}19),"This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation. Language Sources (Number of Documents) Chinese Chinanews (111), Macao Govt. (4), QQ (10), Reuters (31), RFI (2), Tsrus (5) English I ABC News (3), BBC (12), CBS News (2), CNBC (3), CNN (3), Daily Mail (9), Euronews (3), Guardian (3), Independent (3), News Week (6), NY Times (4), Reuters (3), Russia Today (1), The Scotsman (3), The Telegraph (2), UPI (2) English II ABC News (3), BBC (6), CBS News (4), CNBC (2), CNN (3), Daily Mail (2), Euronews (2), Fox News (1), Guardian (2), Independent (1), News Week (5), NY Times (4), Reuters (9), Russia Today (4), The Scotsman (6), The Telegraph (4), The Local (1), UPI (2) Finnish ESS (8), Helsinginsanomat (12), Iltalehti (33), Iltasanomat (34), Kaleva (19), Kansanuutiset (1), Karjalainen (26), Kotiseutu Uutiset (1) German Abdendzeitung München (9), Abendzeitung Nürnberg (1), Aachener Nachrichten (7), Augsburger Allgemine (2), Bergdorfer Zeitung (2), Braunschweiger Zeiting (2), Cuxhavener Nachrichten (1), Come On (2), Der Standart (9), Deutsche Welle (1), Duelmener Zeitung (7), Euronews (2), Frankfurter Neue Presse (2), Frankfurter Rundschau (4), Freipresse (1), Geinhaüser Tageblatt (1), Gmünder Tagespost (1), Göttinger Tageblatt (2), Handelsblatt (3), Hannoversche Allgemeine Zeitung (1), Hersfelder Zeitung (2), HNA (2), Infranken (5), In Süd Thüringen (3), Kieler Nachrichten (6), Merkur Online (5), Morgen Post (1), Nachrichten (4), N TV (3), NW News (1), NZZ (6), OE24 (5), PAZ Online (1), Passauer Neue Presse (1), Rhein Zeitung (1), Rheinische Poste (1), Salzburg (3), Schwarzwälder Bote (2), Söster Anzeiger (2), Südkurier (1), Usinger Anzeiger (1), Westfaelischer Anzeige (2), Welt (2), Wienerzeitung (2), Westfaelische Nachrichten (18), Zeit (1), Zeitungsverlag Waiblingen (2) Gujarati ABP Asmita (13), BBC (3), Divya Bhaskar (20), Global Gujarati News (13), Web Dunia (21) Kazakh 7Kun (4), Aktobe Gazeti (3), Alkyn (4), Astana Akshamy (6), Atyray (1), Kazakh Adabieti (1), Egemen (5), Jaskazaq (11), Akorda/Kazinform (34), SN.kz (5), Zamedia (1) Lithuanian Delfi (22), Diena (25), Lietuvos Zinios (7), TV3 (12), Voruta (2), VZ (8) Russian AIF.ru (14), Altapress (4), Argumenti (3), Euronews (13), Fakty (9), Gazeta (7), Infox (3),",2019,True,False,False,False,True,False,False,"A, E",943,3,946
P19-1018,Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces,"Recent work on bilingual lexicon induction (BLI) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. We then propose Bilingual Lexicon Induction with Semi-Supervision (BLISS) -a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don't appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision. ⇤",2019,False,True,False,True,False,False,False,"B, D",318,3,321
S19-2170,Spider-{J}erusalem at {S}em{E}val-2019 Task 4: Hyperpartisan News Detection,"This paper describes our system for detecting hyperpartisan news articles, which was submitted for the shared task in SemEval 2019 on Hyperpartisan News Detection. We developed a Support Vector Machine (SVM) model that uses TF-IDF of tokens, Language Inquiry and Word Count (LIWC) features, and structural features such as number of paragraphs and hyperlink count in an article. The model was trained on 645 articles from two classes: mainstream and hyperpartisan. Our system was ranked seventeenth out of forty two participating teams in the binary classification task with an accuracy score of 0.742 on the blind test set (the accuracy of the top ranked system was 0.822). We provide a detailed description of our preprocessing steps, discussion of our experiments using different combinations of features, and analysis of our results and prediction errors.",2019,False,False,False,True,False,False,True,"D,G",289,2,291
P19-1267,We Need to Talk about Standard Splits,"It is standard practice in speech & language technology to rank systems according to performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which reports state-of-the-art performance on a widely-used ""standard split"". We fail to reliably reproduce some rankings using randomly generated splits. We suggest that randomly generated splits should be used in system comparison.",2019,False,False,False,False,True,True,False,"E, F",242,3,245
2019.icon-1.6,Event Centric Entity Linking for {H}indi News Articles: A Knowledge Graph Based Approach,"We describe the development of a knowledge graph from an event annotated corpus by presenting a pipeline that identifies and extracts the relations between entities and events from Hindi news articles. Due to the semantic implications of argument identification for events in Hindi, we use a combined syntactic argument and semantic role identification methodology. To the best of our knowledge, no other architecture exists for this purpose. The extracted combined role information is incorporated in a knowledge graph that can be queried via subgraph extraction for basic questions. The architectures presented in this paper can be used for participant extraction and evententity linking in most Indo-Aryan languages, due to similar syntactic and semantic properties of event arguments.",2019,True,True,False,False,False,False,False,"A, B",250,3,253
W19-5352,A Test Suite and Manual Evaluation of Document-Level {NMT} at {WMT}19,"As the quality of machine translation rises and neural machine translation (NMT) is moving from sentence to document level translations, it is becoming increasingly difficult to evaluate the output of translation systems. We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation.",2019,True,False,False,False,True,False,False,"A, E",199,3,202
N19-1304,Analyzing Polarization in Social Media: Method and Application to Tweets on 21 Mass Shootings,"We provide an NLP framework to uncover four linguistic dimensions of political polarization in social media: topic choice, framing, affect and illocutionary force. We quantify these aspects with existing lexical methods, and propose clustering of tweet embeddings as a means to identify salient topics for analysis across events; human evaluations show that our approach generates more cohesive topics than traditional LDA-based models. We apply our methods to study 4.4M tweets on 21 mass shootings. We provide evidence that the discussion of these events is highly polarized politically and that this polarization is primarily driven by partisan differences in framing rather than topic choice. We identify framing devices, such as grounding and the contrasting use of the terms ""terrorist"" and ""crazy"", that contribute to polarization. Results pertaining to topic choice, affect and illocutionary force suggest that Republicans focus more on the shooter and event-specific facts (news) while Democrats focus more on the victims and call for policy changes. Our work contributes to a deeper understanding of the way group divisions manifest in language and to computational methods for studying them. 1",2019,False,False,False,False,True,False,True,"E, G",336,3,339
N19-1364,Let{'}s Make Your Request More Persuasive: Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms,"Modeling what makes a request persuasiveeliciting the desired response from a readeris critical to the study of propaganda, behavioral economics, and advertising. Yet current models can't quantify the persuasiveness of requests or extract successful persuasive strategies. Building on theories of persuasion, we propose a neural network to quantify persuasiveness and identify the persuasive strategies in advocacy requests. Our semi-supervised hierarchical neural network model is supervised by the number of people persuaded to take actions and partially supervised at the sentence level with human-labeled rhetorical strategies. Our method outperforms several baselines, uncovers persuasive strategies-offering increased interpretability of persuasive speechand has applications for other situations with document-level supervision but only partial sentence supervision.",2019,False,True,False,True,False,False,False,"B, D",259,3,262
P19-1180,Visually Grounded Neural Syntax Acquisition,"We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define the concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F 1 scores against gold parse trees. We find that VG-NSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches. 1 * HS and JM contributed equally to the work.",2019,False,True,False,True,False,False,False,"B, D",349,3,352
S19-2073,{GL} at {S}em{E}val-2019 Task 5: Identifying hateful tweets with a deep learning approach.,"This paper describes the system we developed for SemEval 2019 on Multilingual detection of hate speech against immigrants and women in Twitter (HatEval -Task 5). We use an approach based on an Attention-based Long Short-Term Memory Recurrent Neural Network. In particular, we build a Bidirectional LSTM to extract information from the word embeddings over the sentence, then apply attention over the hidden states and finally feed this vector to another LSTM model to get a representation from de data. Then, the output obtained with this model is used to get the prediction of each of the sub-tasks with models based on neural networks and linguistic characteristics.",2019,False,True,False,False,False,False,True,"B, G",247,3,250
P19-1463,"Yes, we can! Mining Arguments in 50 Years of {US} Presidential Campaign Debates","Political debates offer a rare opportunity for citizens to compare the candidates' positions on the most controversial topics of the campaign. Thus they represent a natural application scenario for Argument Mining. As existing research lacks solid empirical investigation of the typology of argument components in political debates, we fill this gap by proposing an Argument Mining approach to political debates. We address this task in an empirical manner by annotating 39 political debates from the last 50 years of US presidential campaigns, creating a new corpus of 29k argument components, labeled as premises and claims. We then propose two tasks: (1) identifying the argumentative components in such debates, and (2) classifying them as premises and claims. We show that feature-rich SVM learners and Neural Network architectures outperform standard baselines in Argument Mining over such complex data. We release the new corpus USElecDeb60To16 and the accompanying software under free licenses to the research community.",2019,True,False,False,False,False,False,True,"A, G",303,3,306
R19-2006,Cross-Lingual Coreference: The Case of {B}ulgarian and {E}nglish,The paper presents several common approaches towards cross-and multi-lingual coreference resolution in a search of the most effective practices to be applied within the work on Bulgarian-English manual coreference annotation of a short story. The work aims at outlining the typology of the differences in the annotated parallel texts. The results of the research prove to be comparable with the tendencies observed in similar works on other Slavic languages and show surprising differences between the types of markables and their frequency in Bulgarian and English.,2019,False,False,False,False,True,True,False,"E,F",216,2,218
D19-5608,Making Asynchronous Stochastic Gradient Descent Work for Transformers,"Asynchronous stochastic gradient descent (SGD) converges poorly for Transformer models, so synchronous SGD has become the norm for Transformer training. This is unfortunate because asynchronous SGD is faster at raw training speed since it avoids waiting for synchronization. Moreover, the Transformer model is the basis for state-of-the-art models for several tasks, including machine translation, so training speed matters. To understand why asynchronous SGD under-performs, we blur the lines between asynchronous and synchronous methods. We find that summing several asynchronous updates, rather than applying them immediately, restores convergence behavior. With this method, the Transformer attains the same BLEU score 1.36 times as fast.",2019,False,False,True,False,False,True,False,"C, F",250,3,253
P19-1568,Zero-shot Word Sense Disambiguation using Sense Definition Embeddings,"Word Sense Disambiguation (WSD) is a longstanding but open problem in Natural Language Processing (NLP). WSD corpora are typically small in size, owing to an expensive annotation process. Current supervised WSD methods treat senses as discrete labels and also resort to predicting the Most-Frequent-Sense (MFS) for words unseen during training. This leads to poor performance on rare and unseen senses. To overcome this challenge, we propose Extended WSD Incorporating Sense Embeddings (EWISE), a supervised model to perform WSD by predicting over a continuous sense embedding space as opposed to a discrete label space. This allows EWISE to generalize over both seen and unseen senses, thus achieving generalized zeroshot learning. To obtain target sense embeddings, EWISE utilizes sense definitions. EWISE learns a novel sentence encoder for sense definitions by using WordNet relations and also ConvE, a recently proposed knowledge graph embedding method. We also compare EWISE against other sentence encoders pretrained on large corpora to generate definition embeddings. EWISE achieves new stateof-the-art WSD performance.",2019,False,True,True,False,False,False,False,"B, C",336,3,339
D19-5717,{YNU}-junyi in {B}io{NLP}-{OST} 2019: Using {CNN}-{LSTM} Model with Embeddings for {S}ee{D}ev Binary Event Extraction,"We participated in the BioNLP 2019 Open Shared Tasks: binary relation extraction of SeeDev task. The model was constructed using convolutional neural networks (CNN) and long short term memory networks (LSTM). The full text information and context information were collected using the advantages of C-NN and LSTM. The model consisted of two main modules: distributed semantic representation construction, such as word embedding, distance embedding and entity type embedding; and CNN-LSTM model. The F1 value of our participated task on the test data set of all types was 0.342. We achieved the second highest in the task. The results showed that our proposed method performed effectively in the binary relation extraction.",2019,False,False,False,True,False,False,True,"D, G",259,3,262
D19-1216,Fact-Checking Meets Fauxtography: Verifying Claims About Images,"The recent explosion of false claims in social media and on the Web in general has given rise to a lot of manual fact-checking initiatives. Unfortunately, the number of claims that need to be fact-checked is several orders of magnitude larger than what humans can handle manually. Thus, there has been a lot of research aiming at automating the process. Interestingly, previous work has largely ignored the growing number of claims about images. This is despite the fact that visual imagery is more influential than text and naturally appears alongside fake news. Here we aim at bridging this gap. In particular, we create a new dataset for this problem, and we explore a variety of features modeling the claim, the image, and the relationship between the claim and the image. The evaluation results show sizable improvements over the baseline. We release our dataset, hoping to enable further research on fact-checking claims about images.",2019,True,False,False,True,False,False,False,"A, D",295,3,298
N19-1412,Benchmarking Hierarchical Script Knowledge,"Understanding procedural language requires reasoning about both hierarchical and temporal relations between events. For example, ""boiling pasta"" is a sub-event of ""making a pasta dish"", typically happens before ""draining pasta,"" and requires the use of omitted tools (e.g. a strainer, sink...). While people are able to choose when and how to use abstract versus concrete instructions, the NLP community lacks corpora and tasks for evaluating if our models can do the same. In this paper, we introduce KIDSCOOK, a parallel script corpus, as well as a cloze task which matches video captions with missing procedural details. Experimental results show that state-of-the-art models struggle at this task, which requires inducing functional commonsense knowledge not explicitly stated in text. * Author now at Google. Work done while unaffiliated. 1. Take the strainer with the pasta and pour the pasta into the sauce. 2. Stir the pasta into sauce while it is in the pan. 3. Let the pasta and sauce simmer for a few minutes.",2019,True,False,False,False,False,False,True,"A, G",329,3,332
D19-1550,Learning Explicit and Implicit Structures for Targeted Sentiment Analysis,"Targeted sentiment analysis is the task of jointly predicting target entities and their associated sentiment information. Existing research efforts mostly regard this joint task as a sequence labeling problem, building models that can capture explicit structures in the output space. However, the importance of capturing implicit global structural information that resides in the input space is largely unexplored. In this work, we argue that both types of information (implicit and explicit structural information) are crucial for building a successful targeted sentiment analysis model. Our experimental results show that properly capturing both information is able to lead to better performance than competitive existing approaches. We also conduct extensive experiments to investigate our model's effectiveness and robustness 1 .",2019,False,True,False,False,False,True,False,"B, F",249,3,252
P19-1602,Generating Sentences from Disentangled Syntactic and Semantic Spaces,"Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE's latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntaxtransfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work. ‡ * Equal contributions. † Corresponding author. ‡ We release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE VAE: The man is in the kitchen. DSS-VAE: There is a airplane in the sky. Ref syn : The shellfish was cooked in a wok. Ref sem : The stadium was packed with people. VAE: The man was filled with people. DSS-VAE: The stadium was packed with people. Ref syn : The child is playing in the garden. Ref sem : There is a dog behind the door. VAE: There is a person in the garden. DSS-VAE: A dog is walking behind the door. Table 7: Case studies of syntax transfer generation.",2019,False,True,False,True,False,False,False,"B, D",428,3,431
N19-1104,Graph Pattern Entity Ranking Model for Knowledge Graph Completion,"Knowledge graphs have evolved rapidly in recent years and their usefulness has been demonstrated in many artificial intelligence tasks. However, knowledge graphs often have lots of missing facts. To solve this problem, many knowledge graph embedding models have been developed to populate knowledge graphs and these have shown outstanding performance. However, knowledge graph embedding models are so-called black boxes, and the user does not know how the information in a knowledge graph is processed and the models can be difficult to interpret. In this paper, we utilize graph patterns in a knowledge graph to overcome such problems. Our proposed model, the graph pattern entity ranking model (GRank), constructs an entity ranking system for each graph pattern and evaluates them using a ranking measure. By doing so, we can find graph patterns which are useful for predicting facts. Then, we perform link prediction tasks on standard datasets to evaluate our GRank method. We show that our approach outperforms other state-of-the-art approaches such as ComplEx and TorusE for standard metrics such as HITS@n and MRR. Moreover, our model is easily interpretable because the output facts are described by graph patterns.",2019,False,True,False,False,False,True,False,"B, F",342,3,345
D19-5005,"Calls to Action on Social Media: Detection, Social Impact, and Censorship Potential","Calls to action on social media are known to be effective means of mobilization in social movements, and a frequent target of censorship. We investigate the possibility of their automatic detection and their potential for predicting real-world protest events, on historical data of Bolotnaya protests in Russia (2011)(2012)(2013). We find that political calls to action can be annotated and detected with relatively high accuracy, and that in our sample their volume has a moderate positive correlation with rally attendance.",2019,True,False,False,False,True,False,False,"A, E",216,3,219
K19-1052,Neural Attentive Bag-of-Entities Model for Text Classification,"This study proposes a Neural Attentive Bagof-Entities model, which is a neural network model that performs text classification using entities in a knowledge base. Entities provide unambiguous and relevant semantic signals that are beneficial for capturing semantics in texts. We combine simple high-recall entity detection based on a dictionary, to detect entities in a document, with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities. We tested the effectiveness of our model using two standard text classification datasets (i.e., the 20 Newsgroups and R8 datasets) and a popular factoid question answering dataset based on a trivia quiz game. As a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/ wikipedia2vec/wikipedia2vec.",2019,False,True,False,True,False,False,False,"B, D",294,3,297
K19-2010,{CUHK} at {MRP} 2019: Transition-Based Parser with Cross-Framework Variable-Arity Resolve Action,"This paper describes our system (RE-SOLVER) submitted to the CoNLL 2019 shared task on Cross-Framework Meaning Representation Parsing (MRP). Our system implements a transition-based parser with a directed acyclic graph (DAG) to tree preprocessor and a novel cross-framework variable-arity resolve action that generalizes over five different representations. Although we ranked low in the competition, we have shown the current limitations and potentials of including variable-arity action in MRP and concluded with directions for improvements in the future.",2019,False,True,False,False,False,True,False,"B, F",225,3,228
W19-4504,Aligning Discourse and Argumentation Structures using Subtrees and Redescription Mining,"In this paper, we investigate similarities between discourse and argumentation structures by aligning subtrees in a corpus containing both annotations. Contrary to previous works, we focus on comparing sub-structures and not only relation matches. Using data mining techniques, we show that discourse and argumentation most often align well, and the double annotation allows to derive a mapping between structures. Moreover, this approach enables the study of similarities between discourse structures and differences in their expressive power.",2019,False,False,False,False,True,True,False,"E,F",208,2,210
W19-3320,Towards Universal Semantic Representation,Natural language understanding at the semantic level and independent of language variations is of great practical value. Existing approaches such as semantic role labeling (SRL) and abstract meaning representation (AMR) still have features related to the peculiarities of the particular language. In this work we describe various challenges and possible solutions in designing a semantic representation that is universal across a variety of languages.,2019,False,False,False,False,True,True,False,"E,F",192,2,194
D19-6106,{BERT} is Not an Interlingua and the Bias of Tokenization,"Multilingual transfer learning can benefit both high-and low-resource languages, but the source of these improvements is not well understood. Cananical Correlation Analysis (CCA) of the internal representations of a pretrained, multilingual BERT model reveals that the model partitions representations for each language rather than using a common, shared, interlingual space. This effect is magnified at deeper layers, suggesting that the model does not progressively abstract semantic content while disregarding languages. Hierarchical clustering based on the CCA similarity scores between languages reveals a tree structure that mirrors the phylogenetic trees hand-designed by linguists. The subword tokenization employed by BERT provides a stronger bias towards such structure than character-and wordlevel tokenizations. We release a subset of the XNLI dataset translated into an additional 14 languages at https://www.github. com/salesforce/xnli_extension to assist further research into multilingual representations.",2019,True,False,False,False,True,False,False,"E, A",300,3,303
W19-7408,Building a Speech Corpus based on {A}rabic Podcasts for Language and Dialect Identification,"In this paper, we present ArPod, a new Arabic speech corpus made of Arabic audio podcasts. We built this dataset, mainly for both speech-based multi-lingual and multi-dialectal identification tasks. It includes two languages: Modern Standard Arabic (MSA) and English, and four Arabic dialects: Saudi, Egyptian, Lebanese and Syrian. A set of supervised classifiers have been used: Support Vector Machines (SVM), Multi Layer Perceptron (MLP), K-Nearest Neighbors (KNN), Extratrees and Convolutional Neural Networks (CNN), using acoustic and spectral features. For both tasks, SVM yielded encouraging results and outperformed the other classifiers.",2019,True,False,False,False,False,False,True,"A, G",258,3,261
P19-1315,Towards Understanding Linear Word Analogies,"A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically downweights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity.",2019,False,False,False,False,True,True,False,"F, E",284,3,287
W19-4026,Creation of a corpus with semantic role labels for {H}ungarian,"In this article, an ongoing research is presented, the immediate goal of which is to create a corpus annotated with semantic role labels for Hungarian that can be used to train a parser-based system capable of formulating relevant questions about the text it processes. We briefly describe the objectives of our research, our efforts at eliminating errors in the Hungarian Universal Dependencies corpus, which we use as the base of our annotation effort, at creating a Hungarian verbal argument database annotated with thematic roles, at classifying adjuncts, and at matching verbal argument frames to specific occurrences of verbs and participles in the corpus.",2019,True,False,False,False,True,False,False,"A, E",235,3,238
D19-5901,Dependency Tree Annotation with {M}echanical {T}urk,"Crowdsourcing is frequently employed to quickly and inexpensively obtain valuable linguistic annotations but is rarely used for parsing, likely due to the perceived difficulty of the task and the limited training of the available workers. This paper presents what is, to the best of our knowledge, the first published use of Mechanical Turk (or similar platform) to crowdsource parse trees. We pay Turkers to construct unlabeled dependency trees for 500 English sentences using an interactive graphical dependency tree editor, collecting 10 annotations per sentence. Despite not requiring any training, several of the more prolific workers meet or exceed 90% attachment agreement with the Penn Treebank (PTB) portion of our data, and, furthermore, for 72% of these PTB sentences, at least one Turker produces a perfect parse. Thus, we find that, supported with a simple graphical interface, people with presumably no prior experience can achieve surprisingly high degrees of accuracy on this task. To facilitate research into aggregation techniques for complex crowdsourced annotations, we publicly release our annotated corpus.",2019,True,False,False,False,True,False,False,"A, E",326,3,329
D19-6123,Neural Unsupervised Parsing Beyond {E}nglish,"Recently, neural network models which automatically infer syntactic structure from raw text have started to achieve promising results. However, earlier work on unsupervised parsing shows large performance differences between non-neural models trained on corpora in different languages, even for comparable amounts of data. With that in mind, we train instances of the PRPN architecture (Shen et al., 2018a)-one of these unsupervised neural network parsers-for Arabic, Chinese, English, and German. We find that (i) the model strongly outperforms trivial baselines and, thus, acquires at least some parsing ability for all languages; (ii) good hyperparameter values seem to be universal; (iii) how the model benefits from larger training set sizes depends on the corpus, with the model achieving the largest performance gains when increasing the number of sentences from 2,500 to 12,500 for English. In addition, we show that, by sharing parameters between the related languages German and English, we can improve the model's unsupervised parsing F1 score by up to 4% in the low-resource setting.",2019,False,False,False,False,True,True,False,"E, F",345,3,348
W19-1301,Stance Detection in Code-Mixed {H}indi-{E}nglish Social Media Data using Multi-Task Learning,"Social media sites like Facebook, Twitter, and other microblogging forums have emerged as a platform for people to express their opinions and views on different issues and events. It is often observed that people tend to take a stance; in favor, against or neutral towards a particular topic. The task of assessing the stance taken by the individual became significantly important with the emergence in the usage of online social platforms. Automatic stance detection system understands the user's stance by analyzing the standalone texts against a target entity. Due to the limited contextual information a single sentence provides, it is challenging to solve this task effectively. In this paper, we introduce a Multi-Task Learning (MTL) based deep neural network architecture for automatically detecting stance present in the code-mixed corpus. We apply our approach on Hindi-English code-mixed corpus against the target entity -""Demonetisation."" Our best model achieved the result with a stance prediction accuracy of 63.2% which is a 4.5% overall accuracy improvement compared to the current supervised classification systems developed using the benchmark dataset for code-mixed data stance detection.",2019,False,True,False,False,False,False,True,"B, G",337,3,340
D19-6122,Natural Language Generation for Effective Knowledge Distillation,"Knowledge distillation can effectively transfer knowledge from BERT, a deep language representation model, to traditional, shallow word embedding-based neural networks, helping them approach or exceed the quality of other heavyweight language representation models. As shown in previous work, critical to this distillation procedure is the construction of an unlabeled transfer dataset, which enables effective knowledge transfer. To create transfer set examples, we propose to sample from pretrained language models fine-tuned on taskspecific text. Unlike previous techniques, this directly captures the purpose of the transfer set. We hypothesize that this principled, general approach outperforms rule-based techniques. On four datasets in sentiment classification, sentence similarity, and linguistic acceptability, we show that our approach improves upon previous methods. We outperform OpenAI GPT, a deep pretrained transformer, on three of the datasets, while using a single-layer bidirectional LSTM that runs at least ten times faster.",2019,False,False,False,True,False,False,True,"D, G",299,3,302
W19-7812,"Creating, Enriching and Valorizing Treebanks of {A}ncient {G}reek","This paper shows the extent to which treebanks of Ancient Greek play a central role in the ongoing Pedalion project at the University of Leuven. Building on diverse treebanks readily available today, the project aims to make progress in the automated parsing of classical and postclassical Greek texts. Rather than developing new technology as such, our project endeavours to make deliberate and methodical use of the technology that already exists, essentially by combining and adapting both technology and data. This contribution offers a 'roadmap' of our project, surveying (a) the existing work on which we can rely, (b) the strategies which we adopt to reach better results in the automated processing of Ancient Greek and (c) the deliverables that have already been realised or are forthcoming.",2019,False,False,False,True,False,False,True,"D, G",270,3,273
D19-1658,A Robust Self-Learning Framework for Cross-Lingual Text Classification,"Based on massive amounts of data, recent pretrained contextual representation models have made significant strides in advancing a number of different English NLP tasks. However, for other languages, relevant training data may be lacking, while state-of-the-art deep learning methods are known to be data-hungry. In this paper, we present an elegantly simple robust self-learning framework to include unlabeled non-English samples in the fine-tuning process of pretrained multilingual representation models. We leverage a multilingual model's own predictions on unlabeled non-English data in order to obtain additional information that can be used during further finetuning. Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effectiveness on document and sentiment classification for a range of diverse languages.",2019,False,False,False,True,False,False,True,"D, G",268,3,271
D19-1524,A Context-based Framework for Modeling the Role and Function of On-line Resource Citations in Scientific Literature,"We introduce a new task of modeling the role and function for on-line resource citations in scientific literature. By categorizing the online resources and analyzing the purpose of resource citations in scientific texts, it can greatly help resource search and recommendation systems to better understand and manage the scientific resources. For this novel task, we are the first to create an annotation scheme, which models the different granularity of information from a hierarchical perspective. And we construct a dataset SciRes, which includes 3,088 manually annotated resource contexts. In this paper, we propose a possible solution by using a multi-task framework to build the scientific resource classifier (SciResCLF) for jointly recognizing the role and function types. Then we use the classification results to help a scientific resource recommendation (SciResREC) task. Experiments show that our model achieves the best results on both the classification task and the recommendation task. The SciRes dataset 1 will be released for future research.",2019,True,True,False,False,False,False,False,"A, B",308,3,311
N19-1284,A Dynamic Speaker Model for Conversational Interactions,"Individual differences in speakers are reflected in their language use as well as in their interests and opinions. Characterizing these differences can be useful in human-computer interaction, as well as analysis of human-human conversations. In this work, we introduce a neural model for learning a dynamically updated speaker embedding in a conversational context. Initial model training is unsupervised, using context-sensitive language generation as an objective, with the context being the conversation history. Further finetuning can leverage task-dependent supervised training. The learned neural representation of speakers is shown to be useful for content ranking in a socialbot and dialog act prediction in human-human conversations. 1",2019,False,True,False,True,False,False,False,"B, D",246,3,249
P19-1247,Encoding Social Information with Graph Convolutional Networks for{P}olitical Perspective Detection in News Media,"Identifying the political perspective shaping the way news events are discussed in the media is an important and challenging task. In this paper, we highlight the importance of contextualizing social information, capturing how this information is disseminated in social networks. We use Graph Convolutional Networks, a recently proposed neural architecture for representing relational information, to capture the documents' social context. We show that social information can be used effectively as a source of distant supervision, and when direct supervision is available, even little social information can significantly improve performance.",2019,False,False,False,True,False,False,True,"D, G",223,3,226
P19-1391,Crowdsourcing and Validating Event-focused Emotion Corpora for {G}erman and {E}nglish,"Sentiment analysis has a range of corpora available across multiple languages. For emotion analysis, the situation is more limited, which hinders potential research on crosslingual modeling and the development of predictive models for other languages. In this paper, we fill this gap for German by constructing deISEAR, a corpus designed in analogy to the well-established English ISEAR emotion dataset. Motivated by Scherer's appraisal theory, we implement a crowdsourcing experiment which consists of two steps. In step 1, participants create descriptions of emotional events for a given emotion. In step 2, five annotators assess the emotion expressed by the texts. We show that transferring an emotion classification model from the original English ISEAR to the German crowdsourced deISEAR via machine translation does not, on average, cause a performance drop.",2019,True,False,False,False,False,False,True,"A, G",284,3,287
S19-1032,Generating Animations from Screenplays,"Automatically generating animation from natural language text finds application in a number of areas e.g. movie script writing, instructional videos, and public safety. However, translating natural language text into animation is a challenging task. Existing text-toanimation systems can handle only very simple sentences, which limits their applications. In this paper, we develop a text-to-animation system which is capable of handling complex sentences. We achieve this by introducing a text simplification step into the process. Building on an existing animation generation system for screenwriting, we create a robust NLP pipeline to extract information from screenplays and map them to the system's knowledge base. We develop a set of linguistic transformation rules that simplify complex sentences. Information extracted from the simplified sentences is used to generate a rough storyboard and video depicting the text. Our sentence simplification module outperforms existing systems in terms of BLEU and SARI metrics.We further evaluated our system via a user study: 68 % participants believe that our system generates reasonable animation from input screenplays.",2019,False,False,False,True,False,False,True,"D, G",319,3,322
2019.jeptalnrecital-tia.5,Entropic characterisation of termino-conceptual structure : A preliminary study,"Caractérisation entropique de la structure termino-conceptuelle : Une enquête préliminaire Les termes représentent des concepts, qui consistent en des caractéristiques conceptuelles. Dans la formation sur le terrain du concept et du terme, qui est effectuée par les chercheurs, le processus est inversé : les éléments / caractéristiques conceptuels sont consolidés pour former des concepts, qui seront représentés par des termes. Les concepts n'existant pas a priori, ce processus est échafaudé par ce que nous pouvons appeler un ""système termino-conceptuel"". Les terminologues, tant dans la pratique que dans la recherche, ne font pas que cueillir et énumérer des termes; en plus ils analysent, décrivent et définissent les termes tout en systématisant les terminologies. Pour mener à bien ces tâches, les terminologues doivent se référer aux systèmes conceptuels, dans la mesure où ils contribuent à systématiser les terminologies ; les terminologues abordent donc également le domaine du système terminologique-conceptuel. Dans cet article nous appuyons le statut du domaine terminologique-conceptuel en proposant un procédé pour caractériser la structure du système terminologique-conceptuel en termes d'entropie. Nous analysons l'entropie des terminologies en langue anglaise de six domaines : l'agriculture, la botanique, la chimie, l'informatique, la physique et la psychologie.",2019,False,False,False,False,True,True,False,"E, F",409,3,412
D19-1043,Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classification,"As an essential component of natural language processing, text classification relies on deep learning in recent years. Various neural networks are designed for text classification on the basis of word embedding. However, polysemy is a fundamental feature of the natural language, which brings challenges to text classification. One polysemic word contains more than one sense, while the word embedding procedure conflates different senses of a polysemic word into a single vector. Extracting the distinct representation for the specific sense could thus lead to fine-grained models with strong generalization ability. It has been demonstrated that multiple senses of a word actually reside in linear superposition within the word embedding so that specific senses can be extracted from the original word embedding. Therefore, we propose to use capsule networks to construct the vectorized representation of semantics and utilize hyperplanes to decompose each capsule to acquire the specific senses. A novel dynamic routing mechanism named 'routing-on-hyperplane' will select the proper sense for the downstream classification task. Our model is evaluated on 6 different datasets, and the experimental results show that our model is capable of extracting more discriminative semantic features and yields a significant performance gain compared to other baseline methods.",2019,False,True,True,False,False,False,False,"B, C",354,3,357
D19-3010,{EGG}: a toolkit for research on Emergence of lan{G}uage in Games,"There is renewed interest in simulating language emergence among deep neural agents that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the evolution of human language. However, optimizing deep architectures connected by a discrete communication channel (such as that in which language emerges) is technically challenging. We introduce EGG, a toolkit that greatly simplifies the implementation of emergent-language communication games. EGG's modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area.",2019,True,True,False,False,False,False,False,"A, B",264,3,267
W19-5607,Classifying {A}rabic dialect text in the Social Media {A}rabic Dialect Corpus ({SMADC}),"In recent years, research in Natural Language Processing (NLP) on Arabic has garnered significant attention. This includes research about classification of Arabic dialect texts, but due to the lack of Arabic dialect text corpora this research has not achieved a high accuracy. Arabic dialects text classification is becoming important due to the increasing use of Arabic dialect in social media, so this text is now considered quite appropriate as a medium of communication and as a source of a corpus. We collected tweets, comments from Facebook and online newspapers representing five groups of Arabic dialects: Gulf, Iraqi, Egyptian, Levantine, and North African. This paper investigates how to classify Arabic dialects in text by extracting lexicons for each dialect which show the distinctive vocabulary differences between dialects. We describe the lexicon-based methods used to classify Arabic dialect texts and present the results, in addition to techniques used to improve accuracy.",2019,True,False,False,True,False,False,False,"A, D",296,3,299
D19-6108,Deep Bidirectional Transformers for Relation Extraction without Supervision,"We present a novel framework to deal with relation extraction tasks in cases where there is complete lack of supervision, either in the form of gold annotations, or relations from a knowledge base. Our approach leverages syntactic parsing and pre-trained word embeddings to extract few but precise relations, which are then used to annotate a larger corpus, in a manner identical to distant supervision. The resulting data set is employed to fine tune a pre-trained BERT model in order to perform relation extraction. Empirical evaluation on four data sets from the biomedical domain shows that our method significantly outperforms two simple baselines for unsupervised relation extraction and, even if not using any supervision at all, achieves slightly worse results than the state-of-the-art in three out of four data sets. Importantly, we show that it is possible to successfully fine tune a large pretrained language model with noisy data, as opposed to previous works that rely on gold data for fine tuning.",2019,True,False,False,True,False,False,False,"A, D",307,3,310
N19-1309,{O}pen{C}eres: {W}hen Open Information Extraction Meets the Semi-Structured Web,"Open Information Extraction (OpenIE), the problem of harvesting triples from natural language text whose predicate relations are not aligned to any pre-defined ontology, has been a popular subject of research for the last decade. However, this research has largely ignored the vast quantity of facts available in semistructured webpages. In this paper, we define the problem of OpenIE from semi-structured websites to extract such facts, and present an approach for solving it. We also introduce a labeled evaluation dataset to motivate research in this area. Given a semi-structured website and a set of seed facts for some relations existing on its pages, we employ a semi-supervised label propagation technique to automatically create training data for the relations present on the site. We then use this training data to learn a classifier for relation extraction. Experimental results of this method on our new benchmark dataset obtained a precision of over 70%. A larger scale extraction experiment on 31 websites in the movie vertical resulted in the extraction of over 2 million triples.",2019,True,False,False,True,False,False,False,"A, D",319,3,322
D19-1198,A Discrete {CVAE} for Response Generation on Short-Text Conversation,"Neural conversation models such as encoderdecoder models are easy to generate bland and generic responses. Some researchers propose to use the conditional variational autoencoder (CVAE) which maximizes the lower bound on the conditional log-likelihood on a continuous latent variable. With different sampled latent variables, the model is expected to generate diverse responses. Although the CVAEbased models have shown tremendous potential, their improvement of generating highquality responses is still unsatisfactory. In this paper, we introduce a discrete latent variable with an explicit semantic meaning to improve the CVAE on short-text conversation. A major advantage of our model is that we can exploit the semantic distance between the latent variables to maintain good diversity between the sampled latent variables. Accordingly, we propose a two-stage sampling approach to enable efficient diverse variable selection from a large latent space assumed in the short-text conversation task. Experimental results indicate that our model outperforms various kinds of generation models under both automatic and human evaluations and generates more diverse and informative responses.",2019,False,True,True,False,False,False,False,"B, C",316,3,319
W19-5015,A Comparison of Word-based and Context-based Representations for Classification Problems in Health Informatics,"Distributed representations of text can be used as features when training a statistical classifier. These representations may be created as a composition of word vectors or as contextbased sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems: influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe and the two adapted using the MESH ontology. There is an improvement of 2-4% in the accuracy when these context-based representations are used instead of word-based representations.",2019,False,False,False,False,True,True,False,"E, F",258,3,261
W19-5056,"{IITP} at {MEDIQA} 2019: Systems Report for Natural Language Inference, Question Entailment and Question Answering","This paper presents the experiments accomplished as a part of our participation in the MEDIQA challenge, an (Abacha et al., 2019)   shared task. We participated in all the three tasks defined in this particular shared task. The tasks are viz. i. Natural Language Inference (NLI) ii. Recognizing Question Entailment(RQE) and their application in medical Question Answering (QA). We submitted runs using multiple deep learning based systems (runs) for each of these three tasks. We submitted five system results in each of the NLI and RQE tasks, and four system results for the QA task. The systems yield encouraging results in all the three tasks. The highest performance obtained in NLI, RQE and QA tasks are 81.8%, 53.2%, and 71.7%, respectively.",2019,False,False,False,True,False,False,True,"G, D",290,3,293
W19-5204,{APE} at Scale and Its Implications on {MT} Evaluation Biases,"In this work, we train an Automatic Post-Editing (APE) model and use it to reveal biases in standard Machine Translation (MT) evaluation procedures. The goal of our APE model is to correct typical errors introduced by the translation process, and convert the ""translationese"" output into natural text. Our APE model is trained entirely on monolingual data that has been round-trip translated through English, to mimic errors that are similar to the ones introduced by NMT. We apply our model to the output of existing NMT systems, and demonstrate that, while the human-judged quality improves in all cases, BLEU scores drop with forward-translated test sets. We verify these results for the WMT18 English→German, WMT15 English→French, and WMT16 English→Romanian tasks. Furthermore, we selectively apply our APE model on the output of the top submissions of the most recent WMT evaluation campaigns. We see quality improvements on all tasks of up to 2.5 BLEU points.",2019,False,False,False,True,True,False,False,"D, E",327,3,330
W19-5927,Zero-shot transfer for implicit discourse relation classification,"Automatically classifying the relation between sentences in a discourse is a challenging task, in particular when there is no overt expression of the relation. It becomes even more challenging by the fact that annotated training data exists only for a small number of languages, such as English and Chinese. We present a new system using zero-shot transfer learning for implicit discourse relation classification, where the only resource used for the target language is unannotated parallel text. This system is evaluated on the discourse-annotated TED-MDB parallel corpus, where it obtains good results for all seven languages using only English training data.",2019,False,False,False,True,False,False,True,"D, G",235,3,238
D19-1428,Using Local Knowledge Graph Construction to Scale {S}eq2{S}eq Models to Multi-Document Inputs,"Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multidocument summarization, feeding graph representations as input can achieve better performance than using retrieved text portions.",2019,False,True,False,True,False,False,False,"B, D",249,3,252
W19-2602,"Scalable, Semi-Supervised Extraction of Structured Information from Scientific Literature","As scientific communities grow and evolve, there is a high demand for improved methods for finding relevant papers, comparing papers on similar topics and studying trends in the research community. All these tasks involve the common problem of extracting structured information from scientific articles. In this paper, we propose a novel, scalable, semi-supervised method for extracting relevant structured information from the vast available raw scientific literature. We extract the fundamental concepts of aim, method and result from scientific articles and use them to construct a knowledge graph. Our algorithm makes use of domain-based word embedding and the bootstrap framework. Our experiments show the domain independence of our algorithm and that our system achieves precision and recall comparable to the state of the art. We also show the research trends of two distinct communities -computational linguistics and computer vision.",2019,True,False,True,False,False,False,False,"A, C",275,3,278
D19-1083,Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention,"The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connections and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified averagebased self-attention sublayer and the encoderdecoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt. 1",2019,False,True,True,False,False,False,False,"B, C",332,3,335
N19-1199,Detecting dementia in {M}andarin {C}hinese using transfer learning from a parallel corpus,"Machine learning has shown promise for automatic detection of Alzheimer's disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of outof-domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.",2019,True,False,False,False,False,False,True,"A, G",231,3,234
W19-5938,From Explainability to Explanation: Using a Dialogue Setting to Elicit Annotations with Justifications,"Despite recent attempts in the field of explainable AI to go beyond black box prediction models, typically already the training data for supervised machine learning is collected in a manner that treats the annotator as a ""black box"", the internal workings of which remains unobserved. We present an annotation method where a task is given to a pair of annotators who collaborate on finding the best response. With this we want to shed light on the questions if the collaboration increases the quality of the responses and if this ""thinking together"" provides useful information in itself, as it at least partially reveals their reasoning steps. Furthermore, we expect that this setting puts the focus on explanation as a linguistic act, vs. explainability as a property of models. In a crowd-sourcing experiment, we investigated three different annotation tasks, each in a collaborative dialogical (two annotators) and monological (one annotator) setting. Our results indicate that our experiment elicits collaboration and that this collaboration increases the response accuracy. We see large differences in the annotators' behavior depending on the task. Similarly, we also observe that the dialog patterns emerging from the collaboration vary significantly with the task.",2019,True,False,False,False,True,False,False,"A, E",351,3,354
D19-6305,Realizing {U}niversal {D}ependencies Structures,We first describe a surface realizer for Universal Dependencies (UD) structures. The system uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing realizer. This approach was then adapted for the two shared tasks of SR'19. The system is quite fast and showed competitive results for English sentences using automatic and manual evaluation measures.,2019,False,False,False,True,False,False,True,"D, G",196,3,199
2019.iwslt-1.22,On Using {S}pec{A}ugment for End-to-End Speech Translation,"This work investigates a simple data augmentation technique, SpecAugment, for end-to-end speech translation. SpecAugment is a low-cost implementation method applied directly to the audio input features and it consists of masking blocks of frequency channels, and/or time steps. We apply SpecAugment on end-to-end speech translation tasks and achieve up to +2.2% BLEU on LibriSpeech Audiobooks En→Fr and +1.2% on IWSLT TED-talks En→De by alleviating overfitting to some extent. We also examine the effectiveness of the method in a variety of data scenarios and show that the method also leads to significant improvements in various data conditions irrespective of the amount of training data.",2019,False,False,False,True,False,False,True,"D, G",263,3,266
K19-1088,Studying Generalisability across Abusive Language Detection Datasets,"Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of redundancy and non-generalisability between datasets. Through experiments on cross-dataset training and testing, the paper reveals that the preconceived notion of including more non-abusive samples in a dataset (to emulate reality) may have a detrimental effect on the generalisability of a model trained on that data. Hence a hierarchical annotation model is utilised here to reveal redundancies in existing datasets and to help reduce redundancy in future efforts.",2019,False,False,False,False,True,True,False,"E, F",231,3,234
W19-2502,Clustering-Based Article Identification in Historical Newspapers,"This article focuses on the problem of identifying articles and recovering their text from within and across newspaper pages when OCR just delivers one text file per page. We frame the task as a segmentation plus clustering step. Our results on a sample of 1912 New York Tribune magazine shows that performing the clustering based on similarities computed with word embeddings outperforms a similarity measure based on character n-grams and words. Furthermore, the automatic segmentation based on the text results in low scores, due to the low quality of some OCRed documents.",2019,False,False,False,True,True,False,False,"D, E",223,3,226
P19-1442,{D}is{S}ent: Learning Sentence Representations from Explicit Discourse Relations,"Learning effective representations of sentences is one of the core missions of natural language understanding. Existing models either train on a vast amount of text, or require costly, manually curated sentence relation datasets. We show that with dependency parsing and rule-based rubrics, we can curate a high quality sentence relation task by leveraging explicit discourse relations. We show that our curated dataset provides an excellent signal for learning vector representations of sentence meaning, representing relations that can only be determined when the meanings of two sentences are combined. We demonstrate that the automatically curated corpus allows a bidirectional LSTM sentence encoder to yield high quality sentence embeddings and can serve as a supervised fine-tuning dataset for larger models such as BERT. Our fixed sentence embeddings achieve high performance on a variety of transfer tasks, including Sen-tEval, and we achieve state-of-the-art results on Penn Discourse Treebank's implicit relation prediction task.",2019,True,False,False,True,False,False,False,"A, D",294,3,297
2019.jeptalnrecital-court.22,Observation de l{'}exp{\'e}rience client dans les restaurants (Mapping Reviewers{'} Experience in Restaurants),"Ces dernières années, les recherches sur la fouille d'opinions ou l'analyse des sentiments sont menées activement dans le domaine du Traitement Automatique des Langues (TAL). De nombreuses études scientifiques portent sur l'extraction automatique des opinions positives ou négatives et de leurs cibles. Ce travail propose d'identifier automatiquement une évaluation, exprimée explicitement ou implicitement par des internautes dans le corpus d'avis tiré du Web. Six catégories d'évaluation sont proposées : opinion positive, opinion négative, opinion mixte, intention, suggestion et description. La méthode utilisée est fondée sur l'apprentissage supervisé qui tient compte des caractéristiques linguistiques de chaque catégorie retenue. L'une des difficultés que nous avons rencontrée concerne le déséquilibre entre les classes d'évaluation créées, cependant, cet obstacle a pu être surmonté dans l'apprentissage grâce aux stratégies de sur-échantillonnage et aux stratégies algorithmiques.",2019,True,False,False,True,False,False,False,"A, D",314,3,317
S19-2223,{YNU}{\_}{DYX} at {S}em{E}val-2019 Task 9: A Stacked {B}i{LSTM} for Suggestion Mining Classification,In this paper we describe a deep-learning system that competed as SemEval 2019 Task 9-SubTask A: Suggestion Mining from Online Reviews and Forums. We use Word2Vec to learn the distributed representations from sentences. This system is composed of a Stacked Bidirectional Long-Short Memory Network (SBiLSTM) for enriching word representations before and after the sequence relationship with context. We perform an ensemble to improve the effectiveness of our model. Our official submission results achieve an F 1 -score 0.5659.,2019,False,True,False,False,False,False,True,"B, G",228,3,231
D19-1194,{D}y{K}g{C}hat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs,"Data-driven, knowledge-grounded neural conversation models are capable of generating more informative responses. However, these models have not yet demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs. This paper proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. Our new task and corpus aids in understanding the influence of dynamic knowledge graphs on responses generation. Also, we propose a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs. To benchmark this new task and evaluate the capability of adaptation, we introduce several evaluation metrics and the experiments show that our proposed approach outperforms previous knowledge-grounded conversation models. The proposed corpus and model can motivate the future research directions 1 .",2019,True,True,False,False,False,False,False,"A, B",302,3,305
N19-1048,Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts,"Learning high-quality embeddings for rare words is a hard problem because of sparse context information. Mimicking (Pinter et al.,  2017)  has been proposed as a solution: given embeddings learned by a standard algorithm, a model is first trained to reproduce embeddings of frequent words from their surface form and then used to compute embeddings for rare words. In this paper, we introduce attentive mimicking: the mimicking model is given access not only to a word's surface form, but also to all available contexts and learns to attend to the most informative and reliable contexts for computing an embedding. In an evaluation on four tasks, we show that attentive mimicking outperforms previous work for both rare and medium-frequency words. Thus, compared to previous work, attentive mimicking improves embeddings for a much larger part of the vocabulary, including the mediumfrequency range.",2019,False,True,False,True,False,False,False,"B, D",290,3,293
W19-5052,{MSIT}{\_}{SRIB} at {MEDIQA} 2019: Knowledge Directed Multi-task Framework for Natural Language Inference in Clinical Domain.,"In this paper, we present Biomedical Multi-Task Deep Neural Network (Bio-MTDNN) on the NLI task of MediQA 2019 challenge (Ben Abacha et al., 2019) . Bio-MTDNN utilizes ""transfer learning"" based paradigm where not only the source and target domains are different but also the source and target tasks are varied, although related. Further, Bio-MTDNN integrates knowledge from external sources such as clinical databases (UMLS) enhancing its performance on the clinical domain. Our proposed method outperformed the official baseline and other prior models (such as ESIM and Infersent on dev set) by a considerable margin as evident from our experimental results.",2019,False,True,False,True,False,False,False,"B, D",258,3,261
