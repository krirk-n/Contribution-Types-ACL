acl_id,title,abstract,year,A,B,C,D,E,F,G,raw_response,input_tokens,output_tokens,total_tokens
W16-0401,Sentiment Analysis - What are we talking about?,"Automatic affect detection and classification from text is a complex task in Natural Language Processing, whose tackling requires not only the use of established methods in the field, but also the use of knowledge extracted from theories in Psychology, Cognitive Science, Social Psychology or Neuropsychology.",2016,False,False,False,True,False,False,True,"D, G",170,3,173
P16-2020,Multiplicative Representations for Unsupervised Semantic Role Induction,"In unsupervised semantic role labeling, identifying the role of an argument is usually informed by its dependency relation with the predicate. In this work, we propose a neural model to learn argument embeddings from the context by explicitly incorporating dependency relations as multiplicative factors, which bias argument embeddings according to their dependency roles. Our model outperforms existing state-of-the-art embeddings in unsupervised semantic role induction on the CoNLL 2008 dataset and the SimLex999 word similarity task. Qualitative results demonstrate our model can effectively bias argument embeddings based on their dependency role.",2016,False,True,False,True,False,False,False,"B, D",232,3,235
2016.jeptalnrecital-demo.13,{LNE}-Visu : a tool to explore and visualize multimedia data,"LNE-Visu est une interface de visualisation et d'exploration de données multimédia qui regroupe les données des campagnes d'évaluation organisées par le LNE. Elle propose 3 fonctionnalités principales : explorer et sélectionner des corpus, visualiser et écouter des données et effectuer des tests de significativités de différences.",2016,True,False,False,False,False,False,True,"A, G",185,3,188
N16-1005,Controlling Politeness in Neural Machine Translation via Side Constraints,"Many languages use honorifics to express politeness, social distance, or the relative social status between the speaker and their addressee(s). In machine translation from a language without honorifics such as English, it is difficult to predict the appropriate honorific, but users may want to control the level of politeness in the output. In this paper, we perform a pilot study to control honorifics in neural machine translation (NMT) via side constraints, focusing on English→German. We show that by marking up the (English) source side of the training data with a feature that encodes the use of honorifics on the (German) target side, we can control the honorifics produced at test time. Experiments show that the choice of honorifics has a big impact on translation quality as measured by BLEU, and oracle experiments show that substantial improvements are possible by constraining the translation to the desired level of politeness.",2016,False,False,False,True,False,False,True,"D, G",311,3,314
W16-5616,News Sentiment and Cross-Country Fluctuations,"What is the information content of news-based measures of sentiment? How are they related to aggregate economic fluctuations? I construct a sentiment index by measuring the net amount of positive expressions in the corpus of Economic news articles produced by Reuters over the period 1987 -2013 and across 12 countries. The index successfully tracks fluctuations in Gross Domestic Product (GDP) at the country level, is a leading indicator of GDP growth and contains information to help forecast GDP growth which is not captured by professional forecasts. This suggests that forecasters do not appropriately incorporate available information in predicting future states of the economy.",2016,False,False,False,False,True,False,True,"E, G",236,3,239
L16-1372,Hypergraph Modelization of a Syntactically Annotated {E}nglish {W}ikipedia Dump,"Wikipedia, the well known internet encyclopedia, is nowadays a widely used source of information. To leverage its rich information, already parsed versions of Wikipedia have been proposed. We present an annotated dump of the English Wikipedia. This dump draws upon previously released Wikipedia parsed dumps. Still, we head in a different direction. In this parse we focus more into the syntactical characteristics of words: aside from the classical Part-of-Speech (PoS) tags and dependency parsing relations, we provide the full constituent parse branch for each word in a succinct way. Additionally, we propose a hypergraph network representation of the extracted linguistic information. The proposed modelization aims to take advantage of the information stocked within our parsed Wikipedia dump. We hope that by releasing these resources, researchers from the concerned communities will have a ready-to-experiment Wikipedia corpus to compare and distribute their work. We render public our parsed Wikipedia dump as well as the tool (and its source code) used to perform the parse. The hypergraph network and its related metadata is also distributed.",2016,True,True,False,False,False,False,False,"A, B",324,3,327
W16-0207,Bilingual Chronological Classification of Hafez{'}s Poems,"We present a novel task: the chronological classification of Hafez's poems (ghazals). We compiled a bilingual corpus in digital form, with consistent idiosyncratic properties. We have used Hooman's labeled ghazals in order to train automatic classifiers to classify the remaining ghazals. Our classification framework uses a Support Vector Machine (SVM) classifier with similarity features based on Latent Dirichlet Allocation (LDA). In our analysis of the results we use the LDA topics' main terms that are passed on to a Principal Component Analysis (PCA) module.",2016,True,False,False,False,False,False,True,"A, G",239,3,242
N16-1141,The Instantiation Discourse Relation: A Corpus Analysis of Its Properties and Improved Detection,"INSTANTIATION is a fairly common discourse relation and past work has suggested that it plays special roles in local coherence, in sentiment expression and in content selection in summarization. In this paper we provide the first systematic corpus analysis of the relation and show that relation-specific features can improve considerably the detection of the relation. We show that sentences involved in INSTANTIATION are set apart from other sentences by the use of gradable (subjective) adjectives, the occurrence of rare words and by different patterns in part-of-speech usage. Words across arguments of INSTANTI-ATION are connected through hypernym and meronym relations significantly more often than in other sentences and that they stand out in context by being significantly less similar to each other than other adjacent sentence pairs. These factors provide substantial predictive power that improves the identification of implicit INSTANTIATION relation by more than 5% F-measure.",2016,False,False,False,True,True,False,False,"E, D",298,3,301
L16-1142,Semantic Links for {P}ortuguese,"This paper describes work on incorporating Princenton's WordNet morphosemantics links to the fabric of the Portuguese OpenWordNet-PT. Morphosemantic links are relations between verbs and derivationally related nouns that are semantically typed (such as for instance ""tune-tuner"" -in Portuguese ""afinar-afinador"" -linked through an agent link). Morphosemantic links have been discussed for Princeton's WordNet for a while, but have not been added to the official database. These links are very useful, they help us to improve our Portuguese WordNet. Thus we discuss the integration of these links in our base and the issues we encountered with the integration.",2016,False,False,False,False,True,False,True,"E, G",257,3,260
C16-1250,A Joint Sentiment-Target-Stance Model for Stance Classification in Tweets,"Classifying the stance expressed in online microblogging social media is an emerging problem in opinion mining. We propose a probabilistic approach to stance classification in tweets, which models stance, target of stance, and sentiment of tweet, jointly. Instead of simply conjoining the sentiment or target variables as extra variables to the feature space, we use a novel formulation to incorporate three-way interactions among sentiment-stance-input variables and three-way interactions among target-stance-input variables. The proposed specification intuitively aims to discriminate sentiment features from target features for stance classification. In addition, regularizing a single stance classifier, which handles all targets, acts as a soft weight-sharing among them. We demonstrate that discriminative training of this model achieves the state-of-the-art results in supervised stance classification, and its generative training obtains competitive results in the weakly supervised setting.",2016,False,True,True,False,False,False,False,"B, C",287,3,290
L16-1429,Aspect based Sentiment Analysis in {H}indi: Resource Creation and Evaluation,"Due to the phenomenal growth of online product reviews, sentiment analysis (SA) has gained huge attention, for example, by online service providers. A number of benchmark datasets for a wide range of domains have been made available for sentiment analysis, especially in resource-rich languages. In this paper we assess the challenges of SA in Hindi by providing a benchmark setup, where we create an annotated dataset of high quality, build machine learning models for sentiment analysis in order to show the effective usage of the dataset, and finally make the resource available to the community for further advancement of research. The dataset comprises of Hindi product reviews crawled from various online sources. Each sentence of the review is annotated with aspect term and its associated sentiment. As classification algorithms we use Conditional Random Filed (CRF) and Support Vector Machine (SVM) for aspect term extraction and sentiment analysis, respectively. Evaluation results show the average F-measure of 41.07% for aspect term extraction and accuracy of 54.05% for sentiment classification.",2016,True,False,False,False,False,False,True,"A, G",320,3,323
L16-1146,{C}4{C}orpus: Multilingual Web-size Corpus with Free License,"Large Web corpora containing full documents with permissive licenses are crucial for many NLP tasks. In this article we present the construction of 12 million-pages Web corpus (over 10 billion tokens) licensed under CreativeCommons license family in 50+ languages that has been extracted from CommonCrawl, the largest publicly available general Web crawl to date with about 2 billion crawled URLs. Our highly-scalable Hadoop-based framework is able to process the full CommonCrawl corpus on 2000+ CPU cluster on the Amazon Elastic Map/Reduce infrastructure. The processing pipeline includes license identification, state-of-the-art boilerplate removal, exact duplicate and near-duplicate document removal, and language detection. The construction of the corpus is highly configurable and fully reproducible, and we provide both the framework (DKPro C4CorpusTools) and the resulting data (C4Corpus) to the research community.",2016,True,False,False,False,False,False,True,"A, G",297,3,300
L16-1667,A {T}urkish-{G}erman Code-Switching Corpus,"Bilingual communities often alternate between languages both in spoken and written communication. One such community, Germany residents of Turkish origin produce Turkish-German code-switching, by heavily mixing two languages at discourse, sentence, or word level. Code-switching in general, and Turkish-German code-switching in particular, has been studied for a long time from a linguistic perspective. Yet resources to study them from a more computational perspective are limited due to either small size or licence issues. In this work we contribute the solution of this problem with a corpus. We present a Turkish-German code-switching corpus which consists of 1029 tweets, with a majority of intra-sentential switches. We share different type of code-switching we have observed in our collection and describe our processing steps. The first step is data collection and filtering. This is followed by manual tokenisation and normalisation. And finally, we annotate data with word-level language identification information. The resulting corpus is available for research purposes.",2016,True,False,False,False,True,False,False,"A, E",316,3,319
W16-3703,Character-Aware Neural Networks for {A}rabic Named Entity Recognition for Social Media,"Named Entity Recognition (NER) is the task of classifying or labelling atomic elements in the text into categories such as Person, Location or Organisation. For Arabic language, recognizing named entities is a challenging task because of the complexity and the unique characteristics of this language. In addition, most of the previous work focuses on Modern Standard Arabic (MSA), however, recognizing named entities in social media is becoming more interesting these days. Dialectal Arabic (DA) and MSA are both used in social media, which is deemed as another challenging task. Most state-of-the-art Arabic NER systems count heavily on hand-crafted engineering features and lexicons which is time consuming. In this paper, we introduce a novel neural network architecture which benefits both from character-and word-level representations automatically, by using combination of bidirectional Long Short-Term Memory (LSTM) and Conditional Random Field (CRF), eliminating the need for most feature engineering. Moreover, our model relies on unsupervised word representations learned from unannotated corpora. Experimental results demonstrate that our model achieves state-of-the-art performance on publicly available benchmark for Arabic NER for social media and surpassing the previous system by a large margin.",2016,False,True,False,False,False,False,True,"B, G",357,3,360
N16-1162,Learning Distributed Representations of Sentences from Unlabelled Data,"Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-bilinear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.",2016,False,False,True,False,False,True,False,"F, C",249,3,252
W16-0403,Rumor Identification and Belief Investigation on {T}witter,"Social media users spend several hours a day to read, post and search for news on microblogging platforms. Social media is becoming a key means for discovering news. However, verifying the trustworthiness of this information is becoming even more challenging. In this study, we attempt to address the problem of rumor detection and belief investigation on Twitter. Our definition of rumor is an unverifiable statement, which spreads misinformation or disinformation. We adopt a supervised rumors classification task using the standard dataset. By employing the Tweet Latent Vector (TLV) feature, which creates a 100-d vector representative of each tweet, we increased the rumor retrieval task precision up to 0.972. We also introduce the belief score and study the belief change among the rumor posters between 2010 and 2016.",2016,True,False,False,False,True,False,False,"A, E",277,3,280
L16-1616,Enhanced {CORILGA}: Introducing the Automatic Phonetic Alignment Tool for Continuous Speech,"The Corpus Oral Informatizado da Lingua Galega (CORILGA) project aims at building a corpus of oral language for Galician, primarily designed to study the linguistic variation and change. This project is currently under development and it is periodically enriched with new contributions. The long-term goal is that all the speech recordings will be enriched with phonetic, syllabic, morphosyntactic, lexical and sentence ELAN-complaint annotations. A way to speed up the process of annotation is to use automatic speech-recognition-based tools tailored to the application. Therefore, CORILGA repository has been enhanced with an automatic alignment tool, available to the administrator of the repository, that aligns speech with an orthographic transcription. In the event that no transcription, or just a partial one, were available, a speech recognizer for Galician is used to generate word and phonetic segmentations. These recognized outputs may contain errors that will have to be manually corrected by the administrator. For assisting this task, the tool also provides an ELAN tier with the confidence measure of each recognized word. In this paper, after the description of the main facts of the CORILGA corpus, the speech alignment and recognition tools are described. Both have been developed using the Kaldi toolkit.",2016,True,False,False,False,False,False,True,"A, G",372,3,375
C16-1047,A Hybrid Deep Learning Architecture for Sentiment Analysis,"In this paper, we propose a novel hybrid deep learning architecture which is highly efficient for sentiment analysis in resource-poor languages. We learn sentiment embedded vectors from the Convolutional Neural Network (CNN). These are augmented to a set of optimized features selected through a multi-objective optimization (MOO) framework. The sentiment augmented optimized vector obtained at the end is used for the training of SVM for sentiment classification.We evaluate our proposed approach for coarse-grained (i.e. sentence level) as well as fine-grained (i.e. aspect level) sentiment analysis on four Hindi datasets covering varying domains. In order to show that our proposed method is generic in nature, we also evaluate it on two benchmark English datasets. Evaluation shows that performance of the proposed method are consistent across all the datasets and often outperform the state-of-art systems. To the best of our knowledge, this is the very first attempt where such a deep learning model is used for sentiment analysis in less-resourced languages such as Hindi.",2016,False,True,False,False,False,False,True,"B, G",318,3,321
C16-2050,{C}on{F}arm: Extracting Surface Representations of Verb and Noun Constructions from Dependency Annotated Corpora of {R}ussian,"ConFarm is a web service dedicated to extraction of surface representations of verb and noun constructions from dependency annotated corpora of Russian texts. Currently, the extraction of constructions with a specific lemma from SynTagRus and Russian National Corpus is available. The system provides flexible interface that allows users to finetune the output. Extracted constructions are grouped by their contents to allow for compact representation, and the groups are visualised as a graph in order to help navigating the extraction results. ConFarm differs from similar existing tools for Russian language in that it offers full constructions, as opposed to extracting separate dependents of search word or working with collocations, and allows users to discover unexpected constructions as opposed to searching for examples of a user-defined construction.",2016,True,False,False,False,False,False,True,"A, G",264,3,267
W16-5706,"Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation","There have been a wide range of recent annotated corpora concerning events, either regarding event coreference, the temporal order of events, hierarchical ""subevent"" structure of events, or causal relationships between events. However, although some believe that these different phenomena will display rich interactions, relatively few corpora annotate all of those layers of annotation in a unified fashion. This paper describes the annotation methodology for the Richer Event Descriptions corpus, which annotates entities, events, times, their coreference and partial coreference relations, and the temporal, causal and subevent relationships between the events. It suggests that such rich annotations of within-document event phenomena can be built with high quality through a multi-stage annotation pipeline, and that the resultant corpus could be useful for systems hoping to transition from the detection of isolated mentions of events toward a richer understanding of events grounded in the temporal, causal, referential and bridging relations that define them.",2016,True,False,False,False,True,False,False,"A, E",302,3,305
L16-1630,Towards Comparability of Linguistic Graph {B}anks for Semantic Parsing,"We announce a new language resource for research on semantic parsing, a large, carefully curated collection of semantic dependency graphs representing multiple linguistic traditions. This resource is called SDP 2016 and provides an update and extension to previous versions used as Semantic Dependency Parsing target representations in the 2014 and 2015 Semantic Evaluation Exercises (SemEval). For a common core of English text, this third edition comprises semantic dependency graphs from four distinct frameworks, packaged in a unified abstract format and aligned at the sentence and token levels. SDP 2016 is the first general release of this resource and available for licensing from the Linguistic Data Consortium from May 2016. The data is accompanied by an open-source SDP utility toolkit and system results from previous contrastive parsing evaluations against these target representations.",2016,True,False,False,False,True,False,False,"A, E",273,3,276
W16-0419,Reputation System: Evaluating Reputation among All Good Sellers,"A reputation system assists people selecting whom to trust. The ""all good reputation"" problem is common in e-commerce domain, making it difficult for buyers to choose credible sellers. Observing high growth of online data in Hindi language, in this paper, we propose a reputation system in this language. The functions of this system include 1) review mining for different criteria of online transactions 2) calculation of reputation rating and reputation weight for each criteria from user reviews and 3) ranking sellers based on computed reputation score. Extensive simulations conducted on eBay dataset show its effectiveness in solving ""all good reputation"" problem. So far as our knowledge is concerned, this is the first work in Hindi language on reputation system.",2016,True,False,False,False,False,False,True,"G, A",260,3,263
L16-1702,Survey of Conversational Behavior: Towards the Design of a Balanced Corpus of Everyday {J}apanese Conversation,"In 2016, we set about building a large-scale corpus of everyday Japanese conversation-a collection of conversations embedded in naturally occurring activities in daily life. We will collect more than 200 hours of recordings over six years, publishing the corpus in 2022. To construct such a huge corpus, we have conducted a pilot project, one of whose purposes is to establish a corpus design for collecting various kinds of everyday conversations in a balanced manner. For this purpose, we conducted a survey of everyday conversational behavior, with about 250 adults, in order to reveal how diverse our everyday conversational behavior is and to build an empirical foundation for corpus design. The questionnaire included when, where, how long, with whom, and in what kind of activity informants were engaged in conversations. We found that ordinary conversations show the following tendencies: i) they mainly consist of chats, business talks, and consultations; ii) in general, the number of participants is small and the duration of the conversation is short; iii) many conversations are conducted in private places such as homes, as well as in public places such as offices and schools; and iv) some questionnaire items are related to each other. This paper describes an overview of this survey study, and then discusses how to design a large-scale corpus of everyday Japanese conversation on this basis.",2016,True,False,False,False,True,False,False,"A, E",381,3,384
D16-1044,Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding,"Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.",2016,False,True,False,True,False,False,False,"B, D",330,3,333
W16-2310,The {JHU} Machine Translation Systems for {WMT} 2016,"This paper describes the submission of Johns Hopkins University for the shared translation task of ACL 2016 First Conference on Machine Translation (WMT 2016). We set up phrase-based, hierarchical phrase-based and syntax-based systems for all 12 language pairs of this year's evaluation campaign. Novel research directions we investigated include: neural probabilistic language models, bilingual neural network language models, morphological segmentation, and the attentionbased neural machine translation model as reranking feature.",2016,False,False,False,True,False,False,True,"D, G",209,3,212
C16-1058,K-{SRL}: Instance-based Learning for Semantic Role Labeling,"Semantic role labeling (SRL) is the task of identifying and labeling predicate-argument structures in sentences with semantic frame and role labels. A known challenge in SRL is the large number of low-frequency exceptions in training data, which are highly context-specific and difficult to generalize. To overcome this challenge, we propose the use of instance-based learning that performs no explicit generalization, but rather extrapolates predictions from the most similar instances in the training data. We present a variant of k-nearest neighbors (kNN) classification with composite features to identify nearest neighbors for SRL. We show that high-quality predictions can be derived from a very small number of similar instances. In a comparative evaluation we experimentally demonstrate that our instance-based learning approach significantly outperforms current state-of-the-art systems on both in-domain and out-of-domain data, reaching F 1 -scores of 89,28% and 79.91% respectively.",2016,False,True,False,True,False,False,False,"B, D",306,3,309
W16-3205,Natural Language Descriptions of Human Activities Scenes: Corpus Generation and Analysis,"There has been continuous growth in the volume and ubiquity of video material. It has become essential to define video semantics in order to aid the searchability and retrieval of this data. Although the method of annotating this data with keywords is relatively well researched, the quality can be improved through describing videos with natural language. We are exploring approaches to generating natural language descriptions of interrelations between human activities in a video stream. This paper focuses on creation of a dataset that can be used for development and evaluation. To this end a corpus of video clips, manually selected from the Hollywood2 dataset, and their natural language descriptions has been generated. Analysis of the hand annotation presents insights into human interests and thoughts. Such resource can be used to evaluate automatic natural language generation systems for video.",2016,True,False,False,False,True,False,False,"A, E",273,3,276
W16-0504,Automated classification of collaborative problem solving interactions in simulated science tasks,"We present a novel situational task that integrates collaborative problem solving behavior with testing in a science domain. Participants engage in discourse, which is used to evaluate their collaborative skills. We present initial experiments for automatic classification of such discourse, using a novel classification schema. Considerable accuracy is achieved with just lexical features. A speech-act classifier, trained on out-of-domain data, can also be helpful.",2016,True,False,False,False,False,False,True,"A, G",196,3,199
W16-1508,Delineating Fields Using Mathematical Jargon,"Tracing ideas through the scientific literature is useful in understanding the origin of ideas and for generating new ones. Machines can be trained to do this at large scale, feeding search engines and recommendation algorithms. Citations and text are the features commonly used for these tasks. In this paper, we focus on a largely ignored facet of scholarly papers-the equations. Mathematical language varies from field to field but original formulae are maintained over generations (e.g., Shannon's Entropy equation). Here we extract a common set of mathematical symbols from more than 250,000 L A T E X source files in the arXiv repository. We compare the symbol distributions across di↵erent fields and calculate the jargon distance between fields. We find a greater difference between the experimental and theoretical disciplines than within these fields. This provides a first step at using equations as a bridge between disciplines that may not cite each other or may speak di↵erent natural languages but use a similar mathematical language.",2016,False,False,False,False,True,False,True,"E, G",313,3,316
P16-1032,"Document-level Sentiment Inference with Social, Faction, and Discourse Context","We present a new approach for documentlevel sentiment inference, where the goal is to predict directed opinions (who feels positively or negatively towards whom) for all entities mentioned in a text. To encourage more complete and consistent predictions, we introduce an ILP that jointly models (1) sentence-and discourse-level sentiment cues, (2) factual evidence about entity factions, and (3) global constraints based on social science theories such as homophily, social balance, and reciprocity. Together, these cues allow for rich inference across groups of entities, including for example that CEOs and the companies they lead are likely to have similar sentiment towards others. We evaluate performance on new, densely labeled data that provides supervision for all pairs, complementing previous work that only labeled pairs mentioned in the same sentence. Experiments demonstrate that the global model outperforms sentence-level baselines, by providing more coherent predictions across sets of related entities.",2016,True,True,False,False,False,False,False,"A, B",301,3,304
C16-1061,Splitting compounds with ngrams,"Compound words with unmarked word boundaries are problematic for many tasks in NLP and computational linguistics, including information extraction, machine translation, and syllabification. This paper introduces a simple, proof-of-concept language modeling approach to automatic compound segmentation, demonstrated with Finnish. The approach utilizes an off-the-shelf morphological analyzer to split training words into their constituent morphemes. A language model is subsequently trained on ngrams composed of morphemes, morpheme boundaries, and word boundaries. Finally, linguistic constraints are used to weed out phonotactically ill-formed segmentations, thereby allowing the language model to select the best grammatical segmentation. This approach achieves an accuracy of ∼97%.",2016,False,False,False,True,False,False,True,"D, G",257,3,260
W16-3011,Extraction of Regulatory Events using Kernel-based Classifiers and Distant Supervision,"This paper describes our system to extract binary regulatory relations from text, used to participate in the SeeDev task of BioNLP-ST 2016. Our system was based on machine learning, using support vector machines with a shallow linguistic kernel to identify each type of relation. Additionally, we employed a distant supervised approach to increase the size of the training data. Our submission obtained the third best precision of the SeeDev-binary task. Although the distant supervised approach did not significantly improve the results, we expect that by exploring other techniques to use unlabeled data should lead to better results.",2016,False,False,False,True,False,False,True,"D, G",234,3,237
Y16-3022,{L}2 Acquisition of {K}orean locative construction by {E}nglish {L}1 speakers: Learnability problem in {K}orean Figure non-alternating verbs,"Korean has locative construction as other languages do such as English. Although L2 acquisition of locative construction has been examined in L2 English research, few experimental investigations of Korean L2 acquisition have been conducted. The current study focused on the syntactic alternation among Figure Framed sentence, Ground Framed sentence, Figure only sentence and Ground only sentence. Forced choice task on 72 locative construction have been conducted by 21 Native Korean speakers and 20 advanced L1 English learners of Korean. L2ers showed different acceptability judgment on Korean locative construction which was distinct from their L1 argument structure. The results showed that these asymmetries were driven by L1 effect when the learnability problem arises due to insufficient input.",2016,False,False,False,False,True,False,True,"E, G",267,3,270
L16-1615,{SPA}: Web-based Platform for easy Access to Speech Processing Modules,"This paper presents SPA, a web-based Speech Analytics platform that integrates several speech processing modules and that makes it possible to use them through the web. It was developed with the aim of facilitating the usage of the modules, without the need to know about software dependencies and specific configurations. Apart from being accessed by a web-browser, the platform also provides a REST API for easy integration with other applications. The platform is flexible, scalable, provides authentication for access restrictions, and was developed taking into consideration the time and effort of providing new services. The platform is still being improved, but it already integrates a considerable number of audio and text processing modules, including: Automatic transcription, speech disfluency classification, emotion detection, dialog act recognition, age and gender classification, non-nativeness detection, hyperarticulation detection, dialog act recognition, and two external modules for feature extraction and DTMF detection. This paper describes the SPA architecture, presents the already integrated modules, and provides a detailed description for the ones most recently integrated.",2016,False,True,False,False,False,False,True,"G, B",322,3,325
W16-2213,Examining the Relationship between Preordering and Word Order Freedom in Machine Translation,"We study the relationship between word order freedom and preordering in statistical machine translation. To assess word order freedom, we first introduce a novel entropy measure which quantifies how difficult it is to predict word order given a source sentence and its syntactic analysis. We then address preordering for two target languages at the far ends of the word order freedom spectrum, German and Japanese, and argue that for languages with more word order freedom, attempting to predict a unique word order given source clues only is less justified. Subsequently, we examine lattices of n-best word order predictions as a unified representation for languages from across this broad spectrum and present an effective solution to a resulting technical issue, namely how to select a suitable source word order from the lattice during training. Our experiments show that lattices are crucial for good empirical performance for languages with freer word order (English-German) and can provide additional improvements for fixed word order languages (English-Japanese).",2016,False,False,False,True,True,False,False,"E, D",305,3,308
W16-2917,Construction of a Personal Experience Tweet Corpus for Health Surveillance,"Studies have shown that Twitter can be used for health surveillance, and personal experience tweets (PETs) are an important source of information for health surveillance. To mine Twitter data requires a relatively balanced corpus and it is challenging to construct such a corpus due to the labor-intensive annotation tasks of large data sets. We developed a bootstrap method of finding PETs with the use of the machine learning-based filter. Through a few iterations, our approach can efficiently improve the balance of two class dataset with a reduced amount of annotation work. To demonstrate the usefulness of our method, a PET corpus related to effects caused by 4 dietary supplements was constructed. In 3 iterations, a corpus of 8,770 tweets was obtained from 108,528 tweets collected, and the imbalance of two classes was significantly reduced from 1:31 to 1:3. In addition, two out of three classifiers used showed improved performance over iterations. It is conceivable that our approach can be applied to various other health surveillance studies that use machine learning-based classifications of imbalanced Twitter data.",2016,True,False,False,True,False,False,False,"A, D",329,3,332
P16-1147,Stack-propagation: Improved Representation Learning for Syntax,"Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call ""stack-propagation"". We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model.",2016,False,True,False,True,False,False,False,"B, D",270,3,273
Q16-1017,Discrete-State Variational Autoencoders for Joint Discovery and Factorization of Relations,"We present a method for unsupervised opendomain relation discovery. In contrast to previous (mostly generative and agglomerative clustering) approaches, our model relies on rich contextual features and makes minimal independence assumptions. The model is composed of two parts: a feature-rich relation extractor, which predicts a semantic relation between two entities, and a factorization model, which reconstructs arguments (i.e., the entities) relying on the predicted relation. The two components are estimated jointly so as to minimize errors in recovering arguments. We study factorization models inspired by previous work in relation factorization and selectional preference modeling. Our models substantially outperform the generative and agglomerative-clustering counterparts and achieve state-of-the-art performance.",2016,False,True,True,False,False,False,False,"B, C",263,3,266
W16-1610,Quantifying the Vanishing Gradient and Long Distance Dependency Problem in Recursive Neural Networks and Recursive {LSTM}s,"Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.",2016,False,False,False,False,True,True,False,"F, E",318,3,321
P16-1096,Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation,"Automatically recognising medical concepts mentioned in social media messages (e.g. tweets) enables several applications for enhancing health quality of people in a community, e.g. real-time monitoring of infectious diseases in population. However, the discrepancy between the type of language used in social media and medical ontologies poses a major challenge. Existing studies deal with this challenge by employing techniques, such as lexical term matching and statistical machine translation. In this work, we handle the medical concept normalisation at the semantic level. We investigate the use of neural networks to learn the transition between layman's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology. We evaluate our approaches using three different datasets, where social media texts are extracted from Twitter messages and blog posts. Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines, which achieved state-of-the-art performance on several medical concept normalisation tasks, by up to 44%.",2016,False,True,False,True,False,False,False,"B, D",312,3,315
P16-4009,{TM}op: a Tool for Unsupervised Translation Memory Cleaning,"We present TMop, the first open-source tool for automatic Translation Memory (TM) cleaning. The tool implements a fully unsupervised approach to the task, which allows spotting unreliable translation units (sentence pairs in different languages, which are supposed to be translations of each other) without requiring labeled training data. TMop includes a highly configurable and extensible set of filters capturing different aspects of translation quality. It has been evaluated on a test set composed of 1,000 translation units (TUs) randomly extracted from the English-Italian version of MyMemory, a large-scale public TM. Results indicate its effectiveness in automatic removing ""bad"" TUs, with comparable performance to a state-of-the-art supervised method (76.3 vs. 77.7 balanced accuracy).",2016,True,False,False,True,False,False,False,"A, D",273,3,276
P16-2056,Bootstrapped Text-level Named Entity Recognition for Literature,"We present a named entity recognition (NER) system for tagging fiction: LitNER. Relative to more traditional approaches, LitNER has two important properties: (1) it makes no use of handtagged data or gazetteers, instead it bootstraps a model from term clusters; and (2) it leverages multiple instances of the same name in a text. Our experiments show it to substantially outperform off-the-shelf supervised NER systems.",2016,False,False,False,True,False,False,True,"D, G",209,3,212
P16-1161,Target-Side Context for Discriminative Models in Statistical Machine Translation,"Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.",2016,False,True,False,False,False,True,False,"B, F",234,3,237
D16-1067,Enhanced Personalized Search using Social Data,"Search personalization that considers the social dimension of the web has attracted a significant volume of research in recent years. A user profile is usually needed to represent a user's interests in order to tailor future searches. Previous research has typically constructed a profile solely from a user's usage information. When the user has only limited activities in the system, the effect of the user profile on search is also constrained. This research addresses the setting where a user has only a limited amount of usage information. We build enhanced user profiles from a set of annotations and resources that users have marked, together with an external knowledge base constructed according to usage histories. We present two probabilistic latent topic models to simultaneously incorporate social annotations, documents and the external knowledge base. Our web search strategy is achieved using personalized social query expansion. We introduce a topical query expansion model to enhance the search by utilizing individual user profiles. The proposed approaches have been intensively evaluated on a large public social annotation dataset. Results show that our models significantly outperformed existing personalized query expansion methods which use user profiles solely built from past usage information in personalized search.",2016,False,True,False,True,False,False,False,"B, D",333,3,336
C16-1335,{R}eddit Temporal N-gram Corpus and its Applications on Paraphrase and Semantic Similarity in Social Media using a Topic-based Latent Semantic Analysis,This paper introduces a new large-scale n-gram corpus that is created specifically from social media text. Two distinguishing characteristics of this corpus are its monthly temporal attribute and that it is created from 1.65 billion comments of user-generated text in Reddit. The usefulness of this corpus is exemplified and evaluated by a novel Topic-based Latent Semantic Analysis (TLSA) algorithm. The experimental results show that unsupervised TLSA outperforms all the state-of-the-art unsupervised and semi-supervised methods in SEMEVAL 2015: paraphrase and semantic similarity in Twitter tasks.,2016,True,False,True,False,False,False,False,"A, C",237,3,240
W16-6327,Learning Non-Linear Functions for Text Classification,"In this paper, we show that generative classifiers are capable of learning non-linear decision boundaries and that non-linear generative models can outperform a number of linear classifiers on some text categorization tasks. We first prove that 3-layer multinomial hierarchical generative (Bayesian) classifiers, under a particular independence assumption, can only learn the same linear decision boundaries as a multinomial naive Bayes classifier. We then go on to show that making a different independence assumption results in nonlinearization, thereby enabling us to learn non-linear decision boundaries. We finally evaluate the performance of these non-linear classifiers on a series of text classification tasks.",2016,False,False,True,False,False,True,False,"C, F",243,3,246
W16-0703,Bridging Relations in {P}olish: Adaptation of Existing Typologies,"The paper attempts at presenting initial verification of existing approaches to annotation of bridging relations by proposing a compiled model based on schemata used in previous annotation projects and testing its validity on a corpus of Polish. The categorization features structural relations, dissimilation, analogy, reference to label, class, entailment and attribution. Multiple categories can be assigned to model situations where several aspects of the relation play a part. The relations are organized hierarchically which allows varied granularity of processing depending on computational needs. The classification is confronted with existing annotation of other-than-identity relations in a portion of Polish Coreference Corpus. Results of manual annotation involving two annotators and adjudicator are presented. Findings from the process are intended to facilitate development of annotation guidelines of a new referencerelated project.",2016,False,False,False,False,True,False,True,"E, G",275,3,278
S16-1051,{AKTSKI} at {S}em{E}val-2016 Task 5: Aspect Based Sentiment Analysis for Consumer Reviews,"This paper describes the polarity classification system designed for participation in SemEval-2016 Task 5 -ABSA. The aim is to determine the sentiment polarity expressed towards certain aspect within a consumer review. Our system is based on supervised learning using Support Vector Machine (SVM). We use standard features for basic classification model. On top this, we include rules to check precedent polarity sequence. This approach is experimental.",2016,False,False,False,True,False,False,True,"D, G",200,3,203
W16-1630,Neural Associative Memory for Dual-Sequence Modeling,"Many important NLP problems can be posed as dual-sequence or sequence-tosequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on autoencoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.",2016,False,True,False,False,False,True,False,"B, F",294,3,297
N16-1167,Lexical Coherence Graph Modeling Using Word Embeddings,"Coherence is established by semantic connections between sentences of a text which can be modeled by lexical relations. In this paper, we introduce the lexical coherence graph (LCG), a new graph-based model to represent lexical relations among sentences. The frequency of subgraphs (coherence patterns) of this graph captures the connectivity style of sentence nodes in this graph. The coherence of a text is encoded by a vector of these frequencies. We evaluate the LCG model on the readability ranking task. The results of the experiments show that the LCG model obtains higher accuracy than state-of-the-art coherence models. Using larger subgraphs yields higher accuracy, because they capture more structural information. However, larger subgraphs can be sparse. We adapt Kneser-Ney smoothing to smooth subgraphs' frequencies. Smoothing improves performance.",2016,False,True,True,False,False,False,False,"B, C",280,3,283
C16-1235,Distance Metric Learning for Aspect Phrase Grouping,"Aspect phrase grouping is an important task in aspect-level sentiment analysis. It is a challenging problem due to polysemy and context dependency. We propose an Attention-based Deep Distance Metric Learning (ADDML) method, by considering aspect phrase representation as well as context representation. First, leveraging the characteristics of the review text, we automatically generate aspect phrase sample pairs for distant supervision. Second, we feed word embeddings of aspect phrases and their contexts into an attention-based neural network to learn feature representation of contexts. Both aspect phrase embedding and context embedding are used to learn a deep feature subspace for measure the distances between aspect phrases for K-means clustering. Experiments on four review datasets show that the proposed method outperforms state-of-the-art strong baseline methods.",2016,False,True,False,True,False,False,False,"B, D",268,3,271
N16-1129,Learning to Recognize Ancillary Information for Automatic Paraphrase Identification,"Previous work on Automatic Paraphrase Identification (PI) is mainly based on modeling text similarity between two sentences. In contrast, we study methods for automatically detecting whether a text fragment only appearing in a sentence of the evaluated sentence pair is important or ancillary information with respect to the paraphrase identification task. Engineering features for this new task is rather difficult, thus, we approach the problem by representing text with syntactic structures and applying tree kernels on them. The results show that the accuracy of our automatic Ancillary Text Classifier (ATC) is promising, i.e., 68.6%, and its output can be used to improve the state of the art in PI.",2016,True,True,False,False,False,False,False,"A, B",252,3,255
L16-1136,The {S}em{D}a{X} Corpus ― Sense Annotations with Scalable Sense Inventories,"We launch the SemDaX corpus which is a recently completed Danish human-annotated corpus available through a CLARIN academic license. The corpus includes approx. 90,000 words, comprises six textual domains, and is annotated with sense inventories of different granularity. The aim of the developed corpus is twofold: i) to assess the reliability of the different sense annotation schemes for Danish measured by qualitative analyses and annotation agreement scores, and ii) to serve as training and test data for machine learning algorithms with the practical purpose of developing sense taggers for Danish. To these aims, we take a new approach to human-annotated corpus resources by double annotating a much larger part of the corpus than what is normally seen: for the all-words task we double annotated 60% of the material and for the lexical sample task 100%. We include in the corpus not only the adjucated files, but also the diverging annotations. In other words, we consider not all disagreement to be noise, but rather to contain valuable linguistic information that can help us improve our annotation schemes and our learning algorithms.",2016,True,False,False,False,True,False,False,"A, E",341,3,344
W16-3705,{S}inhala Short Sentence Similarity Calculation using Corpus-Based and Knowledge-Based Similarity Measures,"Currently, corpus based-similarity, string-based similarity, and knowledge-based similarity techniques are used to compare short phrases. However, no work has been conducted on the similarity of phrases in Sinhala language. In this paper, we present a hybrid methodology to compute the similarity between two Sinhala sentences using a Semantic Similarity Measurement technique (corpus-based similarity measurement plus knowledge-based similarity measurement) that makes use of word order information. Since Sinhala WordNet is still under construction, we used lexical resources in performing this semantic similarity calculation. Evaluation using 4000 sentence pairs yielded an average MSE of 0.145 and a Pearson correlation factor of 0.832.",2016,False,False,False,True,False,False,True,"D, G",250,3,253
C16-1078,Retrieving Occurrences of Grammatical Constructions,"Finding authentic examples of grammatical constructions is central in constructionist approaches to linguistics, language processing, and second language learning. In this paper, we address this problem as an information retrieval (IR) task. To facilitate research in this area, we built a benchmark collection by annotating the occurrences of six constructions in a Swedish corpus. Furthermore, we implemented a simple and flexible retrieval system for finding construction occurrences, in which the user specifies a ranking function using lexical-semantic similarities (lexicon-based or distributional). The system was evaluated using standard IR metrics on the new benchmark, and we saw that lexical-semantical rerankers improve significantly over a purely surface-oriented system, but must be carefully tailored for each individual construction.",2016,True,False,False,False,False,False,True,"A, G",263,3,266
C16-1202,Distributed Representations for Building Profiles of Users and Items from Text Reviews,"In this paper, we propose an approach to learn distributed representations of users and items from text comments for recommendation systems. Traditional recommendation algorithms, e.g. collaborative filtering and matrix completion, are not designed to exploit the key information hidden in the text comments, while existing opinion mining methods do not provide direct support to recommendation systems with useful features on users and items. Our approach attempts to construct vectors to represent profiles of users and items under a unified framework to maximize word appearance likelihood. Then, the vector representations are used for a recommendation task in which we predict scores on unobserved user-item pairs without given texts. The recommendation-aware distributed representation approach is fully supported by effective and efficient learning algorithms over massive text archive. Our empirical evaluations on real datasets show that our system outperforms the state-of-the-art baseline systems.",2016,False,True,False,True,False,False,False,"B, D",280,3,283
P16-1084,How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation,"Recently a few systems for automatically solving math word problems have reported promising results. However, the datasets used for evaluation have limitations in both scale and diversity. In this paper, we build a large-scale dataset which is more than 9 times the size of previous ones, and contains many more problem types. Problems in the dataset are semiautomatically obtained from community question-answering (CQA) web pages. A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results.",2016,True,False,False,False,True,False,False,"A, E",243,3,246
P16-4013,{MUSEEC}: A Multilingual Text Summarization Tool,"The MUSEEC (MUltilingual SEntence Extraction and Compression) summarization tool implements several extractive summarization techniques -at the level of complete and compressed sentences -that can be applied, with some minor adaptations, to documents in multiple languages. The current version of MUSEEC provides the following summarization methods: (1) MUSE -a supervised summarizer, based on a genetic algorithm (GA), that ranks document sentences and extracts top-ranking sentences into a summary, (2) POLY -an unsupervised summarizer, based on linear programming (LP), that selects the best extract of document sentences, and (3) WECOM -an unsupervised extension of POLY that compiles a document summary from compressed sentences. In this paper, we provide an overview of MUSEEC methods and its architecture in general.",2016,False,True,False,True,False,False,False,"D, B",291,3,294
L16-1723,What a Nerd! Beating Students and Vector Cosine in the {ESL} and {TOEFL} Datasets,"In this paper, we claim that Vector Cosine -which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models -can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that -independently of the adopted parameters -outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50%) and several state-of-the-art approaches.",2016,False,False,True,False,False,True,False,"C, F",338,3,341
W16-2320,The {QT}21/{H}im{L} Combined Machine Translation System,This paper describes the joint submission of the QT21 and HimL projects for the English→Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH,2016,False,False,False,True,False,False,True,"D, G",181,3,184
L16-1634,Accuracy of Automatic Cross-Corpus Emotion Labeling for Conversational Speech Corpus Commonization,"There exists a major incompatibility in emotion labeling framework among emotional speech corpora, that is, category-based and dimension-based. Commonizing these requires inter-corpus emotion labeling according to both frameworks, but doing this by human annotators is too costly for most cases. This paper examines the possibility of automatic cross-corpus emotion labeling. In order to evaluate the effectiveness of the automatic labeling, a comprehensive emotion annotation for two conversational corpora, UUDB and OGVC, was performed. With a state-of-the-art machine learning technique, dimensional and categorical emotion estimation models were trained and tested against the two corpora. For the emotion dimension estimation, the automatic cross-corpus emotion labeling for the different corpus was effective for the dimensions of aroused-sleepy, dominant-submissive and interested-indifferent, showing only slight performance degradation against the result for the same corpus. On the other hand, the performance for the emotion category estimation was not sufficient.",2016,False,False,False,False,True,False,True,"E, G",307,3,310
W16-0320,Using Linear Classifiers for the Automatic Triage of Posts in the 2016 {CLP}sych Shared Task,"The 2016 CLPsych Shared Task was to automatically triage posts from a mental health forum into four categories: green (everything is fine), amber (a moderator needs to look at this post), red (a moderator urgently needs to look at this post) and crisis (the person might hurt himself or others). The final results for the task revealed that this problem was not an easy task. I chose to treat the problem as a text categorization task using a system composed of different Support Vector Machines (SVMs) in a one-vs-rest setting. This approach was straight-forward and achieved good performance in the final evaluation. The major difficulty was to find suitable features and feature combinations.",2016,False,False,False,True,False,False,True,"D, G",257,3,260
W16-5506,Automatic Modification of Communication Style in Dialogue Management,"In task-oriented dialogues, there is often only one right answer the system can give. However, a lack of variation can seem repetitive and unnatural. Humans change the way they express something, e.g. by being more or less concise. We aim to approximate this ability by automatically varying the level of verbosity and directness of a given system action. In this work, we illustrate how verbosity and directness may be utilised in adaptive dialogue management and present different approaches to automatically generate varying levels of verbosity and directness for given system actions. Thereby, new and unforeseen system actions can be created dynamically.",2016,False,False,False,True,False,False,True,"D, G",237,3,240
L16-1066,Forecasting Emerging Trends from Scientific Literature,"Text analysis methods for the automatic identification of emerging technologies by analyzing the scientific publications, are gaining attention because of their socio-economic impact. The approaches so far have been mainly focused on retrospective analysis by mapping scientific topic evolution over time. We propose regression based approaches to predict future keyword distribution. The prediction is based on historical data of the keywords, which in our case, are LREC conference proceedings. Considering the insufficient number of data points available from LREC proceedings, we do not employ standard time series forecasting methods. We form a dataset by extracting the keywords from previous year proceedings and quantify their yearly relevance using tf-idf scores. This dataset additionally contains ranked lists of related keywords and experts for each keyword.",2016,True,False,False,True,False,False,False,"A, D",257,3,260
L16-1134,Sense-annotating a Lexical Substitution Data Set with Ubyline,"We describe the construction of GLASS, a newly sense-annotated version of the German lexical substitution data set used at the GERMEVAL 2015: LEXSUB shared task. Using the two annotation layers, we conduct the first known empirical study of the relationship between manually applied word senses and lexical substitutions. We find that synonymy and hypernymy/hyponymy are the only semantic relations directly linking targets to their substitutes, and that substitutes in the target's hypernymy/hyponymy taxonomy closely align with the synonyms of a single GermaNet synset. Despite this, these substitutes account for a minority of those provided by the annotators. The results of our analysis accord with those of a previous study on English-language data (albeit with automatically induced word senses), leading us to suspect that the sense-substitution relations we discovered may be of a universal nature. We also tentatively conclude that relatively cheap lexical substitution annotations can be used as a knowledge source for automatic WSD. Also introduced in this paper is Ubyline, the web application used to produce the sense annotations. Ubyline presents an intuitive user interface optimized for annotating lexical sample data, and is readily adaptable to sense inventories other than GermaNet.",2016,True,False,False,False,True,False,False,"A, E",368,3,371
W16-0523,Cost-Effectiveness in Building a Low-Resource Morphological Analyzer for Learner Language,"In this paper, we describe the development of a morphological analyzer for learner Hungarian, outlining extensions to a resourcelight system that can be developed by different types of experts. Specifically, we discuss linguistic rule writing, resource creation, and different system settings, and our evaluation showcases the amount of improvement one gets for differing levels and kinds of effort, enabling other researchers to spend their time and energy as effectively as possible.",2016,False,False,False,True,True,False,False,"D, E",201,3,204
W16-1307,Demonyms and Compound Relational Nouns in Nominal Open {IE},"Extracting open relational tuples that are mediated by nouns (instead of verbs) is important since titles and entity attributes are often expressed nominally. While appositives and possessives are easy to handle, a difficult and important class of nominal extractions requires interpreting compound noun phrases (e.g., ""Google CEO Larry Page""). We substantially improve the quality of Open IE from compound noun phrases by focusing on phenomena like demonyms and compound relational nouns. We release RELNOUN 2.2, which obtains 3.5 times yield with over 15 point improvement in precision compared to RELNOUN 1.1, a publicly available nominal Open IE system.",2016,True,False,False,True,False,False,False,"A, D",250,3,253
W16-2001,Mining linguistic tone patterns with symbolic representation,"This paper conceptualizes speech prosody data mining and its potential application in data-driven phonology/phonetics research. We first conceptualize Speech Prosody Mining (SPM) in a time-series data mining framework. Specifically, we propose using efficient symbolic representations for speech prosody time-series similarity computation. We experiment with both symbolic and numeric representations and distance measures in a series of time-series classification and clustering experiments on a dataset of Mandarin tones. Evaluation results show that symbolic representation performs comparably with other representations at a reduced cost, which enables us to efficiently mine large speech prosody corpora while opening up to possibilities of using a wide range of algorithms that require discrete valued data. We discuss the potential of SPM using time-series mining techniques in future works.",2016,True,False,False,True,False,False,False,"A, D",269,3,272
P16-2019,Leveraging Lexical Resources for Learning Entity Embeddings in Multi-Relational Data,"Recent work in learning vector-space embeddings for multi-relational data has focused on combining relational information derived from knowledge bases with distributional information derived from large text corpora. We propose a simple approach that leverages the descriptions of entities or phrases available in lexical resources, in conjunction with distributional semantics, in order to derive a better initialization for training relational models. Applying this initialization to the TransE model results in significant new stateof-the-art performances on the WordNet dataset, decreasing the mean rank from the previous best of 212 to 51. It also results in faster convergence of the entity representations. We find that there is a tradeoff between improving the mean rank and the hits@10 with this approach. This illustrates that much remains to be understood regarding performance improvements in relational models.",2016,False,False,False,True,False,True,False,"D, F",275,3,278
W16-3202,Combining Lexical and Spatial Knowledge to Predict Spatial Relations between Objects in Images,"Explicit representations of images are useful for linguistic applications related to images. We design a representation based on first-order models that capture the objects present in an image as well as their spatial relations. We take a supervised learning approach to the spatial relation classification problem and study the effects of spatial and lexical information on prediction performance. We find that lexical information is required to accurately predict spatial relations when combined with location information, achieving an F-score of 0.80, compared to a most-frequent-class baseline of 0.62.",2016,False,False,False,True,True,False,False,"E, D",222,3,225
W16-3905,From Noisy Questions to {M}inecraft Texts: Annotation Challenges in Extreme Syntax Scenario,"User-generated content presents many challenges for its automatic processing. While many of them do come from out-of-vocabulary effects, others spawn from different linguistic phenomena such as unusual syntax. In this work we present a French three-domain data set made up of question headlines from a cooking forum, game chat logs and associated forums from two popular online games (MINECRAFT & LEAGUE OF LEGENDS). We chose these domains because they encompass different degrees of lexical and syntactic compliance with canonical language. We conduct an automatic and manual evaluation of the difficulties of processing these domains for part-of-speech prediction, and introduce a pilot study to determine whether dependency analysis lends itself well to annotate these data. We also discuss the development cost of our data set.",2016,True,False,False,False,True,False,False,"A, E",267,3,270
L16-1178,{SCARE} ― The Sentiment Corpus of App Reviews with Fine-grained Annotations in {G}erman,"The automatic analysis of texts containing opinions of users about, e.g., products or political views has gained attention within the last decades. However, previous work on the task of analyzing user reviews about mobile applications in app stores is limited. Publicly available corpora do not exist, such that a comparison of different methods and models is difficult. We fill this gap by contributing the Sentiment Corpus of App Reviews (SCARE), which contains fine-grained annotations of application aspects, subjective (evaluative) phrases and relations between both. This corpus consists of 1,760 annotated application reviews from the Google Play Store with 2,487 aspects and 3,959 subjective phrases. We describe the process and methodology how the corpus was created. The Fleiss-κ between four annotators reveals an agreement of 0.72. We provide a strong baseline with a linear-chain conditional random field and word-embedding features with a performance of 0.62 for aspect detection and 0.63 for the extraction of subjective phrases. The corpus is available to the research community to support the development of sentiment analysis methods on mobile application reviews.",2016,True,False,False,True,False,False,False,"A, D",344,3,347
L16-1095,{CAT}a{L}og Online: Porting a Post-editing Tool to the Web,"This paper presents CATaLog online, a new web-based MT and TM post-editing tool. CATaLog online is a freeware software that can be used through a web browser and it requires only a simple registration. The tool features a number of editing and log functions similar to the desktop version of CATaLog enhanced with several new features that we describe in detail in this paper. CATaLog online is designed to allow users to post-edit both translation memory segments as well as machine translation output. The tool provides a complete set of log information currently not available in most commercial CAT tools. Log information can be used both for project management purposes as well as for the study of the translation process and translator's productivity.",2016,True,False,False,False,False,False,True,"A, G",261,3,264
C16-1140,Named Entity Disambiguation for little known referents: a topic-based approach,"We propose an approach to Named Entity Disambiguation that avoids a problem of standard work on the task (likewise affecting fully supervised, weakly supervised, or distantly supervised machine learning techniques): the treatment of name mentions referring to people with no (or very little) coverage in the textual training data is systematically incorrect. We propose to indirectly take into account the property information for the ""non-prominent"" name bearers, such as nationality and profession (e.g., for a Canadian law professor named Michael Jackson, with no Wikipedia article, it is very hard to obtain reliable textual training data). The target property information for the entities is directly available from name authority files, or inferrable, e.g., from listings of sportspeople etc. Our proposed approach employs topic modeling to exploit textual training data based on entities sharing the relevant properties. In experiments with a pilot implementation of the general approach, we show that the approach does indeed work well for name/referent pairs with limited textual coverage in the training data.",2016,False,False,False,True,False,False,True,"D, G",321,3,324
2016.jeptalnrecital-jep.21,"Disfluences dans le vieillissement « normal » et la maladie d{'}{A}lzheimer : indices segmentaux, suprasegmentaux et gestuels (Disfluencies in {``}normal{''} aging and {A}lzheimer{'}s disease: segmental, suprasegmental and gestural markers)","L'objectif de cette étude est d'analyser et comparer les productions langagières dans leur multimodalité de 10 personnes atteintes de la maladie d'Alzheimer (MA) appariées à 10 contrôles. Différentes mesures aux niveaux segmental et suprasegmental -erreurs, pauses et allongements vocaliques -ont été réalisées dans une tâche de répétition avec ou sans gestes imposés pour caractériser une disfluence, typique de la MA, puis observées en lien avec les gestes manuels produits. Les résultats montrent la diminution significative de la fluence chez les personnes atteintes de la MA, avec davantage d'erreurs produites au niveau lexical par le groupe Patient et au niveau phonétique par les patients au stade modéré de la maladie, ainsi que de nombreuses pauses silencieuses précédant ou suivant souvent les erreurs produites au niveau segmental. De plus, dans la tâche avec gestes imposés, la répétition de ceux-ci a impacté la fluence des groupes Contrôle et Patient avec une augmentation significative des disfluences au niveau suprasegmental et des erreurs phonétiques au niveau segmental.",2016,False,False,False,False,True,True,False,"E,F",353,2,355
W16-5209,An Ontology for Language Service Composability,"Fragmentation and recombination is a key to create customized language environments for supporting various intercultural activities. Fragmentation provides various language resource components for the customized language environments and recombination builds each language environment according to user's request by combining these components. To realize this fragmentation and recombination process, existing language resources (both data and programs) should be shared as language services and combined beyond mismatch of their service interfaces. To address this issue, standardization is inevitable: standardized interfaces are necessary for language services as well as data format required for language resources. Therefore, we have constructed a hierarchy of language services based on inheritance of service interfaces, which is called language service ontology. This ontology allows users to create a new customized language service that is compatible with existing ones. Moreover, we have developed a dynamic service binding technology that instantiates various executable customized services from an abstract workflow according to user's request. By using the ontology and service binding together, users can bind the instantiated language service to another abstract workflow for a new customized one.",2016,False,True,False,True,False,False,False,"D, B",320,3,323
D16-1264,"{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text","We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.",2016,True,False,False,False,True,False,False,"A, E",266,3,269
P16-4028,new/s/leak {--} Information Extraction and Visualization for Investigative Data Journalists,"We present new/s/leak, a novel tool developed for and with the help of journalists, which enables the automatic analysis and discovery of newsworthy stories from large textual datasets. We rely on different NLP preprocessing steps such named entity tagging, extraction of time expressions, entity networks, relations and metadata. The system features an intuitive web-based user interface based on network visualization combined with data exploring methods and various search and faceting mechanisms. We report the current state of the software and exemplify it with the WikiLeaks PlusD (Cablegate) data.",2016,True,False,False,False,False,False,True,"A, G",227,3,230
W16-2014,Read my points: Effect of animation type when speech-reading from {EMA} data,"Three popular vocal-tract animation paradigms were tested for intelligibility when displaying videos of pre-recorded Electromagnetic Articulography (EMA) data in an online experiment. EMA tracks the position of sensors attached to the tongue. The conditions were dots with tails (where only the coil location is presented), 2D animation (where the dots are connected to form 2D representations of the lips, tongue surface and chin), and a 3D model with coil locations driving facial and tongue rigs. The 2D animation (recorded in VisArtico) showed the highest identification of the prompts.",2016,False,False,False,False,True,False,True,"E, G",240,3,243
W16-5415,Sentiment Analysis for Low Resource Languages: A Study on Informal {I}ndonesian Tweets,"This paper describes our attempt to build a sentiment analysis system for Indonesian tweets. With this system, we can study and identify sentiments and opinions in a text or document computationally. We used four thousand manually labeled tweets collected in February and March 2016 to build the model. Because of the variety of content in tweets, we analyze tweets into eight groups in total, including pos(itive), neg(ative), and neu(tral). Finally, we obtained 73.2% accuracy with Long Short Term Memory (LSTM) without normalizer.",2016,True,False,False,False,False,False,True,"A, G",228,3,231
D16-1072,Fast Coupled Sequence Labeling on Heterogeneous Annotations via Context-aware Pruning,"The recently proposed coupled sequence labeling is shown to be able to effectively exploit multiple labeled data with heterogeneous annotations but suffer from severe inefficiency problem due to the large bundled tag space (Li et al., 2015) . In their case study of part-ofspeech (POS) tagging, Li et al. ( 2015 ) manually design context-free tag-to-tag mapping rules with a lot of effort to reduce the tag space. This paper proposes a context-aware pruning approach that performs token-wise constraints on the tag space based on contextual evidences, making the coupled approach efficient enough to be applied to the more complex task of joint word segmentation (WS) and POS tagging for the first time. Experiments show that using the large-scale People Daily as auxiliary heterogeneous data, the coupled approach can improve F-score by 95.55 − 94.88 = 0.67% on WS, and by 90.58 − 89.49 = 1.09% on joint WS&POS on Penn Chinese Treebank. All codes are released at http://hlt.suda.edu.cn/~zhli.",2016,False,True,False,True,False,False,False,"B, D",341,3,344
L16-1273,The Methodius Corpus of Rhetorical Discourse Structures and Generated Texts,"Using the Methodius Natural Language Generation (NLG) System, we have created a corpus which consists of a collection of generated texts which describe ancient Greek artefacts. Each text is linked to two representations created as part of the NLG process. The first is a content plan, which uses rhetorical relations to describe the high-level discourse structure of the text, and the second is a logical form describing the syntactic structure, which is sent to the OpenCCG surface realization module to produce the final text output. In recent work, White and Howcroft (2015) have used the SPaRKy restaurant corpus, which contains similar combination of texts and representations, for their research on the induction of rules for the combination of clauses. In the first instance this corpus will be used to test their algorithms on an additional domain, and extend their work to include the learning of referring expression generation rules. As far as we know, the SPaRKy restaurant corpus is the only existing corpus of this type, and we hope that the creation of this new corpus in a different domain will provide a useful resource to the Natural Language Generation community.",2016,True,False,False,False,False,False,True,"A, G",345,3,348
N16-3017,Lecture Translator - Speech translation framework for simultaneous lecture translation,"Foreign students at German universities often have difficulties following lectures as they are often held in German. Since human interpreters are too expensive for universities we are addressing this problem via speech translation technology deployed in KIT's lecture halls. Our simultaneous lecture translation system automatically translates lectures from German to English in real-time. Other supported language directions are English to Spanish, English to French, English to German and German to French. Automatic simultaneous translation is more than just the concatenation of automatic speech recognition and machine translation technology, as the input is an unsegmented, practically infinite stream of spontaneous speech. The lack of segmentation and the spontaneous nature of the speech makes it especially difficult to recognize and translate it with sufficient quality. In addition to quality, speed and latency are of the utmost importance in order for the system to enable students to follow lectures. In this paper we present our system that performs the task of simultaneous speech translation of university lectures by performing speech translation on a stream of audio in real-time and with low latency. The system features several techniques beyond the basic speech translation task, that make it fit for real-world use. Examples of these features are a continuous stream speech recognition without any prior segmentation of the input audio, punctuation prediction, run-on decoding and run-on translation with continuously updating displays in order to keep the latency as low as possible.",2016,False,False,False,True,False,False,True,"G, D",382,3,385
S16-1096,{WOLVESAAR} at {S}em{E}val-2016 Task 1: Replicating the Success of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity,This paper describes the WOLVESAAR systems that participated in the English Semantic Textual Similarity (STS) task in SemEval-2016. We replicated the top systems from the last two editions of the STS task and extended the model using GloVe word embeddings and dense vector space LSTM based sentence representations. We compared the difference in performance of the replicated system and the extended variants. Our variants to the replicated system show improved correlation scores and all of our submissions outperform the median scores from all participating systems.,2016,False,False,False,True,False,True,False,"D, F",223,3,226
L16-1559,Finding Alternative Translations in a Large Corpus of Movie Subtitle,"OpenSubtitles.org provides a large collection of user contributed subtitles in various languages for movies and TV programs. Subtitle translations are valuable resources for cross-lingual studies and machine translation research. A less explored feature of the collection is the inclusion of alternative translations, which can be very useful for training paraphrase systems or collecting multi-reference test suites for machine translation. However, differences in translation may also be due to misspellings, incomplete or corrupt data files, or wrongly aligned subtitles. This paper reports our efforts in recognising and classifying alternative subtitle translations with language independent techniques. We use time-based alignment with lexical re-synchronisation techniques and BLEU score filters and sort alternative translations into categories using edit distance metrics and heuristic rules. Our approach produces large numbers of sentence-aligned translation alternatives for over 50 languages provided via the OPUS corpus collection.",2016,True,False,False,True,False,False,False,"A, D",287,3,290
P16-3021,Building a Corpus for {J}apanese Wikification with Fine-Grained Entity Classes,"In this research, we build a Wikification corpus for advancing Japanese Entity Linking. This corpus consists of 340 Japanese newspaper articles with 25,675 entity mentions. All entity mentions are labeled by a fine-grained semantic classes (200 classes), and 19,121 mentions were successfully linked to Japanese Wikipedia articles. Even with the fine-grained semantic classes, we found it hard to define the target of entity linking annotations and to utilize the fine-grained semantic classes to improve the accuracy of entity linking.",2016,True,False,False,False,True,False,False,"A, E",219,3,222
S16-1091,"{S}amsung {P}oland {NLP} Team at {S}em{E}val-2016 Task 1: Necessity for diversity; combining recursive autoencoders, {W}ord{N}et and ensemble methods to measure semantic similarity.","This paper describes our proposed solutions designed for a STS core track within the Se-mEval 2016 English Semantic Textual Similarity (STS) task. Our method of similarity detection combines recursive autoencoders with a WordNet award-penalty system that accounts for semantic relatedness, and an SVM classifier, which produces the final score from similarity matrices. This solution is further supported by an ensemble classifier, combining an aligner with a bi-directional Gated Recurrent Neural Network and additional features, which then performs Linear Support Vector Regression to determine another set of scores.",2016,False,True,False,True,False,False,False,"B, D",234,3,237
L16-1683,Using a Small Lexicon with {CRF}s Confidence Measure to Improve {POS} Tagging Accuracy,"Like most of the languages which have only recently started being investigated for the Natural Language Processing (NLP) tasks, Amazigh lacks annotated corpora and tools and still suffers from the scarcity of linguistic tools and resources. The main aim of this paper is to present a new part-of-speech (POS) tagger based on a new Amazigh tag set (AMTS) composed of 28 tags. In line with our goal we have trained Conditional Random Fields (CRFs) to build a POS tagger for the Amazigh language. We have used the 10-fold technique to evaluate and validate our approach. The CRFs 10 folds average level is 87.95% and the best fold level result is 91.18%. In order to improve this result, we have gathered a set of about 8k words with their POS tags. The collected lexicon was used with CRFs confidence measure in order to have a more accurate POS-tagger. Hence, we have obtained a better performance of 93.82%.",2016,True,False,False,False,False,False,True,"A, G",326,3,329
W16-3419,Measuring Cognitive Translation Effort with Activity Units,"Despite the increased quality of Machine Translation output, human interaction will remain a crucial activity to guarantee the quality of the final translation products. Human-computer interaction in translation will likely be the more successful the more we understand the properties and complementarities of both partners. This paper traces cognitive approaches in machine translation back to the mid-1980s and argues that we now have the technologies available that will allow us to eventually arrive at an in-depth understanding of the human translation processes. It illustrates some of the research methods in empirical translation process research and suggests ngrams of Activity Units for measuring the translation process.",2016,False,False,False,True,True,False,False,"E, D",237,3,240
Q16-1013,Reassessing the Goals of Grammatical Error Correction: Fluency Instead of Grammaticality,"The field of grammatical error correction (GEC) has grown substantially in recent years, with research directed at both evaluation metrics and improved system performance against those metrics. One unvisited assumption, however, is the reliance of GEC evaluation on error-coded corpora, which contain specific labeled corrections. We examine current practices and show that GEC's reliance on such corpora unnaturally constrains annotation and automatic evaluation, resulting in (a) sentences that do not sound acceptable to native speakers and (b) system rankings that do not correlate with human judgments. In light of this, we propose an alternate approach that jettisons costly error coding in favor of unannotated, whole-sentence rewrites. We compare the performance of existing metrics over different gold-standard annotations, and show that automatic evaluation with our new annotation scheme has very strong correlation with expert rankings (ρ = 0.82). As a result, we advocate for a fundamental and necessary shift in the goal of GEC, from correcting small, labeled error types, to producing text that has native fluency.",2016,False,False,False,True,True,False,False,"E, D",333,3,336
S16-1144,{ICL}-{HD} at {S}em{E}val-2016 Task 10: Improving the Detection of Minimal Semantic Units and their Meanings with an Ontology and Word Embeddings,"This paper presents our system submitted for SemEval 2016 Task 10: Detecting Minimal Semantic Units and their Meanings (DiM-SUM; Schneider, Hovy, et al., 2016) . We extend AMALGrAM (Schneider and Smith, 2015) by tapping two additional information sources. The first information source uses a semantic knowledge base (YAGO3; Suchanek et al., 2007) to improve supersense tagging (SST) for named entities. The second information source employs word embeddings (GloVe; Pennington et al., 2014) to capture fine-grained latent semantics and therefore improving the supersense identification for both nouns and verbs. We conduct a detailed evaluation and error analysis for our features and come to the conclusion that both our extensions lead to an improved detection for SST.",2016,False,False,False,True,False,False,True,"D, G",290,3,293
C16-1306,Textual complexity as a predictor of difficulty of listening items in language proficiency tests,In this paper we explore to what extent the difficulty of listening items in an English language proficiency test can be predicted by the textual properties of the item text. We show that a system based on multiple text complexity features can predict item difficulty for several different item types and for some items achieves higher accuracy than human estimates of item difficulty.,2016,False,False,False,False,True,False,True,"E, G",183,3,186
W16-4603,{J}apanese-{E}nglish Machine Translation of Recipe Texts,"Concomitant with the globalization of food culture, demand for the recipes of specialty dishes has been increasing. The recent growth in recipe sharing websites and food blogs has resulted in numerous recipe texts being available for diverse foods in various languages. However, little work has been done on machine translation of recipe texts. In this paper, we address the task of translating recipes and investigate the advantages and disadvantages of traditional phrasebased statistical machine translation and more recent neural machine translation. Specifically, we translate Japanese recipes into English, analyze errors in the translated recipes, and discuss available room for improvements.",2016,False,False,False,True,True,False,False,"D, E",232,3,235
W16-2911,Unsupervised Document Classification with Informed Topic Models,"Document classification is an important and common application in natural language processing. Scaling classification approaches to many targets faces a bottleneck in acquiring gold standard labels. In this work, we develop and evaluate a method for using informed topic models to noisily label documents, creating a noisy but usable set of labels for training discriminative classifiers. We investigate multiple ways to train this noisy classifier, and the best performing method uses Wikipedia-seeded topic models to approximately label training instances without any supervision. We evaluate these methods on the classification task as well as in an active learning setting, in which they are shown to improve learning rates over traditional active learning.",2016,False,False,False,True,False,False,True,"D, G",243,3,246
W16-0711,Error analysis for anaphora resolution in {R}ussian: new challenging issues for anaphora resolution task in a morphologically rich language,"This paper presents a quantitative and qualitative error analysis of Russian anaphora resolvers which participated in the RU-EVAL event. Its aim is to identify and characterize a set of challenging errors common to stateof-the-art systems dealing with Russian. We examined three types of pronouns: 3rd person pronouns, reflexive and relative pronouns. The investigation has shown that a high level of grammatical ambiguity, specific features of reflexive pronouns, free word order and special cases of non-referential pronouns in Russian impact the quality of anaphora resolution systems. Error analysis reveals some specific features of anaphora resolution for morphologically rich and free word order languages with a lack of gold standard resources.",2016,False,False,False,False,True,True,False,"E, F",261,3,264
O16-1022,{F}acebook 活動事件擷取系統({F}acebook Activity Event Extraction System)[In {C}hinese],"The popularity of social networks has made them a perfect medium for activity or advertising campaign promotion. Therefore, many people use Facebook pages to announce their advertising campaign. The purpose of this study is to extract activity events by constructing two named entity recognition models, namely activity name and location, via a Web NER model generation tool [1] . We enhance the tool by improving the tokenizer and alignment technique. In addition, we also use a large database of FB checkin places for location name recognition improvement. For entity relation extraction, we apply sequential pattern mining to",2016,False,True,False,True,False,False,False,"B, D",229,3,232
P16-1133,Cross-Lingual Sentiment Classification with Bilingual Document Representation Learning,"Cross-lingual sentiment classification aims to adapt the sentiment resource in a resource-rich language to a resource-poor language. In this study, we propose a representation learning approach which simultaneously learns vector representations for the texts in both the source and the target languages. Different from previous research which only gets bilingual word embedding, our Bilingual Document Representation Learning model BiDRL directly learns document representations. Both semantic and sentiment correlations are utilized to map the bilingual texts into the same embedding space. The experiments are based on the multilingual multi-domain Amazon review dataset. We use English as the source language and use Japanese, German and French as the target languages. The experimental results show that BiDRL outperforms the state-of-the-art methods for all the target languages.",2016,False,True,False,True,False,False,False,"B, D",268,3,271
C16-1043,Incrementally Learning a Dependency Parser to Support Language Documentation in Field Linguistics,"We present experiments in incrementally learning a dependency parser. The parser will be used in the WordsEye Linguistics Tools (WELT) (Ulinski et al., 2014a; Ulinski et al., 2014b) which supports field linguists documenting a language's syntax and semantics. Our goal is to make syntactic annotation faster for field linguists. We have created a new parallel corpus of descriptions of spatial relations and motion events, based on pictures and video clips used by field linguists for elicitation of language from native speaker informants. We collected descriptions for each picture and video from native speakers in English, Spanish, German, and Egyptian Arabic. We compare the performance of MSTParser (McDonald et al., 2006)  and MaltParser (Nivre et al.,  2006)  when trained on small amounts of this data. We find that MaltParser achieves the best performance. We also present the results of experiments using the parser to assist with annotation. We find that even when the parser is trained on a single sentence from the corpus, annotation time significantly decreases.",2016,True,False,False,False,False,False,True,"A, G",343,3,346
P16-1081,Modeling Social Norms Evolution for Personalized Sentiment Classification,"Motivated by the findings in social science that people's opinions are diverse and variable while together they are shaped by evolving social norms, we perform personalized sentiment classification via shared model adaptation over time. In our proposed solution, a global sentiment model is constantly updated to capture the homogeneity in which users express opinions, while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals. Global model sharing alleviates data sparsity issue, and individualized model adaptation enables efficient online model learning. Extensive experimentations are performed on two large review collections from Amazon and Yelp, and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions.",2016,False,True,False,True,False,False,False,"B, D",258,3,261
W16-6102,Clinical Text Prediction with Numerically Grounded Conditional Language Models,"Assisted text input techniques can save time and effort and improve text quality. In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion. These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word. Our automated evaluation on a clinical dataset shows extended models significantly outperform standard models. Our best system uses both conditioning and grounding, because of their orthogonal benefits. For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%. We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks. We found that at test time numbers have more influence on the document level than on individual word probabilities.",2016,False,True,False,True,False,False,False,"B, D",315,3,318
